<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-component Image Translation for Deep Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Mahfujur</forename><surname>Rahman</surname></persName>
							<email>m27.rahman@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
							<email>c.fookes@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
							<email>m.baktashmotlagh@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
							<email>s.sridharan@qut.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Image and Video Laboratory</orgName>
								<orgName type="institution">Queensland University of Technology (QUT)</orgName>
								<address>
									<settlement>Brisbane</settlement>
									<region>QLD</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-component Image Translation for Deep Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adaption (DA) and domain generalization (DG) are two closely related methods which are both concerned with the task of assigning labels to an unlabeled data set. The only dissimilarity between these approaches is that DA can access the target data during the training phase, while the target data is totally unseen during the training phase in DG. The task of DG is challenging as we have no earlier knowledge of the target samples. If DA methods are applied directly to DG by a simple exclusion of the target data from training, poor performance will result for a given task. In this paper, we tackle the domain generalization challenge in two ways. In our first approach, we propose a novel deep domain generalization architecture utilizing synthetic data generated by a Generative Adversarial Network (GAN). The discrepancy between the generated images and synthetic images is minimized using existing domain discrepancy metrics such as maximum mean discrepancy or correlation alignment. In our second approach, we introduce a protocol for applying DA methods to a DG scenario by excluding the target data from the training phase, splitting the source data to training and validation parts, and treating the validation data as target data for DA. We conduct extensive experiments on four cross-domain benchmark datasets. Experimental results signify our proposed model outperforms the current state-of-the-art methods for DG.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The success of Deep Neural Networks (DNNs) or Deep Learning (DL) largely depends on the availability of large sets of labeled data. When the training and test (source and target) images come from different distributions (domain shift), DL cannot perform well. Domain adaption (DA) and domain generalization (DG) methods have been proposed to address the poor performance due to such domain shift. DA and DG are similar frameworks, but the key difference between DA and DG is the availability of the target data in training phase. DA methods access the target data dur-ing training while DG approaches do not have access to the target data.</p><p>DA is an attractive area of research in computer vision and pattern recognition fields because it handles the task properly with limited target samples. Domain adaptation can be classified into three groups: Unsupervised Domain Adaptation (UDA), Semi-Supervised Domain Adaptation (SSDA) and Supervised Domain Adaptation (SDA). UDA does not require any labeled target data whereas SDA needs all labeled target samples. Unsupervised deep domain adaptation (UDDA) methods require large sets of target data in order to be more successful in producing a desired output.</p><p>To reduce the discrepancy between the source and target data, traditional domain adaptation methods consider seeking domain-invariant information of the data, adapting classifiers, or both. In the conventional domain adaptation task, we still access the target data in training phase. However, in real world scenarios, the target data are out of reach in the training phase. Consequently, ordinary domain adaptation methods do not perform well without having target data in the training phase.</p><p>When target data is inaccessible, the issue of DG arises. Without the target data, DG makes full use of and derives benefit from all available source domains. It then tries to acquire a domain independent model by merging data sources which can be used for unknown target data. For an example, we have the images from ImageNet <ref type="bibr" target="#b5">[6]</ref> and Caltech-256 <ref type="bibr" target="#b14">[15]</ref> datasets and we want to classify images in the LabelMe dataset <ref type="bibr" target="#b34">[35]</ref>. We have source data from multiple sources for training without having any knowledge about the target data. That is, the target data is completely unseen during the training phase. DG models utilize the captured information from various source domains and generalize to unseen target data, which is used only in the test phase. Although DA and DG frameworks are very close and both have similar goals (producing a strong classifier on target data), the existing DA techniques do not perform well when directly applied in DG.</p><p>In this paper, we propose a novel deep domain generalization framework by utilizing synthetic data that are generated by a GAN during the training stage where the discrep-ancy between the real and synthetic data is minimized using the existing domain adaptation metrics such as maximum mean discrepancy or correlation alignment. The motivation behind this research is that if we can generate different images from a single image whose styles are similar to the available source domains, the framework will learn a more agnostic model that can be applied for unseen data. Our approach takes advantage of the natural image correspondence built by CycleGAN <ref type="bibr" target="#b44">[45]</ref> and ComboGAN <ref type="bibr" target="#b0">[1]</ref>. The contributions of this paper are two-fold:</p><p>? We implement a novel deep domain generalization framework utilizing synthetic data that is generated by a GAN. We generate images from one source domain into the distributions of other available source domains which are used in the training phase as validation data. The discrepancy between the real data and synthetic data is decreased using existing domain discrepancy metrics in DG settings.</p><p>? We introduce a protocol for applying DA methods on DG scenarios where the source datasets are split into training and validation sets, and the validation data acts as target data for DG.</p><p>We conduct extensive experiments to evaluate the image classification accuracy of our proposed method across a large set of alternatives in DG settings. Our method performs significantly better than previous state-of-the-art approaches across a range of visual image classification tasks with domain mismatch between the source and target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Research</head><p>This section reviews existing research on DA and DG, especially in the avenue of object classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Domain Adaptation</head><p>To address the problem of domain shift, many DA methods <ref type="bibr">[13, 23-25, 31, 37, 40-42, 44]</ref> have been introduced in recent years. All the DA techniques can be divided into two main categories: Conventional Domain Adaptation methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref> and Deep Domain Adaptation methods <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. The conventional domain adaptation methods have been developed into two phases, the features are extracted in the first phase and these extracted features are used to train the classifier in the second phase. However, the performance of these DA methods is not satisfactory.</p><p>Obtaining the features using a deep neural network even without adaptation techniques outperforms conventional DA methods by a large margin. The image classification accuracy obtained with Deep Convolutional Activation Features (DeCAF) <ref type="bibr" target="#b7">[8]</ref> even without using any adaptation algorithm is remarkably better than any conventional domain adaptation methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref> due to the capacity of a DNN to extract more robust features using nonlinear function. Inspired by the success of DL, researchers widely adopted DL based DA techniques.</p><p>Maximum Mean Discrepancy (MMD) is a well known metric for comparing the discrepancy between the source and target data. Eric Tzeng et al. <ref type="bibr" target="#b40">[41]</ref> introduced the Deep Domain Confusion (DDC) domain adaptation method where the discrepancy is reduced by introducing a confusion layer. In <ref type="bibr" target="#b39">[40]</ref>, the previous work <ref type="bibr" target="#b40">[41]</ref> is improved by adopting a soft label distribution matching loss which is used to reduce the discrepancy between the source and target domains. Long et al. proposed the Domain Adaptation Network (DAN) <ref type="bibr" target="#b22">[23]</ref> that introduced the sum of MMDs defined between several layers which are used to mitigate the domain discrepancy problem. This idea was further boosted by the Joint Adaptation Networks <ref type="bibr" target="#b24">[25]</ref> and Residual Transfer Networks <ref type="bibr" target="#b23">[24]</ref>. Hemanth et al. <ref type="bibr" target="#b41">[42]</ref> proposed a new Deep Hasing Network for UDA where hash codes are used to address the DA problem.</p><p>Correlation Alignment is another popular metric for feature adaptation where the covariances or second order statistics of the source and target data are aligned. In <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>, UDDA approaches have been proposed where the discrepancy between the source and target data is reduced by aligning the second order statistics of the source and target data. The idea of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> is similar to Deep Domain Confusion (DDC) <ref type="bibr" target="#b40">[41]</ref> and Deep Adaptation Network (DAN) <ref type="bibr" target="#b22">[23]</ref> except that instead of MMD, they adopted CORAL loss to minimize the discrepancy. CORAL loss is modified in <ref type="bibr" target="#b19">[20]</ref> by introducing a Riemannian Metric which performs better than <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref>. The aforementioned methods utilized two streams of Convolutional Neural Networks (CNN) where the source and target networks are fused at the classifier level. Domain-Adversarial Neural Networks (DANN) <ref type="bibr" target="#b9">[10]</ref> introduced a deep domain adaptation approach by integrating a gradient reversal layer into the traditional architecture.</p><p>Current DA approaches still suffer from DG problems when the target data is unavailable during training. In this paper, we explore how the DA approaches can be applied more efficiently on DG scenarios to improve their generalization capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Domain Generalization</head><p>In recent research, DG is a less explored issue than DA. The aim of DG is to acquire knowledge from the multiple source domains available to obtain a domain-independent model that can be used for a particular task, such as classification, to an unseen domain. Blanchard et al. <ref type="bibr" target="#b1">[2]</ref> first introduced an augmented Support Vector Machine (SVM) based model that solved automatic gating of flow cytometry by encoding empirical marginal distributions into the kernel. In <ref type="bibr" target="#b27">[28]</ref>, a DG model was proposed which learned a domain invariant transformation by reducing the discrepancy among available source domains. Xu et al. <ref type="bibr" target="#b42">[43]</ref> proposed a domain generalization method that worked on unseen domains by using low-rank structure from various latent source domains. Ghifary et al. <ref type="bibr" target="#b11">[12]</ref> proposed an autoencoder based technique to extract domain-invariant information through multi-task learning. These DG methods aggregate all the samples from training datasets to learn a shared invariant representation which is then used for the unseen target domain.</p><p>In <ref type="bibr" target="#b18">[19]</ref>, a domain generalization technique was introduced based on a multi-task max-margin classifier that regulates the weights of the classifier to improve the performance on unseen datasets which was inaccessible in the training phase. Fang et al. <ref type="bibr" target="#b8">[9]</ref> proposed a domain generalization method using Unbiased Metric Learning (UML). It makes a less biased distance metric that provides better object classification accuracy for an unknown target dataset. In <ref type="bibr" target="#b42">[43]</ref>, a domain generalization approach was proposed that combined the score of the classifiers. For multi-view domain generalization, <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> proposed another DG approach where multiple types of features of the source samples were used to learn a robust classifier. All the above methods exploit all the information from the training data to train a classifier or adjust weights.</p><p>The above mentioned DG methods use shallow models, hence the performance still needs to improve. Moreover, these shallow models extract the features first, which are then used to train the classifier. As a result, these are not end-to-end methods. Motiian et al. <ref type="bibr" target="#b26">[27]</ref> proposed supervised deep domain adaptation and generalization using contrastive loss to align the source data and target data. In this method, the source domains are aggregated and the learned model from source data is used for unseen target data. Li et al. <ref type="bibr" target="#b21">[22]</ref> proposed another DG method based on a low-rank parameterized CNN. A deep domain generalization architecture with a structured low-rank constraint to mitigate the domain generalization issue was proposed in <ref type="bibr" target="#b6">[7]</ref> where consistent information across multiple related source domains were captured.</p><p>Although many DA techniques based on deep architectures are proposed in recent years, very few DG approaches based on deep architecture are introduced. In this paper, we explore GAN along with a deep architecture to address the DG challenges. We propose a deep domain generalization framework using synthetic data generated by a GAN where the styles are transferred from one domain into another domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In this section, we will introduce our proposed Deep Domain Generalization approach by using synthetic data during training to produce an agnostic classifier that can be ap-plied for unseen target data. We build a model to generate images with the distributions of one source domain into other available source domains. We follow the definitions and notation of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b31">32]</ref>. Let, X and Y stand for the input (data) and the corresponding output (label) random variables, and P (X, Y ) represents the joint probability distribution of the input and output. We can define a domain as the probability distribution P (X, Y ) on X ? Y . We formalize domain adaptation and domain generalization as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptation</head><p>Let us consider that the source domain data samples are S D = {X s i } with available labels L s = {Y i } and the target data samples are T D = {X t i } without labels. The data distribution of the source and target data are different, i.e.,</p><formula xml:id="formula_0">P s (X s i , Y s ) = P t (X t i , Y t )</formula><p>where Y t is the label of target samples. The task of domain adaptation is to learn a classifying function f : X ? Y able to classify {X t i } to the corresponding {Y t i } given S D and T D during training as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Generalization</head><p>Let us consider that we have a set of n number of source domains such as</p><formula xml:id="formula_1">? = S 1 D ; S 2 D ; S 3 D ;. . . ;S n D and target domain T n D where T n D / ? ?. The aim of domain generalization is to gain a classifying function f : X ? Y able to classify {X t i } to the corresponding {Y t i } given S 1 D ; S 2 D ; S 3 D ;. . . ;S n D</formula><p>during training as the input, but T n D is unavailable during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Our Approach</head><p>Generative Adversarial Networks (GANs) have accomplished great outcomes in different applications, for example, image generation <ref type="bibr" target="#b13">[14]</ref>, text2image <ref type="bibr" target="#b33">[34]</ref>, image-toimage translation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b44">45]</ref>, person re-identification <ref type="bibr" target="#b17">[18]</ref> and image editing <ref type="bibr" target="#b32">[33]</ref>. The strong success of GANs is influenced by the concept of an adversarial loss that drives the generated images to be identical to the real images. Zhu et al. <ref type="bibr" target="#b45">[46]</ref> proposed a conditional multimodal image-to-image translation method where the training procedure requires paired data which is a supervised solution of image translation from one domain to other domains. As we do not have paired data on domain adaptation or generalization settings, we do not consider this method for synthetic image generation. Zhu et al. <ref type="bibr" target="#b44">[45]</ref> proposed CycleGAN that learned a mapping between an input image and an output image using both adversarial and cycle consistency loss where unpaired data were used for image generation. Anoosheh et al. <ref type="bibr" target="#b0">[1]</ref> modified CycleGAN by reducing the number of generators and discriminators while there are more than two domains. Choi et al. <ref type="bibr" target="#b3">[4]</ref> proposed Stargan for image translation from one domain to another domain. <ref type="bibr">Huang</ref>    <ref type="bibr" target="#b0">[1]</ref> which are used during the training phase and a discrepancy loss is used to mitigate the distribution discrepancy between the real source domain images and generated synthetic images. For an example, in Office-Caltech dataset <ref type="bibr" target="#b12">[13]</ref>, 4 different domains are available: Amazon, Webcam, DSLR and Caltech. When Caltech domain is unseen, we generate the images from the other three domains: Amazon, Webcam and DSLR. These generated images are used to train the network while the images from Caltech domain are classified during the test phase.</p><p>proposed another multi-modal image translation technique known as MUNIT which can translate image from one domain to other domains. However, the experiments so far have been tailored by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref> to merely two domains at a time whereas ComboGAN <ref type="bibr" target="#b0">[1]</ref> is capable to translate images at a time when more than two domains exist in the dataset. Our approach takes advantage of the natural image correspondence built by <ref type="bibr" target="#b0">[1]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> shows the overall architecture of our proposed method for DG based on synthetic image. We use ComboGAN <ref type="bibr" target="#b0">[1]</ref> to translate the samples in the source domain D n s to those other domains. Having 4 domains of Office-Home dataset <ref type="bibr" target="#b41">[42]</ref>, we take the image from one domain i.e., Art and translate it to 4 different image domains which are similar to Art, Clipart, P roduct and Real ? world domains.</p><p>GANs comprise of a generator G and a discriminator D. The input of the generator is random numbers and it generates an image. This generated image is fed into the discriminator with a stream of images taken from real datasets and the discriminator returns the probabilities representing the probability of a real or fake prediction. The training process is called adversarial training. Training itself is executed in two alternating steps; first the discriminator D is trained to distinguish between one or more pairs of real and generated samples, and the generator is trained to fool the discriminator with generated samples. At beginning, the discriminator should be too powerful so that it can identify the real and fake images. After that the generator would be more powerful than the discriminator so that the discriminator cannot distinguish the real and generated images. The training procedure of GAN is a min-max game between a generator and a discriminator. The optimization problem of GAN can be expressed as, </p><formula xml:id="formula_2">min G max D E x [logD(x)] + E z [log(1 ? D(G(z))].<label>(</label></formula><formula xml:id="formula_3">D ? S 2 D ; G 2 : S 1 D ? S 3 D ;. . . ; G n : S 1 D ? S n D .</formula><p>In our model, we have n number of discriminators where the discriminators distinguish between the real images and translated images. For transferring one domain's images into the style of other domain's images, we use adversarial losses <ref type="bibr" target="#b13">[14]</ref> and cycle consistency losses <ref type="bibr" target="#b44">[45]</ref>. To compare the distribution of the synthetic images to the distribution in the target domain, adversarial losses are used. On the other hand, to preclude the learned mappings G 1 , G 2 , G 3 and G n to be in conflict with each other, cycle consistency losses are used. The adversarial losses, for an example, mapping function G 1 : S 1 D ? S 2 D and its discriminator D 1 can be expressed as, <ref type="formula" target="#formula_2">(2)</ref> where the generator attempts to generate images with the distribution of a new domain (S 2 D ) and the discriminator D 1 tries to differentiate the generated images from the real images.</p><formula xml:id="formula_4">L GAN (G 1 , D 1 , S 1 D , S 2 D ) = E y?P S2 [logD 1 (y)] +E x?P S1 [log(1 ? D 1 (G 1 (x))],</formula><p>We need to map an individual input x i to an output y i , however, adversarial losses cannot ensure that it can map the input to the output properly. In addition to decrease the discrepancy among mapping functions, we use cycle consistency loss,</p><formula xml:id="formula_5">L cyc (G 1 , G 2 ) = E x?Ps1 [||G 2 (G 1 (x)) ? x||] +E y?Ps2 [||G 1 (G 2 (y)) ? y||].<label>(3)</label></formula><p>To transfer the style of one domain into another domain, for an example, to generate images in the style in domain S 2 D from the image of domain S 1 D , the full objective of the GAN is,</p><formula xml:id="formula_6">L(G 1 , G 2 , D 1 , D 2 ) = L GAN (G 1 , D 2 , S 2 D , S 1 D ) +L GAN (G 2 , D 1 , S 2 D , S 1 D ) + ?L cyc (G 1 , G 2 ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Domain Generalization using Synthetic Data</head><p>The previous domain generalization method <ref type="bibr" target="#b10">[11]</ref> divided the source domain datasets into a training set and a test set by random selection from the source datasets. It minimized the discrepancy between these training and test samples during training. In our method, we decrease the discrepancy between the real images and generated images. If there is no domain mismatch between the source and target data, the classifying function f is trained by minimizing the classification loss,</p><formula xml:id="formula_7">L C (f ) = E[l(f (X s ), Y ],<label>(5)</label></formula><p>where E[.] and l represent mathematical expectation and any loss functions (such as, categorical cross-entropy for multi-class classification) respectively. On the other hand, if the distribution of the source and target data are different, a DA algorithm is used to minimize the discrepancy between the source and target distributions. As in UDA techniques, there is no labeled data, thus the discrepancy is minimized using the following Equation,</p><p>L D (g) = d(p(g(X s )), p(g(X t ))).</p><p>The motivation behind Equation <ref type="formula" target="#formula_2">(6)</ref> is to adjust the distributions of the features within the embedding space, mapped from the source and the target data. Overall, in most of the deep domain adaptation algorithms, the objective is,</p><formula xml:id="formula_9">L T (f ) = L C + L D .<label>(7)</label></formula><p>We use GAN to generate images from one source domain into the distributions of other available source domains. The discrepancy between the synthetic and real images is minimized by domain discrepancy loss during training. Thus, the learned model can be applied to an unseen domain more effectively. We propose the new loss for training the model as follows, L D (g) = d(p(g(X s )), p(g(X G ))),</p><p>where X s are the real images and X G are the synthetic images, d can be any domain discrepancy metric such as coral loss or maximum mean discrepancy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Protocol for Applying DA Methods on DG Scenario</head><p>In this section, we will discuss our second approach for domain generalization where we randomly split each source domain data into a training (70%) and a validation set (30%). Most of the DA methods use Siamese based architecture where two stream of CNN is used. This 70% (from all source domain) is fed into one stream of CNN and the other 30% (from all source domain) data is fed into the second stream of CNN during the training phase. The discrepancy is minimized between these data in the training phase. In the test phase, we use unlabeled target data which is completely unseen during the training stage to evaluate our method for domain generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct substantial experiments to evaluate the proposed method and compare with state-ofthe-art UDDA and DG techniques.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate all the methods on four standard domain adaptation and generalization benchmark datasets: Office-31 <ref type="bibr" target="#b35">[36]</ref>, Office-Caltech <ref type="bibr" target="#b12">[13]</ref>, Office-Home <ref type="bibr" target="#b41">[42]</ref> and PACS <ref type="bibr" target="#b21">[22]</ref>.</p><p>Office-31 <ref type="bibr" target="#b35">[36]</ref> is the most prominent benchmark dataset for domain adaptation. It has 3 domains: Amazon (A) domain is formed with the downloaded images from amazon.com, DSLR (D), containing images captured by Digital SLR camera, and Webcam (W) contains images that captured by web camera with different photo graphical settings. For all experiments, we use labeled source data and unlabeled target data for unsupervised domain generalization where the target data is totally unseen during training. We conduct experiments on 3 transfer tasks (A, W ? D; A, D ? W and D, W ? A) where two domains are used as source domains and other is as target domain. For an example, the transfer task of A, W ? D, Amazon(A) and W ebcam(W ) are used as source domains whereas DSLR(D) is used as target domain. We also calculate the average performance of all transfer tasks.</p><p>Office-Caltech <ref type="bibr" target="#b12">[13]</ref> dataset is formed by taking the 10 common classes of two datasets: Office-31 and Caltech-256. It has 4 domains: Amazon (A), Webcam (W), DSLR (D) and Caltech (C). We conduct experiments on 4 transfer tasks where 3 domains or 2 domains are used as source domains (W, D, C ? A; A, W, D ? C; A, C ? D, W and D, W ? A, C).</p><p>Office-Home <ref type="bibr" target="#b41">[42]</ref> is a benchmark dataset for domain adaptation which contains 4 domains where each domain consists of 65 categories. The four domains are: Art (Ar): artistic images in the form of sketches, paintings, ornamentation, etc.; Clipart (Cl): collection of clipart images; Product (Pr): images of objects without a background and Real-World (Rw): images of objects captured with a regular camera. It contains 15,500 images, with an average of around 70 images per class and a maximum of 99 images in a class. We conduct experiments on 4 transfer tasks where 3 domains are used as source domains (P r, Rw, Cl ? Ar; P r, Rw, Ar ? Cl; P r, Ar, Cl ? Rw and Ar, Cl, Rw ? P r).</p><p>PACS <ref type="bibr" target="#b21">[22]</ref> is also a recently released benchmark dataset for domain generalization which is created by considering the common classes among Caltech256 , Sketchy, TU-Berlin and Google Images. It has 4 domains, each domain consists of 7 categories. It contains total 9991 images. We conduct experiments on 4 transfer tasks where 3 domains are used as source domains (P, C, S ? A; P, S, A ? C; S, A, C ? P and P, C, A ? S).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Network Architecture</head><p>We adopt the network model for generative architecture as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45]</ref> where impressive results for image translation in different domains are shown. The GAN network consists of two convolution layers with two stride-2, several residual blocks and two fractionally strided convolution layers with stride 0.5. We use 9 blocks for 256 ? 256 resolution images. We use 70 ? 70 PatchGANs for the discriminator networks. For domain generalization, we use DAN <ref type="bibr" target="#b22">[23]</ref>, D-CORAL <ref type="bibr" target="#b38">[39]</ref>, RTN <ref type="bibr" target="#b23">[24]</ref> and JAN <ref type="bibr" target="#b24">[25]</ref> architecture where Alexnet <ref type="bibr" target="#b20">[21]</ref> is used, comprising of five convolution layers and three fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Setup</head><p>For synthetic image generation, we use ComboGAN <ref type="bibr" target="#b0">[1]</ref> where we set the value of ? as 10 in Equation 4. In this model, Adam solver is used with a batch size of 1. We trained all the networks from scratch and we set the learning rate to 0.0002. We set total 200 epochs, and for the first 100 epochs, the learning rate does not change whereas it linearly decays the rate towards zero over the next 100 epochs.</p><p>For domain generalization, we use two streams of CNN. In each stream, we extend AlexNet <ref type="bibr" target="#b20">[21]</ref> model which is pretrained on the ImageNet <ref type="bibr" target="#b5">[6]</ref> dataset. We set the dimension of the last fully connected layer (fc8) to the number of categories (for instance, 65 for Office-Home dataset). We set the learning rate to 0.0001 to optimize the network. Moreover, we set the batch size to 128, weight decay to 5 ? 10 ?4 and momentum to 0.9 during the training phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results and Discussion</head><p>In this section, we will discuss our experimental results in detail. For fair comparison, we use the same net-work architecture (AlexNet <ref type="bibr" target="#b20">[21]</ref>) that are used in the existing domain adaptation methods such as DAN <ref type="bibr" target="#b22">[23]</ref>, D-CORAL <ref type="bibr" target="#b38">[39]</ref>, JAN <ref type="bibr" target="#b24">[25]</ref> and RTN <ref type="bibr" target="#b23">[24]</ref> in domain generalization settings. DAN <ref type="bibr" target="#b22">[23]</ref> is a deep domain adaptation model where the discrepancy between the source and target data is minimized using MMD. JAN <ref type="bibr" target="#b24">[25]</ref> is also a deep domain adaptation method where the discrepancy between the source and target data is mitigated using Joint Maximum Mean Discrepancy (JMMD) criterion. RTN <ref type="bibr" target="#b23">[24]</ref> is used for minimizing domain discrepancy between different distributions data using residual transfer network and MMD. On the other hand, D-CORAL <ref type="bibr" target="#b38">[39]</ref> is a DDA method where the discrepancy between two domains is minimized by the second order statistics (covariances) which is known as correlation alignment.    <ref type="table">Table 3</ref>: Recognition accuracies for domain generalization on the Office-Home dataset <ref type="bibr" target="#b41">[42]</ref> with synthetic images that are generated using MUNIT. The subscript S represents synthetic data.</p><p>We evaluate our DG model on different datasets. At first, we follow the same experimental setting as in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> for Office-Home dataset, and randomly split each source domain into a training set (70% ) and a validation set (30%).  <ref type="table">Table 4</ref>: Recognition accuracies for domain generalization on the Office-Home dataset <ref type="bibr" target="#b41">[42]</ref> with synthetic images that are generated using Stargan. The subscript S represents synthetic data.</p><p>Then we conduct experiments of having one domain totally unseen during the training stage. The 70% data is fed into one stream of CNN and 30% data is fed into the second stream of CNN. In the test phase, we use unlabeled target data which is completely unseen during the training stage. It is noted that the target data is not splitting. In the above setting, we evaluate 4 existing domain adaptation (DAN <ref type="bibr" target="#b22">[23]</ref>, D-CORAL <ref type="bibr" target="#b38">[39]</ref>, JAN <ref type="bibr" target="#b24">[25]</ref> and RTN <ref type="bibr" target="#b23">[24]</ref>) methods for domain generalization settings. We report these comparative results in <ref type="table" target="#tab_3">Table 1</ref>.</p><p>After that, we generate synthetic images using Combo-GAN <ref type="bibr" target="#b0">[1]</ref> in the training phase. <ref type="figure" target="#fig_2">Figure 2</ref> shows some generated images. The real images are fed into one stream of CNN and synthetic images are fed into another stream of CNN. In <ref type="table" target="#tab_4">Table 2</ref>, we report the comparative results. From <ref type="table" target="#tab_3">Table 1</ref> and <ref type="table" target="#tab_4">Table 2</ref>, we can observe that our method improves on an average 3% higher than 70%-30% settings in each transfer task on Office-Home dataset.</p><p>To further increase the justification of adopting Com-boGAN in our framework, we also experimentally evaluate our domain generalization model using synthetic images that are generated by MUNIT <ref type="bibr" target="#b15">[16]</ref> and Stargan <ref type="bibr" target="#b3">[4]</ref> on Office-Home dataset. The domain generalization on different tasks are reported in <ref type="table">Table 3</ref> and <ref type="table">Table 4</ref> using MUNIT and Stargan respectively. From the results (Tables 1, 2, 3 and 4), we make two important observations: (1) The multi-component synthetic data generation using GAN boosts the domain generalization performance; and (2) As ComboGAN can handle more than two domains at a time compared to MUNIT and Stargan, it can generate more multi-component images which are more effective for domain generalization.</p><p>We further evaluate and compare our proposed approach with both shallow and deep domain generalization stateof-the-art methods: Undoing the Damage of Dataset Bias (Undo-Bias) <ref type="bibr" target="#b18">[19]</ref>, Unbiased Metric Learning (UML) <ref type="bibr" target="#b8">[9]</ref>, Low-Rank Structure from Latent Domains for Domain Generalization (LRE-SVM) <ref type="bibr" target="#b42">[43]</ref>, Multi-Task Autoencoders (MTAE) <ref type="bibr" target="#b11">[12]</ref>, Domain Separation Network (DSN) <ref type="bibr" target="#b2">[3]</ref>, Deep Domain Generalization with Structured Low-Rank Constraint(DGLRC) <ref type="bibr" target="#b6">[7]</ref>, Domain Generalization via Invari-    <ref type="table">Table 7</ref>: Recognition accuracies for domain generalization on the PACS dataset <ref type="bibr" target="#b21">[22]</ref> using synthetic images that are generated by ComboGAN. The subscript S represents synthetic data.</p><p>ant Feature Representation (uDICA) <ref type="bibr" target="#b27">[28]</ref>, Deeper, Broader and Artier Domain Generalization (DBADG) <ref type="bibr" target="#b21">[22]</ref>.</p><p>We report comparative results in <ref type="table" target="#tab_7">Table 5</ref>, 6 and 7 on Office 31, Office-Caltech, and PACS datasets respectively. From <ref type="table" target="#tab_7">Table 5</ref>, it can be seen that, the previous best method <ref type="bibr" target="#b6">[7]</ref> achieved 80.02% average accuracy where the domain generalization issue is solved by using a structured low rank constraint. In contrast, our domain generalization method using synthetic data with D-CORAL <ref type="bibr" target="#b38">[39]</ref>, JAN <ref type="bibr" target="#b24">[25]</ref>, and RTN <ref type="bibr" target="#b23">[24]</ref> network architectures achieves 80.33%, 81.21% and 80.87% average accuracies respectively which outperforms the state-of-the-art methods. For Office-Caltech dataset (see <ref type="table" target="#tab_8">Table 6</ref>), although DGLRC <ref type="bibr" target="#b6">[7]</ref> achieved best performance, our proposed method is different in network architecture as we use generative adversarial network to generate synthetic data. Using synthetic data with DAN <ref type="bibr" target="#b22">[23]</ref>, D-CORAL <ref type="bibr" target="#b38">[39]</ref>, JAN <ref type="bibr" target="#b24">[25]</ref> and RTN <ref type="bibr" target="#b23">[24]</ref> network architecture, we achieve 85.52%, 85.29%, 86.87% and 86.04% average accuracies respectively. For PACS dataset, our proposed method achieves state-of-the-art performance using synthetic data and JAN architecture <ref type="bibr" target="#b24">[25]</ref>. It is worth noting that for PACS dataset, our result is 69.45% while the previous state-of-the-art method achieved 69.21%. Our image translation model is unsupervised, and capable of translating images from one domain to another in open-set domain adaptation/generalization scenario. However, to minimize discrepancy among different domains, we considered close set domain generalization settings where we assume that every source domains share similar classes as the target domain. In the future work, we will explore our method on outdoor scene dataset where the source domains may not share the same class labels with the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we developed a novel deep domain generalization architecture using synthetic images which were generated by a GAN and existing domain discrepancy minimizing metrics which aims to learn a domain agnostic model from the real and synthetic data that can be applied for unseen datasets. More specifically, we built an architecture to transfer the style from one domain image to another domain. After generating synthetic data, we used maximum mean discrepancy and correlation alignment metrics to minimize the discrepancy between the synthetic data and real data. Extensive experimental results on several benchmark datasets demonstrate our proposed method achieves stateof-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Our proposed methodology of Deep Domain Generalization. At first, synthetic images are generated using Com-boGAN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Sample synthetic images from the Office-Caltech dataset [13] which comprises 4 domains. (a) Four images are generated from one image of Webcam domain, (b) four images are generated from one image of Amazon domain, (c) four images are generated from one image of Caltech domain and (d) four images are generated from one image of DSLR domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al.<ref type="bibr" target="#b15">[16]</ref> </figDesc><table><row><cell></cell><cell>G1</cell><cell>D1</cell><cell>Synthetic Image</cell></row><row><cell></cell><cell></cell><cell></cell><cell>from Domain 2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Synthetic Image</cell></row><row><cell></cell><cell></cell><cell></cell><cell>from Domain 3</cell></row><row><cell>Real Image from</cell><cell></cell><cell></cell></row><row><cell>Domain 1</cell><cell>G2</cell><cell></cell><cell>Synthetic Images</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Synthetic Image</cell></row><row><cell></cell><cell></cell><cell></cell><cell>from Domain N -1</cell></row><row><cell>Labeled Source Data</cell><cell></cell><cell></cell></row><row><cell>Unlabeled Synthetic Data</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Recognition accuracies for domain generalization on the Office-Home dataset<ref type="bibr" target="#b41">[42]</ref>. Here, we split the source datasets into 70-30% training-validation samples.</figDesc><table><row><cell>Sources ? Target</cell><cell>Ours</cell><cell>Ours (D?</cell><cell>Ours</cell><cell>Ours</cell></row><row><cell></cell><cell>(DAN S )</cell><cell>CORAL S )</cell><cell>(JAN S )</cell><cell>(RT N S )</cell></row><row><cell>P r, Rw, Cl ?Ar</cell><cell>47.85</cell><cell>47.98</cell><cell>48.09</cell><cell>47.90</cell></row><row><cell>P r, Rw, Ar ?Cl</cell><cell>43.85</cell><cell>44.73</cell><cell>45.20</cell><cell>45.30</cell></row><row><cell>P r, Ar, Cl ?Rw</cell><cell>67.27</cell><cell>67.58</cell><cell>68.35</cell><cell>68.09</cell></row><row><cell>Ar, Cl, Rw ?Pr</cell><cell>64.29</cell><cell>64.78</cell><cell>66.52</cell><cell>66.21</cell></row><row><cell>Avg.</cell><cell>55.82</cell><cell>56.27</cell><cell>57.04</cell><cell>56.88</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Recognition accuracies for domain generalization on the Office-Home dataset<ref type="bibr" target="#b41">[42]</ref> with synthetic images that are generated using ComboGAN. The subscript S represents synthetic data.</figDesc><table><row><cell>Sources ? Target</cell><cell>Ours</cell><cell>Ours (D?</cell><cell>Ours</cell><cell>Ours</cell></row><row><cell></cell><cell>(DAN S )</cell><cell>CORAL S )</cell><cell>(JAN S )</cell><cell>(RT N S )</cell></row><row><cell>P r, Rw, Cl ?Ar</cell><cell>45.03</cell><cell>45.27</cell><cell>46.24</cell><cell>45.71</cell></row><row><cell>P r, Rw, Ar ?Cl</cell><cell>41.92</cell><cell>42.60</cell><cell>42.89</cell><cell>42.85</cell></row><row><cell>P r, Ar, Cl ?Rw</cell><cell>65.39</cell><cell>65.72</cell><cell>66.08</cell><cell>65.41</cell></row><row><cell>Ar, Cl, Rw ?Pr</cell><cell>62.56</cell><cell>63.43</cell><cell>64.90</cell><cell>64.37</cell></row><row><cell>Avg.</cell><cell>53.73</cell><cell>54.26</cell><cell>54.78</cell><cell>54.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Recognition accuracies for domain generalization on the Office31 dataset<ref type="bibr" target="#b35">[36]</ref> using synthetic images that are generated by ComboGAN. The subscript S represents synthetic data.</figDesc><table><row><cell>Sources ? Target</cell><cell>Undo-Bias</cell><cell>UML</cell><cell>LRE-SVM</cell><cell>MTAE</cell><cell>DSN</cell><cell>DGLRC</cell><cell>Ours (DAN S )</cell><cell>Ours (D ? CORAL S )</cell><cell>Ours (JAN S )</cell><cell>Ours (RT N S )</cell></row><row><cell>W, D, C ?A</cell><cell>90.98</cell><cell>91.02</cell><cell>91.87</cell><cell>93.13</cell><cell>-</cell><cell>94.21</cell><cell>92.27</cell><cell>92.79</cell><cell>93.31</cell><cell>93.06</cell></row><row><cell>A, W, D ?C</cell><cell>85.95</cell><cell>84.59</cell><cell>86.38</cell><cell>86.15</cell><cell>-</cell><cell>87.63</cell><cell>84.41</cell><cell>86.74</cell><cell>86.28</cell><cell>85.31</cell></row><row><cell>A, C ?D, W</cell><cell>80.49</cell><cell>82.29</cell><cell>84.59</cell><cell>85.35</cell><cell>85.76</cell><cell>86.32</cell><cell>85.17</cell><cell>82.07</cell><cell>85.17</cell><cell>84.27</cell></row><row><cell>D, W ?A, C</cell><cell>69.98</cell><cell>79.54</cell><cell>81.17</cell><cell>80.52</cell><cell>81.22</cell><cell>82.24</cell><cell>80.24</cell><cell>79.56</cell><cell>82.74</cell><cell>81.53</cell></row><row><cell>Avg.</cell><cell>81.85</cell><cell>84.36</cell><cell>86.00</cell><cell>86.28</cell><cell>-</cell><cell>87.60</cell><cell>85.52</cell><cell>85.29</cell><cell>86.87</cell><cell>86.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Recognition accuracies for domain generalization on the Office-Caltech dataset<ref type="bibr" target="#b12">[13]</ref> using synthetic images that are generated by ComboGAN. The subscript S represents synthetic data.</figDesc><table><row><cell>Sources ? Target</cell><cell>uDICA</cell><cell>LRE-SVM</cell><cell>MTAE</cell><cell>DSN</cell><cell>DBADG</cell><cell>Ours (DAN S )</cell><cell>Ours (D ? CORAL S )</cell><cell>Ours (JAN S )</cell><cell>Ours (RT N S )</cell></row><row><cell>P, C, S ?A</cell><cell>64.57</cell><cell>59.74</cell><cell>60.27</cell><cell>61.13</cell><cell>62.86</cell><cell>64.69</cell><cell>61.37</cell><cell>62.64</cell><cell>62.64</cell></row><row><cell>P, S, A ?C</cell><cell>64.54</cell><cell>52.81</cell><cell>58.65</cell><cell>66.54</cell><cell>66.97</cell><cell>63.60</cell><cell>66.16</cell><cell>65.98</cell><cell>66.52</cell></row><row><cell>S, A, C ?P</cell><cell>91.78</cell><cell>85.53</cell><cell>91.12</cell><cell>83.25</cell><cell>89.50</cell><cell>90.11</cell><cell>89.16</cell><cell>90.44</cell><cell>89.86</cell></row><row><cell>P, C, A ?S</cell><cell>51.12</cell><cell>37.89</cell><cell>47.86</cell><cell>58.58</cell><cell>57.51</cell><cell>58.08</cell><cell>58.92</cell><cell>58.76</cell><cell>57.68</cell></row><row><cell>Avg.</cell><cell>68.00</cell><cell>58.99</cell><cell>64.48</cell><cell>63.38</cell><cell>69.21</cell><cell>69.12</cell><cell>68.90</cell><cell>69.45</cell><cell>69.18</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The research presented in this paper was supported by Australian Research Council (ARC) Discovery Project Grants DP170100632 and DP140100793.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Combogan: Unrestrained scalability for image domain translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anoosheh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1712.06909</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalizing from several related classification tasks to a new unlabeled sample</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Stargan: Unified generative adversarial networks for multidomain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Domain Adaptation in Computer Vision Applications. Advances in Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep domain generalization with structured low-rank constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scatter component analysis: A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multimodal unsupervised image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep four-stream siamese convolutional neural network with joint verification and identification loss for person re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khatun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Denman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Correlation alignment by riemannian metric for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<idno>abs/1705.08180</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unified deep supervised domain adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Piccirilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-view domain generalization for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An exemplar-based multiview domain generalization framework for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="259" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual domain adaptation: A survey of recent advances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="53" to="69" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Invertible Conditional GANs for image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS) Workshop on Adversarial Training</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Freeman. Labelme: A database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Exploiting low-rank structure from latent domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning and evaluating classifiers under sample selection bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zadrozny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Toward multimodal image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
