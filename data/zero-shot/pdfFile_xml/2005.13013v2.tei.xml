<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
							<email>jasonphang@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
							<email>iacer.calixto@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">ILLC</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phu</forename><forename type="middle">Mon</forename><surname>Htut</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Vania</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Kann</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Colorado Boulder</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
							<email>bowman@nyu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">English Intermediate-Task Training Improves Zero-Shot Cross-Lingual Transfer Too</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Intermediate-task training-fine-tuning a pretrained model on an intermediate task before fine-tuning again on the target task-often improves model performance substantially on language understanding tasks in monolingual English settings. We investigate whether English intermediate-task training is still helpful on non-English target tasks. Using nine intermediate language-understanding tasks, we evaluate intermediate-task transfer in a zeroshot cross-lingual setting on the XTREME benchmark.</p><p>We see large improvements from intermediate training on the BUCC and Tatoeba sentence retrieval tasks and moderate improvements on question-answering target tasks. MNLI, SQuAD and HellaSwag achieve the best overall results as intermediate tasks, while multi-task intermediate offers small additional improvements. Using our best intermediate-task models for each target task, we obtain a 5.4 point improvement over XLM-R Large on the XTREME benchmark, setting the state of the art 1 as of June 2020. We also investigate continuing multilingual MLM during intermediate-task training and using machine-translated intermediatetask data, but neither consistently outperforms simply performing English intermediate-task training. arXiv:2005.13013v2 [cs.CL] 30 Sep 2020 Self-supervised multilingual pre-training Intermediate task training in English Target task fine-tuning in English XLM-R EN ????? MLM ... ? EN ??? EN EN ??? EN ?????</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Zero-shot cross-lingual transfer involves training a model on task data in one set of languages (or language pairs, in the case of translation) and evaluating the model on the same task in unseen languages (or pairs). In the context of natural language understanding tasks, this is generally done using a pretrained multilingual language-encoding model * * Equal contribution. <ref type="bibr">1</ref> The state of art on XTREME at the time of final publication in September 2020 is held by <ref type="bibr" target="#b14">Fang et al. (2020)</ref>, who introduce an orthogonal method. such as mBERT <ref type="bibr" target="#b12">(Devlin et al., 2019a)</ref>, XLM <ref type="bibr" target="#b10">(Conneau and Lample, 2019)</ref> or XLM-R <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref> that has been pretrained with a masked language modeling (MLM) objective on large corpora of multilingual data, fine-tune it on task data in one language, and evaluate the tuned model on the same task in other languages.</p><p>Intermediate-task training (STILTs; <ref type="bibr" target="#b34">Phang et al., 2018)</ref> consists of fine-tuning a pretrained model on a data-rich intermediate task, before fine-tuning a second time on the target task. Despite its simplicity, this two-phase training setup has been shown to be helpful across a range of Transformer models and target tasks <ref type="bibr">(Wang et al., 2019a;</ref><ref type="bibr">Pruksachatkun et al., 2020)</ref>, at least within English settings.</p><p>In this work, we propose to use intermediate training on English tasks to improve zero-shot cross-lingual transfer performance. Starting with a pretrained multilingual language encoder, we perform intermediate-task training on one or more English tasks, then fine-tune on the target task in English, and finally evaluate zero-shot on the same task in other languages.</p><p>Intermediate-task training on English data introduces a potential issue: We train the pretrained multilingual model extensively on only English data before evaluating it on non-English target task data, potentially causing the model to lose the knowledge of the other languages that was acquired during pretraining <ref type="bibr" target="#b22">(Kirkpatrick et al., 2017;</ref><ref type="bibr">Yogatama et al., 2019)</ref>. To mitigate this issue, we experiment with mixing in multilingual MLM training updates during the intermediate-task training. In the same vein, we also conduct a case study where we machine-translate intermediate task data from English into three other languages (German, Russian and Swahili) to investigate whether intermediate training on these languages improves target task performance in the same languages.</p><p>Concretely, we use the pretrained XLM-R (Con- <ref type="figure">Figure 1</ref>: We investigate the benefit of injecting an additional phase of intermediate-task training on English language task data. We also consider variants using multi-task intermediate-task training, as well as continuing multilingual MLM during intermediate-task training. Best viewed in color.</p><p>neau et al., 2020) encoder and perform experiments on 9 target tasks from the recently introduced XTREME benchmark <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>, which aims to evaluate zero-shot cross-lingual transfer performance across diverse target tasks across up to 40 languages each. We investigate how training on 9 different intermediate tasks, including question answering, sentence tagging, sentence completion, paraphrase detection, and natural language inference impacts zero-shot cross-lingual transfer performance. We find the following:</p><p>? Intermediate-task training on SQuAD, MNLI, and HellaSwag yields large target-task improvements of 8.2, 7.5, and 7.0 points on the development set, respectively. Multi-task intermediate-task training on all 9 tasks performs best, improving by 8.7 points. ? Evaluating our best performing models for each target task on the XTREME benchmark yields an average improvement of 5.4 points, setting the state of the art as of writing.</p><p>? Training on English intermediate tasks outperforms the more complex alternatives of (i) continuing multilingual MLM during intermediate-task training, and (ii) using machine-translated intermediate-task data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>We follow a three-phase approach to training, illustrated in <ref type="figure">Figure 1</ref>: (i) we use a publicly available model pretrained on raw multilingual text using MLM; (ii) we perform intermediate-task training on one or more English intermediate tasks; and (iii) we fine-tune the model on English target-task training data, before evaluating it on target-task test data in each target language. In phase (ii), our intermediate tasks have English input data. In Section 2.4, we investigate an alternative where we machine-translate intermediate-task data to other languages, which we use for training. We experiment with both single-and multi-task training for intermediate-task training. We use target tasks from the recent XTREME benchmark for zero-shot cross-lingual transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Intermediate Tasks</head><p>We study the effect of intermediate-task training (STILTs; <ref type="bibr" target="#b34">Phang et al., 2018)</ref> with nine different English intermediate tasks, described in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>We choose the tasks below based to cover a variety of task formats (classification, question answering, and multiple choice) and based on evidence  MNLI In additional to the full ANLI + , we also consider the MNLI task as a standalone intermediate task because of its already large and diverse training set.</p><p>QQP Quora Question Pairs 3 is a paraphrase detection dataset. Examples in the dataset consist of two questions, labeled for whether they are semantically equivalent.</p><p>SQuAD Stanford Question Answering Dataset <ref type="bibr">(Rajpurkar et al., 2016</ref><ref type="bibr">(Rajpurkar et al., , 2018</ref>) is a questionanswering dataset consisting of passages extracted from Wikipedia articles and crowd-sourced questions and answers. In SQuAD version 1.1, each example consists of a context passage and a question, and the answer is a text span from the context. SQuAD version 2.0 includes additional questions with no answers, written adversarially by crowdworkers. We use both versions in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Target Tasks</head><p>We use the 9 target tasks from the XTREME benchmark, which span 40 different languages (hereafter referred to as the target languages): Crosslingual Question Answering <ref type="table">(XQuAD; Artetxe</ref>   <ref type="bibr">baum et al., 2017, 2018)</ref>, which requires identifying parallel sentences from corpora of different languages; and Tatoeba <ref type="bibr" target="#b5">(Artetxe and Schwenk, 2019)</ref>, which involves aligning pairs of sentences with the same meaning. Among the 9 tasks, BUCC and Tatoeba are sentence retrieval tasks that do not include training sets, and are scored based on the similarity of learned representations (see Appendix A). XQuAD, Ty-DiQA and Tatoeba do not include development sets separate from the test sets. 4 For all XTREME tasks, we follow the training and evaluation protocol described in the benchmark paper <ref type="bibr" target="#b16">(Hu et al., 2020)</ref> 3 http://data.quora.com/ First-Quora-DatasetRelease-Question-Pairs 4 UDPOS also does not include development sets for Kazakh, Thai, Tagalog or Yoruba. and their sample implementation. 5 Intermediateand target-task statistics are shown in <ref type="table" target="#tab_2">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multilingual Masked Language Modeling</head><p>Our setup requires that we train the pretrained multilingual model extensively on English data before using it on a non-English target task, which can lead to the catastrophic forgetting of other languages acquired during pretraining. We investigate whether continuing to train on the multilingual MLM pretraining objective while fine-tuning on an English intermediate task can prevent catastrophic forgetting of the target languages and improve downstream transfer performance.</p><p>We construct a multilingual corpus across the 40 languages covered by the XTREME benchmark using Wikipedia dumps from April 14, 2020 for each language and the MLM data creation scripts from the jiant 1.3 library <ref type="bibr">(Phang et al., 2020)</ref>. In total, we use 2 million sentences sampled across all 40 languages using the sampling ratio from <ref type="bibr" target="#b10">Conneau and Lample (2019)</ref> with ? = 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Translated Intermediate-Task Training</head><p>Large-scale labeled datasets are rarely available in languages other than English for most languageunderstanding benchmark tasks. Given the availability of increasingly performant machine translation models, we investigate if using machinetranslated intermediate-task data can improve samelanguage transfer performance, compared to using English intermediate task data.</p><p>We translate training and validation data of three intermediate tasks: QQP, HellaSwag, and MNLI. We choose these tasks based on the size of the training sets and because their examplelevel (rather than word-level) labels can be easily mapped onto translated data. To translate QQP and HellaSwag, we use pretrained machine translation models from OPUS-MT (Tiedemann and Thottingal, 2020). These models are trained with Marian-NMT (Junczys-Dowmunt et al., 2018) on OPUS data (Tiedemann, 2012), which integrates several resources depending on the available corpora for the language pair. For MNLI, we use the publicly available machine-translated training data of XNLI provided by the XNLI authors. <ref type="bibr">6</ref> We use German, Russian, and Swahili translations of all three datasets instead of English data for the intermediate-task training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models</head><p>We use the pretrained XLM-R Large model <ref type="bibr" target="#b9">(Conneau et al., 2020)</ref> as a starting point for all our experiments, as it currently achieves state-of-theart performance on many zero-shot cross-lingual transfer tasks. 7 Details on intermediate-and targettask training can be found in Appendix A.</p><p>XLM-R For our baseline, we directly fine-tune the pretrained XLM-R model on each target task's English training data (if available) and evaluate zero-shot on non-English data, closely following the sample implementation for the XTREME benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XLM-R + Intermediate Task</head><p>In our main approach, as described in <ref type="figure">Figure 1</ref>, we include an additional intermediate-task training phase before training and evaluating on the target tasks as described above.</p><p>We also experiment with multi-task training on all available intermediate tasks. We follow <ref type="bibr">Raffel et al. (2020)</ref> and sample batches of examples for each task with probability r m = min(em,K) (min(em,K) , where e m is the number of examples in task m and the constant K = 2 17 limits the oversampling of data-rich tasks.</p><p>XLM-R + Intermediate Task + MLM To incorporate multilingual MLM into the intermediatetask training, we treat multilingual MLM as an additional task for intermediate training, using the same multi-task sampling strategy as above.</p><p>XLM-R + Translated Intermediate Task We translate intermediate-task training and validation data for three tasks and fine-tune XLM-R on translated intermediate-task data before we train and evaluate on the target tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Software</head><p>Experiments were carried out using the jiant <ref type="bibr">(Phang et al., 2020)</ref> library (2.0 alpha), based on PyTorch <ref type="bibr" target="#b32">(Paszke et al., 2019)</ref> and <ref type="bibr">Transformers (Wolf et al., 2019)</ref>. 7 XLM-R Large <ref type="bibr" target="#b9">(Conneau et al., 2020</ref>) is a 550m-parameter variant of the RoBERTa masked language model <ref type="bibr" target="#b26">(Liu et al., 2019b)</ref> trained on a cleaned version of CommonCrawl on 100 languages. Notably, Yoruba is used in the POS and NER XTREME tasks but not is not in the set of 100 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>We train three versions of each intermediate-task model with different random seeds. For each run, we compute the average target-task performance across languages, and report the median performance across the three random seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intermediate-Task Training</head><p>As shown in Table 2, no single intermediate task yields positive transfer across all target tasks. The target tasks TyDiQA, BUCC and Tatoeba see consistent gains from most or all intermediate tasks. In particular, BUCC and Tatoeba, the two sentence retrieval tasks with no training data, benefit universally from intermediate-task training. PAWS-X, NER, XQuAD and MLQA also exhibit gains with the additional intermediate-task training on some intermediate tasks. On the other hand, we find generally no or negative transfer to XNLI and POS.</p><p>Among the intermediate tasks, we find that MNLI performs best; with meaningful improvements across the PAWS-X, TyDiQA, BUCC and Tatoeba tasks. ANLI + , SQuAD v1.1, SQuAD v2.0 and HellaSwag also show strong positive transfer performance: SQuAD v1.1 shows strong positive transfer across all three QA tasks, SQuAD v2.0 shows the most positive transfer to TyDiQA, while HellaSwag shows the most positive transfer to NER and BUCC tasks. ANLI + does not show any improvement over MNLI (of which it is a superset), even on XNLI for which it offers additional directly relevant training data. This mirrors negative findings from <ref type="bibr" target="#b29">Nie et al. (2020)</ref> on NLI evaluations and Bowman et al. (2020) on transfer within English. QQP significantly improves sentence retrieval-task performance, but has broadly negative transfer to the other target tasks. 8 CCG also has relatively poor transfer performance, consistent with Pruksachatkun et al. <ref type="formula">(2020)</ref>.</p><p>Among our intermediate tasks, both SQuAD v1.1 and MNLI also serve as training sets for target tasks (for XNLI and XQuAD/MLQA respectively). While both tasks show overall positive transfer, SQuAD v1.1 actually markedly improves the performance in XQuAD and MLQA, while MNLI slightly hurts XNLI performance. We hypothesize that the somewhat surprising improvements to XQuAD and MLQA performance from SQuAD v1.1 arise due to the baseline XQuAD and MLQA </p><formula xml:id="formula_0">ANLI + -0.8 -0.0 -1.4 -3.5 -1.1 / -0.5 -0.6 / -0.8 -0.6 / -3.0 +19.9 +48.2 + 6.6 MNLI -1.2 + 1.4 -0.7 + 0.5 -0.3 / -0.1 + 0.2 / + 0.2 -1.0 / -1.6 +20.0 +48.8 + 7.5 QQP -4.4 -4.8 -6.5 -45.4 -3.8 / -3.8 -3.9 / -4.4 -11.1 / -10.2 +17.1 +49.5 -1.5 SQuADv1.1 -1.9 + 1.2 -0.8 -0.4 + 1.8 / + 2.5 + 2.2 / + 2.6 + 9.7 / +10.8 +18.9 +41.3 + 8.1 SQuADv2 -1.6 + 1.9 -1.1 + 0.8 -0.5 / + 0.7 -0.4 / + 0.1 +10.4 / +11.3 +19.3 +43.4 + 8.2 HellaSwag -7.1 + 1.8 -0.7 + 1.6 -0.0 / + 0.5 -0.1 / + 0.2 -0.0 / -1.0 +20.3 +47.6 + 7.0 CCG -2.6 -3.4 -2.0 -1.5 -1.5 / -1.3 -1.6 / -1.5 -2.8 / -6.2 +11.7 +41.9 + 4.1 CosmosQA -2.1 -0.3 -1.4 -1.5 -0.9 / -1.3 -1.5 / -2.0 + 0.5 / -0.6 +19.2 +43.9 + 6.1 CSQA -2.9 -2.8 -1.7 -1.6 -1.0 / -1.8 -1.0 / -0.6 + 3.5 / + 2.9</formula><p>+18.1 +48.6 + 6.5 Multi-task -0.9 + 1.7 -1.0 + 1.8 + 0.3 / + 0.9 + 0.2 / + 0.5 + 5.8 / + 6.0 +19.6 +49.9 + 8.7</p><p>With MLM    to development set performance on each target task. Based on the results in <ref type="table" target="#tab_5">Table 2</ref>, which reflect the median over 3 runs, we pick the best intermediatetask configuration for each target task, and then choose the best model out of the 3 runs. Scores on the XTREME benchmark are computed based on the respective test sets where available, and based on development sets for target tasks without separate held-out test sets. We are generally able to replicate the best reported XLM-R baseline results, except for Tatoeba, where our implementation significantly underperforms the reported scores in <ref type="bibr" target="#b16">Hu et al. (2020)</ref>, and TyDiQA, where our implementation outperforms the reported scores. We also highlight that there is a large margin of difference between development and test set scores for BUCCthis is likely because BUCC is evaluated based on sentence retrieval over the given set of input sentences, and the test sets for BUCC are generally much larger than the development sets.</p><formula xml:id="formula_1">ANLI + -1.1 + 1.4 + 0.0 + 0.4 -1.9 / -1.7 -0.7 / -0.6 + 0.9 / + 0.5 +18.6 +46.2 + 7.1 MNLI -0.7 + 1.6 -1.6 + 1.0 -0.7 / + 0.1 + 0.4 / + 0.8 -1.8 / -3.2 +17.1 +44.3 + 6.6 QQP -1.3 -1.1 -2.4 -0.9 -0.3 / -0.2 + 0.0 / + 0.2 -1.6 / -4.2 +14.4 +39.8 + 5.0 SQuADv1.1 -2.6 + 0.3 -2.0 -0.9 + 0.2 / + 1.6 + 0.1 / + 1.1 + 8.5 / + 9.5 +16.0 +40.3 + 6.8 SQuADv2 -1.7 + 2.1 -1.4 + 1.0 -0.8 / + 0.1 -0.8 / -0.5 + 8.3 / + 8.9 +15.6 +31.3 + 6.1 HellaSwag -3.3 + 2.0 -0.7 + 0.8 -0.8 / -0.0 + 0.1 / + 0.6 + 0.3 / + 1.0 + 6.3 +22.3 + 3.1 CCG -1.0 -1.3 -1.2 -1.9 -1.9 / -2.2 -2.1 / -2.6 -5.5 / -6.2 + 8.8 +36.1 + 3.3 CosmosQA -1.0 -1.0 -1.6 -3.8 -3.1 / -3.3 -3.7 / -4.2 -0.6 / -3.2 +15.5 +42.7 + 4.7 CSQA -0.5 + 0.3 -1.0 -0.7 -0.9 / -1.0 -0.7 / -0.6 + 2.1 / + 0.4 +11</formula><formula xml:id="formula_2">MNLIen -0.8 + 0.9 -0.1 -0.8 -0.3 / -1.0 -1.0 / -0.2 - +16.5 +32.7 MNLIde -0.4 + 0.5 -0.3 -0.9 + 0.2 / -0.3 -2.4 / -2.0 - +17.0 +33.7 QQPen -2.2 -4.2 -3.2 -7.3 -4.5 / -4.7 -6.7 / -6.4 - +16.5 +32.6 QQPde -2.6 -9.1 -3.2 -22.9 -6.6 / -5.9 -7.7 / -6.6 - +16.0 +33.5 HellaSwagen -0.3 + 0.3 + 0.1 + 0.5 + 1.0 / + 0.2 -0.3 / + 0.4 - +16.9 +33.8 HellaSwagde -0.2 + 0.2 -0.4 -0.4 + 0.2 / -0.2 -3.5 / -2.5 - +16.3 +33.</formula><formula xml:id="formula_3">MNLIen + 0.3 - -0.0 + 0.8 + 0.1 / + 1.5 - -1.5 / -4.6 +14.3 +47.1 MNLIru -0.6 - -0.3 + 1.9 -0.4 / + 1.3 - +11.2 / +16.1 +13.1 +48.3 QQPen -0.7 - -2.9 -18.6 -3.5 / -2.4 - -8.1 / -5.4 +14.1 +49.5 QQPru -3.0 - -10.6 -59.1 -5.2 / -3.9 - -14.4 / -12.1 +13.3 +46.7 HellaSwagen -0.9 - -0.0 + 1.4 + 0.8 / + 2.9 - -4.0 / -10.6 +14.7 +49.9 HellaSwagru -0.3 - -0.4 + 2.8 + 0.2 / + 0.2 - + 8.5 / +13.2 -71.6 -23.5 Swahili XLM-R 72.4 - - 69.8 - - 67.2 / 48.7 - 7.9 MNLIen -3.0 - - + 0.6 - - -0.3 / -0.2 - +24.9 MNLIsw -1.1 - - -2.4 - - +13.8 / +23.4 - +47.9 QQPen -2.8 - - -4.6 - - -12.7 / -12.2 - +27.2 QQPsw -7.1 - - -32.1 - - -7.0 / -0.4 - +41.8 HellaSwagen -0.4 - - + 0.1 - - -0.9 / -0.4 - +27.2 HellaSwagsw -9.8 - - + 0.4 - - +15.6 / +26.3 - -0.5</formula><p>Our best models show gains in 8 out of the 9 XTREME tasks relative to both baseline implementations, attaining an average score of 73.5 across target tasks, a 5.4 point improvement over the pre-vious best reported average score of 68.1. We set the state of the art on the XTREME benchmark as of June 2020, though <ref type="bibr" target="#b14">Fang et al. (2020)</ref> achieve higher results and hold the state of the art using an orthogonal approach at the time of our final publication in September 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Translated Intermediate-Task Training Data</head><p>In <ref type="table" target="#tab_7">Table 3</ref>, we show results for experiments using machine-translated intermediate-training data, and evaluated on the available target-task languages. Surprisingly, even when evaluating inlanguage, using target-language intermediate-task data does not consistently outperform using English intermediate-task data in any of the intermediate tasks on average.</p><p>In general, cross-lingual transfer to XNLI is negative regardless of the intermediate-task or the target language. In contrast, we observe mostly positive transfer on BUCC, and Tatoeba, with a few notable exceptions where models fail catastrophically. TyDiQA exhibits positive transfer where the intermediate-and target-task languages aligned: intermediate training on Russian or German helps TyDiQA performance in that respective language, whereas intermediate training on English hurts non-English performance somewhat. For the remaining tasks, there appears to be little correlation between performance and the alignment of intermediateand target-task languages. English language QQP already has mostly negative transfer to all target tasks except for BUCC and Tatoeba (see <ref type="table" target="#tab_5">Table 2</ref>), and also shows a similar trend when translated into any of the three target languages.</p><p>We note that the quality of translations may affect the transfer performance. While validation performance on the translated intermediate tasks <ref type="table" target="#tab_2">(Table 15</ref>) for MNLI and QQP is only slightly worse than the original English versions, the performance for the Russian and Swahili HellaSwag is much worse and close to chance. Despite this, intermediate-task training on Russian and Swahili HellaSwag improve performance on PAN-X and TyDiQA, while we see generally poor transfer performance from QQP. The interaction between translated intermediate-task data and transfer performance continues to be a complex open question. <ref type="bibr" target="#b3">Artetxe et al. (2020a)</ref> found that translating or back-translating training data for a task can improve zero-shot cross-lingual performance for tasks such as XNLI depending on how the multilingual datasets are created. In contrast, we train on translated intermediate-task data and then fine-tune on a target task with English training data (excluding BUCC2018 and Tatoeba). The authors of the XTREME benchmark have also recently released translated versions of all the XTREME task training data, which we hope will prompt further investigation into this matter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Sequential transfer learning using pretrained Transformer-based encoders <ref type="bibr" target="#b34">(Phang et al., 2018)</ref> has been shown to be effective for many text classification tasks. This setup generally involves finetuning on a single task <ref type="bibr">(Pruksachatkun et al., 2020;</ref><ref type="bibr">Vu et al., 2020)</ref> or multiple tasks <ref type="bibr" target="#b25">(Liu et al., 2019a;</ref><ref type="bibr">Wang et al., 2019b;</ref><ref type="bibr">Raffel et al., 2020)</ref>, sometimes referred to as the intermediate task(s), before finetuning on the target task. We build upon this line of work, focusing on intermediate-task training for improving cross-lingual transfer.</p><p>Early work on cross-lingual transfer mostly relies on the availability of parallel data, where one can perform translation <ref type="bibr" target="#b27">(Mayhew et al., 2017)</ref> or project annotations from one language into another <ref type="bibr" target="#b19">(Hwa et al., 2005;</ref><ref type="bibr" target="#b1">Agi? et al., 2016)</ref>. For dependency parsing, <ref type="bibr" target="#b28">McDonald et al. (2011)</ref> use delexicalized parsers trained on source languages and labeled training data for parsing target-language data. <ref type="bibr">Agi? (2017)</ref> proposes a parser selection method to select the single best parser for a target language.</p><p>For large-scale cross-lingual transfer outside NLU, <ref type="bibr" target="#b20">Johnson et al. (2017)</ref> train a single multilingual neural machine translation system with up to 7 languages and perform zero-shot translation without explicit bridging between the source and target languages. <ref type="bibr" target="#b2">Aharoni et al. (2019)</ref> expand this approach to cover over 100 languages in a single model. Recent works on extending pretrained Transformer-based encoders to multilingual settings show that these models are effective for cross-lingual tasks and competitive with strong monolingual models on the XNLI benchmark <ref type="bibr" target="#b13">(Devlin et al., 2019b;</ref><ref type="bibr" target="#b10">Conneau and Lample, 2019;</ref><ref type="bibr" target="#b9">Conneau et al., 2020;</ref><ref type="bibr" target="#b17">Huang et al., 2019a)</ref>. More recently, <ref type="bibr" target="#b3">Artetxe et al. (2020a)</ref> showed that cross-lingual transfer performance can be sensitive to translation artifacts arising from a multilingual datasets' creation procedure.</p><p>Finally, <ref type="bibr" target="#b33">Pfeiffer et al. (2020)</ref> propose adapter modules that learn language and task representations for cross-lingual transfer, which allow adaptation to languages not seen during pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We evaluate the impact of intermediate-task training on zero-shot cross-lingual transfer. We investigate 9 intermediate tasks and how intermediate-task training impacts the zero-shot cross-lingual transfer to the 9 target tasks in the XTREME benchmark. Overall, intermediate-task training significantly improves the performance on BUCC and Tatoeba, the two sentence retrieval target tasks in the XTREME benchmark, across almost every intermediate-task configuration. Our best models obtain 5.9 and 23.9 point gains on BUCC and Tatoeba, respectively, compared to the best available XLM-R baseline scores <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>. We also observed gains in question-answering tasks, particularly using SQuAD v1.1 and v2.0 as intermediate tasks, with absolute gains of 2.1 F1 for XQuAD, 0.8 F1 for MLQA, and 10.4 for F1 Ty-DiQA, again over the best available baseline scores. We improve over XLM-R by 5.4 points on average on the XTREME benchmark. Additionally, we found multi-task training on all 9 intermedi-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 XTREME Benchmark Target Tasks</head><p>We follow the sample implementation for the XTREME benchmark unless otherwise stated. We use a learning rate of 3e-6, and use the same optimization procedure as for intermediate tasks. Hyperparameters in the <ref type="table" target="#tab_11">Table 5</ref> follow the sample implementation. For POS and NER, we use the same strategy as for CCG for matching tags to tokens. For BUCC and Tatoeba, we extract the representations for each token from the 13th self-attention layer, and use the mean-pooled representation as the embedding for that example, as in the sample implementation. Similarly, we follow the sample implementation and set an optimal threshold for each language sub-task for BUCC as a similarity score cut-off for extracting parallel sentences based on the development set and applied to the test set. We randomly initialize the corresponding output heads for each task, regardless of the similarity between intermediate and target tasks (e.g. even if both the intermediate and target tasks train on SQuAD, we randomly initialize the output head in between phases).          <ref type="table" target="#tab_2">Table 15</ref>: Intermediate task performance on trained and evaluated on translated data. We report the median result for English (original) task data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>et al., 2020b); Multilingual Question Answering (MLQA; Lewis et al., 2020); Typologically Diverse Question Answering (TyDiQA-GoldP; Clark et al., 2020); Cross-lingual Natural Language Inference (XNLI; Conneau et al., 2018); Crosslingual Paraphrase Adversaries from Word Scrambling (PAWS-X; Yang et al., 2019); Universal Dependencies v2.5 (Nivre et al., 2018) POS tagging; Wikiann NER (Pan et al., 2017); BUCC (Zweigen</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>Applying intermediate-task training to BUCC and Tatoeba, the two sentence retrieval target tasks that have no training data of their own, yields dramatic improvements with almost every intermediate training configuration. Ty-DiQA shows consistent improvements with many intermediate tasks, whereas XNLI does not see benefits from intermediate training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>and</cell></row></table><note>Overview of the intermediate tasks (top) and target tasks (bottom) in our experiments. For target tasks, Train and Dev correspond to the English training and development sets, while Test shows the range of sizes for the target-language test sets for each task. XQuAD, TyDiQA and Tateoba do not have separate held-out development sets.of positive transfer from literature. Pruksachatkun et al. (2020) shows that MNLI (of which ANLI + is a superset), CommonsenseQA, Cosmos QA+ . For all three natural language inference tasks, examples consist of premise and hypothesis sentence pairs, and the task is to classify the relationship between the premise and hypothe- sis as entailment, contradiction, or neutral.(Zellers et al., 2018; Le Bras et al., 2020) with BERT.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Intermediate-task training results. We compute the average target task performance across all languages, and report the median over 3 separate runs with different random seeds. Multi-task experiments use all intermediate tasks. We underline the best results per target task with and without intermediate MLM co-training, and bold-face the best overall scores for each target task. ? : XQuAD, TyDiQA and Tatoeba do not have held-out test data and are scored using development sets in the benchmark. ? : Results obtained with our best-performing intermediate task configuration for each target task, selected based on the development set. The results for individual languages can be found in Appendix B. models being under-trained. For all target-task finetuning, we follow the sample implementation for target task training in the XTREME benchmark, which trains on SQuAD for only 2 epochs. This may explain why an additional phase of SQuAD training can improve performance. Conversely, the MNLI-to-XNLI model might be over-trained, given the MNLI training set is approximately 4 times as large as the SQuAD v1.1 training set.Multi-Task Training Multi-task training on all intermediate tasks attains the best overall average performance on the XTREME tasks, and has the most positive transfer to NER and Tatoeba tasks. However, the overall margin of improvement over the best single intermediate-task model is relatively small (only 0.3, over MNLI), while requiring significantly more training resources. Many single intermediate-task models also outperform the multitask model in individual target tasks. Wang et al. (2019b) also found more mixed results from a having an initial phase of multi-task training, albeit only among English language tasks across a different set of tasks. On the other hand, multi-task training precludes the need to do intermediate-task model selection, and is a useful method for incorporating multiple, diverse intermediate tasks.MLM Incorporating MLM during intermediatetask training shows no clear trend. It reduces negative transfer, as seen in the cases of Common-senseQA and QQP, but it also tends to somewhat reduce positive transfer. The reductions in positive transfer are particularly significant for the BUCC and Tatoeba tasks, although the impact on TyDiQA is more mixed. On balance, we do not see that incorporating MLM improves transfer performance.</figDesc><table><row><cell>XTREME Benchmark Results At the bottom</cell></row><row><cell>of Table 2, we show results obtained by XLM-R</cell></row><row><cell>on the XTREME benchmark as reported by</cell></row><row><cell>Hu et al. (2020), results obtained with our re-</cell></row><row><cell>implementation of XLM-R (i.e. our baseline), and</cell></row><row><cell>results obtained with our best models, which use</cell></row><row><cell>intermediate-task configuration selected according</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Experiments with translated intermediate-task training and validation data evaluated on all XTREME target tasks. In each target language (TL) block, models are evaluated on a single target language. We show results for models trained on original intermediate-task training data (en) and compare it to models trained on translated data {de,ru,sw}. '-' indicates that target task data is not available for that target language.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>ate tasks to slightly outperform individual intermediate training. On the other hand, we found that neither incorporating multilingual MLM into the intermediate-task training phase nor translating intermediate-task data consistently led to improved transfer performance. Ellie Pavlick, and Samuel R. Bowman. 2019a. Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4465-4476, Florence, Italy. Association for Computational Linguistics. Annual Meeting of the Association for Computational Linguistics, pages 4791-4800, Florence, Italy. Association for Computational Linguistics.</figDesc><table><row><cell>Shuning Jin, Berlin Chen, Benjamin Van Durme, A Implementation Details</cell><cell>Bowman. 2020. Intermediate-task transfer learning a machine really finish your sentence? In Pro-</cell></row><row><cell>While we have explored the extent to which En-glish intermediate-task training can improve cross-lingual transfer, a clear next avenue of investigation for future work is how the choice of intermediate-and target-task languages influences transfer across different tasks. Acknowledgments This project has benefited from support to SB by Eric and Wendy Schmidt (made by recommenda-tion of the Schmidt Futures program), by Sam-sung Research (under the project Improving Deep Learning using Latent Structure), by Intuit, Inc., by NVIDIA Corporation (with the donation of a Titan V GPU), by Google (with the donation of Google Cloud credits). IC has received funding from the A.1 Intermediate Tasks For intermediate-task training, we use a learning rate of 1e-5 without MLM, and 5e-6 with MLM. Hyperparameters in the Table 4 were chosen based on intermediate task validation performance in an Edouard Grave, Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pap-pagari, R. Thomas McCoy, Roma Patel, Najoung Kim, Ian Tenney, Yinghui Huang, Katherin Yu, preliminary search. We use a warmup of 10% of the total number of steps, and perform early stopping based on the first 500 development set examples of Shuning Jin, Berlin Chen, Benjamin Van Durme, each task with a patience of 30. For CCG, where Edouard Grave, Ellie Pavlick, and Samuel R. Bow-man. 2019b. Can you tell me how to get past sesame street? sentence-level pretraining beyond language tags are assigned for each word, we use the repre-sentation of first sub-word token of each word for modeling. In Proceedings of the 57th Annual Meet-prediction. ing of the Association for Computational Linguistics, pages 4465-4476, Florence, Italy. Association for Computational Linguistics. Task Batch size # Epochs ANLI + 24 2 Adina Williams, Nikita Nangia, and Samuel Bowman. MNLI 24 2 2018. A broad-coverage challenge corpus for sen-CCG 24 15 tence understanding through inference. In Proceed-CommonsenseQA 4 10 ings of the 2018 Conference of the North American Cosmos QA 4 15 Chapter of the Association for Computational Lin-HellaSwag 24 7 guistics: Human Language Technologies, Volume QQP 24 3 1 (Long Papers), pages 1112-1122. Association for SQuAD 8 3 European Unions Horizon 2020 research and inno-vation program under the Marie Sk?odowska-Curie grant agreement No 838188. This project has ben-efited from direct support by the NYU IT High based upon work supported by the National Sci-ence Foundation under Grant No. 1922658. Any ing. Unpublished manuscript available on arXiv. formers: State-of-the-art natural language process-icz, and Jamie Brew. 2019. Huggingface's trans-Performance Computing Center. This material is Computational Linguistics. MLM 8 -ric Cistac, Tim Rault, R'emi Louf, Morgan Funtow-Chaumond, Clement Delangue, Anthony Moi, Pier-Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Multi-task Mixed 3</cell><cell>with pretrained language models: When and why does it work? In Proceedings of the 58th Annual Meeting of the Association for Computational Lin-guistics, pages 5231-5247, Online. Association for Computational Linguistics. Colin Raffel, Noam Shazeer, Adam Roberts, Kather-ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Re-search, 21(140):1-67. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable ques-tions for SQuAD. In Proceedings of the 56th An-nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784-ceedings of the 57th Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2017. Overview of the second BUCC shared task: Spotting parallel sentences in comparable cor-pora. In Proceedings of the 10th Workshop on Build-ing and Using Comparable Corpora, pages 60-67, Vancouver, Canada. Association for Computational Linguistics. Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2018. Overview of the third BUCC shared task:spotting parallel sentences in comparable cor-pora. In Proceedings of 11th Workshopon Building and Using Comparable Corpora, pages 39-42. 789, Melbourne, Australia. Association for Compu-tational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu-ral Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics. tion answering challenge targeting commonsense Jonathan Berant. 2019. CommonsenseQA: A ques-Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Robert Speer, Joshua Chin, and Catherine Havasi. 2017. Artificial Intelligence. eral knowledge. In Thirty-First AAAI Conference on Conceptnet 5.5: An open multilingual graph of gen-</cell></row><row><cell>opinions, findings, and conclusions or recommen-dations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019. PAWS-x: A cross-lingual adversar-ial dataset for paraphrase identification. In Proceed-ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter-national Joint Conference on Natural Language Pro-</cell><cell>knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Associ-ation for Computational Linguistics.</cell></row><row><cell>cessing (EMNLP-IJCNLP), pages 3687-3692, Hong Kong, China. Association for Computational Lin-guistics.</cell><cell>J?rg Tiedemann. 2012. Parallel Data, Tools and In-terfaces in OPUS. In Proceedings of the Eight In-ternational Conference on Language Resources and</cell></row><row><cell>Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Ling-</cell><cell>Evaluation (LREC'12), Istanbul, Turkey. European Language Resources Association (ELRA).</cell></row><row><cell>peng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, et al. 2019. Learning and evaluat-ing general linguistic intelligence. arXiv preprint arXiv:1901.11373.</cell><cell>J?rg Tiedemann and Santhosh Thottingal. 2020. OPUS-MT Building open translation services for the World. In Proceedings of the 22nd Annual Con-ferenec of the European Association for Machine</cell></row><row><cell>Rowan Zellers, Yonatan Bisk, Roy Schwartz, and</cell><cell>Translation (EAMT), Lisbon, Portugal.</cell></row><row><cell>Yejin Choi. 2018. SWAG: A large-scale adversar-</cell><cell>Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-</cell></row><row><cell>ial dataset for grounded commonsense inference. In</cell><cell>dro Sordoni, Adam Trischler, Andrew Mattarella-</cell></row><row><cell>Proceedings of the 2018 Conference on Empirical</cell><cell>Micke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-</cell></row><row><cell>Methods in Natural Language Processing, pages 93-</cell><cell>ploring and predicting transferability across NLP</cell></row><row><cell>104, Brussels, Belgium. Association for Computa-</cell><cell>tasks.</cell></row><row><cell>tional Linguistics.</cell><cell></cell></row><row><cell></cell><cell>Alex Wang, Jan Hula, Patrick Xia, Raghavendra Pap-</cell></row><row><cell>Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali</cell><cell>pagari, R. Thomas McCoy, Roma Patel, Najoung</cell></row><row><cell>Farhadi, and Yejin Choi. 2019. HellaSwag: Can</cell><cell>Kim, Ian Tenney, Yinghui Huang, Katherin Yu,</cell></row></table><note>Yada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Intermediate-task training configuration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Target-task training configuration. 82.7 83.8 81.3 89.3 84.4 83.7 77.3 79.2 72.4 77.1 78.9 72.6 80.0 79.6 80.1</figDesc><table><row><cell>B Per-Language Results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">: Full XNLI Results</cell><cell></cell></row><row><cell></cell><cell></cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>fr</cell><cell>ja</cell><cell>ko</cell><cell>zh</cell><cell>Avg</cell></row><row><cell></cell><cell>XLM-R</cell><cell cols="7">88.1 93.4 89.2 89.3 81.8 81.8 82.0 86.5</cell></row><row><cell></cell><cell>ANLI +</cell><cell cols="7">88.0 94.1 89.6 90.7 82.0 82.2 81.9 87.0</cell></row><row><cell>Without MLM</cell><cell cols="8">MNLI QQP SQuADv2.0 88.9 95.2 91.7 91.3 84.7 84.5 85.4 88.8 89.0 95.0 90.7 90.9 82.9 83.8 84.2 88.1 83.9 93.0 87.7 88.7 79.2 78.6 79.7 84.4 SQuADv1.1 89.4 94.2 91.1 91.1 83.8 83.5 83.9 88.1 HellaSwag 88.4 95.0 90.2 91.1 84.8 84.6 84.5 88.4 CCG 83.5 92.3 86.5 88.1 78.0 77.0 78.6 83.5 Cosmos QA 88.4 93.8 90.4 90.3 84.3 84.3 85.0 88.1</cell></row><row><cell></cell><cell>CSQA</cell><cell cols="7">85.9 93.7 88.6 89.8 81.7 80.4 81.5 86.0</cell></row><row><cell></cell><cell>Multi-task</cell><cell cols="7">89.0 95.0 90.2 91.1 83.8 83.5 85.5 88.3</cell></row><row><cell></cell><cell>ANLI +</cell><cell cols="7">88.1 94.5 90.1 90.4 84.0 84.2 84.2 87.9</cell></row><row><cell>With MLM</cell><cell cols="8">MNLI QQP SQuADv2.0 88.9 95.0 91.7 92.0 85.2 83.9 84.7 88.8 90.1 95.5 91.3 91.3 84.4 84.1 84.5 88.7 88.6 94.3 89.8 90.6 81.7 82.8 82.3 87.1 SQuADv1.1 89.0 93.8 90.3 88.9 82.7 82.2 82.2 87.0 HellaSwag 90.3 95.0 91.0 90.5 84.9 85.9 84.8 88.9 CCG 87.5 93.3 88.3 88.4 81.5 81.2 81.3 85.9</cell></row><row><cell></cell><cell cols="8">Cosmos QA 88.1 94.0 89.4 90.0 82.5 82.4 82.3 87.0</cell></row><row><cell></cell><cell>CSQA</cell><cell cols="7">88.7 94.1 89.1 89.8 82.5 82.9 82.2 87.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>: Full PAWS-X Results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Full POS Results. kk, th, tl and yo do not have development set data.</figDesc><table><row><cell></cell><cell></cell><cell>af</cell><cell>ar</cell><cell>bg</cell><cell>bn</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>et</cell><cell>eu</cell><cell>fa</cell><cell>fi</cell><cell>fr</cell><cell>he</cell><cell>hi</cell><cell>hu</cell><cell>id</cell><cell>it</cell><cell>ja</cell><cell>jv</cell><cell>ka</cell></row><row><cell></cell><cell>XLM-R</cell><cell cols="20">77.7 47.1 81.9 74.9 78.6 76.3 81.6 74.7 77.2 61.2 58.2 78.3 78.3 50.2 68.7 80.6 53.7 80.8 15.6 56.2 61.4</cell></row><row><cell></cell><cell>ANLI +</cell><cell cols="20">75.4 52.7 78.1 72.7 76.4 76.3 80.9 71.6 72.8 52.2 60.7 75.8 77.4 49.1 69.6 79.6 52.7 78.9 13.1 54.3 62.1</cell></row><row><cell>Without MLM</cell><cell cols="21">MNLI QQP SQuADv2.0 76.0 48.0 81.1 71.8 78.4 78.2 84.3 74.7 78.4 53.9 56.9 78.9 82.5 56.0 68.9 79.8 56.4 80.8 18.1 61.8 67.3 76.9 48.3 80.5 72.8 77.7 77.9 84.2 76.9 78.5 62.1 58.3 78.7 81.1 55.1 69.0 81.1 55.7 80.8 16.4 54.2 68.1 73.8 40.9 75.5 66.0 71.3 71.6 75.8 65.5 69.3 55.5 49.9 73.1 72.8 42.6 59.8 74.3 49.2 75.9 5.7 54.4 51.1 SQuADv1.1 79.1 52.6 80.1 75.5 77.8 78.1 80.8 75.3 76.7 54.3 61.9 78.7 78.4 52.8 65.6 80.3 54.6 80.8 18.7 52.1 62.4 HellaSwag 77.0 54.9 82.7 76.6 79.1 78.9 84.3 77.8 78.0 58.8 65.0 77.5 80.3 57.0 71.2 81.8 54.3 81.4 19.6 56.9 70.6 CCG 77.4 51.5 78.7 72.5 78.4 76.2 80.8 73.0 78.0 56.9 62.1 78.2 77.3 48.6 67.3 79.7 54.9 79.9 15.9 60.3 58.9 Cosmos QA 76.6 49.3 79.2 76.0 77.8 76.1 81.2 73.2 76.6 59.8 55.8 77.8 77.0 46.8 67.8 79.4 53.2 80.0 14.1 55.5 57.8</cell></row><row><cell></cell><cell>CSQA</cell><cell cols="20">77.6 46.1 78.9 75.4 78.4 76.2 81.3 77.3 75.2 59.8 61.9 78.0 78.2 48.9 67.6 79.6 55.6 80.1 11.6 53.8 57.7</cell></row><row><cell></cell><cell>Multi-task</cell><cell cols="20">78.5 49.2 82.0 73.3 78.9 80.1 84.5 76.6 78.5 59.4 49.4 79.1 81.2 56.4 70.6 81.0 57.0 80.7 20.7 64.7 68.6</cell></row><row><cell></cell><cell>ANLI +</cell><cell cols="20">76.4 51.5 80.7 73.3 79.2 77.8 84.3 75.4 78.0 57.7 49.7 77.6 80.1 54.8 68.9 80.8 54.8 80.5 14.4 54.9 64.5</cell></row><row><cell>With MLM</cell><cell cols="21">MNLI QQP SQuADv2.0 78.0 46.5 82.8 71.7 79.0 77.3 84.2 74.8 79.0 61.6 63.3 79.5 80.0 57.6 67.5 81.9 62.0 80.7 20.0 62.3 68.2 78.0 52.3 81.7 73.0 79.6 78.1 84.4 77.2 79.4 59.6 60.6 79.2 81.4 55.1 68.6 81.0 51.3 81.0 14.0 62.0 64.3 77.1 46.7 79.0 72.9 79.4 76.3 81.9 74.2 78.7 61.8 66.0 78.3 78.0 50.4 69.1 81.6 53.2 80.1 15.1 62.6 60.7 SQuADv1.1 77.7 58.0 81.4 75.2 78.0 77.4 82.1 69.6 76.1 54.1 58.4 77.5 78.7 54.8 67.5 78.8 49.9 79.5 14.5 55.9 68.3 HellaSwag 78.7 47.0 81.8 73.8 79.7 78.2 84.8 73.6 79.2 55.8 55.6 78.2 79.4 55.0 69.8 81.3 54.1 81.3 18.5 58.1 67.5 CCG 74.5 46.4 76.7 74.5 76.9 75.7 80.5 72.6 77.7 58.9 59.6 77.7 77.0 48.1 66.3 80.1 53.4 78.7 13.8 57.1 58.2</cell></row><row><cell></cell><cell cols="21">Cosmos QA 78.2 39.1 80.0 73.8 79.0 77.2 81.4 70.3 78.8 65.4 48.9 78.7 77.7 48.3 68.0 80.8 55.1 81.2 13.2 58.9 59.0</cell></row><row><cell></cell><cell>CSQA</cell><cell cols="20">77.4 48.8 78.9 73.9 78.8 76.3 81.9 75.2 79.5 66.7 58.6 79.6 78.5 47.7 68.2 81.0 55.3 81.3 12.2 60.4 58.9</cell></row><row><cell></cell><cell></cell><cell>kk</cell><cell>ko</cell><cell>ml</cell><cell>mr</cell><cell>ms</cell><cell>my</cell><cell>nl</cell><cell>pt</cell><cell>ru</cell><cell>sw</cell><cell>ta</cell><cell>te</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>yo</cell><cell>zh</cell><cell>Avg</cell></row><row><cell></cell><cell>XLM-R</cell><cell cols="12">48.7 54.5 58.8 61.8 54.1 53.7 83.2 80.7 69.3 69.8 58.2 50.8</cell><cell>2.2</cell><cell cols="7">73.2 81.1 67.0 74.9 33.2 23.6 62.8</cell><cell>-</cell></row><row><cell></cell><cell>ANLI +</cell><cell cols="12">50.2 52.6 61.2 63.0 66.8 46.5 81.8 78.7 67.0 66.9 55.0 52.1</cell><cell>2.5</cell><cell cols="7">71.2 78.0 67.3 73.9 43.3 18.9 62.0</cell><cell>-</cell></row><row><cell>Without MLM</cell><cell cols="13">MNLI QQP SQuADv2.0 49.9 58.1 61.6 62.5 72.1 50.0 83.1 82.3 70.8 65.4 62.6 53.6 51.7 58.8 64.8 61.3 69.8 54.9 83.0 80.8 70.2 70.3 59.3 55.4 50.4 40.1 51.2 51.4 61.4 32.5 78.2 73.0 50.8 65.1 47.3 41.4 SQuADv1.1 51.8 57.1 61.7 59.8 50.4 52.2 83.3 80.8 69.8 69.2 58.3 49.5 HellaSwag 50.5 58.4 56.6 66.6 72.8 59.4 83.2 82.5 70.8 69.9 63.7 53.0 CCG 52.4 52.7 57.7 59.6 52.3 50.0 82.5 79.0 67.1 67.0 55.3 49.1 Cosmos QA 48.4 52.4 60.3 62.1 56.9 50.2 82.8 79.5 67.4 67.8 57.2 51.4</cell><cell>1.0 1.6 0.6 0.8 1.1 2.6 1.3</cell><cell cols="7">74.8 80.5 56.9 78.1 38.9 25.2 64.2 67.4 72.3 57.2 67.9 43.9 8.6 55.9 74.8 80.0 63.2 78.9 41.2 22.5 64.1 71.6 79.1 58.6 76.3 47.5 26.2 63.0 75.1 78.0 70.0 75.0 42.1 29.7 65.5 70.0 81.0 65.3 74.2 37.6 23.3 62.1 74.6 80.7 60.8 74.9 34.8 19.5 61.8</cell><cell>-------</cell></row><row><cell></cell><cell>CSQA</cell><cell cols="12">49.7 52.0 59.1 62.9 62.4 46.1 82.5 80.3 65.4 69.0 57.1 51.2</cell><cell>1.8</cell><cell cols="7">73.1 80.2 73.3 73.5 35.3 19.3 62.3</cell><cell>-</cell></row><row><cell></cell><cell>Multi-task</cell><cell cols="12">53.2 57.8 60.8 61.0 69.3 54.2 83.8 80.8 69.4 70.6 58.9 53.7</cell><cell>2.2</cell><cell cols="7">75.2 77.2 57.7 75.6 46.1 30.4 64.7</cell><cell>-</cell></row><row><cell></cell><cell>ANLI +</cell><cell cols="12">52.9 56.8 60.0 61.1 75.4 49.5 83.4 80.9 68.3 71.0 57.2 49.8</cell><cell>0.9</cell><cell cols="7">74.5 79.0 59.8 76.3 31.7 22.5 63.2</cell><cell>-</cell></row><row><cell>With MLM</cell><cell cols="13">MNLI QQP SQuADv2.0 52.1 60.8 65.1 63.2 54.7 54.8 83.4 80.9 71.6 72.6 63.0 54.1 54.7 57.5 63.5 63.3 66.3 49.6 83.4 81.1 70.3 72.2 57.0 53.5 49.9 54.5 63.3 64.6 54.7 49.0 82.9 78.9 68.7 70.9 58.0 50.7 SQuADv1.1 51.6 57.7 62.7 60.2 62.2 52.9 81.8 77.7 71.4 68.5 59.7 49.9 HellaSwag 53.6 58.9 62.5 63.2 72.4 54.7 82.8 80.9 71.3 70.6 59.5 52.0 CCG 54.6 53.5 60.6 62.8 69.1 41.6 80.7 78.1 65.4 68.1 55.1 51.6</cell><cell>1.1 1.1 0.4 1.5 2.4 1.3</cell><cell cols="7">74.1 80.9 61.1 75.1 43.4 22.8 64.3 74.0 82.3 70.2 77.1 40.3 24.9 63.5 75.3 80.4 59.8 77.6 33.6 28.0 64.7 72.9 78.1 54.2 71.5 34.3 22.4 62.6 73.6 80.1 58.4 78.3 36.8 24.9 64.2 68.7 79.8 61.9 68.8 37.9 19.8 61.6</cell><cell>------</cell></row><row><cell></cell><cell cols="13">Cosmos QA 49.7 52.5 55.7 60.2 52.1 48.1 82.9 78.9 67.1 66.6 55.3 47.7</cell><cell>0.9</cell><cell cols="7">74.7 80.8 59.5 74.0 34.9 19.3 61.3</cell><cell>-</cell></row><row><cell></cell><cell>CSQA</cell><cell cols="12">52.2 54.4 60.4 61.1 52.9 47.8 83.4 80.7 68.5 69.0 57.9 50.1</cell><cell>1.4</cell><cell cols="7">73.6 81.5 63.2 74.0 43.6 19.3 62.9</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9</head><label>9</label><figDesc>/ 56.0 79.5 / 62.0 78.6 / 60.6 86.7 / 75.5 81.5 / 63.6 72.7 / 56.2 79.2 / 61.8 71.0 / 56.8 75.0 / 59.1 78.6 / 58.9 68.8 / 57.6 76.9 / 60.7 SQuADv1.1 75.9 / 59.9 80.3 / 63.6 80.3 / 62.1 88.3 / 77.4 81.8 / 63.2 76.1 / 59.2 80.0 / 64.1 75.6 / 65.5 75.8 / 59.2 80.5 / 61.2 70.8 / 61.3 78.7 / 63.3 / 53.2 77.4 / 60.5 75.7 / 56.9 84.8 / 72.9 79.3 / 60.1 73.1 / 55.8 75.8 / 57.1 70.3 / 58.3 71.7 / 55.6 77.2 / 57.0 66.9 / 57.4 74.9 / 58.6 Cosmos QA 72.5 / 53.9 77.2 / 61.2 76.9 / 59.1 85.1 / 72.9 79.2 / 60.6 73.4 / 57.5 76.4 / 57.7 72.0 / 61.7 72.1 / 55.1 77.4 / 57.6 68.6 / 59.0 75.5 / 59.6 CSQA 73.0 / 54.0 77.6 / 60.7 77.4 / 58.7 86.2 / 74.5 80.3 / 61.1 73.1 / 57.3 77.8 / 59.9 71.4 / 59.6 72.1 / 55.0 77.9 / 58.7 71.2 / 60.7 76.2 / 60.0</figDesc><table><row><cell>: Full NER Results</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc></figDesc><table /><note>Full XQuAD Results</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13</head><label>13</label><figDesc>20.4 39.0 13.3 63.9 18.9 48.0 25.8 19.9 42.0 41.5 48.1 28.0 38.3 42.5 47.0 42.3 41.8 10.2 Without MLMANLI + 78.8 74.0 88.0 72.3 97.4 82.4 91.2 70.9 53.3 91.5 88.6 89.8 82.1 92.8 86.2 92.1 82.6 88.7 31.7 MNLI 79.6 70.7 84.8 71.2 96.6 82.5 93.1 74.3 59.2 90.0 89.0 89.6 81.8 91.7 86.0 91.7 86.3 89.5 30.7 QQP 80.4 74.9 87.3 74.3 96.5 84.1 93.8 74.7 60.2 91.0 90.3 89.9 86.0 93.3 88.4 92.1 86.3 89.9 35.6 SQuADv2.0 73.7 67.7 84.2 63.2 96.0 74.3 89.2 70.5 54.0 87.9 85.5 87.1 77.1 88.0 83.5 89.5 80.2 86.4 32.2 SQuADv1.1 76.9 68.9 85.7 65.7 96.4 76.3 89.5 76.9 58.4 88.0 88.5 88.5 77.3 89.9 84.0 90.4 83.0 88.7 30.2 HellaSwag 78.9 75.4 89.9 75.4 97.7 84.8 93.1 79.8 64.8 91.8 92.0 92.2 84.9 93.4 89.5 92.1 86.7 91.6 37.1 CCG 71.9 59.1 82.1 62.5 95.5 74.4 87.0 67.3 49.0 84.7 82.6 84.4 77.2 85.4 80.7 87.2 79.1 78.7 24.9 Cosmos QA 78.6 70.6 86.6 71.0 96.4 80.5 91.8 77.6 60.7 89.8 91.3 89.4 83.0 91.5 87.7 91.4 83.7 88.2 37.1 CSQA 79.5 74.5 87.7 74.0 96.9 83.6 92.9 79.1 65.8 90.0 92.0 90.7 83.1 92.2 88.4 91.8 85.4 88.9 33.7 Multi-task 81.2 71.9 88.0 73.6 97.1 82.9 92.6 73.1 58.6 90.4 89.6 89.6 84.1 92.6 87.2 92.6 83.9 91.0 34.1 With MLM ANLI + 78.6 65.2 86.6 67.8 97.0 78.2 90.2 79.1 59.3 89.3 89.1 90.4 78.7 89.3 86.5 91.0 84.6 87.0 26.3 MNLI 77.3 65.2 83.8 64.9 97.2 76.1 92.1 77.7 57.3 88.1 88.8 87.5 81.0 89.0 87.1 90.5 82.6 85.6 27.3 QQP 74.4 61.3 83.7 64.6 96.2 75.7 88.1 76.7 59.4 86.3 87.0 86.9 76.6 85.9 84.2 89.8 79.8 84.0 28.8 SQuADv2.0 70.8 57.6 80.9 52.7 96.6 63.4 84.5 71.5 47.4 85.4 86.9 85.1 71.9 85.2 83.9 90.4 78.1 83.2 16.1 SQuADv1.1 79.2 67.7 86.5 71.4 96.7 80.4 91.6 83.1 66.3 90.8 91.1 89.8 77.5 92.3 87.4 91.8 84.6 87.4 26.3 HellaSwag 57.1 45.2 69.4 40.4 89.7 57.8 73.4 64.0 42.2 77.1 76.4 76.5 62.6 75.1 76.2 82.5 69.7 77.5 22.0 CCG 71.9 52.3 80.4 51.0 95.0 72.6 86.0 73.5 51.0 83.3 84.1 81.8 71.3 79.1 81.6 87.2 78.7 76.2 12.7 Cosmos QA 69.7 63.7 84.0 58.8 95.1 74.2 84.6 76.5 58.6 85.7 85.2 84.5 76.2 87.1 84.7 88.5 81.4 85.5 24.9 CSQA 54.3 45.3 63.6 33.5 87.0 50.5 70.0 58.8 35.7 74.1 71.0 70.7 58.2 70.2 72.5 80.4 64.2 75.5 16.6 67.3 84.6 90.8 80.5 93.6 91.0 90.5 30.8 76.5 85.5 91.2 59.9 87.9 79.7 94.6 93.0 80.8 -MNLI 77.9 67.7 84.3 89.8 80.4 92.5 91.3 89.2 32.8 70.0 78.2 86.7 60.9 88.8 74.5 92.5 91.2 80.2 -QQP 78.7 69.4 86.4 92.9 82.9 93.3 92.5 91.6 35.1 81.4 90.6 90.0 64.6 91.4 81.7 95.0 92.3 82.7 -SQuADv2.0 67.0 63.0 80.8 82.8 71.6 89.7 90.4 86.9 27.7 60.9 74.4 80.7 54.2 85.9 70.6 92.5 89.3 76.1 -SQuADv1.1 70.9 63.7 83.3 87.3 74.7 91.7 90.2 89.1 31.5 60.6 77.8 82.3 59.3 88.3 68.3 92.8 90.8 77.9 -HellaSwag 80.8 72.0 86.5 92.1 81.1 93.2 91.9 92.0 35.1 79.2 87.2 89.6 64.5 90.6 82.4 95.1 92.6 83.3 -CCG 65.1 56.9 76.8 82.5 70.3 88.9 88.8 84.5 24.9 60.3 65.4 72.8 53.3 82.6 64.7 89.7 84.8 72.9 -Cosmos QA 75.7 69.9 83.6 90.1 78.7 92.0 91.3 89.7 34.1 72.3 84.6 89.1 59.7 89.6 79.8 93.3 90.9 80.9 -CSQA 80.8 70.3 85.5 91.7 82.7 93.3 91.4 90.4 35.9 73.3 84.6 89.4 65.4 90.2 77.1 94.8 92.53.8 54.8 77.5 72.5 61.5 90.0 87.0 87.2 20.3 41.7 51.7 80.5 38.0 81.8 63.3 90.6 89.1 70.4 -SQuADv1.1 73.2 66.8 83.9 89.8 78.9 93.0 90.4 89.7 33.8 76.2 85.0 90.0 54.5 90.0 78.6 93.6 90.9 80.6 -HellaSwag 38.5 43.1 70.5 63.2 39.7 79.1 78.4 80.0 19.2 30.9 55.6 66.6 33.1 71.5 49.8 80.4 77.7 61.4 -CCG 58.3 51.3 74.6 76.3 58.4 89.0 86.9 82.9 23.3 46.9 60.3 72.6 40.9 82.5 55.8 87.9 80.3 69.4 -Cosmos QA 63.3 56.0 80.7 79.0 63.1 89.4 87.2 86.1 26.2 55.7 71.8 80.5 44.6 83.0 63.7 91.0 85.1 73.8 -CSQA 33.4 36.2 65.9 47.0 30.9 76.6 74.7 75.5 19.0 28.3 49.6 64.1 26.0 64.1 53.0 78.4 75.1 56.9 -</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">: Full BUCC Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>af</cell><cell>ar</cell><cell>bg</cell><cell>bn</cell><cell>de</cell><cell>el</cell><cell>es</cell><cell>et</cell><cell>eu</cell><cell>fa</cell><cell>fi</cell><cell>fr</cell><cell>he</cell><cell>hi</cell><cell>hu</cell><cell>id</cell><cell>it</cell><cell>ja</cell><cell>jv</cell></row><row><cell></cell><cell>XLM-R</cell><cell>30.5 ka</cell><cell>kk</cell><cell>ko</cell><cell>ml</cell><cell>mr</cell><cell>nl</cell><cell>pt</cell><cell>ru</cell><cell>sw</cell><cell>ta</cell><cell>te</cell><cell>th</cell><cell>tl</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>zh</cell><cell>Avg</cell></row><row><cell></cell><cell>XLM-R</cell><cell cols="8">11.8 17.4 35.5 19.4 15.2 52.6 47.2 42.1</cell><cell>7.9</cell><cell>9.1</cell><cell cols="8">19.7 27.4 10.3 37.8 22.5 38.3 41.2 31.0</cell><cell>-</cell></row><row><cell>Without MLM</cell><cell>ANLI +</cell><cell cols="18">76.9 9 82.2</cell><cell>-</cell></row><row><cell></cell><cell>Multi-task</cell><cell cols="18">78.7 68.2 85.0 91.4 80.4 92.1 92.0 90.2 34.4 68.7 83.8 89.1 62.3 88.9 77.6 95.0 92.8 81.2</cell><cell>-</cell></row><row><cell></cell><cell>ANLI +</cell><cell cols="18">70.6 64.7 83.6 88.9 75.6 92.0 91.0 88.1 29.0 70.0 76.9 84.7 51.6 88.0 71.7 93.6 91.6 78.5</cell><cell>-</cell></row><row><cell></cell><cell>MNLI</cell><cell cols="18">67.7 63.3 81.8 84.3 75.0 90.8 90.5 87.8 29.7 62.2 73.5 85.2 53.4 87.6 71.2 93.3 88.5 77.4</cell><cell>-</cell></row><row><cell>With MLM</cell><cell>QQP SQuADv2.0</cell><cell cols="18">66.0 64.2 80.2 82.0 70.6 89.4 89.8 86.7 30.5 60.9 76.1 83.6 52.3 84.9 72.7 90.5 88.0 76.0</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14 :</head><label>14</label><figDesc>Full Tatoeba Results</figDesc><table><row><cell></cell><cell cols="3">MNLI QQP HellaSwag</cell></row><row><cell>en</cell><cell>87.1</cell><cell>88.0</cell><cell>71.6</cell></row><row><cell>Translated to de</cell><cell>82.2</cell><cell>84.6</cell><cell>55.1</cell></row><row><cell>Translated to ru</cell><cell>70.1</cell><cell>83.8</cell><cell>27.4</cell></row><row><cell>Translated to sw</cell><cell>70.8</cell><cell>79.3</cell><cell>25.1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">If a word is tokenized into sub-word tokens, we use the representation of the first token for the tag prediction for that word as in<ref type="bibr" target="#b12">Devlin et al. (2019a)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/google-research/ xtreme 6 According to<ref type="bibr" target="#b11">Conneau et al. (2018)</ref>, these data are translated using a Facebook internal machine translation system.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">For QQP, on 2 of the 3 random seeds the NER model performed extremely poorly, leading to the large negative transfer of -45.4.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cross-lingual parser selection for low-resource languages</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)</title>
		<meeting>the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multilingual projection for parsing truly low-resource languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeljko</forename><surname>Agi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Johannsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalie</forename><surname>H?ctor Mart?nez Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Schluter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S?gaard</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00100</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Massively multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1388</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3874" to="3884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Translation artifacts in cross-lingual transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04721</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4623" to="4637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="597" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Palomaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11997</idno>
		<title level="m">Livio Baldini Soares, and Emily Pitler. 2020. Collecting entailment data for pretraining: New protocols and negative results</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Nikolaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32 (NeurIPS)</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7059" to="7069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">XNLI: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05166</idno>
		<title level="m">FILTER: An Enhanced Fusion Method for Cross-lingual Language Understanding. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CCGbank: A corpus of CCG derivations and dependency structures extracted from the Penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Steedman</surname></persName>
		</author>
		<idno type="DOI">10.1162/coli.2007.33.3.355</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="355" to="396" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cosmos QA: Machine reading comprehension with contextual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1243</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2391" to="2401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bootstrapping parsers via syntactic projection across parallel texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Weinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Okan</forename><surname>Kolak</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324905003840</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Eng</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">311325</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Google&apos;s multilingual neural machine translation system: Enabling zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernanda</forename><surname>Vi?gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macduff</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00065</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="339" to="351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Marian: Fast neural machine translation in C++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Grundkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Dwojak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Heafield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Neckermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Germann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alham</forename><surname>Fikri Aji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Bogoychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Andr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="121" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1611835114</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Overcoming catastrophic forgetting in neural networks</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adversarial filters of dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating cross-lingual extractive question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7315" to="7330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-task deep neural networks for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1441</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4487" to="4496" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cheap translation for cross-lingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Tse</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1269</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2545" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-source transfer of delexicalized dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="62" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial NLI: A new benchmark for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Universal Dependencies 2.2. LINDAT/CLARIN digital library at the Institute of Formal and Applied Linguistics (?FAL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?eljko</forename><surname>Agi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ahrenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lene</forename><surname>Antonsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><forename type="middle">Jesus</forename><surname>Aranzabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gashaw</forename><surname>Arutie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luma</forename><surname>Ateyah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Attia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Faculty of Mathematics and Physics, Charles University</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Crosslingual name tagging and linking for 282 languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoman</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1178</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1946" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos; Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MAD-X: An adapter-based framework for multi-task cross-lingual transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<title level="m">Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks. Unpublished manuscript available on arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">2020. jiant 2.0: A software toolkit for research on general-purpose text understanding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Yeres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">F</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><surname>Phu Mon Htut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghu</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Thomas</forename><surname>Shuning Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roma</forename><surname>Mccoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghui</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najoung</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berlin</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhad</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Mohananey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikha</forename><surname>Kann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Bordia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowman</surname></persName>
		</author>
		<ptr target="http://jiant.info/.ardeeneshivizhAvgXLM-R62" />
		<imprint/>
	</monogr>
	<note>7 / 42.4 69.1 / 52.0 81.6 / 68.6 72.2 / 53.0 68.0 / 50.7 69.5 / 47.6 67.9 / 46.2 70.1 / 51.5</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Cosmos QA</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Full MLQA Results ar bn en fi id ko ru sw te Avg XLM</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
	<note>R 64.5 / 46.9 59.5 / 41.6 70.4 / 56.6 64.9 / 49.2 75.1 / 59.8 54.7 / 39.5 65.4 / 43.6 67.2 / 48.7 68.8 / 48.3 65.6 / 48.2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
