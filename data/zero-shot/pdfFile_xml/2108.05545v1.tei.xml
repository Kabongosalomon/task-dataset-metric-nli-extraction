<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Hwan</forename><surname>Ko</surname></persName>
							<email>jhko@skku.edu</email>
							<affiliation key="aff1">
								<orgName type="department">College of Information and Communication Engineering</orgName>
								<orgName type="institution">Sungkyunkwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With increasing applications of 3D hand pose estimation in various human-computer interaction applications, convolution neural networks (CNNs) based estimation models have been actively explored. However, the existing models require complex architectures or redundant computational resources to trade with the acceptable accuracy. To tackle this limitation, this paper proposes HandFoldingNet, an accurate and efficient hand pose estimator that regresses the hand joint locations from the normalized 3D hand point cloud input. The proposed model utilizes a folding-based decoder that folds a given 2D hand skeleton into the corresponding joint coordinates. For higher estimation accuracy, folding is guided by multi-scale features, which include both global and joint-wise local features. Experimental results show that the proposed model outperforms the existing methods on three hand pose benchmark datasets with the lowest model parameter requirement. Code is available at https://github.com/cwc1260/HandFold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J?S?(3+ + ?3)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation</head><p>Folding Layers ?3 ? ?2 ?(3 + ) ?(3 + ) ?(3 + ) Set Abstract Levels 1 Set Abstract Levels 2 Set Abstract Levels 3 ? ? ?( + ) ?3 2D Hand Skeleton Super Link Folding Layers J?S?(3+ + ?3) Aggregation Folding Layers Global Feature Replicate ?3 Super Link</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D hand pose estimation aims to estimate joint locations from input hand images. Accurate and real-time estimation is critical in various human-computer interaction applications, especially in virtual reality and augmented reality <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>. Recently, many studies achieved impressive progress by utilizing hand depth images from depth cameras. However, it still remains challenging to achieve accurate and real-time estimation, due to various issues such as self-occlusion, noise, high dimensionality, and various orientations of a hand <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>With the advancement of deep neural networks (DNNs), various DNN-based hand pose estimation techniques achieved powerful performances. In most of these techniques, 2D convolution neural networks (CNNs) have been * Jong Hwan Ko is the corresponding author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Point cloud</head><p>3D pose 2D hand skeleton "force" "fold" <ref type="figure">Figure 1</ref>. Illustration of the folding concept. The network can be interpreted as emulating the "force" through multi-scale features extracted from the point cloud. The "force" will drive a 2D hand skeleton to "fold" into the 3D joint coordinates representing the hand pose.</p><p>adopted to perform direct hand depth image processing <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3]</ref>. However, 2D CNNs cannot fully take advantage of 3D spatial information of the depth image, which is essential for achieving high accuracy. An intuitive solution is to discretize hand depth images into a 3D voxelized representation and perform 3D-to-3D inference using a 3D CNN <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref>. However, its critical limitation is the cubic growth of memory consumption with an increase in the image resolution <ref type="bibr" target="#b30">[31]</ref>. Thus, application of 3D CNNs has been limited to low-resolution images, which may lead to lose of critical details for estimation. In contrast, the point cloud is being regarded as an efficient and precise representation for 3D hand pose estimation, as it models hand depth images into the continuous 3D coordinates without discretization. However, the point cloud could not be directly processed by conventional DNNs due to the irregular order of points, until the emergence of PointNet <ref type="bibr" target="#b27">[28]</ref>. With a concise symmetric architecture composed of a point-wise shared-weights multi-layer perceptron (MLP) and a max-pooling layer, PointNet is invariant with the order of the input points.</p><p>Based on this architecture, a series of PointNet-based hand pose estimation models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref> have been pro-arXiv:2108.05545v1 [cs.CV] 12 Aug 2021 <ref type="figure">Figure 2</ref>. The HandFoldingNet architecture. It takes the preprocessed normalized point cloud with surface normal vectors from a 2D depth image as an input. The hierarchical PointNet encoder is then exploited to extract features of various levels to summarize a global feature from the input point cloud. The global folding decoder receives the global feature to guide the folding of a pre-defined 2D hand skeleton into the initial joint coordinates. In the end, the local features near the initial joint coordinates are grouped and fed into the local folding blocks to estimate the accurate joint coordinates. posed. They can be summarized into two categories: 1) regression-based methods and 2) detection-based methods. Regression-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b3">4]</ref> encode the hand shape into a single global feature through a PointNet-based feature extractor. The global feature representing the hand pose in the high dimensional latent space is fed into a non-linear regression network that performs inference of the joint coordinates. On the other hand, detection-based methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b20">21]</ref> adopt hierarchical features to compute heat-map features for each point. The point-wise features represent the possibility distribution of each joint. However, the existing regression-based and detection-based strategies have limitations. The regression-based methods process only a single global feature, which is not sufficient for highly complex mapping into 3D hand poses. On the other hand, the detection-based methods propagate hierarchical features to each point including the points that contribute little to the specific joint estimation. Therefore, this redundant feature propagation significantly increases the computational cost and slows down the estimation.</p><p>To tackle these limitations, we propose HandFoldingNet, an accurate and efficient 3D hand pose estimation network.</p><p>The key idea of HandFoldingNet is to fold a 2D hand skeleton into the 3D pose, guided by multi-scale features extracted from both global and local information. The motivation of adopting the folding-based design in FoldingNet <ref type="bibr" target="#b44">[45]</ref> is that it is suitable for a 3D hand pose estimation task. Essentially, a specific hand pose is a result of applying a force on the human hand skeleton. The folding operation can be interpreted as emulating the "force" applied to the fixed 2D hand skeleton, as shown in <ref type="figure">Figure 1</ref>. In order to guide folding, HandFoldingNet introduces two novel modules that handle different scales of features: 1) a global-feature guided folding (global folding) decoder and 2) a joint-wise local-feature guided folding (local folding) block. Inspired by FoldingNet, a global folding decoder folds a 2D hand skeleton into the 3D hand joint coordi-nates. The global feature that guides folding is extracted from the input hand point cloud by a PointNet-based encoder <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12]</ref>. The local folding block utilizes local features as well as spatial dependencies between the joints, in order to augment joint-wise features and correct the coordinate estimation. Utilization of local features is supposed to compensate for the weakness of conventional regressionbased methods. Additionally, unlike the detection-based methods that propagate local features to all the points, we only extract a small region of local features near each joint, in order to avoid massive computations.</p><p>We evaluate our network on ICVL <ref type="bibr" target="#b35">[36]</ref>, MSRA <ref type="bibr" target="#b34">[35]</ref> and NYU <ref type="bibr" target="#b39">[40]</ref> datasets, which are challenging benchmarks commonly used for evaluation of a 3D hand pose estimation task. The results show that our network generally outperforms the previous state-of-the-art methods in terms of both accuracy and efficiency. The proposed network achieves the mean distance errors of 5.95mm, 7.34mm and 8.58mm on the ICVL, MSRA and NYU datasets, respectively. Meanwhile, it contains only 1.28M parameters and runs in realtime with 84 frames per second on a single GPU.</p><p>The key contributions of this paper are as follows:</p><p>? We propose a novel neural network, HandFoldingNet, which takes the hand point cloud as input and estimates the 3D hand joint coordinates based on the multiscalefeature guided folding. ? We propose a global-feature guided folding decoder that infers joint-wise features and coordinates. The joint-wise features help the model exploit natural spatial dependencies between the joints for better estimation performance. ? We propose joint-wise local-feature guided folding to capture local features and spatial dependencies that augments joint-wise features for higher accuracy. ? We conduct extensive experiments to analyse the efficiency and accuracy of our proposed network and its key components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Depth-based 3D Hand Pose Estimation</head><p>Traditional 3D hand pose estimation approaches based on depth images are mainly implemented in three categories: generative methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32]</ref>, discriminative methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>, and hybrid methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>. In recent years, DNN-based models showed superior performance on 3D hand pose estimation tasks. Representative 2D CNNs are commonly adopted to pose estimation in various implementations. A series of studies <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b9">10]</ref> exploited 2D CNNs in order to extract a 2D heat-map that represents the possibility distribution of hand joints from a depth image.</p><p>Another line of work proposed regression-based methods based on 2D CNNs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b2">3]</ref>, which act as feature extractors that provide efficient features for joint coordinates regression. Instead of processing in the 2D space, several approaches <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> encoded 2D depth images into 3D voxels and adopted 3D CNNs to estimate the 3D hand pose. As depth images can be easily transformed into the point cloud by multiplying the camera intrinsic matrix, several point cloud based models <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b20">21]</ref> have been proposed. They showed acceptable efficiency and performance by directly processing the input coordinates to estimate the joint coordinates in the identical 3D space.</p><p>HandFoldingNet is inspired by these point cloud based methods, but it differs from them in the following aspects. The proposed network does not directly regress the hand joint coordinates nor estimate the point-wise probability distribution. Instead, it first regresses the initial joint coordinates for grouping local features. Meanwhile, it also provides joint-wise features for modeling spatial dependencies. In the end, the network aggregates these local features and spatial dependencies to estimate the accurate joint coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Point Cloud Reconstruction</head><p>Deep point cloud reconstruction aims to reconstruct the point cloud based on the features extracted from images, point clouds, or other types of data. An intuitive way of achieving the point cloud reconstruction is to adopt 3D CNNs, as in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>. However, these approaches reconstruct the voxelized representation of the point cloud. Instead of CNN-based methods, other approaches <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b4">5]</ref> proposed direct reconstruction of the point cloud.</p><p>Theoretically, our main task, estimating hand joint coordinates for a given hand point cloud, can be transformed into the point cloud reconstruction task, because the estimated joint coordinates can be treated as a small set of points that need to be reconstructed. Therefore, we inherit the idea of FoldingNet <ref type="bibr" target="#b44">[45]</ref> to reconstruct the joint point cloud. FoldingNet proposed a novel folding operation im-plemented by a sequence of shared-weights MLPs. This folding operation can be intuitively interpreted as learning the "force" to fold a given 2D grid lattice into the target point cloud. There are two critical differences between our network and FoldingNet: 1) we introduce folding of a 2D hand skeleton instead of a regular grid lattice in order to adapt it to the hand pose estimation task, 2) we exploit multi-scale features for higher estimation accuracy, unlike FoldingNet that processes only a single global feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HandFoldingNet</head><p>HandFoldingNet aims to perform hand pose estimation using 2D hand joint skeleton folding. The network architecture is shown in <ref type="figure">Figure 2</ref>. It takes an N ? 6 matrix (P nor , F nor ), which represents a set of normalized points, as an input. Each row of the input matrix is composed of a normalized 3D xyz coordinate p nor i ? P nor and the corresponding 3D surface normal vector f nor i ? F nor . The output is a J ? 3 matrix, representing the 3D coordinates of estimated J joints. The N points are firstly input to the hierarchical PointNet encoder that extracts local features of various levels and a single global feature. Then the global feature is fed into the global-feature guided folding decoder and guides folding of the fixed 2D hand skeleton into the 3D joint coordinates. In order to augment the estimation performance, the output from the global folding decoder and local features near them are processed by joint-wise local-feature based folding blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point Cloud Preprocessing</head><p>First, the 2D depth image is converted into a point cloud by reprojecting the pixels in the 3D space, forming the model input (P nor , F nor ). We follow the point cloud preprocessing method described in HandPointNet <ref type="bibr" target="#b8">[9]</ref>. The input depth images are first transformed into point cloud representations through camera intrinsic parameters, to adapt to our point cloud based network. Then, in order to deal with various hand orientations, an oriented bounding box (OBB) is created from the 3D point cloud. After that, the point cloud is rotated into the OBB coordinate system, whose axes are aligned with the principle components of the hand points distribution. The oriented points are subsampled and normalized into the range of [-0.5, 0.5] to form the final input coordinates P nor . In the end, point-wise surface normal vectors F nor are calculated from the normalized point cloud. Please refer <ref type="bibr" target="#b8">[9]</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Hierarchical PointNet Encoder</head><p>We exploit the same hierarchical PointNet encoder as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> to extract features from the unordered point cloud. As shown in <ref type="figure">Figure 2</ref>, the encoder consists of a cascade of L point set abstraction levels. The l-th level (l ? {1, 2, . . . , L}) takes N l?1 ?(3+C l?1 ) matrix from the  The joint coordinates are used as centroids that group local features from the local feature map. Folding embeddings are rearranged to be aligned with the corresponding adjacent joints to collect spatial dependencies. Ultimately, the aggregated feature map composed with grouped local features and rearranged embeddings is fed into a symmetric architecture to compute the residual with respect to the previously-estimated joint locations for more accurate joint estimation.</p><p>previous (l ? 1)-th level as an input, of which the i-th row is composed of a 3D coordinate p l?1 i and the corresponding feature f l?1 i . Then it outputs N l ? (3 + C l ) matrix, which is composed of N l of sub-sampled centroids p l i and their corresponding C l -dim local features f l i . Specifically, for the first level, the input coordinate is p nor i and the corresponding feature is a 3D surface normal vector f nor i . The N l centroids are randomly sampled from the input coordinates. Then, S neighbor points with their corresponding features around each centroid p l i are gathered as a local region {p l?1 s,i , f l?1 s,i } S s=1 by using the ball query <ref type="bibr" target="#b28">[29]</ref> within a specified radius r. The coordinates in the local region are then translated to the local frame relative to their centroid: p l?1 s,i ? p l i . For each local region, a symmetric PointNet <ref type="bibr" target="#b27">[28]</ref> with a 3-layer MLP is adopted to generate a C l -dim feature for each point in the region. Subsequently, a maxpooling operation aggregates these point-wise features into a single local feature representing the corresponding centroid. Therefore, the local feature of the j-th sub-sampled centroid in the l-th level is represented as:</p><formula xml:id="formula_0">f l i = M AX 1?s?S (h([p l?1 s,i ? p l i , f l?1 s,i ])),<label>(1)</label></formula><p>where h is the MLP, MAX is the channel-wise max-pooling operation, and '[?]' is the concatenation operation. For the last level, it directly adopts the shared-weights MLP and max-pooling operation on the whole input (without sampling) in order to generate the single C g -dim global feature, which is represented as:</p><formula xml:id="formula_1">g = M AX 1?i?N L?1 (h([p L?1 i , f L?1 i ])).<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Global-Feature Guided Folding Decoder</head><p>The proposed decoder folds a fixed 2D hand skeleton into the 3D coordinates of joints, being guided by a global  feature. The hand skeleton is a set of hand joint coordinates in a 2D plane and is handcrafted by the following steps: 1) randomly choosing samples from the training set, 2) measuring the average length of links between each pair of adjacent ground truth joints from the samples, 3) unfolding links in a 2D plane, 4) collecting the coordinates of joints across every two connected links. An example of the 2D hand skeleton for the ICVL dataset is shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>After the hierarchical PointNet encoder extracts the global feature g, it is fed into the global folding decoder. Before inserting the global feature g, we replicate it J times and concatenate the replicated features with the fixed hand skeleton, whose size is J ? 2. The result of the concatenation is supplied to a 2-layer MLP that generates a highdimensional folding embedding e j for each joint. A subsequent 1-layer MLP predicts the initial 3D joint coordinates by processing input embeddings. Hence, the output coordinate j 0 j of the j-th joint is represented as:</p><formula xml:id="formula_2">j 0 j = h p (e j ) where e j = h e ([skel j , g]),<label>(3)</label></formula><p>where h p and h e denote the MLPs, e j denotes the intermediate folding embedding, and skel j denotes the j-th point of 2D coordinate of the fixed skeleton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Joint-Wise Local-Feature Guided Folding Block</head><p>Using only a single global feature (i.e. global-feature guided folding and other regression-based methods) is not sufficient to accurately estimate the joint coordinates . We believe that the use of additional joint-wise local features encourages the network to correct the joint coordinates.Therefore, we propose a novel joint-wise local-feature guided folding block for capturing local features and spatial dependencies that help better estimation.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, the output coordinates from the (k ? 1)-th folding block are firstly used as centroids for the current k-th local folding block. The J centroids group J local regions from the output of the l-th set abstraction level within radius r. From each region, S neighbors are sampled, each of which is composed of a 3D local coordinate p l s,j ? j k?1 j and a C l -dim corresponding local feature f l s,j , where 1 ? s ? S. Therefore, the output size of this grouping is J ? S ? (3 + C l ). Note that l is set to 1 as default, while the selection of l will be discussed in Section 4.4.</p><p>In addition, we introduce a rearrangement process that explicitly models spatial dependencies. It is worth mentioning that, the feature of a specific joint is represented by the corresponding row of the folding embeddings from the global folding decoder. Similarly, the local folding block provides joint-wise folding embeddings as well, enabling the network to stack more local folding blocks for accurate estimation. The rearrangement process first permutes the folding embeddings in order to form rearranged embeddings, which match the spatial dependency mapping as shown in <ref type="figure">Figure 5</ref>. The j-th row of each rearranged embedding is the folding embedding of the adjacent joints of the j-th joint. Then, we form the spatial dependency feature map by concatenating rearranged embeddings with the input folding embeddings. In the dependency mapping, as shown in <ref type="figure">Figure 5</ref>, each joint links with the other two adjacent joints. Therefore, this rearrangement process takes the folding embeddings of size J ? C f and outputs a spatial dependency map with size J ? (C f + C f + C f ). Specifically, since the fingertips only have one adjacent joint, we concatenate them with themselves to keep a uniform shape of the spatial dependency map. As shown in <ref type="figure">Figure 5</ref>, there are self-relations for fingertips. Moreover, we replicate the spatial dependency feature map S times to align the dimension with the previous grouping output before the following aggregation.</p><p>After local features and the spatial dependency feature map are prepared, we concatenate them together, to form an aggregated feature map. The aggregated feature map is then fed to aggregation folding layers with symmetric structure, as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. In this structure, we introduce a 3-layer MLP and a max-pooling, which aggregate the features into a single folding embedding for each joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? ? ?</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Folding Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rearranged Embeddings 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rearranged Embeddings 1</head><p>For joint 1</p><p>For joint j For joint J <ref type="figure">Figure 5</ref>. The spatial dependency mapping between hand joints of the ICVL dataset (left). Each joint permutes its embedding ej to map with its two adjacent joints along the mapping direction of the arrows forming two rearranged embeddings e 1 j and e 2 j (right). Exceptionally, fingertips are forced to map with themselves (red dotted arrows) to keep consistency. Subsequently, we introduce another 3-layer MLP that maps the high-dimensional embedding into the 3D coordinates. Intuitively, since each joint focuses on its individual local region, only a relative displacement can be effectively computed by this MLP-MAX-MLP structure. Therefore, we inherit the residual block design <ref type="bibr" target="#b14">[15]</ref>. The final joint coordinates are calculated by adding relative displacement outputs with the previously predicted coordinates. Hence, the j-th estimated joint of the k-th block is represented as:</p><formula xml:id="formula_3">j k j = h r (M AX 1?s?S (h f ([p l s,j ?j k?1 j , f l s,j , e k?1 j ])))+j k?1 j ,<label>(4)</label></formula><p>where h r and h f denote the shared-weights MLPs. j k?1 j indicates the j-th output joint coordinate of the previous global folding decoder or local folding block. p l s,j and f l s,j are the s-th neighbor coordinate and feature of the j-th joint where l denotes the l-th set abstraction level. e k?1 j indicates the concatenation of the j-th row of the folding embeddings and its two adjacent joints embeddings from the previous global folding decoder or the local folding block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Loss Function</head><p>As our loss function, we adopt smooth L1 loss, which is less sensitive to outliers than L2 loss. The smooth L1 loss is defined as</p><formula xml:id="formula_4">L1 smooth (x) = 0.5|x|, |x| &lt; 0.01 |x| ? 0.005, otherwise .<label>(5)</label></formula><p>Since the global folding and local folding blocks of our network output their respective estimated coordinates, we supervise all outputs by the following joint loss function:</p><formula xml:id="formula_5">L = J j=1 L1 smooth (j 0 j ? j * j ) + K k=1 J j=1 L1 smooth (j k j ? j * ),<label>(6)</label></formula><p>where j * j indicates the ground-truth coordinate of the j-th joint, and K indicates the quantity of stacked local folding blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experiment Settings</head><p>We conducted experiments on an NVIDIA TITAN RTX GPU with PyTorch. For training, we used the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with beta1 = 0.5, beta2 = 0.999, and learning rate ? = 0.001. The number of input points to the network was preprocessed to 1,024 and the batch size was set to 32. The network implementation details are shown in <ref type="table" target="#tab_1">Table 1</ref>. Batch normalization <ref type="bibr" target="#b15">[16]</ref> and the ReLU <ref type="bibr" target="#b24">[25]</ref> activation function are adopted in all MLP layers except the layers that output coordinates and residuals. Meanwhile, to avoid overfitting, we adopted online data augmentation with random rotation ([-37.5, 37.5] degrees around z-axis), 3D scaling ([0.9, 1.1]), and 3D translation ([-10, 10]mm). We evaluated the performance of the proposed model using public hand pose datasets, the ICVL <ref type="bibr" target="#b35">[36]</ref>, MSRA <ref type="bibr" target="#b34">[35]</ref> and NYU <ref type="bibr" target="#b39">[40]</ref> datasets. We trained the model for 400 epochs on ICVL, 200 epochs on NYU and 80 epochs (with a learning rate decay of 0.1 after 60 epochs) on MSRA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Evaluation Metrics</head><p>MSRA Dataset. The MSRA dataset <ref type="bibr" target="#b34">[35]</ref> provides more than 76K frames from 9 subjects. Each subject contains 17 hand gestures. The ground truth of each frame contains J = 21 joints, including one joint for a wrist and four joints for each finger. Following the most recent work <ref type="bibr" target="#b34">[35]</ref>, we evaluate this dataset with the leave-one-subject-out crossvalidation strategy.</p><p>ICVL Dataset. The ICVL dataset <ref type="bibr" target="#b35">[36]</ref> is a commonlyused depth stream hand pose dataset that provides 22K and 1.6K depth frames for training and testing, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Mean error (mm) Input Type ICVL MSRA NYU DeepModel <ref type="bibr" target="#b45">[46]</ref> 11.56 -17.04 2D R DeepPrior <ref type="bibr" target="#b26">[27]</ref> 10. The ground truth of each frame contains J = 16 joints, including one joint for a palm and three joints for each finger. Since the frames also contain the human body area, we firstly crop the hand area from a depth image with the method proposed in <ref type="bibr" target="#b25">[26]</ref>, and take the output joint locations of the global folding decoder to segment the image of the hand area. NYU Dataset. The NYU dataset is captured from three different views. Each view contains 72K training 8K testing depth images captured with the Microsoft Kinect sensor. Following recent works, we only use one view and 14 joints out of total of 36 annotated joints for training and testing. We also follow the same hand area segmenting process as in the ICVL dataset.</p><p>Evaluation metrics. We evaluate the hand pose estimation performance with two commonly-used metrics: the mean distance error and the success rate. The mean distance error measures the average Euclidean distance between the estimated coordinates and ground-truth ones for all the joints over the entire testing set. The success rate is the fraction of the frames whose mean distance error is less than a certain distance threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts</head><p>We compare HandFoldingNet with other state-of-the-art methods, including methods with 2D (depth image) input: model-based method (DeepModel) <ref type="bibr" target="#b45">[46]</ref>, DeepPrior <ref type="bibr" target="#b26">[27]</ref>, improved DeepPrior (DeepPrior++) <ref type="bibr" target="#b25">[26]</ref>, region ensemble network , Ren-9x6x6 <ref type="bibr" target="#b41">[42]</ref>), Pose-Ren <ref type="bibr" target="#b2">[3]</ref>, dense regression network (DenseReg) <ref type="bibr" target="#b41">[42]</ref>, CrossInfoNet  [6] and JGR-P2O <ref type="bibr" target="#b7">[8]</ref>, and methods with 3D (point cloud or voxel) input: 3DCNN <ref type="bibr" target="#b10">[11]</ref>, SHPR-Net <ref type="bibr" target="#b3">[4]</ref>, HandPointNet <ref type="bibr" target="#b8">[9]</ref>, Point-to-Point <ref type="bibr" target="#b11">[12]</ref> and V2V <ref type="bibr" target="#b23">[24]</ref>. <ref type="figure" target="#fig_3">Figure 6</ref> shows the success rate on the ICVL, NYU, and MSRA dataset. The qualitative results are represented in <ref type="figure" target="#fig_4">Figure 7</ref>. <ref type="table">Table 2</ref> summarizes the performance based on the mean distance error on the three datasets. The results show that our method outperforms the existing methods on the ICVL dataset, achieving the mean distance error of 5.95mm. The proposed model also achieves the second-lowest error on the MSRA dataset and third-lowest error on the NYU dataset. Among methods using the 3D input, our method outperforms other state-of-the-art methods on both ICVL and MSRA datasets. Also, HandFoldingNet shows the state-of-the-art performance among regression-based methods on all three datasets. <ref type="figure" target="#fig_3">Figure 6</ref> represents that our method achieves the highest success rate when the error threshold is lower than 10mm, 13mm and 25mm on the ICVL, MSRA and NYU datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>We conduct ablation experiments evaluating the performance impact of each component in our model. The following experiments are evaluated based on the ICVL dataset. Effectiveness of the local folding block. This experiment evaluates the accuracy improvement by attaching the proposed local folding block. To compare with the proposed network having one global folding and two local folding blocks (triple fold), we introduce a shallow network (single fold) that only provides the global folding, a network with only one local folding block (double fold), and a network with three local folding blocks (quadra fold). <ref type="table">Table 3</ref> shows the performance comparison between the models with different number of local folding.</p><p>The result shows that local folding significantly reduces the distance error. This experiment proves that the global folding that only accepts a single global feature for estimation is relatively weak, and the local features contributes the correction of the final joint coordinates. Although attaching more local folding blocks increases the inference overhead, the number of parameters and operations of the proposed model (triple fold) are not significant compared to the existing models, as analyzed in Section 4.5. However, the result also shows that the model performance is saturated at triple fold. The reason is that the additional gradients from the third local fold corrupt the back propagation and make the training harder. Note that double fold still outperforms several point cloud based networks with smaller parameter size and operation count. Effectiveness of local features and spatial dependencies. We evaluate the contribution of the critical feature components of the aggregated feature map, which are the local feature and spatial dependency feature. We conduct two independent experiments: 1) without local feature and 2) without spatial dependency. For without local feature, we remove the grouped local feature component of the aggregated map and maintain the spatial dependency component. For Without spatial dependency, we remove rearranged folding embeddings and maintain the local feature. 1.55mm without the local features. Similarly, without the spatial dependency, the mean distance error increases by 0.40mm. These experiments show that the both features are critical for improving estimation accuracy. Meanwhile, the local feature contributes to the performance more efficiently, as it requires smaller parameters and FLOPs while achieving better performance than using the spatial dependency. Sampling level of local features. HandFoldingNet is composed of three set abstraction levels in the PointNet encoder, where each level has different input points density and feature complexity. Therefore, we should carefully determine the abstraction level so that the local folding blocks can effectively collect extra local features. To analyze the per-formance impact of the abstraction level, we experiment with the input, first, and second set abstraction levels as the input to the local folding blocks. <ref type="table">Table 5</ref> indicates that adopting the output point cloud from the first set abstraction level achieves the highest performance because the neighbor points around the joints are adequate (input points are dense) and the features they provide are effectively informed (input features are complex). On the other hand, the input point cloud is not complex enough as it only includes 3D surface normal vectors. Consequently, directly using the input point cloud for local folding is not effective in capturing necessary features that can improve the performance. Conversely, using higher abstraction level (sampling level 2) degrades the performance. Although the second level features are sufficiently complex, the points are actually sparse in the 3D space. Therefore, the local folding can not group enough points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Runtime and Model Size</head><p>The runtime of HandFoldingNet measured on an NVIDIA TITAN RTX GPU is 11.9ms per point frame in average, including 8.2ms for preprocessing and 3.7ms for network inference. Thus, it can run in real-time at about 84.0fps. <ref type="table">Table 6</ref> shows our method has the lowest total latency among the 3D-input based methods. Our method also achieves the fastest inference within the point cloud based methods that require 8.2ms of preprocessing time. Moreover, the number of parameters of our proposed network is sufficiently small, which is only 1.28M. Compared with previous state-of-the-art models, our model requires the least parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed HandFoldingNet, a novel and efficient neural network that takes the point cloud as the input and estimates the 3D hand pose. The proposed network achieves the accurate joint coordinates estimation by leveraging the multi-scale features, including the global feature and the joint-wise local feature. Experimental results on three challenging benchmarks showed that our network outperforms previous state-of-the-art methods while requiring the minimal computational resources. Ablation experiments demonstrated the contribution of its key components for better accuracy and efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Joint-wise local feature guided folding block. The local folding block accepts three inputs, which are the previously estimated joint coordinates, folding embeddings from intermediate layers of the previous folding block, and a local feature map extracted by the previous set abstraction level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>An example of a 2D hand skeleton based on the ICVL dataset. The skeleton contains J = 16 points, each of which is represented as a 2D coordinate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Comparison with the state-of-the-art methods using the ICVL (left), MSRA (middle) and NYU (right) dataset. The success rate is shown in thisfigure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results of HandFoldingNet on the ICVL (left), MSRA (middle) and NYU (right) dataset. Hand depth images are transformed into 3D points as shown in the figure. Ground truth is shown in black, and the estimated joint coordinates are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Implementation specifications. Each block contains four types of hyperparameters: search radius (r), the number of grouping neighbors (S), sampling centroids (N l ), and the number of output channels of each MLP layer. Max stands for the existence of a max-pooling layer at the end of the block. SA stands for the set abstraction level of PointNet encoder. The local folding blocks are divided into two parts at max-pooling for the clear representation.</figDesc><table><row><cell>Block type SA (l=1) SA (l=2) SA (l=3)</cell><cell cols="5">r 0.12 64 512 S N l MLP channels max ? 32, 32, 128 0.2 64 128 ? 64, 64, 256 -128 1 128, 128, 512 ?</cell></row><row><cell>global fold (k=0) local fold (k=1) local fold (k=2)</cell><cell>-0.4 -0.4 -</cell><cell>-64 -64 -</cell><cell>J J J J J</cell><cell>256, 256, 3 256, 256, 256 256, 256, 3 256, 256, 256 256, 256, 3</cell><cell>? ? ? ? ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 Table 3 .Table 6 .</head><label>436</label><figDesc>shows that the mean distance error increases by Comparison of different numbers of local folding blocks used in the model. # Local fold indicates the number of local folding blocks attached after the global folding decoder. # Params indicates the total number of parameters of the network. FLOPs indicates the total number of floating-point operations required for the network inference. Comparison of the model size and inference time for the methods using the 3D input. Speed stands for the frame rate (fps) on a single GPU. Time stands for the total computation time including preprocessing time and model inference time.</figDesc><table><row><cell cols="2">Global # Local fold fold ? ? ? 1 ? 2 ? 3</cell><cell cols="2">Mean error (mm) 8.13 6.34 5.95 6.08</cell><cell>#Params FLOPs 0.38M 0.46G 0.78M 0.78G 1.28M 1.10G 1.78M 1.48G</cell></row><row><cell cols="4">Local feature dependency error (mm) Spatial Mean ? ? 7.90 ? ? 6.35 ? ? 5.95</cell><cell># Params FLOPs 1.21M 1.04G 1.08M 0.91G 1.28M 1.10G</cell></row><row><cell cols="5">Table 4. Comparison of different settings between the local feature</cell></row><row><cell cols="2">and spatial dependency.</cell><cell></cell><cell></cell></row><row><cell cols="5">Sampling level Mean error (mm) #Params FLOPs</cell></row><row><cell>input</cell><cell></cell><cell cols="2">6.58</cell><cell>1.21M</cell><cell>1.04G</cell></row><row><cell>first (l=1)</cell><cell></cell><cell cols="2">5.95</cell><cell>1.28M</cell><cell>1.10G</cell></row><row><cell>second (l=2)</cell><cell></cell><cell cols="2">6.48</cell><cell>1.34M</cell><cell>1.17G</cell></row><row><cell cols="5">Table 5. Comparison of different set abstraction levels for local</cell></row><row><cell>features.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="4"># Param Speed Time (ms) GPU Type</cell></row><row><cell cols="4">V2V-PoseNet [24] 457.5M 3.5</cell><cell>23 + 5.5</cell><cell>TITAN X</cell></row><row><cell cols="3">HandPointNet [9] 2.58M</cell><cell cols="2">48 8.2 + 11.3 GTX1080</cell></row><row><cell cols="3">Point-to-Point [12] 4.3M</cell><cell cols="2">41.8 8.2 + 15.7 TITAN XP</cell></row><row><cell>Ours</cell><cell cols="2">1.28M</cell><cell>84</cell><cell>8.2 + 3.7 TITAN RTX</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panos</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Diamanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
	<note>Ioannis Mitliagkas, and Leonidas Guibas</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose guided structured region ensemble network for cascaded hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="page" from="138" to="149" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shpr-net: Deep semantic hand pose regression from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="43425" to="43439" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Point auto-encoder and its application to 2d-3d transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukhan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="66" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crossinfonet: Multi-task information sharing based hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="9896" to="9905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Vision-based hand pose estimation: A review. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xander</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Twombly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="52" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jgr-p2o: Joint graph reasoning based pixel-to-offset prediction network for 3d hand pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linpu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiong</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hand pointnet: 3d hand pose estimation using point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="8417" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust 3d hand pose estimation in single depth images: from single-view cnn to multiview cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Point-topoint regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a predictable and generative vector representation for objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Region ensemble network: Improving convolutional network for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengkai</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cairong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hand pose estimation and hand shape classification using multi-layered randomized decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>K?ra?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><forename type="middle">Emre</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lale</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="852" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning an efficient model of hand shape variation from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameh</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahram</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2540" to="2548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A survey on 3d hand pose estimation: Cameras, methods, and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrong</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="251" to="272" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Point-to-pose voting based hand pose estimation using residual permutation equivariant layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shile</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongheui</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parsing the hand in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1241" to="1253" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose estimation for augmented reality: a hands-on survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Uchiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Spindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on visualization and computer graphics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2633" to="2651" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern Recognition</title>
		<meeting>the IEEE conference on computer vision and pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Icml</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep-prior++: Improving fast and accurate 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision Workshops</title>
		<meeting>the IEEE international conference on computer vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<title level="m">Hands deep in deep learning for hand pose estimation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Srn: Stacked regression network for real-time 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiting</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3577" to="3586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Vconv-dae: Deep volumetric shape learning without object labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="236" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Accurate, robust, and flexible real-time hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duncan</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Leichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Vinnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems</title>
		<meeting>the 33rd Annual ACM Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3633" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Opening the black box: Hierarchical sampling optimization for estimating human hand pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danhang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3325" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Corish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Luff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Online generative model personalization for hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasia</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edoardo</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murphy</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Capturing hands in action using discriminative salient points and physics simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilash</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="172" to="193" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dense 3d regression for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3dn: 3d deformation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radomir</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1038" to="1046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Model-based deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingfu</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06854</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
