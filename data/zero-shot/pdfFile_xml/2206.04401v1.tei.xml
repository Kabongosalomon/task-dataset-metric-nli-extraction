<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-modal Local Shortest Path and Global Enhancement for Visible-Thermal Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Cross-modal Local Shortest Path and Global Enhancement for Visible-Thermal Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Visible-Thermal person re-identification</term>
					<term>cross- modal</term>
					<term>Local feature alignment</term>
					<term>Multi-branch</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In addition to considering the recognition difficulty caused by human posture and occlusion, it is also necessary to solve the modal differences caused by different imaging systems in the Visible-Thermal cross-modal person re-identification (VT-ReID) task. In this paper,we propose the Cross-modal Local Shortest Path and Global Enhancement (CM-LSP-GE) modules, a two-stream network based on joint learning of local and global features. The core idea of our paper is to use local feature alignment to solve occlusion problem, and to solve modal difference by strengthening global feature. Firstly, Attentionbased two-stream ResNet network is designed to extract dualmodality features and map to a unified feature space. Then, to solve the cross-modal person pose and occlusion problems, the image are cut horizontally into several equal parts to obtain local features and the shortest path in local features between two graphs is used to achieve the fine-grained local feature alignment. Thirdly, a batch normalization enhancement module applies global features to enhance strategy, resulting in difference enhancement between different classes. The multi granularity loss fusion strategy further improves the performance of the algorithm. Finally, joint learning mechanism of local and global features is used to improve cross-modal person re-identification accuracy. The experimental results on two typical datasets show that our model is obviously superior to the most state-of-the-art methods. Especially, on SYSU-MM01 datasets, our model can achieve a gain of 2.89%and 7.96%in all search term of Rank-1 and mAP. The source code will be released soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ERSON re-identification(Re-ID) <ref type="bibr" target="#b0">[1]</ref> is an important technology in video tracking, which mainly studies matching person images from different camera perspectives. The traditional Re-ID mainly focuses on the visible modality, but in the actual application environment, there are often problems that cannot be identified due to low illumination, so that full-time tracking cannot be realized. In order to obtain the information of pedestrian identification at night, researchers turned to VT-Reid <ref type="bibr" target="#b1">[2]</ref> research which mainly studies the matching of pedestrian images from one modality (infrared) to another modality (visible), which is a cross-modal tracking technology for intelligent tracking of pedestrians in full time.</p><p>As shown in <ref type="figure">Figure 1</ref>, now the main challenge of VT-Reid is not only to solve the problem of pedestrian posture difference and body part occlusion, but also to solve the problem of image inconsistency caused by modal difference.</p><p>In order to improve the accuracy of cross modal pedestrian re-recognition, researchers have proposed machine learning <ref type="bibr">Xiaohong</ref>   <ref type="figure">Fig. 1</ref>: The cross-modal person re-identification technology difficulties. Sample images from SYSU-MM01 dataset <ref type="bibr" target="#b1">[2]</ref> models that based on hand-designed feature and deep learning network. Because manual features can only represent limited pedestrian low-level features which are only part of all features , the machine learning methods such as HOG <ref type="bibr" target="#b2">[3]</ref> ,LOMO <ref type="bibr" target="#b3">[4]</ref> cannot fulfill the re-identification task satisfactorily. At present, deep learning is widely used to solve the re-identification problem and its common solution is to encode the pedestrian characteristics of different modes into a common feature space for similarity measurement.</p><p>To alleviate the modal differences in cross matching, in this paper we use the attention-based two-stream ResNet network to extract the features of pedestrians in visible and infrared images, and map them to the unified feature space for similarity measurement.</p><p>Most of the work only focuses on the global coarse grained feature extraction process and ignore the research on the feature enhancement method after extraction, in this paper we design a feature enhancement module based on batch normalization to enhance the global feature after extraction and improve the feature representation of different pedestrians.</p><p>However, it is not enough to focus only on the global features, local features are also play an important role in VT-ReID task. When the body parts are missed due to pedestrian occlusion, it is difficult to extract the global features from these images and truly characterize this person, which is easy to lead incorrect classification. Considering that local information(e.g., head, body) of pedestrians in the images can be well distinguished and aids global feature learning, so in this paper, the pedestrian images under two different modes arXiv:2206.04401v1 [cs.CV] 9 Jun 2022 are segmented equally in the horizontal direction, and then the shortest path algorithm is used to achieve cross-modal local feature alignment. Finally, the joint learning mechanism based on local and global features can effectively improve the algorithm performance.</p><p>Finally, different backbone networks in the classification task can affect the final classification accuracy. In this context, we investigate the impact of different variants of the ResNet two-stream feature extraction network on the final identification accuracy to further promote the performance of the network. In summary, the contributions of this paper are:</p><p>? We propose an attention-based two-stream ResNet network for VT cross-modal feature acquisition. ? We propose a method for cross-modal local feature alignment based on the shortest path (CM-LSP), which effectively solves the occlusion problem in cross-modal pedestrian re-identification and improves the robustness of the algorithm. ? We design a batch normalized global feature enhancement (BN-GE) method to solve the problem of insufficient global feature discrimination and propose a multi granularity loss fusion strategy to guide network learning. ? Ours method achieves preferably results on datasets SYSU-MM01 and RegDB. This can be used as a research baseline to improve the quality of future research.</p><p>II. RELATED WORK In order to improve cross-modal recognition accuracy in VT-ReID not only needs to solve the problem of pedestrian posture and occlusion, but also needs to break through the dilemma of cross-modal discrepancy. Machine learning based on artificial features has proved poor performance since they represent only some low-level pedestrian features. Therefore, Researchers turn to more powerful methods, Deep Learning, for feature acquisition and the mainstream approaches include: image generation-based methods, feature extractor-based methods and metric learning-based methods.</p><p>Image Generation Based Methods fulfil the reidentification task by generating fake images through generative adversarial network (GAN) to reduce the difference between cross-patterns from the image level. Vladimir <ref type="bibr" target="#b4">[5]</ref> et al.firstly proposed ThermalGAN to transform visible images into infrared images and then accomplish pedestrian recognition in the infrared modality. Zhang <ref type="bibr" target="#b5">[6]</ref> et al.considered a teacher-student GAN model (TS-GAN) which used modal transitions to better guide the learning of discriminative features. Xia <ref type="bibr" target="#b6">[7]</ref> et al.pointed out an image modal panning network which performed image modal transformation through a cycle consistency adversarial network. The above methods all use GAN to generate fake images reducing cross-modal differences from the image level. However, multiple seemingly reasonable images may be generated due to the change of the color attribute of the pedestrian appearance. It is difficult to determine which generated target is correct,resulting in a false identification process. The methods based on image generation often have the problem of algorithm performance uncertainty.</p><p>Feature Extractor Based Methods are mainly used to extract the distinction and consistency characteristics from different modes according to the discrepancy of different modes. Therefore, the extraction of rich features is the key of the algorithm.Wu <ref type="bibr" target="#b1">[2]</ref> et al.analyzed the performance of different network structures, which include one-stream and twostream networks, and proposed deep zero-padding for training one-stream network towards automatically evolving domainspecific nodes in the network for cross-modality matching. Kang <ref type="bibr" target="#b7">[8]</ref> et al.rendered a one-stream model that placed visible and infrared images in different channels or created input images by connecting different channels. Fan <ref type="bibr" target="#b8">[9]</ref> et al.advanced a cross-spectral bi-subspace matching one-stream model to solve the matching difference between cross-modal classes. All three of the above single-stream algorithms have low accuracy due to the single-stream network structure defects, which can only extract some common features and cannot extract the discriminative features in dual-modality.</p><p>On the contrary to single stream network, the two-stream can extract different modal features by using the parallel network, so that the network has the advantage of extracting distinguishing features. Ye <ref type="bibr" target="#b9">[10]</ref> et al. applied the two-stream AlextNet network to gain the dual-mode features, and then project these features into the public feature space. Based on this, Jiang <ref type="bibr" target="#b10">[11]</ref> et al. designed a multi-granularity attention network to extract coarse-grained features separately. Ran <ref type="bibr" target="#b11">[12]</ref> et al. mapped global features to the same feature space and added local discriminative feature learningand the algorithms performance were improved to some extent.To verify the effect of the network flow structure to acquire features ability on the performance of the algorithm, Emrah Basaran <ref type="bibr" target="#b12">[13]</ref> designed a four-stream ResNet network framework which composed of gray flow and LZM upon the two-stream network. However, the experiment result showed that this method has large amount of calculation, high training cost and unsatisfactory experimental results. Overall, the two-stream network performs best in the structure of VT-ReID tasks.</p><p>Most of the above methods use ResNet as the feature extraction network. However, the variants of ResNet, like SE <ref type="bibr" target="#b13">[14]</ref> , CBAM <ref type="bibr" target="#b14">[15]</ref> , GC <ref type="bibr" target="#b15">[16]</ref> , SK <ref type="bibr" target="#b16">[17]</ref> , ST <ref type="bibr" target="#b17">[18]</ref> , NAM <ref type="bibr" target="#b18">[19]</ref> , ResNetXT <ref type="bibr" target="#b19">[20]</ref> and SN <ref type="bibr" target="#b20">[21]</ref> are widely used in classification and recognition tasks, and have achieved good accuracy improvement.</p><p>Metric Learning Based Methods are mainly focus on forcibly shortening the distance between similar samples across models and widening the distance between different samples by designing loss function. Based on the hierarchical feature extraction of the two-stream network, Ye <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> et al. designed the dual-constrained top-ranking loss and the Bidirectional exponential angular triplet loss from the global feature. Zhu <ref type="bibr" target="#b23">[24]</ref> et al.proposed to use the Hetero-center loss to constrain the intra-class center distance between two heterogeneous modes to monitor the learning of cross-modal invariant information from the perspective of global features.Ling <ref type="bibr" target="#b24">[25]</ref> et al.advanced a center-guided metric learning method for enforcing the distance constraints among cross-modal class centers and samples. Liu <ref type="bibr" target="#b25">[26]</ref> et al.raised a dual-modality triplet loss which considering both inter-mode difference and intra-mode change and introduced a mid-level feature fusion module. Hao <ref type="bibr" target="#b26">[27]</ref> et al.projected an end-to-end two-stream hypersphere manifold embedding network with classification and identification losses, which constrained the intra-mode change and cross-mode change on the hypersphere. Zhao <ref type="bibr" target="#b27">[28]</ref> et al. introduced difficult sample quintuple loss which is used to guide global feature learning. Liu <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> et al. introduced heterogeneous center-based triple loss and dual-granularity triple loss from cross-modal global feature alignment, and coarsegrained feature learning as well as part-level feature extraction block. Ling <ref type="bibr" target="#b30">[31]</ref> et al. designed the Earth Mover's Distance can alleviate the impact of the intra-identity variations during modality alignment, and the Multi-Granularity Structure is designed to enable it to align modalities from both coarse-and fine-grained features. In order to find the nuances features, Wu <ref type="bibr" target="#b31">[32]</ref> et al. proposed the center clustering loss, separation loss and the mode alignment module to find the nuances of different modes in an unsupervised manner.</p><p>From the above literature, it is known that coarse-grained global feature plays a major role in recognition, while finegrained local feature is a very good help in addition to global features to improve the ReID accuracy. In order to solve the problem of occlusion and modal difference in cross-modal VT-ReID, we use multi-granularity fusion loss to guide network learning. Firstly, in order to solve the modal difference, we design the classification loss based on global features and the loss of hard sample triples from the overall level. At the same time, the classification loss based on local features is designed from the partial level. Secondly, in order to solve the problem of component occlusion and ensure the shortest distance between components of the same kind, a part alignment loss is devised. Finally, multi-granularity loss is used to jointly constrain feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head><p>Overview. In this section, we propose a CM-LSP-GE method. As shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attention-based two-stream network</head><p>In cross-modal person re-identification project, the twostream network is often used for feature extraction due to its excellent characteristics of discriminative learning of different modal features, which mainly include feature extraction and feature mapping. At present, most of the mainstream methods use ResNet50 as the backbone for feature extraction. However, some of the latest improved ResNet have achieved better performance in image classification, recognition and so on. Therefore, it is necessary to find a better backbone for VT-Reid task, and provide a new reference for future research. As shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, We will introduce how to obtain the feature of sample. Assume that the visible and infrared image are respectively defined as X rgb and X ir , they are respectively fed into the VMN and TMN networks for feature learning to obtain shallow features F s rgb and F s ir ,then the two shallow features are connected as new features, which are input into the FMN network for fusion feature learning and feature mapping , and then the segmented features F d rgb and F d ir represented in a uniform feature space are obtained. The segmentation features are processed by the adaptive average pooling layer to obtain the global featuresF glo rgb , F glo ir ,while the local featuresF loc rgb and F loc ir are obtained via the horizontal adaptive maximum pooling layer. Finally, the global features are enhanced by the EM module to get the enhanced featuresF eglo rgb and F eglo ir .In the network inference stage, the distance matrix is constructed using the F eglo rgb for similarity analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Cross-modal Local Feature Alignment Module</head><p>When pedestrian occlusion occurs, it is difficult to truly recognize the global pedestrian features using missing components as distinguishing features. Therefore, we designed the shortest path alignment module based on local features, which achieves component alignment by equalizing the image segmentation and calculating the shortest path between local features in the two graphs. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the visible and infrared images are equally divided into five parts, then the local feature representation is defined as F loc where i is the horizontal feature position, d is the local feature dimension, h is height of the input image. When using similar methods to calculate infrared local features, we obtain the local feature representation of the bimodal state and subsequently define the Eq. 2for calculating the distance between the two graphs. The distance equation is defined as:</p><formula xml:id="formula_0">rgb = {f 1 r , f 2 r ...f i r } and F loc ir = {f 1 t , f 2 t ...f i t },</formula><formula xml:id="formula_1">di,j = f i r ? M ean(f i r ) M ax(f i r ) ? M in(f i r ) ? f j t ? M ean(f j t ) M ax(f j t ) ? M in(f j t ) 1<label>(2)</label></formula><p>where i, j ? (1, 2, 3, ..., h) are the respective parts of the images. d i,j is the distance between local features of different modes. Then, we construct the distance matrix D from d i,j ,and define S i,j is the total distance between the local features of the two images as the shortest distance from (1, 1) to (H, H).the shortest path between two graphs is calculated by the Eq. 3 It is no doubt that global features play an important role in person re-identification, and a BN-based global feature enhancement module is designed to further improve the discrimination of different pedestrian global representations under cross-modality. As shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, taking the visible mode as an example.</p><formula xml:id="formula_2">Si,j = ? ? ? ? ? di,j i = 1, j = 1 Si,j?1 + di,j i = 1, j = 1 Si?1,j + di,j i = 1, j = 1 min(Si,j?1, Si?1,j) + di,j i = 1, j = 1 (3)</formula><p>Firstly, the unified spatial output features F d rgb are normalized through the batch-normalization layer (BN) At the same time, the vector v bn is calculated according to the BN weights. Normalize the features F d rgb using BN and multiply them with v bn to obtain a new feature and use this feature to obtain a new weight matrix by adaptive averaging pooling layer. To obtain the final F eglo rgb , we linearly sum F glo rgb and this feature. Then, we derive the calculation process from the perspective of the formula. We first define the BN layer calculation, as in Eq. 4:</p><formula xml:id="formula_3">BN (x) = ? x ? ? b ? 2 b + ? + ?<label>(4)</label></formula><p>where ? is scale factor and ? is translation factor, ? b and ? 2 b is mean and variance of one batch, respectively. ? is a hyper parameter. We define the weight vector v bn = ? j / j=0 ? j ,where ? j is the weight factor in the BN layer. Finally, The enhanced global features are calculated as in Eq. 5:</p><formula xml:id="formula_4">F eglo rgb = F glo rgb + Avg(v bn ? BN (F d rgb ))<label>(5)</label></formula><p>where Avg is adaptive average pooling calculation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. The Multi-granularity Fusion Loss</head><p>The multi-granularity fusion loss proposed in this paper is described in detail below. We use two-stream networks to obtain global and local features, and then use them to compute classification loss and triple loss. Classification loss is widely used in the Reid task to calculate cross-entropy loss mainly by pedestrian identity labels, which is referred to as id loss in this paper.</p><p>Firstly, we define the global and local classification losses as L g id and L lv id , respectively, as Eq. 6 and Eq. 7:</p><formula xml:id="formula_5">L g id = N i=1 ?qi log(p g i ) (6) L lv id = S j=2 N i=1</formula><p>?qi log(p j i ) <ref type="bibr" target="#b6">(7)</ref> where N is the total number of categories in the training dataset, q i is the sample true probability distribution , S is the number of horizontal slices, p j i and p g i are the predicted probability distributions.</p><p>For each of P that randomly selected person identities, K visible images and K thermal images are randomly sampled, totally are 2 ? P ? K images, We define the heterogeneous center-based triad loss as Eq. 8 Considering the auxiliary role of local features to the performance of the algorithm, the cross-modal local feature alignment loss L vt P A is designed in this paper and defined as Eq. 9:</p><formula xml:id="formula_6">L g T ri = P i=1 [mg + f c i v ? f c i t 2 ? min k?{v,t} f c i v ? f c j k 2 ] + + P i=1 [mg + f c i t ? f c i v 2 ? min k?{v,t} f c i t ? f c j k 2 ] + (8) where m g is a hyper parameter.f c i v = 1 K K j=1 f i v,j and f c i t = 1 K K j=1 f i t,</formula><formula xml:id="formula_7">L vt P A = P i=1 2K a=1 H j=2 [m l + max k?{v,t} f ka i.j ? f kp i,j 2 ? min k?{v,t} f ka i,j ? f kn i,j 2 ]<label>(9)</label></formula><p>where m l is a hyper parameter, f ka i.j is the infrared/visible modal local features, f kp i,j is the visible/infrared mode with the most distant positive sample, f kn i,j is the visible/infrared mode distance nearest negative sample and k represents different modes. In this paper, the total loss L total is defined by the Eq. 10: RegDB <ref type="bibr" target="#b32">[33]</ref> contains 412 individuals, each with 10 images from the visible camera and 10 images from the infrared camera.</p><formula xml:id="formula_8">L total = Globalid L g id + L eg id + GlobalT ri L g T ri + L eg T ri + Localid L lv id + L lt id + LocalT ri L vt P A<label>(</label></formula><p>Evaluation metrics. The CMC (Cumulative Matching Characteristics), the mINP (mean inverse negative penalty) and mAP (mean Average Precision) are used to evaluate the retrieval performance. For CMC, we report the rank-1, rank-10, and rank-20 precision.</p><p>Official method evaluation was performed on the SYSU-MM01dataset, based on 10 replicate random splits of the query set dataset and the mean of the group to be tested. RegDB results were based on 10 replicate random splits of the training and test sets and the mean was calculated.</p><p>Implementation Details. We implemented our model with Pytorch. The training input images are first padded with 10 and randomly cropped to 288?144,random horizontal flipping is further imposed as data augmentation. The train-batch size is set to 120. We set the ImageNet pre-trained CNN part as 0.01 and the classifier as 0.1, optimized them via SGD. The learning rate decreases by a factor of 10 every 10 epochs.</p><p>Re-ranking. In recent studies <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref> , the Re-ranking algorithm ,which is an algorithm for reordering Retrieval Results, contributes significantly to person Re-Identification, so that we used ECN <ref type="bibr" target="#b34">[35]</ref> (Expanded Cross Neighborhood) as a post-processing algorithm in the inference phase. We used the relevant parameter sources in 35.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Comparison with The State-of-The-Art Algorithms</head><p>To demonstrate the superiority of our method, we compared it against the state-of-the-art approaches on SYSU-MM01 and RegDB. These methods include:traditional characterization:HOG <ref type="bibr" target="#b2">[3]</ref>,LOMO <ref type="bibr" target="#b3">[4]</ref>,deep learning:HCML <ref type="bibr" target="#b9">[10]</ref>,LZM <ref type="bibr" target="#b12">[13]</ref>,IPVT+MSR <ref type="bibr" target="#b7">[8]</ref>.</p><p>Metric Learning Methods: HC <ref type="bibr" target="#b23">[24]</ref>,DCTR <ref type="bibr" target="#b22">[23]</ref>,CML <ref type="bibr" target="#b24">[25]</ref> ,EDFL <ref type="bibr" target="#b25">[26]</ref>,HSME <ref type="bibr" target="#b26">[27]</ref>,HPILN <ref type="bibr" target="#b27">[28]</ref>,DGTL <ref type="bibr" target="#b28">[29]</ref>,Hc-Tri <ref type="bibr" target="#b29">[30]</ref>,CM-EMD <ref type="bibr" target="#b30">[31]</ref>,MPANet <ref type="bibr" target="#b31">[32]</ref>.</p><p>Image generation methods:cmGAN <ref type="bibr" target="#b35">[36]</ref>,AlignGAN <ref type="bibr" target="#b36">[37]</ref>, JSIA <ref type="bibr" target="#b38">[39]</ref>, CoSiGAN <ref type="bibr" target="#b37">[38]</ref>,TS-GAN <ref type="bibr" target="#b5">[6]</ref>. The experimental results respectively correspond to Tabel II and Tabel I.</p><p>Results on SYSU-MM01. The results are shown in Tabel I. Our method respectively obtains 76.28% and 76.52% of Rank-1 and mAP accuracy in the full scene search, and obtain 82.31% and 85.16% of Rank-1 and mAP accuracy in the indoor search. The test results are better than the latest methods.</p><p>Results on RegDB. We use the method of this paper on the RegDB dataset to compare with the current state-of-theart methods, and the results are shown in Tabel II. In Visible-Thermal test results, our method achieves Rank-1 accuracy of 94.13% and mAP accuracy of 88.86%, and in Thermal-Visible test our method obtains Rank-1 accuracy of 93.16% and mAP accuracy of 87.26%. In VT test, the results are basically the same as the latest literature CM-EMD <ref type="bibr" target="#b30">[31]</ref> results were essentially unchanged and in the TV test, 0.39% improvement was achieved on Rank-1 and 0.41% improvement on mAP.  Compared with HcTri <ref type="bibr" target="#b29">[30]</ref>, our method has better performance in all aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation.</head><p>Effectiveness of attention mechanism. In order to study the effect of different attention methods on re-identification accuracy, the following eight different methods are selected for experimental study. such as SEResNet based on channel attention, CBAMResNet based on convolutional block attention, GCResNet based on full-text information, ResNetST based on scattered attention, NAMResNet based on normalized attention, SKResNet based on selective kernel, ResNetXT based on design aggregated residual block, ResNetSN based on switchable normalized layer. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, the experimental results prove the improved backbone network based on GC attention mechanism has the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of GE.</head><p>To verify the effectiveness of the GE module designed in this paper, we select global features and enhanced global features in the network for visualization and analysis which are shown in <ref type="figure">Figure 7</ref>. We randomly select 20 different pedestrians from the SYSU-MM01 query dataset, and take 10 images of each person separately for visualization and analysis. In <ref type="figure">Fig. 7 (a)</ref> the image is mapped to a two-dimensional plane using PCA, (b) shows the global features mapped onto the plane visualization and (c) shows the enhanced global features. Comparing (b) and (c), it can be found that the distance between different classes is farther after global feature enhancement.</p><p>Effectiveness of CM-LSP. In order to verify the effectiveness of the proposed method for cross-modal local feature alignment based on the shortest path the cutting quantity experiment on SYSU-MM01 is firstly carried out. From the result in Tabel III.it can be found that the best result is obtained by dividing the body image into three equal parts.</p><p>We chose the number of parts in Tabel III. as 2 for the validation dimension experiment and selectively verified that the dimensions 128, 256, 516 and 1024. As shown in Tabel IV, we found that the 1024 dims features have the best performance from the combined results.</p><p>Experimental results demonstrated that joint learning using global and local features is superior to global features alone, and with the help of local features, the network pays more attention to the similarity of pedestrian body parts in images rather than overfitting background information.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>Considering the important role of feature fusion of global and local features for the overall re-identification task accuracy improvement,we proposed a novel Visible-Thermal person re-Identification network in this paper, called Cross-modal Local Shortest path and Batch Normalized Global Enhancement (CM-LSP-GE), which can mitigate cross-modal differences and resolve occlusion problems in images. Moreover, we introduce four different methods to facilitate the CM-LSP-GE proposed in this paper, which are the attention-based twostream network, cross-modal local feature alignment based on the shortest path (CM-LSP),batch normalized global feature enhancement (GE) and the multi-granularity fusion loss.CM-LSP can ensure that the model learns pedestrian details in the image without overfitting the background at a fine-grained level, while GE can enhance the global feature representation of the image at a coarse-grained level. Loss function guides network learning from global and local levels. Eventually, the accuracy is effectively improved, and our method achieves better results on RegDB and SYSU-MM01.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The model proposed in this paper consists of three main components, an attention-based two-stream backbone network, the cross-modal local feature alignment module and a global feature enhancement module. Firstly, attention-based two-stream networks extract bimodal features and map them to the same feature space. Segmentation by FMN output features to obtain unimodal After Split Feature, the segmented features continue to be divided into unimodal global features and local features containing a certain number of horizontal cuts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>this model mainly consists of four parts:(1) Attention-based two-stream network includes thermal mode network (TMN), visible mode network (VMN)and fusion module network (FMN). (2) Cross-modal local feature alignment module.(3) Batch normalized global feature enhancement module. (4) The multi-granularity fusion loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Schematic diagram of two-stream characteristic flow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Cross-modal Local Feature Alignment</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Global feature enhancement module C. Batch normalized global feature enhancement module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>j are the features centers of different modality. f c j v and f c j t represent pedestrian features in different modality respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>(a) Impacts of Backbone in terms of Rank-1,(b) Impacts of Backbone in terms of mAP, (c) Impacts of Backbone in terms of mINP Fig. 7: (a) Impacts of Backbone in terms of Rank-1,(b) Impacts of Backbone in terms of mAP, (c) Impacts of Backbone in terms of mINP</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Wang with the University of Shanghai for Science and Technology , Shanghai,30332 China E-mail:wangxiaohong@st.usst.edu.</figDesc><table /><note>cn Chaoqi Li and Xiangcai Ma are with the University of Shanghai for Science and Technology.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>where the visible local features are calculated as Eq. 1</figDesc><table><row><cell>f i r = HM ax</cell></row></table><note>i?(1,2,...,h) (F d rgb ) [i?d] d = 2 n &amp;n ? (0, 1, 2, ..., 11) (1)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>SYSU-MM01 is the first cross-modal pedestrian dataset built by Wu et al. All data are collected from 4 RGB cameras and 2 IR cameras, including a total of 491 different pedestrians. 395 people are included in the training set, with a total of 19,659 visible images and 12,792 IR images, and 96 people are included in the test set, including full-scene search and indoor scene search. For indoor search only search in cameras1,2,3and6, while the full scene uses all images.</figDesc><table><row><cell>10)</cell></row><row><cell>IV. EXPERIMENT</cell></row><row><cell>A. Experimental settings</cell></row><row><cell>Datasets.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I :</head><label>I</label><figDesc>Comparison with the state-of-the-art methods on the SYSU-MM01 dataset.</figDesc><table><row><cell>Methods</cell><cell>Time</cell><cell cols="10">All search rank-1 rank-10 rank-20 mAP mINP rank-1 rank-10 rank-20 mAP mINP Indoor search</cell></row><row><cell>HOG [3]</cell><cell>2005</cell><cell>2.76</cell><cell>18.25</cell><cell>31.91</cell><cell>4.24</cell><cell>-</cell><cell>3.22</cell><cell>24.68</cell><cell>44.52</cell><cell>7.25</cell><cell>-</cell></row><row><cell>LOMO [4]</cell><cell>2015</cell><cell>3.64</cell><cell>23.18</cell><cell>37.28</cell><cell>4.53</cell><cell>-</cell><cell>5.75</cell><cell>34.35</cell><cell>57.90</cell><cell>10.19</cell><cell>-</cell></row><row><cell>HCML [10]</cell><cell>2018</cell><cell>14.32</cell><cell>53.16</cell><cell>69.17</cell><cell>16.16</cell><cell>-</cell><cell>20.58</cell><cell>63.38</cell><cell>85.79</cell><cell>26.92</cell><cell>-</cell></row><row><cell>DCTR [23]</cell><cell>2018</cell><cell>17.01</cell><cell>55.43</cell><cell>71.96</cell><cell>19.66</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell>cmGAN [36]</cell><cell>2018</cell><cell>26.97</cell><cell>67.51</cell><cell>80.56</cell><cell>31.49</cell><cell>-</cell><cell>31.63</cell><cell>77.23</cell><cell>89.62</cell><cell>42.46</cell><cell>-</cell></row><row><cell>HSME [27]</cell><cell>2019</cell><cell>20.68</cell><cell>62.74</cell><cell>77.95</cell><cell>23.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>IPVT+MSR [8]</cell><cell>2019</cell><cell>23.20</cell><cell>51.20</cell><cell>61.70</cell><cell>22.50</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>AlignGAN [37]</cell><cell>2019</cell><cell>51.50</cell><cell>89.40</cell><cell>95.70</cell><cell>33.90</cell><cell>-</cell><cell>57.10</cell><cell>92.70</cell><cell>97.40</cell><cell>45.30</cell><cell>-</cell></row><row><cell>CoSiGAN [38]</cell><cell>2020</cell><cell>35.55</cell><cell>81.54</cell><cell>90.43</cell><cell>38.33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDFL [26]</cell><cell>2020</cell><cell>36.94</cell><cell>84.52</cell><cell>93.22</cell><cell>40.77</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HPILN [28]</cell><cell>2020</cell><cell>41.36</cell><cell>84.78</cell><cell>94.51</cell><cell>42.95</cell><cell>-</cell><cell>45.77</cell><cell>91.82</cell><cell>98.46</cell><cell>56.52</cell><cell>-</cell></row><row><cell>JSIA [39]</cell><cell>2020</cell><cell>45.10</cell><cell>85.70</cell><cell>93.80</cell><cell>29.50</cell><cell>-</cell><cell>52.70</cell><cell>91.10</cell><cell>96.40</cell><cell>42.70</cell><cell>-</cell></row><row><cell>CML [25]</cell><cell>2020</cell><cell>56.27</cell><cell>94.08</cell><cell>98.12</cell><cell>43.39</cell><cell>-</cell><cell>60.42</cell><cell>95.88</cell><cell>99.5</cell><cell>53.52</cell><cell>-</cell></row><row><cell>HC [24]</cell><cell>2020</cell><cell>59.96</cell><cell>91.50</cell><cell>96.82</cell><cell>54.95</cell><cell>-</cell><cell>59.74</cell><cell>92.07</cell><cell>96.22</cell><cell>64.91</cell><cell>-</cell></row><row><cell>HcTri [30]</cell><cell>2020</cell><cell>61.68</cell><cell>93.10</cell><cell>97.17</cell><cell cols="2">57.51 39.54</cell><cell>63.41</cell><cell>91.69</cell><cell>95.28</cell><cell cols="2">68.10 64.26</cell></row><row><cell>LZM [13]</cell><cell>2020</cell><cell>63.05</cell><cell>93.62</cell><cell>96.30</cell><cell>67.13</cell><cell>-</cell><cell>69.06</cell><cell>96.30</cell><cell>97.16</cell><cell>76.95</cell><cell>-</cell></row><row><cell>TS-GAN [6]</cell><cell>2021</cell><cell>55.90</cell><cell>91.20</cell><cell>96.60</cell><cell>39.70</cell><cell>-</cell><cell>59.30</cell><cell>91.80</cell><cell>97.90</cell><cell>50.90</cell><cell>-</cell></row><row><cell>AGW [7]</cell><cell>2021</cell><cell>56.52</cell><cell>90.26</cell><cell>95.59</cell><cell cols="2">57.47 38.75</cell><cell>68.72</cell><cell>94.61</cell><cell>97.42</cell><cell cols="2">75.11 64.22</cell></row><row><cell>DGTL [29]</cell><cell>2021</cell><cell>57.34</cell><cell>-</cell><cell>-</cell><cell>55.13</cell><cell>-</cell><cell>63.11</cell><cell>-</cell><cell>69.2</cell><cell>-</cell><cell>-</cell></row><row><cell>MPANet [32]</cell><cell>2021</cell><cell>70.58</cell><cell>96.21</cell><cell>98.80</cell><cell>68.24</cell><cell>-</cell><cell>76.74</cell><cell>98.21</cell><cell>99.57</cell><cell>80.95</cell><cell>-</cell></row><row><cell>CM-EMD [31]</cell><cell>2022</cell><cell>73.39</cell><cell>96.24</cell><cell>98.82</cell><cell>68.56</cell><cell>-</cell><cell>80.53</cell><cell>98.31</cell><cell>99.91</cell><cell>82.71</cell><cell>-</cell></row><row><cell>CM-GE</cell><cell>-</cell><cell>58.59</cell><cell>87.14</cell><cell>92.64</cell><cell cols="2">54.95 39.00</cell><cell>65.84</cell><cell>93.22</cell><cell>96.95</cell><cell cols="2">67.31 61.99</cell></row><row><cell>CM-LSP-GE</cell><cell>-</cell><cell>60.91</cell><cell>89.03</cell><cell>94.43</cell><cell cols="2">57.12 41.39</cell><cell>69.38</cell><cell>95.14</cell><cell>98.25</cell><cell>70.3</cell><cell>65.14</cell></row><row><cell>CM-LSP-GE-RK</cell><cell>-</cell><cell>76.28</cell><cell>94.38</cell><cell>97.08</cell><cell cols="2">76.52 42.03</cell><cell>82.31</cell><cell>98.12</cell><cell>99.91</cell><cell cols="2">85.16 66.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Comparison with the state-of-the-art methods on the RegDB dataset.</figDesc><table><row><cell>Methods</cell><cell>Time</cell><cell cols="4">Visible-Thermal Thermal-Visible rank-1 mAP rank-1 mAP</cell></row><row><cell>HCML [10]</cell><cell>2018</cell><cell>24.44</cell><cell>20.08</cell><cell>21.70</cell><cell>22.24</cell></row><row><cell>DCTR [29]</cell><cell>2018</cell><cell>33.47</cell><cell>31.83</cell><cell>32.72</cell><cell>31.10</cell></row><row><cell>HSME [27]</cell><cell>2019</cell><cell>50.85</cell><cell>47.00</cell><cell>50.15</cell><cell>46.16</cell></row><row><cell cols="2">AlignGAN [37] 2019</cell><cell>57.9</cell><cell>53.6</cell><cell>56.3</cell><cell>53.4</cell></row><row><cell>CoSiGAN [38]</cell><cell>2020</cell><cell>47.18</cell><cell>46.16</cell><cell>-</cell><cell>-</cell></row><row><cell>EDFL [26]</cell><cell>2020</cell><cell>52.58</cell><cell>52.98</cell><cell>51.89</cell><cell>52.13</cell></row><row><cell>CML [25]</cell><cell>2020</cell><cell>-</cell><cell>-</cell><cell>59.81</cell><cell>60.86</cell></row><row><cell>HcTri [30]</cell><cell>2020</cell><cell>91.05</cell><cell>83.28</cell><cell>89.3</cell><cell>81.46</cell></row><row><cell>LZM [13]</cell><cell>2020</cell><cell>60.58</cell><cell>63.36</cell><cell>57.17</cell><cell>57.56</cell></row><row><cell>AGW [7]</cell><cell>2021</cell><cell>78.3</cell><cell>70.37</cell><cell>75.22</cell><cell>67.28</cell></row><row><cell>DGTL [29]</cell><cell>2021</cell><cell>83.92</cell><cell>73.78</cell><cell>81.59</cell><cell>71.65</cell></row><row><cell>MPANet [32]</cell><cell>2021</cell><cell>83.7</cell><cell>80.9</cell><cell>82.8</cell><cell>80.7</cell></row><row><cell>CM-EMD [31]</cell><cell>2022</cell><cell>94.37</cell><cell>88.23</cell><cell>92.77</cell><cell>86.85</cell></row><row><cell>CM-GE</cell><cell>-</cell><cell>92.32</cell><cell>85.35</cell><cell>90.16</cell><cell>83.43</cell></row><row><cell>CM-LSP-GE</cell><cell>-</cell><cell>94.13</cell><cell>88.86</cell><cell>93.16</cell><cell>87.26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison with Part quantity in SYSU-MM01 do indoor search.</figDesc><table><row><cell cols="3">Parts Rank-1 mAP mINP</cell></row><row><cell>None</cell><cell>65.84</cell><cell>67.31 61.99</cell></row><row><cell>2</cell><cell>67.26</cell><cell>68.08 62.52</cell></row><row><cell>3</cell><cell>69.38</cell><cell>70.30 65.14</cell></row><row><cell>4</cell><cell>68.55</cell><cell>69.47 64.09</cell></row><row><cell>5</cell><cell>66.66</cell><cell>68.22 62.42</cell></row><row><cell>6</cell><cell>67.82</cell><cell>68.80 63.28</cell></row><row><cell>7</cell><cell>67.76</cell><cell>68.56 62.62</cell></row><row><cell>8</cell><cell>67.17</cell><cell>68.21 62.90</cell></row><row><cell>9</cell><cell>66.02</cell><cell>67.52 62.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison with two Part and differ Dim in SYSU-MM01 do indoor search.</figDesc><table><row><cell cols="5">Dim rank-1 rank-10 rank-20 mAP mINP</cell></row><row><cell>128</cell><cell>67.26</cell><cell>93.79</cell><cell>97.15</cell><cell>68.08 62.52</cell></row><row><cell>256</cell><cell>65.25</cell><cell>93.96</cell><cell>97.21</cell><cell>66.65 60.92</cell></row><row><cell>512</cell><cell>65.58</cell><cell>93.51</cell><cell>97.40</cell><cell>66.88 61.04</cell></row><row><cell cols="2">1024 67.13</cell><cell>93.72</cell><cell>97.35</cell><cell>68.72 62.78</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="2872" to="2893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rgb-infrared cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5380" to="5389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE computer society conference on computer vision and pattern recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Thermalgan: Multimodal color-to-thermal image translation for person re-identification in multispectral dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Kniaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hladuvka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Kropatsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mizginov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rgb-ir crossmodality person reid based on teacher-student gan model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visible-infrared person reidentification with data augmentation via cycle-consistent adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">443</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Person re-identification between visible and thermal camera images based on deep residual cnn using single input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cross-spectrum dual-subspace pairing for rgb-infrared cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00213</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical discriminative learning for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A cross-modal multi-granularity attention network for rgb-ir person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">406</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="59" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving visiblethermal reid with structural common space embedding and part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="31" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An efficient framework for visible-infrared cross modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="510" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Resnest: Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nam: Normalization-based attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hoffmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.12419</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Differentiable learning-to-normalize via switchable normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.10779</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bi-directional exponential angular triplet loss for rgb-infrared person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1583" to="1595" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visible thermal person reidentification via dual-constrained top-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>p. 2. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hetero-center loss for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">386</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Class-aware modality mix and center-guided metric learning for visible-thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhancing the discriminative feature learning for visible-thermal cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">398</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hsme: Hypersphere manifold embedding for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hpiln: a feature learning framework for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Strong but simple baseline with dual-granularity triplet loss for visible-thermal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parameter sharing exploration and heterocenter triplet loss for visible-thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Cross-modality earth mover&apos;s distance for visible thermal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01675,2022.3</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discover cross-modality nuances for visible-infrared person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Person recognition system based on a combination of body images from visible light and thermal cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">G</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">605</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A posesensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification with generative adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rgb-infrared cross-modality person re-identification via joint pixel and feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Visibleinfrared person re-identification via colorization-based siamese generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cross-modality paired-images generation for rgb-infrared person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-G</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
