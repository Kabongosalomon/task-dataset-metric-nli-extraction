<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Global Filter Networks for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="laboratory">Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="institution">Tsinghua University State Key</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="laboratory">Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="institution">Tsinghua University State Key</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="laboratory">Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="institution">Tsinghua University State Key</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="laboratory">Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="institution">Tsinghua University State Key</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Automation</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology</orgName>
								<orgName type="laboratory">Lab of Intelligent Technologies and Systems</orgName>
								<orgName type="institution">Tsinghua University State Key</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Global Filter Networks for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in self-attention and pure multi-layer perceptrons (MLP) models for vision have shown great potential in achieving promising performance with fewer inductive biases. These models are generally based on learning interaction among spatial locations from raw data. The complexity of self-attention and MLP grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. In this paper, we present the Global Filter Network (GFNet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. Our architecture replaces the self-attention layer in vision transformers with three key operations: a 2D discrete Fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity trade-offs of our models on both ImageNet and downstream tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness. Code is available at https://github.com/raoyongming/GFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The transformer architecture, originally designed for the natural language processing (NLP) tasks <ref type="bibr" target="#b46">[46]</ref>, has shown promising performance on various vision problems recently <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b5">5]</ref>. Different from convolutional neural networks (CNNs), vision transformer models use self-attention layers to capture long-term dependencies, which are able to learn more diverse interactions between spatial locations. The pure multi-layer perceptrons (MLP) models <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> further simplify the vision transformers by replacing the self-attention layers with MLPs that are applied across spatial locations. Since fewer inductive biases are introduced, these two kinds of models have the potential to learn more generic and flexible interactions among spatial locations from raw data.</p><p>One primary challenge of applying self-attention and pure MLP models to vision tasks is the considerable computational complexity that grows quadratically as the number of tokens increases. Therefore, typical vision transformer style models usually consider a relatively small resolution for the intermediate features (e.g. 14 ? 14 tokens are extracted from the input images in both ViT <ref type="bibr" target="#b10">[10]</ref> and MLP-Mixer <ref type="bibr" target="#b41">[41]</ref>). This design may limit the applications of downstream dense prediction tasks like detection and segmentation. A possible solution is to replace the global self-attention with several local self-attention like Swin transformer <ref type="bibr" target="#b29">[29]</ref>. Despite the effectiveness in practice, local self-attention brings quite a few hand-made choices (e.g., window size, padding strategy, etc.) and limits the receptive field of each layer.  <ref type="figure">Figure 1</ref>: The overall architecture of the Global Filter Network. Our architecture is based on Vision Transformer (ViT) models with some minimal modifications. We replace the self-attention sub-layer with the proposed global filter layer, which consists of three key operations: a 2D discrete Fourier transform to convert the input spatial features to the frequency domain, an element-wise multiplication between frequency-domain features and the global filters, and a 2D inverse Fourier transform to map the features back to the spatial domain. The efficient fast Fourier transform (FFT) enables us to learn arbitrary interactions among spatial locations with log-linear complexity.</p><p>In this paper, we present a new conceptually simple yet computationally efficient architecture called Global Filter Network (GFNet), which follows the trend of removing inductive biases from vision models while enjoying the log-linear complexity in computation. The basic idea behind our architecture is to learn the interactions among spatial locations in the frequency domain. Different from the self-attention mechanism in vision transformers and the fully connected layers in MLP models, the interactions among tokens are modeled as a set of learnable global filters that are applied to the spectrum of the input features. Since the global filters are able to cover all the frequencies, our model can capture both long-term and short-term interactions. The filters are directly learned from the raw data without introducing human priors. Our architecture is largely based on the vision transformers only with some minimal modifications. We replace the self-attention sub-layer in vision transformers with three key operations: a 2D discrete Fourier transform to convert the input spatial features to the frequency domain, an element-wise multiplication between frequency-domain features and the global filters, and a 2D inverse Fourier transform to map the features back to the spatial domain. Since the Fourier transform is used to mix the information of different tokens, the global filter is much more efficient compared to the self-attention and MLP thanks to the O(L log L) complexity of the fast Fourier transform algorithm (FFT) <ref type="bibr" target="#b7">[7]</ref>. Benefiting from this, the proposed global filter layer is less sensitive to the token length L and thus is compatible with larger feature maps and CNN-style hierarchical architectures without modifications. The overall architecture of GFNet is illustrated in <ref type="figure">Figure 1</ref>. We also compare our global filter with prevalent operations in deep vision models in <ref type="table">Table 1</ref>.</p><p>Our experiments on ImageNet verify the effectiveness of GFNet. With a similar architecture, our model outperform the recent vision transformer and MLP models including DeiT <ref type="bibr" target="#b43">[43]</ref>, ResMLP <ref type="bibr" target="#b42">[42]</ref> and gMLP <ref type="bibr" target="#b28">[28]</ref>. When using the hierarchical architecture, GFNet can further enlarge the gap. GFNet also works well on downstream transfer learning and semantic segmentation tasks. Our results demonstrate that GFNet can be a very competitive alternative to transformer-style models and CNNs in efficiency, generalization ability and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Vision transformers. Since Dosovitskiy et al. <ref type="bibr" target="#b10">[10]</ref> introduce transformers to the image classification and achieve a competitive performance compared to CNNs, transformers begin to exhibit their</p><formula xml:id="formula_0">Complexity (FLOPs) # Parameters Depthwise Convolution O(k 2 HW D) k 2 D Self-Attention O(HW D 2 + H 2 W 2 D) 4D 2 Spatial MLP O(H 2 W 2 D) H 2 W 2</formula><p>Global Filter O (HW D log 2 (HW ) + HW D) HW D <ref type="table">Table 1</ref>: Comparisons of the proposed Global Filter with prevalent operations in deep vision models. H, W and D are the height, width and the number of channels of the feature maps. k is the kernel size of the convolution operation. The proposed global filter is much more efficient than self-attention and spatial MLP.</p><p>potential in various vision tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b53">53]</ref>. Recently, there are a large number of works which aim to improve the transformers <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b52">52]</ref>. These works either seek for better training strategies <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b11">11]</ref> or design better architectures <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b52">52]</ref> or both <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b11">11]</ref>. However, most of the architecture modification of the transformers <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b52">52]</ref> introduces additional inductive biases similar to CNNs. In this work, we only focus on the standard transformer architecture <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b43">43]</ref> and our goal is to replace the heavy self-attention layer (O(L 2 )) to an more efficient operation which can still model the interactions among different spatial locations without introducing the inductive biases associated with CNNs.</p><p>MLP-like models. More recently, there are several works that question the importance of selfattention in the vision transformers and propose to use MLP to replace the self-attention layer in the transformers <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b28">28]</ref>. The MLP-Mixer <ref type="bibr" target="#b41">[41]</ref> employs MLPs to perform token mixing and channel mixing alternatively in each block. ResMLP <ref type="bibr" target="#b42">[42]</ref> adopts a similar idea but substitutes the Layer Normalization with an Affine transformation for acceleration. The recently proposed gMLP <ref type="bibr" target="#b28">[28]</ref> uses a spatial gating unit to re-weight tokens in the spatial dimension. However, all of the above models include MLPs to mix the tokens spatially, which brings two drawbacks: (1) like the self-attention in the transformers, the spatial MLP still requires computational complexity quadratic to the length of tokens. (2) unlike transformers, MLP models are hard to scale up to higher resolution since the weights of the spatial MLPs have fixed sizes. Our work follows this trend and successfully resolves the above issues in MLP-like models. The proposed GFNet enjoys log-linear complexity and can be easily scaled up to any resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Applications of Fourier transform in vision.</head><p>Fourier transform has been an important tool in digital image processing for decades <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b0">1]</ref>. With the breakthroughs of CNNs in vision <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b13">13]</ref>, there are a variety of works that start to incorporate Fourier transform in some deep learning method <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b6">6]</ref> for vision tasks. Some of these works employ discrete Fourier transform to convert the images to the frequency domain and leverage the frequency information to improve the performance in certain tasks <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b50">50]</ref>, while others utilize the convolution theorem to accelerate the CNNs via fast Fourier transform (FFT) <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b9">9]</ref>. FFC <ref type="bibr" target="#b6">[6]</ref> replaces the convolution in CNNs with an Local Fourier Unit and perform convolutions in the frequency domain. Very recent works also try to leverage Fourier transform to develop deep learning models to solve partial differential equations <ref type="bibr" target="#b27">[27]</ref> and NLP tasks <ref type="bibr" target="#b25">[25]</ref>. In this work, we propose to use learnable filters to interchange information globally among the tokens in the Fourier domain, inspired by the frequency filters in the digital image processing <ref type="bibr" target="#b35">[35]</ref>. We also take advantage of some properties of FFT to reduce the computational costs and the number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries: discrete Fourier transform</head><p>We start by introducing the discrete Fourier transform (DFT), which plays an important role in the area of digital signal processing and is a crucial component in our GFNet. For clarity, We first consider the 1D DFT. Given a sequence of N complex numbers x[n], 0 ? n ? N ? 1, the 1D DFT converts the sequence into the frequency domain by:  DFT is widely used in modern signal processing algorithms for mainly two reasons: (1) the input and output of DFT are both discrete thus can be easily processed by computers; (2) there exist efficient algorithms for computing the DFT. The fast Fourier transform (FFT) algorithms take advantage of the symmetry and periodicity properties of W kn N and reduce the complexity to compute DFT from O(N 2 ) to O(N log N ). The inverse DFT (3.2), which has a similar form to the DFT, can also be computed efficiently using the inverse fast Fourier transform (IFFT).</p><formula xml:id="formula_1">X[k] = N ?1</formula><p>The DFT described above can be extend to 2D signals. Given the 2D signal X[m, n], 0 ? m ? M ? 1, 0 ? n ? N ? 1, the 2D DFT of x[m, n] is given by:</p><formula xml:id="formula_2">X[u, v] = M ?1 m=0 N ?1 n=0 x[m, n]e ?j2?( um M + vn N ) . (3.3)</formula><p>The 2D DFT can be viewed as performing 1D DFT on the two dimensions alternatively. Similar to 1D DFT, 2D DFT of real input x[m, n] satisfied the conjugate symmetry property</p><formula xml:id="formula_3">X[M ? u, N ? v] = X * [u, v].</formula><p>The FFT algorithms can also be applied to 2D DFT to improve computational efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Filter Networks</head><p>Overall architecture. Recent advances in vision transformers <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b43">43]</ref> demonstrate that models based on self-attention can achieve competitive performance even without the inductive biases associated with the convolutions. Henceforth, there are several works <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b41">41]</ref> that exploit approaches (e.g., MLPs) other than self-attention to mix the information among the tokens. The proposed Global Filter Networks (GFNet) follows this line of work and aims to replace the heavy self-attention layer (O(N 2 )) with a simpler and more efficient one.</p><p>The overall architecture of our model is depicted in <ref type="figure">Figure 1</ref>. Our model takes as an input H ? W non-overlapping patches and projects the flattened patches into L = HW tokens with dimension D. The basic building block of GFNet consists of: 1) a global filter layer that can exchange spatial information efficiently (O(L log L)); 2) a feedforward network (FFN) as in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b43">43]</ref>. The output tokens of the last block are fed into a global average pooling layer followed by a linear classifier.</p><p>Global filter layer. We propose global filter layer as an alternative to the self-attention layer which can mix tokens representing different spatial locations. Given the tokens x ? R H?W ?D , we first perform 2D FFT (see Section 3.1) along the spatial dimensions to convert x to the frequency domain:</p><formula xml:id="formula_4">X = F[x] ? C H?W ?D , (3.4)</formula><p>where F[?] denotes the 2D FFT. Note that X is a complex tensor and represents the spectrum of x. We can then modulate the spectrum by multiplying a learnable filter K ? C H?W ?D to the X:</p><formula xml:id="formula_5">X = K X, (3.5)</formula><p>where is the element-wise multiplication (also known as the Hadamard product). The filter K is called the global filter since it has the same dimension with X, which can represent an arbitrary filter in the frequency domain. Finally, we adopt the inverse FFT to transform the modulated spectrumX back to the spatial domain and update the tokens:</p><formula xml:id="formula_6">x ? F ?1 [X]. (3.6)</formula><p>The formulation of the global filter layer is motivated by the frequency filters in the digital image processing <ref type="bibr" target="#b35">[35]</ref>, where the global filter K can be regarded as a set of learnable frequency filters for different hidden dimensions. It can be proved (see Appendix A) that the global filter layer is equivalent to a depthwise global circular convolution with the filter size H ? W . Therefore, the global filter layer is different from the standard convolutional layer which adopts a relatively small filter size to enforce the inductive biases of the locality. We also find although the proposed global filter can also be interpreted as a spatial domain operation, the filters learned in our networks exhibit more clear patterns in the frequency domain than the spatial domain, which indicates our models tend to capture relation in the frequency domain instead of spatial domain (see <ref type="figure" target="#fig_3">Figure 4</ref>). Note that the global filter implemented in the frequency domain is also much more efficient compared to the spatial domain, which enjoys a complexity of O(DL log L) while the vanilla depthwise global circular convolution in the spatial domain has O(DL 2 ) complexity. We will also show that the global filter layer is better than its local convolution counterparts in the experiments.</p><p>It is also worth noting that in the implementation, we make use of the property of DFT to reduce the redundant computation. Since x is a real tensor, its DFT X is conjugate symmetric, i.e. X[H ? u, W ? v, :] = X * [H, W, :]. Therefore, we can take only the half of the values in the X but preserve the full information at the same time:</p><formula xml:id="formula_7">X r = X[:, 0 : W ] := F r [x], W = W/2 , (3.7)</formula><p>where F r denotes the 2D FFT for real input. In this way, we can implement the global filter as K r ? C H? W ?D , which can reduce half the parameters. This can also ensure F ?1 r [K r X r ] is a real tensor, thus it can be added directly to the input x. The global filter layer can be easily in modern deep learning frameworks (e.g., PyTorch <ref type="bibr" target="#b34">[34]</ref>), as is shown in Algorithm 1. The FFT and IFFT are well supported by GPU and CPU thanks to the acceleration libraries like cuFFT and mkl-fft, which makes our models perform well on hardware.</p><p>Relationship to other transformer-style models. The GFNet follows the line of research about the exploration of approaches to mix the tokens. Compared to existing architectures like vision transformers and pure MLP models, we exhibit that GFNet has several favorable properties: 1) GFNet is more efficient. The complexity of both the vision transformers <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref> and the MLP models <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b42">42]</ref> is O(L 2 ). Different from them, global filter layer only consists an FFT (O(L log L)), an element-wise multiplication (O(L)) and an IFFT (O(L log L)), which means the total computational complexity is O(L log L). 2) Although pure MLP models are simpler compared to transformers, it is hard to fine-tune them on higher resolution (e.g., from 224 ? 224 resolution to 384 ? 384 resolution) since they can only process a fixed number of tokens. As opposed to pure MLP models, we will show that our GFNet can be easily scaled up to higher resolution. Our model is more flexible since both the FFT and the IFFT have no learnable parameters and can process sequences with arbitrary length. We can simply interpolate the global filter K to K ? C H ?W ?D for different inputs, where H ? W is the target size. The interpolation is reasonable due to the property of DFT. Each element of the global filter K[u, v] corresponds to the spectrum of the filter at ? u = 2?u/H, ? v = 2?v/W and thus, the global filter K can be viewed as a sampling of a continuous spectrum</p><formula xml:id="formula_8">K(? u , ? v ), where ? u , ? v ? [0, 2?].</formula><p>Hence, changing the resolution is equivalent to changing the sampling interval of K(? u , ? v ). Therefore, we only need to perform interpolation to shift from one resolution to another. We also notice recently a concurrent work FNet <ref type="bibr" target="#b25">[25]</ref> leverages Fourier transform to mix tokens. Our work is distinct from FNet in three aspects: (1) FNet performs FFT to the input and directly adds the real part of the spectrum to the input tokens, which blends the information from different domains (spatial/frequency) together. On the other hand, GFNet draws motivation from the frequency filters, which is more reasonable.</p><p>(2) FNet only keeps the real part of the spectrum. Note that the spectrum of real input is conjugate symmetric, which means the real part is exactly symmetric and thus contains redundant information. Our GFNet, however, utilizes this property to simplify the computation. <ref type="formula">(3)</ref> FNet is designed for NLP tasks, while our GFNet focuses on vision tasks. In our experiments, we also implement the FNet and show that our model outperforms it.</p><p>Architecture variants. Due to the limitation from the quadratic complexity in the self-attention, vision transformers <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b43">43]</ref> are usually designed to process a relatively small feature map (e.g., 14 ? 14). However, our GFNet, which enjoys log-linear complexity, avoids that problem. Since in our GFNet the computational costs do not grow such significantly when the feature map size increases, we can adopt a hierarchical architecture inspired by the success of CNNs <ref type="bibr" target="#b23">[23,</ref><ref type="bibr" target="#b14">14]</ref>. Generally speaking, we can start from a large feature map (e.g., 56 ? 56) and gradually perform downsampling after a few blocks. In this paper, we mainly investigate two kinds of variants of GFNet: transformer-style models with a fixed number of tokens in each block and CNN-style hierarchical models with gradually downsampled tokens. For transformer-style models, we begin with a 12-layer model (GFNet-XS) with a similar architecture with DeiT-S and ResMLP-12. Then, we obtain 3 variants of the model (GFNet-Ti, GFNet-S and GFNet-B) by simply adjusting the depth and embedding dimension, which have similar computational costs with ResNet-18, 50 and 101 <ref type="bibr" target="#b14">[14]</ref>. For hierarchical models, we also design three models (GFNet-H-Ti, GFNet-H-S and GFNet-H-B) that have these three levels of complexity following the design of PVT <ref type="bibr" target="#b47">[47]</ref>. We use 4 ? 4 patch embedding to form the input tokens and use a non-overlapping convolution layer to downsample tokens following <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b29">29]</ref>. Unlike PVT <ref type="bibr" target="#b47">[47]</ref> and Swin <ref type="bibr" target="#b29">[29]</ref>, we directly apply our building block on different stages without any modifications. The detailed architectures are summarized in <ref type="table" target="#tab_1">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conduct extensive experiments to verify the effectiveness of our GFNet. We present the main results on ImageNet <ref type="bibr" target="#b8">[8]</ref> and compare them with various architectures. We also test our models on the downstream transfer learning datasets including CIFAR-10/100 <ref type="bibr" target="#b22">[22]</ref>, Stanford Cars <ref type="bibr" target="#b21">[21]</ref> and Flowers-102 <ref type="bibr" target="#b33">[33]</ref>. Lastly, we investigate the efficiency and robustness of the proposed models and provide visualization to have an intuitive understanding of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ImageNet Classification</head><p>Setups. We conduct our main experiments on ImageNet <ref type="bibr" target="#b8">[8]</ref>, which is a widely used large-scale benchmark for image classification. ImageNet contains roughly 1.2M images from 1,000 categories. Following common practice <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b43">43]</ref>, we train our models on the training set of ImageNet and report the single-crop top-1 accuracy on 50,000 validation images. To fairly compare with previous works <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b42">42]</ref>, we follow the most training details for our models and do not add extra regularization methods like <ref type="bibr" target="#b19">[19]</ref>. Different from <ref type="bibr" target="#b43">[43]</ref>, we does not use EMA model <ref type="bibr" target="#b36">[36]</ref>, RandomEarse <ref type="bibr" target="#b54">[54]</ref> and   <ref type="bibr" target="#b17">[17]</ref>, which are important to train DeiT while sightly hurting the performance of our models. We set the gradient clipping norm to 1 for all of our models. During finetuning at the higher resolution, we use the hyper-parameters suggested by the implementation of <ref type="bibr" target="#b43">[43]</ref> and train the model for 30 epochs. All of our models are trained on a single machine with 8 GPUs. More details can be found in Appendix B.</p><p>Comparisons with transformer-style architectures. The results are presented in <ref type="table" target="#tab_2">Table 3</ref>. We compare our method with different transformer-style architectures for image classification including vision transformers (DeiT <ref type="bibr" target="#b43">[43]</ref>) and MLP-like models (ResMLP <ref type="bibr" target="#b42">[42]</ref> and gMLP <ref type="bibr" target="#b28">[28]</ref>) that have similar complexity and number of parameters. We see that our method can clearly outperform recent MLP-like models such as ResMLP <ref type="bibr" target="#b42">[42]</ref> and gMLP <ref type="bibr" target="#b28">[28]</ref>, and show similar performance to DeiT. Specifically, GFNet-XS outperforms ResMLP-12 by 2.0% while having slightly fewer FLOPs. GFNet-S also achieves better top-1 accuracy compared to gMLP-S and DeiT-S. Our tiny model is significantly better compared to both DeiT-Ti (+2.4%) and gMLP-Ti (+2.6%) with the similar level of complexity.</p><p>Comparisons with hierarchical architectures. We compare different kinds of hierarchical models in <ref type="figure" target="#fig_3">Figure 4</ref>. ResNet <ref type="bibr" target="#b14">[14]</ref> is the most widely used convolutional model while RegNet <ref type="bibr" target="#b37">[37]</ref> is a family of carefully designed CNN models. We also compare with recent hierarchical vision transformers PVT <ref type="bibr" target="#b47">[47]</ref> and Swin <ref type="bibr" target="#b29">[29]</ref>. Benefiting from the log-linear complexity, GFNet-H models show significantly better performance than ResNet, RegNet and PVT and achieve similar performance with Swin while having a much simpler and more generic design.</p><p>Fine-tuning at higher resolution. One prominent problem of MLP-like models is that the feature resolution is not adjustable. On the contrary, the proposed global filter is more flexible. We demonstrate the advantage of GFNet by finetuning the model trained at 224 ? 224 resolution to higher resolution following the practice in vision transformers <ref type="bibr" target="#b43">[43]</ref>. As shown in <ref type="table" target="#tab_2">Table 3</ref>, our model can easily adapt to higher resolution with only 30 epoch finetuning and achieve better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transfer learning</head><p>To test the generality of our architecture and the learned representation, we evaluate GFNet on a set of commonly used transfer learning benchmark datasets including CIFAR-10 <ref type="bibr" target="#b22">[22]</ref>, CIFAR-100 <ref type="bibr" target="#b22">[22]</ref>, Stanford Cars <ref type="bibr" target="#b21">[21]</ref> and Flowers-102 <ref type="bibr" target="#b33">[33]</ref>. We follow the setting of previous works <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b42">42]</ref>, where the model is initialized by the ImageNet pre-trained weights and finetuned on the new datasets. We evaluate the transfer learning performance of our basic model and best model. The results are presented in <ref type="table" target="#tab_4">Table 5</ref>. The proposed models generally work well on downstream datasets. GFNet models outperform ResMLP models by a large margin and achieve very competitive performance with state-of-the-art EfficientNet-B7. Our models also show competitive performance compared to state-of-the-art CNNs and vision transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis and visualization</head><p>Efficiency of GFNet. We demonstrate the efficiency of our GFNet in <ref type="figure" target="#fig_5">Figure 2</ref>, where the models are compared in theoretical FLOPs, actual latency and peak memory usage on GPU. We test a single building block of each model (including one token mixing layer and one FFN) with respect to the different numbers of tokens and set the feature dimension and batch size to 384 and 32 respectively. The self-attention model quickly runs out of memory when feature resolution exceeds 56 2 , which is also the feature resolution of our hierarchical model. The advantage of the proposed architecture becomes larger as the resolution increases, which strongly shows the potential of our model in vision tasks requiring high-resolution feature maps.  Complexity/accuracy trade-offs. We show the computational complexity and accuracy trade-offs of various transformer-style architectures in <ref type="figure">Figure 3</ref>. It is clear that GFNet achieves the best trade-off among all kinds of models.</p><p>Ablation study on the global filter. To more clearly show the effectiveness of the proposed global filters, we compare GFNet-XS with several baseline models that are equipped with different token mixing operations. The results are presented in <ref type="table" target="#tab_5">Table 6</ref>. All models have a similar building block ( token mixing layer + FFN ) and the same feature dimension of D = 384. We also implement the recent FNet <ref type="bibr" target="#b25">[25]</ref> for comparison, where a 1D FFT on feature dimension and a 2D FFT on spatial dimensions are used to mix tokens. As shown in <ref type="table" target="#tab_5">Table 6</ref>, our method outperforms all baseline methods except DeiT-S that has 64% higher FLOPs.</p><p>Robustness &amp; generalization ability. Inspired by the <ref type="bibr" target="#b32">[32]</ref>, we further conduct experiments to evaluate the robustness and the generalization ability of the GFNet. For robustness, we consider ImageNet-A, ImageNet-C, FGSM and PGD. ImageNet-A <ref type="bibr" target="#b16">[16]</ref>  <ref type="figure">(IN-A)</ref> is a challenging dataset that contains natural adversarial examples. ImageNet-C <ref type="bibr" target="#b15">[15]</ref> (IN-C) is used to validate the robustness of the model under various types of corruption. We use the mean corruption error (mCE, lower is better) on ImageNet-C as the evaluation metric. FGSM <ref type="bibr" target="#b12">[12]</ref> and PGD <ref type="bibr" target="#b31">[31]</ref> are two widely used algorithms that are targeted to evaluate the adversarial robustness of the model by single-step attack and multistep attack, respectively. For generalization ability, we adopt two variants of ImageNet validation set: ImageNet-V2 <ref type="bibr" target="#b39">[39]</ref> (IN-V2) and ImageNet-Real <ref type="bibr" target="#b1">[2]</ref> (IN-Real). ImageNet-V2 is a re-collected version of ImageNet validation set following the same data collection procedure of ImageNet, while ImageNet-Real contains the same images as ImageNet validation set but has reassessed labels. We compare GFNet-S with various baselines in <ref type="table">Table 7</ref> including CNNs, Transformers and MLP-like architectures and find the GFNet enjoys both favorable robustness and generalization ability. <ref type="table">Table 7</ref>: Evaluation of robustness and generalization ability. We measure the robustness from different aspects, including the adversarial robustness by adopting adversarial attack algorithms including FGSM and PGD and the performance on corrupted/out-of-distribution datasets including ImageNet-A <ref type="bibr" target="#b16">[16]</ref> (top-1 accuracy) and ImageNet-C <ref type="bibr" target="#b15">[15]</ref> (mCE, lower is better). The generalization ability is evaluated on ImageNet-V2 <ref type="bibr" target="#b39">[39]</ref> and ImageNet-Real <ref type="bibr" target="#b1">[2]</ref>.  Visualization. The core operation in GFNet is the element-wise multiplication between frequencydomain features and the global filter. Therefore, it is easy to visualize and interpret. We visualize the frequency domain filters as well as their corresponding spatial domain filters in <ref type="figure" target="#fig_3">Figure 4</ref>. The learned global filters have more clear patterns in the frequency domain, where different layers have different characteristics. Interestingly, the filters in the last layer particularly focus on the low-frequency component. The corresponding filters in the spatial domain are less interpretable for humans.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented the Global Filter Network (GFNet), which is a conceptually simple yet computationally efficient architecture for image classification. Our model replaces the self-attention sub-layer in vision transformer with 2D FFT/IFFT and a set of learnable global filters in the frequency domain.</p><p>Benefiting from the token mixing operation with log-linear complexity, our architecture is highly efficient. Our experimental results demonstrated that GFNet can be a very competitive alternative to vision transformers, MLP-like models and CNNs in accuracy/complexity trade-offs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discrete Fourier transform</head><p>In this section, we will elaborate on the derivation and the properties of the discrete Fourier transform.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 From Fourier transform to discrete Fourier transform</head><p>Discrete Fourier transform (DFT) can be derived in many ways. Here we will introduce the formulation of DFT from the standard Fourier transform (FT), which is originally designed for continuous signals. The FT converts a continuous signal from the time domain to the frequency domain and can be viewed as an extension of the Fourier series. Specifically, the Fourier transform of the signal x(t) is given by</p><formula xml:id="formula_9">X(j?) = ? ?? x(t)e ?j?t dt := F[x(t)]. (A.1)</formula><p>The inverse Fourier transform (IFT) has a similar form to the Fourier transform:</p><formula xml:id="formula_10">x(t) = 1 2? ? ?? X(j?)e j?t d?. (A.2)</formula><p>From the formulas of the FT and the IFT we can have a glimpse of the duality property of the FT between the time domain and the frequency domain. The duality indicates that the properties in the time domain always have their counterparts in the frequency domain. There are a variety of properties of Fourier transform. To name a few basic ones, the FT of a unit impulse function (a.k.a. Dirac delta function) is</p><formula xml:id="formula_11">F[?(t)] = ? ?? ?(t)e ?j?t dt = 0+ 0? ?(t)dt = 1, (A.3)</formula><p>and the time shifting property:</p><formula xml:id="formula_12">F[?(t ? t 0 )] = ? ?? x(t ? t 0 )e ?j?t dt = e ?j?t0 ? ?? x(t)e ?j?t dt = e ?j?t0 X(j?). (A.4)</formula><p>However, we rarely deal with continuous signal in the real application. A general practice is to perform sampling to the continuous signal to obtain a sequence of discrete signal. The sampling can be achieved using a sequence of unit impulse functions,</p><formula xml:id="formula_13">x s (t) = x(t) ? n=?? ?(t ? nT s ) = ? n=?? x(nT s )?(t ? nT s ), (A.5)</formula><p>where T s is the sampling interval. Taking the FT of the sampled signal x s (t) and applying Equation (A.3) and Equation (A.4), we have</p><formula xml:id="formula_14">X s (j?) = ? n=?? x(nT s )e ?j?nTs . (A.6)</formula><p>In the above equation, it is direct to show that X s (j?) is a periodic function with the fundamental period as 2?/T s . Actually, there is always a correspondence between the discrete signal in one domain and the periodic signal in the other domain. Usually, we prefer a normalized frequency ? ? ?T s such that the period of X s (j?) is exact 2?. We can further denote x[n] = x(nT s ) as the sequence of discrete signal and derive the discrete-time Fourier transform (DTFT):</p><p>X(e j? ) = where we assume the non-zero terms lie in [0, N ?1] without loss of generality. Note that the DTFT is a continuous function of ? and we can obtain a sequence of X[k] by sampling X(e j? ) at frequencies ? k = 2?k/N :</p><formula xml:id="formula_15">X[k] = X(e j? )| ?=2?k/N = N ?1 n=0 x[n]e ?j(2?/N )kn , (A.9)</formula><p>which is exactly the formulation of DFT. The extension from 1D DFT to 2D DFT is straightforward.</p><p>In fact, The 2D DFT can be viewed as performing 1D DFT on the two dimensions alternatively, i.e., the 2D DFT of x[m, n] is given by: For 2D signals, we have a similar result:</p><formula xml:id="formula_16">X[u, v] = M ?1 m=0 N ?1</formula><formula xml:id="formula_17">X[M ? u, N ? v] = M ?1 m=0 N ?1 n=0 x[m, n]e ?j2?( (M ?u)m M + (N ?v)n N ) = M ?1 m=0 N ?1 n=0 x[m, n]e j2?( um M + vn N ) = X * [u, v].</formula><p>(A.12)</p><p>In our GFNet, we leverage this property to reduce the number of learnable parameters and redundant computation. where the right hand is exactly the multiplication of the signal and the filter in the frequency domain. The convolution theorem in 2D scenario can be derived similarly. Therefore, our global filter layer is equivalent to a depth-wise circular convolution, where the filter has the same size as the feature map. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>The detailed architectures. To better compare with previous methods, we use the identical overall architecture to DeiT Samll <ref type="bibr" target="#b43">[43]</ref> and ResMLP-12 <ref type="bibr" target="#b42">[42]</ref> for GFNet-XS, where only the selfattention/MLP sub-layers, the final classifier and the residual connection are modified (using a single residual connection in each block will lead to 0.2% top-1 accuracy improvement on Ima-geNet for GFNet-XS). We set the number of layers and embedding dimension to {12, 19, 19} and {256, 384, 512} for GFNet-{Ti, S, B}, respectively. The architectures of our hierarchical models are shown in <ref type="table" target="#tab_7">Table 8</ref>. We use the similar strategy as ResNet <ref type="bibr" target="#b14">[14]</ref> to increase network depth where we fix the number of blocks for the stage 1,2,4 to 3 and adjust the number of blocks in stage 3. For small and base hierarchical models, we adopt the LayerScale normalization <ref type="bibr" target="#b45">[45]</ref> for more stable training. The high efficiency of our GFNet makes it possible to directly process a large feature map in the early stages (e.g., H/4 ? W/4) without introducing any handcraft structures like Swin <ref type="bibr" target="#b29">[29]</ref>.</p><p>Details about ImageNet experiments. We train our models for 300 epochs using the AdamW optimizer <ref type="bibr" target="#b30">[30]</ref>. We set the initial learning rate as batch size 1024 ? 0.001 and decay the learning rate to 1e ?5 using the cosine schedule. We use a linear warm-up learning rate in the first 5 epochs and apply gradient clipping to stabilize the training process. We set the stochastic depth coefficient <ref type="bibr" target="#b18">[18]</ref> to 0, 0, 0.15 and 0.25 for GFNet-Ti, GFNet-XS, GFNet-S and GFNet-B. For hierarchical models, we use the stochastic depth coefficient of 0.1, 0.2, and 0.4 for GFNet-H-Ti, GFNet-H-S, and GFNet-H-B. During finetuning at the higher resolution, we use the hyper-parameters suggested by the implementation of <ref type="bibr" target="#b43">[43]</ref> and train the model for 30 epochs with a learning rate of 5e ?6 and set the weight decay to 1e ?6 . We set the stochastic depth coefficient to 0.1 for GFNet-S and GFNet-B during finetuning.</p><p>Details about transfer learning experiments. We evaluate generality of learned representation of GFNet on a set of commonly used transfer learning benchmark datasets including CIFAR-10 <ref type="bibr" target="#b22">[22]</ref>, CIFAR-100 <ref type="bibr" target="#b22">[22]</ref>, Stanford Cars <ref type="bibr" target="#b21">[21]</ref> and Flowers-102 <ref type="bibr" target="#b33">[33]</ref>. We follow the setting of previous works <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b42">42]</ref>, where the model is initialized by the ImageNet pre-trained weights and finetuned on the new datasets. During finetuning, we use the AdamW optimizer and set the weight decay to 1e ?4 . We use batch size 512 and a smaller initial learning rate of 0.0001 with cosine decay. Linear learning rate warm-up in the first 5 epochs and gradient clipping with a max norm of 1 are also applied to stabilize the training. We keep most of the regularization methods unchanged except for removing stochastic depth following <ref type="bibr" target="#b43">[43]</ref>. For relatively larger datasets including CIFAR-10 and CIFAR-100, we train the model for 200 epochs. For other datasets, the model is trained for 1000 GFNet-S DeiT-S <ref type="figure">Figure 6</ref>: ImageNet accuracy of GFNet and DeiT <ref type="bibr" target="#b43">[43]</ref> when directly evaluated on different resolutions without fine-tuning. The GFNet can better adapt to various resolutions. <ref type="table">Table 9</ref>: Transfer learning datasets. We provide the training set size, test set size and the number of categories as references.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Train Size Test size #Categories CIFAR-10 <ref type="bibr" target="#b22">[22]</ref> 50,000 10,000 10 CIFAR-100 <ref type="bibr" target="#b22">[22]</ref> 50,000 10,000 100 Stanford Cars <ref type="bibr" target="#b21">[21]</ref> 8,144 8,041 196 Flowers-102 <ref type="bibr" target="#b33">[33]</ref> 2,040 6,149 102 <ref type="table">Table 10</ref>: Semantic segmentation results on ADE20K. We report the mIoU on the validation set. All models are equipped with Semantic FPN <ref type="bibr" target="#b20">[20]</ref> and trained for 80K iterations following <ref type="bibr" target="#b47">[47]</ref>. The FLOPs are tested with 1024 ? 1024 input. We compare the models that have similar computational costs and divide the models into three groups: 1) tiny models using ResNet-18, PVT-Ti and GFNet-H-Ti; 2) small models using ResNet-50, PVT-S, Swin-Ti and GFNet-H-S and 3) base models using ResNet-101, PVT-M, Swin-S and GFNet-H-B. epoch. Our models are trained and evaluated on commonly used splits following <ref type="bibr" target="#b40">[40]</ref>. The detailed splits are provided in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Results &amp; Analysis</head><p>Semantic segmentation. To show the potential of our models on dense prediction tasks, we evaluate our GFNet on ADE20K <ref type="bibr" target="#b55">[55]</ref>, a challenging semantic segmentation dataset that is commonly used to test vision transformers. We use the Semantic FPN framework <ref type="bibr" target="#b20">[20]</ref> and follow the experiment settings in PVT <ref type="bibr" target="#b47">[47]</ref>. We train our model for 80K steps with a batch size of 16 on the training set and report the mIoU on the validation set following common practice. We compare the performance and the computational costs of the GFNet series and other commonly used baselines in <ref type="table">Table 10</ref>.</p><p>To produce hierarchical feature maps, we adopt the GFNet-H series in the semantic segmentation experiments. We observe that our GFNet works well on the dense prediction task and can achieve very competitive performance in different levels of complexity.</p><p>Power distribution. We plot the power of the global filters on different frequency ranges of each layer in <ref type="figure" target="#fig_7">Figure 5</ref>, where we can have a clearer picture of how the global filters of different layers capture the information of different frequencies.</p><p>Directly adapting to other resolutions. As is discussed in Section 3.2, one of the advantages of GFNet is the ability to deal with arbitrary resolutions. To verify this, we directly evaluate GFNet-S trained with 224 ? 224 images on different resolutions (from 128 to 448). We plot the accuracy of GFNet-S and DeiT-S in <ref type="figure">Figure 6</ref> and find our GFNet can adapt to different resolutions with less performance drop than DeiT-S.</p><p>Filter visualization for hierarchical models. We also provide the visualization of the frequencydomain global filters for the hierarchical model GFNet-H-B in <ref type="figure" target="#fig_4">Figure 7</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 3 . 1 ) 1</head><label>311</label><figDesc>n=0 x[n]e ?j(2?/N )kn := N ?1 n=0 x[n]W kn N Algorithm Pseudocode of Global Filter Layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Comparisons among GFNet, ViT<ref type="bibr" target="#b10">[10]</ref> and ResMLP<ref type="bibr" target="#b42">[42]</ref> in (a) FLOPs (b) latency and (c) GPU memory with respect to the number of tokens (feature resolution). The dotted lines indicate the estimated values when the GPU memory has run out. The latency and GPU memory is measured using a single NVIDIA RTX 3090 GPU with batch size 32 and feature dimension 384. ImageNet acc. vs model complexity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the learned global filters in GFNet-XS. We visualize the original frequency domain global filters in (a) and show the corresponding spatial domain filters for the first 6 columns in (b). There are more clear patterns in the frequency domain than the spatial domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>7 )</head><label>7</label><figDesc>If the discrete signal x[n] has finite length N (which is common in digital signal processing), the DTFT becomesX(e j? ) = N ?1 n=0 x[n]e ?j?n , (A.8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>A. 2</head><label>2</label><figDesc>Some properties of DFT DFT of real signals. Given a real signal x[n], the DFT of it is conjugate symmetric, which can be proved as follows: X[N ? k] = N ?1 n=0 x[n]e ?j(2?/N )(N ?k)n = N ?1 n=0 x[n]e j(2?/N )kn = X * [k]. (A.11)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>1 n=0 N ? 1 m=0h</head><label>11</label><figDesc>The convolution theorem. One of the most important property of Fourier transform is the convolution theorem. Specifically, for the DFT, the convolutional theorem states that the multiplication in the frequency domain is equivalent to the circular convolutionin the time domain. The circular convolution of a signal x[n] and a filter h[n] can be defined as y[n] = N ?1 m=0 h[m]x[((n ? m)) N ], (A.13) where we use ((n)) N to denote n modulo N . Consider the DFT of y[n], we have Y [k] = N ?[m]x[((n ? m)) N ]e ?j(2?/N )kn = (n ? m)) N ]e ?j(2?/N )k(n?m) ? m]e ?j(2?/N )k(n?m) + m?1 n=0 x[n ? m + N ]e ?j(2?/N )k(n?m) ]e ?j(2?/N )kn = H[k] N ?1 n=0 x[n]e ?j(2?/N )kn = H[k]X[k], (A.14)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>[-/3, /3] u, v [-2 /3, -/3] [ /3, 2 /3] u, v [-, -2 /3] [2 /3, ]The average power on different frequency ranges of each layer. We can observe that the global filters of different layers focus on different frequencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of the learned global filters in GFNet-H-B. We visualize the frequency domain global filters from different stages with different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed configurations of different variants of GFNet. For hierarchical models, we provide the number of channels and blocks in 4 stages. The FLOPs are calculated with 224 ? 224 input.</figDesc><table><row><cell>Model</cell><cell>#Blocks</cell><cell>#Channels</cell><cell cols="2">Params (M) FLOPs (G)</cell></row><row><cell>GFNet-Ti</cell><cell>12</cell><cell>256</cell><cell>7</cell><cell>1.3</cell></row><row><cell>GFNet-XS</cell><cell>12</cell><cell>384</cell><cell>16</cell><cell>2.9</cell></row><row><cell>GFNet-S</cell><cell>19</cell><cell>384</cell><cell>25</cell><cell>4.5</cell></row><row><cell>GFNet-B</cell><cell>19</cell><cell>512</cell><cell>43</cell><cell>7.9</cell></row><row><cell>GFNet-H-Ti</cell><cell>[3, 3, 10, 3]</cell><cell>[64, 128, 256, 512]</cell><cell>15</cell><cell>2.1</cell></row><row><cell>GFNet-H-S</cell><cell>[3, 3, 10, 3]</cell><cell>[96, 192, 384, 768]</cell><cell>32</cell><cell>4.6</cell></row><row><cell>GFNet-H-B</cell><cell>[3, 3, 27, 3]</cell><cell>[96, 192, 384, 768]</cell><cell>54</cell><cell>8.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with transformer-style architectures on ImageNet. We compare different transformer-style architectures for image classification including vision transformers<ref type="bibr" target="#b43">[43]</ref>, MLP-like models<ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b28">28]</ref> and our models that have comparable FLOPs and the number of parameters. We report the top-1 accuracy on the validation set of ImageNet as well as the number of parameters and FLOPs. All of our models are trained with 224 ? 224 images. We use "?384" to represent models finetuned on 384 ? 384 images for 30 epochs.</figDesc><table><row><cell>Model</cell><cell cols="5">Params (M) FLOPs (G) Resolution Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>DeiT-Ti [43]</cell><cell>5</cell><cell>1.2</cell><cell>224</cell><cell>72.2</cell><cell>91.1</cell></row><row><cell>gMLP-Ti [28]</cell><cell>6</cell><cell>1.4</cell><cell>224</cell><cell>72.0</cell><cell>-</cell></row><row><cell>GFNet-Ti</cell><cell>7</cell><cell>1.3</cell><cell>224</cell><cell>74.6</cell><cell>92.2</cell></row><row><cell>ResMLP-12 [42]</cell><cell>15</cell><cell>3.0</cell><cell>224</cell><cell>76.6</cell><cell>-</cell></row><row><cell>GFNet-XS</cell><cell>16</cell><cell>2.9</cell><cell>224</cell><cell>78.6</cell><cell>94.2</cell></row><row><cell>DeiT-S [43]</cell><cell>22</cell><cell>4.6</cell><cell>224</cell><cell>79.8</cell><cell>95.0</cell></row><row><cell>gMLP-S [28]</cell><cell>20</cell><cell>4.5</cell><cell>224</cell><cell>79.4</cell><cell>-</cell></row><row><cell>GFNet-S</cell><cell>25</cell><cell>4.5</cell><cell>224</cell><cell>80.0</cell><cell>94.9</cell></row><row><cell>ResMLP-36 [42]</cell><cell>45</cell><cell>8.9</cell><cell>224</cell><cell>79.7</cell><cell>-</cell></row><row><cell>GFNet-B</cell><cell>43</cell><cell>7.9</cell><cell>224</cell><cell>80.7</cell><cell>95.1</cell></row><row><cell>GFNet-XS?384</cell><cell>18</cell><cell>8.4</cell><cell>384</cell><cell>80.6</cell><cell>95.4</cell></row><row><cell>DeiT-B [43]</cell><cell>86</cell><cell>17.5</cell><cell>224</cell><cell>81.8</cell><cell>95.6</cell></row><row><cell>gMLP-B [28]</cell><cell>73</cell><cell>15.8</cell><cell>224</cell><cell>81.6</cell><cell>-</cell></row><row><cell>GFNet-S?384</cell><cell>28</cell><cell>13.2</cell><cell>384</cell><cell>81.7</cell><cell>95.8</cell></row><row><cell>GFNet-B?384</cell><cell>47</cell><cell>23.3</cell><cell>384</cell><cell>82.1</cell><cell>95.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Model</cell><cell cols="4">Params (M) FLOPs (G) Top-1 Acc. (%) Top-5 Acc. (%)</cell></row><row><cell>ResNet-18 [14]</cell><cell>12</cell><cell>1.8</cell><cell>69.8</cell><cell>89.1</cell></row><row><cell>RegNetY-1.6GF [37]</cell><cell>11</cell><cell>1.6</cell><cell>78.0</cell><cell>-</cell></row><row><cell>PVT-Ti [28]</cell><cell>13</cell><cell>1.9</cell><cell>75.1</cell><cell>-</cell></row><row><cell>GFNet-H-Ti</cell><cell>15</cell><cell>2.1</cell><cell>80.1</cell><cell>95.1</cell></row><row><cell>ResNet-50 [43]</cell><cell>26</cell><cell>4.1</cell><cell>76.1</cell><cell>92.9</cell></row><row><cell>RegNetY-4.0GF [37]</cell><cell>21</cell><cell>4.0</cell><cell>80.0</cell><cell>-</cell></row><row><cell>PVT-S [28]</cell><cell>25</cell><cell>3.8</cell><cell>79.8</cell><cell>-</cell></row><row><cell>Swin-Ti [29]</cell><cell>29</cell><cell>4.5</cell><cell>81.3</cell><cell>-</cell></row><row><cell>GFNet-H-S</cell><cell>32</cell><cell>4.6</cell><cell>81.5</cell><cell>95.6</cell></row><row><cell>ResNet-101 [43]</cell><cell>45</cell><cell>7.9</cell><cell>77.4</cell><cell>93.5</cell></row><row><cell>RegNetY-8.0GF [37]</cell><cell>39</cell><cell>8.0</cell><cell>81.7</cell><cell>-</cell></row><row><cell>PVT-M [28]</cell><cell>44</cell><cell>6.7</cell><cell>81.2</cell><cell>-</cell></row><row><cell>Swin-S [29]</cell><cell>50</cell><cell>8.7</cell><cell>83.0</cell><cell>-</cell></row><row><cell>GFNet-H-B</cell><cell>54</cell><cell>8.6</cell><cell>82.9</cell><cell>96.2</cell></row><row><cell>repeated augmentation</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Comparisons with hierarchical architectures on ImageNet. We compare different hi- erarchical architectures for image classification including convolutional neural networks [14, 37], hierarchical vision transformers [47, 29] and our hierarchical models that have comparable FLOPs and number of parameters. We report the top-1 accuracy on the validation set of ImageNet as well as the number of parameters and FLOPs. All models are trained and tested with 224 ? 224 images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on transfer learning datasets. We report the top-1 accuracy on the four datasets as well as the number of parameters and FLOPs.</figDesc><table><row><cell>Model</cell><cell cols="2">FLOPs Params</cell><cell>CIFAR-10</cell><cell cols="2">CIFAR-100 Flowers-102</cell><cell>Cars-196</cell></row><row><cell>ResNet50 [14]</cell><cell>4.1G</cell><cell>26M</cell><cell>-</cell><cell>-</cell><cell>96.2</cell><cell>90.0</cell></row><row><cell>EfficientNet-B7 [40]</cell><cell>37G</cell><cell>66M</cell><cell>98.9</cell><cell>91.7</cell><cell>98.8</cell><cell>94.7</cell></row><row><cell>ViT-B/16 [10]</cell><cell>55.4G</cell><cell>86M</cell><cell>98.1</cell><cell>87.1</cell><cell>89.5</cell><cell>-</cell></row><row><cell>ViT-L/16 [10]</cell><cell>190.7G</cell><cell>307M</cell><cell>97.9</cell><cell>86.4</cell><cell>89.7</cell><cell>-</cell></row><row><cell>Deit-B/16 [43]</cell><cell>17.5G</cell><cell>86M</cell><cell>99.1</cell><cell>90.8</cell><cell>98.4</cell><cell>92.1</cell></row><row><cell>ResMLP-12 [42]</cell><cell>3.0G</cell><cell>15M</cell><cell>98.1</cell><cell>87.0</cell><cell>97.4</cell><cell>84.6</cell></row><row><cell>ResMLP-24 [42]</cell><cell>6.0G</cell><cell>30M</cell><cell>98.7</cell><cell>89.5</cell><cell>97.9</cell><cell>89.5</cell></row><row><cell>GFNet-XS</cell><cell>2.9G</cell><cell>16M</cell><cell>98.6</cell><cell>89.1</cell><cell>98.1</cell><cell>92.8</cell></row><row><cell>GFNet-H-B</cell><cell>8.6G</cell><cell>54M</cell><cell>99.0</cell><cell>90.3</cell><cell>98.8</cell><cell>93.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Comparisons among the GFNet and other</figDesc><table><row><cell cols="4">variants based on the transformer-like architecture</cell></row><row><cell cols="4">on ImageNet. We show that GFNet outperforms the</cell></row><row><cell cols="4">ResMLP [42], FNet [25] and models with local depth-</cell></row><row><cell cols="4">wise convolutions. We also report the number of pa-</cell></row><row><cell cols="4">rameters and theoretical complexity in FLOPs.</cell></row><row><cell>Model</cell><cell cols="3">Acc Param FLOPs (%) (M) (G)</cell></row><row><cell>DeiT-S [43]</cell><cell>79.8</cell><cell>22</cell><cell>4.6</cell></row><row><cell cols="2">Local Conv (3 ? 3) 77.7</cell><cell>15</cell><cell>2.8</cell></row><row><cell cols="2">Local Conv (5 ? 5) 78.1</cell><cell>15</cell><cell>2.9</cell></row><row><cell cols="2">Local Conv (7 ? 7) 78.2</cell><cell>15</cell><cell>2.9</cell></row><row><cell>ResMLP [42]</cell><cell>76.6</cell><cell>15</cell><cell>3.0</cell></row><row><cell>FNet [25]</cell><cell>71.2</cell><cell>15</cell><cell>2.9</cell></row><row><cell>GFNet-XS</cell><cell>78.6</cell><cell>16</cell><cell>2.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>The detailed architectures of hierarchical GFNet variants. We adopt hierarchical architectures where the we use patch embedding layer to perform downsampling. "?n" indicates the stride of the downsampling is n. "GFBlock(D)" represents one building block of GFNet with embedding dimension D. We set the MLP expansion ratio to 4 for all the feedforward networks.</figDesc><table><row><cell></cell><cell cols="3">Output Size</cell><cell>GFNet-H-Ti</cell><cell>GFNet-H-S</cell><cell>GFNet-H-B</cell></row><row><cell>Stage1</cell><cell>H 4</cell><cell>?</cell><cell>W 4</cell><cell>Patch Embedding?4 GFBlock(64) ? 3</cell><cell>Patch Embedding?4 GFBlock(96) ? 3</cell><cell>Patch Embedding?4 GFBlock(96) ? 3</cell></row><row><cell>Stage2</cell><cell>H 8</cell><cell>?</cell><cell>W 8</cell><cell>Patch Embedding?2 GFBlock(128) ? 3</cell><cell>Patch Embedding?2 GFBlock(192) ? 3</cell><cell>Patch Embedding?2 GFBlock(192) ? 3</cell></row><row><cell>Stage3</cell><cell>H 16</cell><cell>?</cell><cell>W 16</cell><cell cols="3">Patch Embedding?2 GFBlock(256) ? 10 GFBlock(384) ? 10 GFBlock(384) ? 27 Patch Embedding?2 Patch Embedding?2</cell></row><row><cell>Stage4</cell><cell>H 32</cell><cell>?</cell><cell>W 32</cell><cell>Patch Embedding?2 GFBlock(512) ? 3</cell><cell>Patch Embedding?2 GFBlock(768) ? 3</cell><cell>Patch Embedding?2 GFBlock(768) ? 3</cell></row><row><cell>Classifier</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Global Average Pooling, Linear</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Digital image processing: principles and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baxes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den Oord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Are we done with imagenet</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<idno>2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transformer tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadong</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fast fourier convolution. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An algorithm for the machine calculation of complex fourier series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Cooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tukey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">90</biblScope>
			<biblScope unit="page" from="297" to="301" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Circnn: accelerating and compressing deep neural networks using block-circulant weight matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youwei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="395" to="408" />
			<pubPlace>MICRO</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improve vision transformers training by suppressing over-smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12753</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Augment your batch: Improving generalization through instance repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8129" to="8138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Token labeling: Training a 85</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
	</analytic>
	<monogr>
		<title level="m">5% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Imagenet classification with deep convolutional neural networks. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Han</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhyeok</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung-Rae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">Fnet: Mixing tokens with fourier transforms</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Falcon: A fourier transform based approach for fast and secure convolutional neural network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiping</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenkai</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8705" to="8714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fourier neural operator for parametric partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Kovachki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burigede</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaushik</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking the design principles of robust vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gege</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuefeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaokai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07926</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICVGIP</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Digital image processing algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Pitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Resmlp: Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fda: Fourier domain adaptation for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="4085" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pointr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
