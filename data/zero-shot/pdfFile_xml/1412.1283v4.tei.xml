<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Feature Masking for Joint Object and Stuff Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>jifdai@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<email>kahe@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>jiansun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Feature Masking for Joint Object and Stuff Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The topic of semantic segmentation has witnessed considerable progress due to the powerful features learned by convolutional neural networks (CNNs) <ref type="bibr" target="#b12">[13]</ref>. The current leading approaches for semantic segmentation exploit shape information by extracting CNN features from masked image regions. This strategy introduces artificial boundaries on the images and may impact the quality of the extracted features. Besides, the operations on the raw image domain require to compute thousands of networks on a single image, which is time-consuming.</p><p>In this paper, we propose to exploit shape information via masking convolutional features. The proposal segments (e.g., super-pixels) are treated as masks on the convolutional feature maps. The CNN features of segments are directly masked out from these maps and used to train classifiers for recognition. We further propose a joint method to handle objects and "stuff" (e.g., grass, sky, water) in the same framework. State-of-the-art results are demonstrated on benchmarks of PASCAL VOC and new PASCAL-CONTEXT, with a compelling computational speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b1">2]</ref> aims to label each image pixel to a semantic category. With the recent breakthroughs <ref type="bibr" target="#b12">[13]</ref> by convolutional neural networks (CNNs) <ref type="bibr" target="#b14">[15]</ref>, R-CNN based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> for semantic segmentation have substantially advanced the state of the art.</p><p>The R-CNN methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> for semantic segmentation extract two types of CNN features -one is region features <ref type="bibr" target="#b7">[8]</ref> extracted from proposal bounding boxes <ref type="bibr" target="#b21">[22]</ref>; the other is segment features extracted from the raw image content masked by the segments <ref type="bibr" target="#b9">[10]</ref>. The concatenation of these features are used to train classifiers <ref type="bibr" target="#b9">[10]</ref>. These methods have demonstrated compelling results on this long-standing challenging task.</p><p>However, the raw-image-based R-CNN methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b9">10]</ref> have two issues. First, the masks on the image content can lead to artificial boundaries. These boundaries do not exhibit on the samples during the network pre-training (e.g., in the 1000-category ImageNet <ref type="bibr" target="#b4">[5]</ref>). This issue may degrade the quality of the extracted segment features. Second, similar to the R-CNN method for object detection <ref type="bibr" target="#b7">[8]</ref>, these methods need to apply the network on thousands of raw image regions with/without the masks. This is very timeconsuming even on high-end GPUs.</p><p>The second issue also exists in R-CNN based object detection. Fortunately, this issue can be largely addressed by a recent method called SPP-Net <ref type="bibr" target="#b10">[11]</ref>, which computes convolutional feature maps on the entire image only once and applies a spatial pyramid pooling (SPP) strategy to form cropped features for classification. The detection results via these cropped features have shown competitive detection accuracy <ref type="bibr" target="#b10">[11]</ref>, and the speed can be ?50? faster. Therefore, in this paper, we raise a question: for semantic segmentation, can we use the convolutional feature maps only?</p><p>The first part of this work says yes to this question. We design a convolutional feature masking (CFM) method to extract segment features directly from feature maps instead of raw images. With the segments given by the region proposal methods (e.g., selective search <ref type="bibr" target="#b21">[22]</ref>), we project them to the domain of the last convolutional feature maps. The projected segments play as binary functions for masking the convolutional features. The masked features are then fed into the fully-connected layers for recognition. Because the convolutional features are computed from the unmasked image, their quality is not impacted. Besides, this method is efficient as the convolutional feature maps only need to be computed once. The aforementioned two issues involving semantic segmentation are thus both addressed. <ref type="figure" target="#fig_0">Figure 1</ref> compares the raw-image-based pipeline and our featuremap-based pipeline.</p><p>The second part of this paper further generalizes our method for joint object and stuff segmentation <ref type="bibr" target="#b17">[18]</ref>. Different from objects, "stuff" <ref type="bibr" target="#b17">[18]</ref> (e.g., sky, grass, water) is usually treated as the context in the image. Stuff mostly exhibits as colors or textures and has less well-defined shapes. It is thus inappropriate to use a single rectangular box or a single segment to represent stuff. Based on our masked  <ref type="bibr" target="#b7">[8]</ref> and "Simultaneous Detection and Segmentation" (SDS) <ref type="bibr" target="#b9">[10]</ref> that operate on the raw image domain. Bottom: our method that masks the convolutional feature maps.</p><p>convolutional features, we propose a training procedure that treats a stuff as a compact combination of multiple segment features. This allows us to address the object and stuff in the same framework. Based on the above methods, we show state-of-the-art results on the PASCAL VOC 2012 benchmark <ref type="bibr" target="#b6">[7]</ref> for object segmentation. Our method can process an image in a fraction of a second, which is ?150? faster than the R-CNNbased SDS method <ref type="bibr" target="#b9">[10]</ref>. Further, our method is also the first deep-learning-based method ever applied to the newly labeled PASCAL-CONTEXT benchmark <ref type="bibr" target="#b17">[18]</ref> for both object and stuff segmentation, where our result substantially outperforms previous states of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Convolutional Feature Masking</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Convolutional Feature Masking Layer</head><p>The power of CNNs as a generic feature extractor has been gradually revealed in the computer vision area <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>. In Krizhevsky et al.'s work <ref type="bibr" target="#b12">[13]</ref>, they suggest that the features of the fully-connected layers can be used as holistic image features, e.g., for image retrieval. In <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>, these holistic features are used as generic features for full-image classification tasks in other datasets via transfer learning. In the breakthrough object detection paper of R-CNN <ref type="bibr" target="#b7">[8]</ref>, the CNN features are also used like holistic features, but are extracted from sub-images which are the crops of raw images. In the CNN-based semantic segmentation paper <ref type="bibr" target="#b9">[10]</ref>, the R-CNN idea is generalized to masked raw image regions. For all these methods, the entire network is treated as a holistic feature extractor, either on the entire image or on sub-images.</p><p>In the recent work of SPP-Net <ref type="bibr" target="#b10">[11]</ref>, it shows that the convolutional feature maps can be used as localized features. On a full-image convolutional feature map, the local rectangular regions encode both the semantic information (by strengths of activations) and spatial information (by positions). The features from these local regions can be pooled <ref type="bibr" target="#b10">[11]</ref> directly for recognition.</p><p>The spatial pyramid pooling (SPP) in <ref type="bibr" target="#b10">[11]</ref> actually plays two roles: 1) masking the feature maps by a rectangular region, outside which the activations are removed; 2) generating a fixed-length feature from this arbitrary sized region. So, if masking by rectangles can be effective, what if we mask the feature maps by a fine segment with an irregular shape?</p><p>The Convolutional Feature Masking (CFM) layer is thus developed. We first obtain the candidate segments (like super-pixels) on the raw image. Many regional proposal methods (e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b0">1]</ref>) are based on super-pixels. Each proposal box is given by grouping a few super-pixels. We call such a group as a segment proposal. So we can obtain the candidate segments together with their proposal boxes (referred to as "regions" in this paper) without extra effort. These segments are binary masks on the raw images.</p><p>Next we project these binary masks to the domain of the last convolutional feature maps. Because each activation in the convolutional feature maps is contributed by a receptive field in the image domain, we first project each activation onto the image domain as the center of its receptive field (following the details in <ref type="bibr" target="#b10">[11]</ref>). Each pixel in the binary masks on the image is assigned to its nearest center of the receptive fields. Then these pixels are projected back onto  the convolutional feature map domain based on this center and its activation's position. On the feature map, each position will collect multiple pixels projected from a binary mask. These binary values are then averaged and thresholded (by 0.5). This gives us a mask on the feature maps ( <ref type="figure" target="#fig_2">Figure 2</ref>). This mask is then applied on the convolutional feature maps. Actually, we only need to multiply this binary mask on each channel of the feature maps. We call the resulting features as segment features in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Network Designs</head><p>In <ref type="bibr" target="#b9">[10]</ref>, it has been shown that the segment features alone are insufficient. These segment features should be used together with the regional features (from bounding boxes) generated in a way like R-CNN <ref type="bibr" target="#b7">[8]</ref>. Based on our CFM layer, we can have two possible ways of doing this.</p><p>Design A: on the last convolutional layer. As shown in <ref type="figure" target="#fig_3">Figure 3</ref> (left part), after the last convolutional layer, we generate two sources of features. One is the regional feature produced by the SPP layer as in <ref type="bibr" target="#b10">[11]</ref>. The other is the segment feature produced in the following way. The CFM layer is applied on the full-image convolutional feature map. This gives us an arbitrary-sized (in terms of its bounding box) segment feature. Then we use another SPP layer on this feature to produce a fixed-length output. The two pooled features are fed into two separate fc layers. The features of the last fc layers are concatenated to train a classifier, as is the classifier in <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this design, we have two pathways of the fc layers in both training and testing.</p><p>Design B: on the spatial pyramid pooling layer. We first adopt the SPP layer <ref type="bibr" target="#b10">[11]</ref> to pool the features. We use a 4level pyramid of {6 ? 6, 3 ? 3, 2 ? 2, 1 ? 1} as in <ref type="bibr" target="#b10">[11]</ref>. The 6 ? 6 level is actually a 6 ? 6 tiny feature map that still has plenty spatial information. We apply the CFM layer on this tiny feature map to produce the segment feature. This feature is then concatenated with the other three levels and fed onto the fc layers, as shown in <ref type="figure" target="#fig_3">Figure 3</ref> (right).</p><p>In this design, we keep one pathway of the fc layers to reduce the computational cost and over-fitting risk.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training and Inference</head><p>Based on these two designs and the CFM layer, the training and inference stages can be easily conducted following the common practices in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>. In both stages, we use the region proposal algorithm (e.g., selective search <ref type="bibr" target="#b21">[22]</ref>) to generate about 2,000 region proposals and associated segments. The input image is resized to multiple scales (the shorter edge s ? {480, 576, 688, 864, 1200}) <ref type="bibr" target="#b10">[11]</ref>, and the convolutional feature maps are extracted from full images and then fixed (not further tuned).</p><p>Training. We first apply the SPP method [11] 1 to finetune a network for object detection. Then we replace the finetuned network with the architecture as in Design A or B, and further finetune the network for segmentation. In the second fine-tuning step, the segment proposal overlapping a ground-truth foreground segment by [0.5, 1] is considered as positive, and [0.1, 0.3] as negative. The overlap is measured by intersection-over-union (IoU) score based on the two segments' areas (rather than their bounding boxes). After fine-tuning, we train a linear SVM classifier on the network output, for each category. In the SVM training, only the ground-truth segments are used as positive samples.</p><p>Inference. Each region proposal is assigned to a proper scale as in <ref type="bibr" target="#b10">[11]</ref>. The features of each region and its associated segment are extracted as in Design A or B. The SVM classifier is used to score each region.</p><p>Given all the scored region proposals, we obtain the pixel-level category labeling by the pasting scheme in SDS <ref type="bibr" target="#b9">[10]</ref>. This pasting scheme sequentially selects the region proposal with the highest score, performs region refinement, inhibits overlapping proposals, and pastes the pixel labels onto the labeling result. Region refinement improves the accuracy by about 1% on PASCAL VOC 2012 for both SDS and our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Results on Object Segmentation</head><p>We evaluate our method on the PASCAL VOC 2012 semantic segmentation benchmark <ref type="bibr" target="#b6">[7]</ref> that has 20 object categories. We follow the "comp6" evaluation protocol, which is also used in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. The training set of PASCAL VOC 2012 and the additional segmentation annotations from <ref type="bibr" target="#b8">[9]</ref> are used for training and evaluation as in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. Two scenarios are studied: semantic segmentation and simultaneous detection and segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario I: Semantic Segmentation</head><p>In the experiments of semantic segmentation, category labels are assigned to all the pixels in the image, and the accuracy is measured by region IoU scores <ref type="bibr" target="#b6">[7]</ref>. We first study using the "ZF SPPnet" model <ref type="bibr" target="#b10">[11]</ref> as our feature extractor. This model is based on Zeiler and Fergus's fast model <ref type="bibr" target="#b24">[25]</ref> but with the SPP layer <ref type="bibr" target="#b10">[11]</ref>. It has five convolutional layers and three fc layers. This model is released with the code of <ref type="bibr" target="#b10">[11]</ref>. We note that the results in R-CNN <ref type="bibr" target="#b7">[8]</ref> and SDS <ref type="bibr" target="#b9">[10]</ref> use the "AlexNet" <ref type="bibr" target="#b12">[13]</ref> instead. To understand the impacts of the pre-trained models, we report their object detection mAP on the val set of PASCAL VOC 2012: SPP-Net (ZF) is 51.3%, R-CNN (AlexNet) is 51.0%, and SDS (AlexNet) is 51.9%. This means that both pre-trained models are comparable as generic feature extractors. So the following gains of CFM are not simply due to pre-trained models.</p><p>To show the effect of the CFM layer, we present a baseline with no CFM -in our Design B, we remove the CFM layer but still use the same entire pipeline. We term this baseline as the "no-CFM" version of our method. Actually, this baseline degrades to the original SPP-net usage <ref type="bibr" target="#b10">[11]</ref>, except that the definitions of positive/negative samples are for segmentation. <ref type="table" target="#tab_0">Table 1</ref> compares the results of no-CFM and the two designs of CFM. We find that the CFM has obvious advantages over the no-CFM baseline. This is as expected, because the no-CFM baseline has not any segmentbased feature. Further, we find that the designs A and B perform just comparably, while A needs to compute two pathways of the fc layers. So in the rest of this paper, we adopt Design B for ZF SPPnet.</p><p>In <ref type="table" target="#tab_1">Table 2</ref> we evaluate our method using different region proposal algorithms. We adopt two proposal algorithms: Selective Search (SS) <ref type="bibr" target="#b21">[22]</ref>, and Multiscale Combinatorial Grouping (MCG) <ref type="bibr" target="#b0">[1]</ref>. Following the protocol in <ref type="bibr" target="#b9">[10]</ref>, the "fast" mode is used for SS, and the "accurate" mode is used for MCG. <ref type="table" target="#tab_1">Table 2</ref> shows that our method achieves higher accuracy on the MCG proposals. This indicates that our feature masking method can exploit the information generated by more accurate segmentation proposals.     In <ref type="table" target="#tab_1">Table 2</ref> we also evaluate the impact of pre-trained networks. We compare the ZF SPPnet with the public VGG-16 model <ref type="bibr" target="#b19">[20]</ref> 2 . Recent advances in image classification have shown that very deep networks <ref type="bibr" target="#b19">[20]</ref> can significantly improve the classification accuracy. The VGG-16 model has 13 convolutional and 3 fc layers. Because this model has no SPP layer, we consider its last pooling layer (7?7) as a special SPP layer which has a single-level pyramid of {7 ? 7}. In this case, our Design B does not apply because there is no coarser level. So we apply our Design A instead. <ref type="table" target="#tab_1">Table  2</ref> shows that our results improve substantially when using the VGG net. This indicates that our method benefits from the more representative features learned by deeper models.  <ref type="table" target="#tab_2">Table 3</ref> we evaluate the impact of image scales. Instead of using the 5 scales, we simply extract features from single-scale images whose shorter side is s = 576. <ref type="table" target="#tab_2">Table 3</ref> shows that our single-scale variant has negligible degradation. But the single-scale variant has a faster computational speed as in <ref type="table" target="#tab_3">Table 4</ref>.</p><p>Next we compare with the state-of-the-art results on the PASCAL VOC 2012 test set in <ref type="table" target="#tab_5">Table 5</ref>. Here SDS <ref type="bibr" target="#b9">[10]</ref> is the previous state-of-the-art method on this task, and O 2 P [4] is a leading non-CNN-based method. Our method with ZF SPPnet and MCG achieves a score of 55.4. This is 3.8% higher than the SDS result reported in <ref type="bibr" target="#b9">[10]</ref> which uses AlexNet and MCG. This demonstrates that our CFM method can produce effective features without masking raw-pixel images. With the VGG net, our method has a score of 61.8 on the test set.</p><p>Besides the high accuracy, our method is much faster than SDS. The running time of the feature extraction steps in SDS and our method is shown in <ref type="table" target="#tab_3">Table 4</ref>. Both approaches are run on an Nvidia GTX Titan GPU based on the Caffe library <ref type="bibr" target="#b11">[12]</ref>. The time is averaged over 100 random images from PASCAL VOC. Using 5 scales, our method with ZF SPPnet is ? 47? faster than SDS; using 1 scale, our method with ZF SPPnet is ?150? faster than SDS and is more accurate. The speed gain is because our method only needs to compute the feature maps once. <ref type="table" target="#tab_3">Table 4</ref> also shows that our method is still feasible using the VGG net.</p><p>Concurrent with our work, a Fully Convolutional Network (FCN) method <ref type="bibr" target="#b15">[16]</ref> is proposed for semantic segmentation. It has a score (62.2 on test set) comparable with our method, and has a fast speed as it also performs convolutions once on the entire image. But FCN is not able to generate instance-wise results, which is another metric evaluated in <ref type="bibr" target="#b9">[10]</ref>. Our method is also applicable in this case, as evaluated below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario II: Simultaneous Detection and Segmentation</head><p>In the evaluation protocol of simultaneous detection and segmentation <ref type="bibr" target="#b9">[10]</ref>, all the object instances and their segmentation masks are labeled. In contrast to semantic segmentation, this scenario further requires to identify different object instances in addition to labeling pixel-wise semantic categories. The accuracy is measured by mean AP r score defined in <ref type="bibr" target="#b9">[10]</ref>.</p><p>We report the mean AP r results on VOC 2012 validation set following <ref type="bibr" target="#b9">[10]</ref>, as the ground-truth labels for the test set are not available. As shown in <ref type="table" target="#tab_6">Table 6</ref>, our method has a mean AP r of 53.2 when using ZF SPPnet and MCG. This is better than the SDS result (49.7) reported in <ref type="bibr" target="#b9">[10]</ref>. With the VGG net, our mean AP r is 60.7, which is the state-of-theart result reported in this task. Note that the FCN method <ref type="bibr" target="#b15">[16]</ref> is not applicable when evaluating the mean AP r metric,   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Joint Object and Stuff Segmentation</head><p>The semantic categories in natural images can be roughly divided into objects and stuff. Objects have consistent shapes and each instance is countable, while stuff has consistent colors or textures and exhibits as arbitrary shapes, e.g., grass, sky, and water. So unlike an object, a stuff region is not appropriate to be represented as a rectangular region or a bounding box. While our method can generate segment features, each segment is still associated with a bounding box due to its way of generation. When the region/segment proposals are provided, it is rarely that the stuff can be fully covered by a single segment. Even if the stuff is covered by a single rectangular region, it is almost certain that there are many pixels in this region that do not belong to the stuff. So stuff segmentation has issues different from object segmentation.</p><p>Next we show a generalization of our framework to address this issue involving stuff. We can simultaneously handle objects and stuff by a single solution. Especially, the convolutional feature maps need only to be computed once. So there will be little extra cost if the algorithm is required to further handle stuff.</p><p>Our generalization is to modify the underlying probabilistic distributions of the samples during training. Instead of treating the samples equally, our training will bias toward the proposals that can cover the stuff as compact as possible (discussed below). A Segment Pursuit procedure is proposed to find the compact proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stuff Representation by Segment Combination</head><p>We treat stuff as a combination of multiple segment proposals. We expect that each segment proposal can cover a stuff portion as much as possible, and a stuff can be fully covered by several segment proposals. At the same time, we hope the combination of these segment proposals is compact -the fewer the segments, the better.</p><p>We first define a candidate set of segment proposals (in a single image) for stuff segmentation. We define a "purity score" as the IoU ratio between a segment proposal and the stuff portion that is within the bounding box of this segment. Among all the segment proposals in a single image, those having high purity scores (&gt; 0.6) with stuff consist of the candidate set for potential combinations.</p><p>To generate one compact combination from this candidate set, we adopt a procedure similar to the matching pursuit <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17]</ref>. We sequentially pick segments from the candidate set without replacement. At each step, the largest segment proposal is selected. This selected proposal then inhibits its highly overlapped proposals in the candidate set (they will not be selected afterward). In this paper, the inhibition overlap threshold is set as IoU=0.2. The process is repeated till the remaining segments all have areas smaller than a threshold, which is the average of the segment areas in the initial candidate set (of that image). We call this procedure segment pursuit. <ref type="figure" target="#fig_7">Figure 4 (b)</ref> shows an example if segment proposals are randomly sampled from the candidate set. We see that there are many small segments. It is harmful to define these small, less discriminative segments as either positive or negative samples (e.g., by IoU) -if they are positive, they are just a very small part of the stuff; if they are negative, they share the same textures/colors as a larger portion of the stuff. So we prefer to ignore these samples in the training, so the classifier will not bias toward any side about these small samples. <ref type="figure" target="#fig_7">Figure 4 (c)</ref> shows the segment proposals selected by segment pursuit. We see that they can cover the stuff (grass here) by only a few but large segments. We expect the solver to rely more on such a compact combination of proposals.</p><p>However, the above process is deterministic and can only give a small set of samples from each image. For example, in <ref type="figure" target="#fig_7">Figure 4 (c</ref>   give a small set of samples from each image. For example, in <ref type="figure" target="#fig_7">Figure 4</ref> (c) it only provides 5 segment proposals. In the fine-tuning process, we need a large number of stochastic samples for the training. So we inject randomness into the above segment pursuit procedure. In each step, we randomly sample a segment proposal from the candidate set, rather than using the largest. The picking probability is proportional to the area size of segment (so a larger one is still preferred). This can give us another "compact" combination in a stochastic way. <ref type="figure" target="#fig_7">Figure 4 (d)</ref> shows an example of the segment proposals generated in a few trials of this way. All the segment proposals given by this way are considered as the positive samples of a category of stuff. The negative samples are the segment proposals whose purity scores are below 0.3. These samples can then be used for fine-tuning and SVM training as detailed below.</p><p>During the fine-tuning stage, in each epoch each image generates a stochastic "compact" combination. All the segment proposals in this combination for all images consist of the samples of this epoch. These samples are randomly permuted and fed into the SGD solver. Although now the samples appear mutually independent to the SGD solver, they are actually sampled jointly by the rule of segment pursuit. Their underlying probabilistic distributions will impact the SGD solver. This process is repeated for each epoch. For the SGD solver, we halt the training process after 200k mini-batches. For the SVM training stage, we only use the single combination given by the deterministic segment pursuit.</p><p>Using this way, we can treat object+stuff segmentation in the same framework as for object-only. The only difference is that the stuff samples are provided in a way given by segment pursuit, rather than purely randomly. To bal- the fine-tuning process, we need a large number of stochastic samples for the training. So we inject randomness into the above segment pursuit procedure. In each step, we randomly sample a segment proposal from the candidate set, rather than using the largest. The picking probability is proportional to the area size of a segment (so a larger one is still preferred). This can give us another compact combination in a stochastic way. <ref type="figure" target="#fig_7">Figure 4 (d)</ref> shows an example of the segment proposals generated in a few trials.</p><p>All the segment proposals given by this way are considered as the positive samples of a category of stuff. The negative samples are the segment proposals whose purity scores are below 0.3. These samples can then be used for fine-tuning and SVM training as detailed below.</p><p>During the fine-tuning stage, in each epoch each image generates a stochastic compact combination. All the segment proposals in this combination for all images consist of the samples of this epoch. These samples are randomly permuted and fed into the SGD solver. Although now the samples appear mutually independent to the SGD solver, they are actually sampled jointly by the rule of segment pursuit. Their underlying probabilistic distributions will impact the SGD solver. This process is repeated for each epoch. For the SGD solver, we halt the training process after 200k mini-batches. For SVM training, we only use the single combination given by the deterministic segment pursuit.</p><p>Using this way, we can treat object+stuff segmentation in the same framework as for object-only. The only difference is that the stuff samples are provided in a way given by segment pursuit, rather than purely randomly. To balance different categories, the portions of objects, stuff, and   <ref type="bibr" target="#b17">[18]</ref>. The categories marked by ? are the 33 easier categories identified in <ref type="bibr" target="#b17">[18]</ref>. The results of SuperParsing <ref type="bibr" target="#b20">[21]</ref> and O 2 P [4] are from the errata of <ref type="bibr" target="#b17">[18]</ref>.</p><p>background samples in each mini-batch are set to be approximately 30%, 30%, and 40%. The testing stage is the same as in the object-only case. While the testing stage is unchanged, the classifiers learned are biased toward those compact proposals.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results on Joint Object and Stuff Segmentation</head><p>We conduct experiments on the newly labeled PASCAL-CONTEXT dataset <ref type="bibr" target="#b17">[18]</ref> for joint object and stuff segmentation. In this enriched dataset, every pixel is labeled with a semantic category. It is a challenging dataset with various images, diverse semantic categories, and balanced ratios of object/stuff pixels. Following the protocol in <ref type="bibr" target="#b17">[18]</ref>, the semantic segmentation is performed on the most frequent 59 categories and one background category ( <ref type="table" target="#tab_9">Table 7</ref>). The segmentation accuracy is measured by mean IoU scores over the 60 categories. Following <ref type="bibr" target="#b17">[18]</ref>, the mean of the scores over a subset of 33 easier categories (identified by <ref type="bibr" target="#b17">[18]</ref>) is reported in this 60-way segmentation task as well. The training and evaluation are performed on the train and val sets respectively. We compare with two leading methods -SuperParsing <ref type="bibr" target="#b20">[21]</ref> and O 2 P [4], whose results are reported in <ref type="bibr" target="#b17">[18]</ref>. For fair comparisons, the region refinement <ref type="bibr" target="#b9">[10]</ref> is not used in all methods. The pasting scheme is the same as in O 2 P <ref type="bibr" target="#b3">[4]</ref>. In this comparison, we ignore R-CNN <ref type="bibr" target="#b7">[8]</ref> and SDS <ref type="bibr" target="#b9">[10]</ref> because they have not been developed for stuff. <ref type="table" target="#tab_9">Table 7</ref> shows the mean IoU scores. Here "no-CFM" is our baseline (no CFM, no segment pursuit); "CFM w/o SP" is our CFM method but without segment pursuit; and "CFM" is our CFM method with segment pursuit. When segment pursuit is not used, the positive stuff samples are uniformly sampled from the candidate set (in which the segments have purity scores &gt; 0.6).</p><p>SuperParsing <ref type="bibr" target="#b20">[21]</ref> gets a mean score of 15.2 on the easier 33 categories, and the overall score is unavailable in <ref type="bibr" target="#b17">[18]</ref>. The O 2 P method <ref type="bibr" target="#b3">[4]</ref> results in 29.2 on the easier 33 cate-gories and 18.1 overall, as reported in <ref type="bibr" target="#b17">[18]</ref>. Both methods are not based on CNN features.</p><p>For the CNN-based results, the no-CFM baseline (20.7, with ZF and SS) is already better than O 2 P (18.1). This is mainly due to the generic features learned by deep networks. Our CFM method without segment pursuit improves the overall score to 24.0. This shows the effects of the masked convolutional features. With our segment pursuit, the CFM method further improves the overall score to 26.6. This justifies the impact of the samples generated by segment pursuit. When replacing the ZF SPPnet by the VGG net, and the SS proposals by MCG, our method yields an over score of 34.4. So our method benefits from deeper models and more accurate segment proposals. Some of our results are shown in <ref type="figure" target="#fig_10">Figure 5</ref>.</p><p>It is worth noticing that although only mean IoU scores are evaluated in this dataset, our method is also able to generate instance-wise results for objects.</p><p>Additional Results. We also run our trained model on an external dataset of MIT-Adobe FiveK <ref type="bibr" target="#b2">[3]</ref>, which consists of images taken by professional photographers to cover a broad range of scenes, subjects, and lighting conditions. Although our model is not trained for this dataset, it produces reasonably good results (see <ref type="figure">Figure 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We have presented convolutional feature masking, which exploits the shape information at a late stage in the network. We have further shown that convolutional feature masking  <ref type="figure">Figure 6</ref>: Some visual results of our trained model (with VGG and MCG) for cross-dataset joint object and stuff segmentation. The network is trained on the PASCAL-CONTEXT training set <ref type="bibr" target="#b17">[18]</ref>, and is applied on MIT-Adobe FiveK <ref type="bibr" target="#b2">[3]</ref>.</p><p>is applicable for joint object and stuff segmentation. We plan to further study improving object detection by convolutional feature masking. Exploiting the context information provided by joint object and stuff segmentation would also be interesting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>System pipeline. Top: the methods of "Regions with CNN features" (R-CNN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An illustration of the CFM layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Two network designs in this paper. The input image is processed as a whole at the convolutional layers from conv1 to conv5. Segments are exploited at a deeper hierarchy by: (Left) applying CFM on the feature map of conv5, where " b" means for "bounding boxes" and " s" means for segments; (Right) applying CFM on the finest feature map of the spatial pyramid pooling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>2</head><label></label><figDesc>www.robots.ox.ac.uk/?vgg/research/very_deep/ In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Stuff segment proposals sampled by different methods. (a) input image; (b) 43 regions uniformly sampled; (c) 5 regions sampled by deterministic segment pursuit; (d) 43 regions sampled by stochastic segment pursuit for finetuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Stuff segment proposals sampled by different methods. (a) input image; (b) 43 regions uniformly sampled; (c) 5 regions sampled by deterministic segment pursuit; (d) 43 regions sampled by stochastic segment pursuit for finetuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :</head><label>5</label><figDesc>Some example results of our CFM method (with VGG and MCG) for joint object and stuff segmentation. The images are from the PASCAL-CONTEXT validation set<ref type="bibr" target="#b17">[18]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean IoU on PASCAL VOC 2012 validation set using our various designs. Here we use ZF SPPnet and Selective Search.</figDesc><table><row><cell></cell><cell cols="2">ZF SPPnet VGG net</cell></row><row><cell>SS</cell><cell>50.9</cell><cell>56.3</cell></row><row><cell>MCG</cell><cell>53.0</cell><cell>60.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Mean IoU on PASCAL VOC 2012 validation set using different pre-trained networks and proposal methods. SS denotes Selective Search<ref type="bibr" target="#b21">[22]</ref>, and MCG denotes Multiscale Combinatorial Grouping<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell></cell><cell cols="2">ZF SPPnet VGG net</cell></row><row><cell>5-scale</cell><cell>53.0</cell><cell>60.9</cell></row><row><cell>1-scale</cell><cell>52.9</cell><cell>60.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Mean IoU on PASCAL VOC 2012 validation set using different scales. Here we use MCG for proposals.</figDesc><table><row><cell></cell><cell cols="3">conv time fc time total time</cell></row><row><cell>SDS (AlexNet) [10]</cell><cell>17.8s</cell><cell>0.14s</cell><cell>17.9s</cell></row><row><cell>CFM, (ZF, 5 scales)</cell><cell>0.29s</cell><cell>0.09s</cell><cell>0.38s</cell></row><row><cell>CFM, (ZF, 1 scale)</cell><cell>0.04s</cell><cell>0.09s</cell><cell>0.12s</cell></row><row><cell>CFM, (VGG, 5 scales)</cell><cell>1.74s</cell><cell>0.36s</cell><cell>2.10s</cell></row><row><cell>CFM, (VGG, 1 scale)</cell><cell>0.21s</cell><cell>0.36s</cell><cell>0.57s</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Feature extraction time per image on GPU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>mean areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv O2P [4] 47.8 64.0 27.3 54.1 39.2 48.7 56.6 57.7 52.5 14.2 54.8 29.6 42.2 58.0 54.8 50.2 36.6 58.6 31.6 48.4 38.6 SDS (AlexNet + MCG) [10] 51.6 63.3 25.7 63.0 39.8 59.2 70.9 61.4 54.9 16.8 45.0 48.2 50.5 51.0 57.7 63.3 31.8 58.7 31.2 55.7 48.5 CFM (ZF + SS) 53.5 63.3 21.5 59.1 40.3 52.4 68.6 55.4 66.6 25.4 60.5 48.5 60.0 53.6 58.6 59.8 40.5 68.6 31.7 49.3 53.6 CFM (ZF + MCG) 55.4 65.2 23.5 59.0 40.4 61.1 68.9 57.9 70.8 23.9 59.4 44.7 66.2 57.5 62.1 57.6 44.1 64.5 42.5 52.9 55.7</figDesc><table><row><cell>CFM (VGG + MCG)</cell><cell>61.8 75.7 26.7 69.5 48.8 65.6 81.0 69.2 73.3 30.0 68.7 51.5 69.1 68.1 71.7 67.5 50.4 66.5 44.4 58.9 53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Mean IoU scores on the PASCAL VOC 2012 test set.</figDesc><table><row><cell>method</cell><cell>mean AP r</cell></row><row><cell>SDS (AlexNet + MCG) [10]</cell><cell>49.7</cell></row><row><cell>CFM (ZF + SS)</cell><cell>51.0</cell></row><row><cell>CFM (ZF + MCG)</cell><cell>53.2</cell></row><row><cell>CFM (VGG + MCG)</cell><cell>60.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Instance-wise semantic segmentation evaluated by mean AP r [10] on PASCAL VOC 2012 validation set.because it cannot produce object instances.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Segmentation accuracy measured by IoU scores on the new PASCAL-CONTEXT validation set</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>table person</head><label>person</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>sofa</cell><cell></cell><cell>wall</cell><cell>light</cell><cell>sky</cell><cell>building</cell><cell>sky</cell><cell>building</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tree</cell><cell>bus</cell><cell>tree</cell><cell>bus</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>road</cell><cell>fence ground</cell><cell>person</cell><cell>road</cell><cell>fence ground</cell></row><row><cell></cell><cell>wall</cell><cell>cat</cell><cell>table</cell><cell>wall</cell><cell>cat</cell><cell></cell><cell cols="2">ground</cell><cell>water</cell><cell>ground</cell><cell>water</cell></row><row><cell></cell><cell>shelves</cell><cell></cell><cell></cell><cell>shelves</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bird</cell></row><row><cell></cell><cell cols="2">book</cell><cell>floor</cell><cell cols="2">book</cell><cell>floor</cell><cell cols="2">grass</cell><cell>grass</cell></row><row><cell>input</cell><cell cols="3">ground-truth</cell><cell cols="2">our results</cell><cell>input</cell><cell cols="3">ground-truth</cell><cell>our results</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/ShaoqingRen/SPP_net</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<title level="m">Multiscale combinatorial grouping. CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Object segmentation by alignment of poselet activations to image contours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input / output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2524</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.4729</idno>
		<title level="m">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Obj cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
	<note>Neural computation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.4038</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Matching pursuits with timefrequency dictionaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Mallat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Textonboost: Joint appearance, shape and context modeling for mulit-class object recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Superparsing: scalable nonparametric image parsing with superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning active basis model for object detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Layered object detection for multi-class segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hallman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.2901</idno>
		<title level="m">Visualizing and understanding convolutional neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
