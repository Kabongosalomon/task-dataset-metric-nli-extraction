<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohaddeseh</forename><surname>Bastan</surname></persName>
							<email>mbastan@cs.stonybrook.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
							<email>msurdeanu@email.arizona.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Arizona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
							<email>niranjan@cs.stonybrook.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Stony Brook University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BioNLI: Generating a Biomedical NLI Dataset Using Lexico-semantic Constraints for Adversarial Examples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural language inference (NLI) is critical for complex decision-making in biomedical domain. One key question, for example, is whether a given biomedical mechanism is supported by experimental evidence. This can be seen as an NLI problem but there are no directly usable datasets to address this. The main challenge is that manually creating informative negative examples for this task is difficult and expensive. We introduce a novel semi-supervised procedure that bootstraps an NLI dataset from existing biomedical dataset that pairs mechanisms with experimental evidence in abstracts. We generate a range of negative examples using nine strategies that manipulate the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, as perturbations via logical constraints in a neuro-logical decoding system <ref type="bibr" target="#b16">(Lu et al., 2021b)</ref>.</p><p>We use this procedure to create a novel dataset for NLI in the biomedical domain, called BioNLI and benchmark two state-of-the-art biomedical classifiers. The best result we obtain is around mid 70s in F1, suggesting the difficulty of the task. Critically, the performance on the different classes of negative examples varies widely, from 97% F1 on the simple role change negative examples, to barely better than chance on the negative examples generated using neuro-logic decoding. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Biomedical research has progressed at a tremendous pace, to the point where PubMed 2 has indexed well over 1M publications per year in the past eight years. Many of these publications include high-level mechanistic knowledge, e.g., proteinsignaling pathways, which is critical for the under-Premise:The outflow of uracil from the yeast Saccharomyces cerevisiae is known to be relatively fast in certain circumstances, to be retarded by proton conductors and to occur in strains lacking a uracil proton symport. In the present work, it was shown that uracil exit from washed yeast cells is an active process, creating a uracil gradient of the order of -80 mV relative to the surrounding medium. Glucose accelerated uracil exit, while retarding its entry. DNP or sodium azide each lowered the gradient to about -30 mV, simultaneously increasing the rate of uracil entry. They also lowered cellular ATP content. Manipulation of the external ionic conditions governing delta mu H+ at the plasma membrane had no detectable effect on uracil transport in yeast preparations thoroughly depleted of ATP.</p><p>Consistent Hypothesis:It was concluded that &lt;re&gt; uracil &lt;er&gt; exit is probably not driven by the s &lt;el&gt; proton &lt;le&gt; gradient but may utilize ATP directly.</p><p>Adversarial Hypothesis:It is concluded that &lt;el&gt; uracil &lt;le&gt; exit from S. cerevisiae is an active process facilitated by a &lt;re&gt; proton &lt;er&gt; gradient and ATP. <ref type="table">Table 1</ref>: Example of a premise/hypothesis pair in the BioNLI dataset, as well as of an adversarial hypothesis that was automatically generated by an encoderdecoder network that manipulated the lexico-semantic constraints in the original hypothesis. Here the regulator entity is marked as &lt;re&gt; entity &lt;er&gt;, and the regulated entity is marked as &lt;el&gt; entity &lt;le&gt;. standing of many diseases <ref type="bibr" target="#b25">(Valenzuela-Escarcega et al., 2018)</ref>, but which must be supported by lowerlevel experimental evidence to be trustworthy. Developing models that can understand and reason about such mechanisms is crucial for supporting effective access to the rich biomedical knowledge <ref type="bibr">(Bastan et al., 2022)</ref>. In particular, the current information deluge motivates the need for developing tools that can answer the question: "Is a given mechanism supported by experimental evidence?". This can be seen as a biomedical natural language inference (NLI) problem. Despite the prevalence of many biomedical NLP datasets <ref type="bibr">(Demner-Fushman et al., 2020;</ref><ref type="bibr">Bastan et al., 2022;</ref><ref type="bibr" target="#b11">Krallinger et al., 2017)</ref>, there are no datasets that can be directly used to address this task.</p><p>However, manually creating a biomedical NLI dataset that focuses on mechanistic information is challenging. <ref type="table">Table 1</ref>, which contains an actual example from our proposed dataset, highlights several difficulties. First, understanding biomedical mechanisms and the necessary experimental evidence that supports (or does not support) them requires tremendous expertise and effort <ref type="bibr" target="#b9">(Kaushik et al., 2019)</ref>. For example, the premise shown is considerably larger than the average premise in other open-domain NLI datasets such as SNLI <ref type="bibr" target="#b4">(Bowman et al., 2015)</ref>, and is packed with domain-specific information. Second, negative examples are seldom explicit in publications. Creating them manually risks introducing biases, simplistic information, and systematic omissions <ref type="bibr" target="#b27">(Wu et al., 2021)</ref>.</p><p>In this work, we introduce a novel semisupervised procedure for the creation of biomedical NLI datasets that include mechanistic information.</p><p>Our key contribution is automating the creation of negative examples that are informative without being simplistic. Intuitively, we achieve this by defining lexico-semantic constraints based on the mechanism structures in the biomedical literature abstracts. Our dataset creation is as follows:</p><p>(1) We extract positive entailment examples consisting of a premise and hypothesis from abstracts of PubMed publications. We focus on abstracts that contain an explicit conclusion sentence, which describes a biomedical interaction between two entities (a regulator and a regulated protein or chemical). This yields premises that are considerably larger than premises in other open-domain NLI datasets: between 3 -15 sentences.</p><p>(2) We generate a wide range of negative examples by manipulating the structure of the underlying mechanisms both with rules, e.g., flip the roles of the entities in the interaction, and, more importantly, by imposing the perturbed conditions as logical constraints in a neuro-logical decoding system <ref type="bibr" target="#b16">(Lu et al., 2021b)</ref>. This battery of strategies produces a variety of negative examples, which range in difficulty, and, thus, provide an important framework for the evaluation of NLI methods.</p><p>We employ this procedure to create a new dataset for natural language inference (NLI) in the biomedical domain, called BioNLI. <ref type="table">Table 1</ref> shows an actual example from BioNLI. The dataset contains 13489 positive entailment examples, and 26907 adversarial negative examples generated using nine different strategies. An evaluation of a sample of these negative examples by human biomedical experts indicated that 86% of these examples are indeed true negatives. We trained two state-of-the-art neural NLI classifiers on this dataset, and show that the overall F1 score remains relatively low, in the mid 70s, which indicates that this NLI task remains to be solved. Critically, we observe that the performance on the different classes of negative examples varies widely, from 97% accuracy on the simple negative examples that change the role of the entities in the hypothesis, to 55% (i.e., barely better than chance) on the negative examples generated using neuro-logic decoding. Further, given how the dataset is constructed we can also test if models produce consistent decisions on all adversarial negatives associated with a mechanism, giving deeper insight into model behavior. Thus, in addition of its importance in the biomedical field, we hope that this dataset will serve as a benchmark to test models' language understanding abilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous work on NLI in scientific domains include: medical question answering (Abacha and Demner-Fushman, 2016), entailment based text exploration in health care <ref type="bibr" target="#b2">(Adler et al., 2012)</ref>, entailment recognition in medical texts <ref type="bibr" target="#b1">(Abacha et al., 2015)</ref>, textual inference in clinical trials <ref type="bibr" target="#b23">(Shivade et al., 2015)</ref>, NLI on medical history <ref type="bibr" target="#b22">(Romanov and Shivade, 2018)</ref>, and SciTail <ref type="bibr" target="#b10">(Khot et al., 2018)</ref> which is created from multiple-choice science exams and web sentences. These datasets either have modest sizes <ref type="bibr" target="#b1">(Abacha et al., 2015)</ref>, target specific NLP problems such as coreference resolution or named entity extraction <ref type="bibr" target="#b23">(Shivade et al., 2015)</ref>, and make use of experts in the domain to generate inconsistent data which is costly and labor-intensive. Additionally, they also focus on sentence-to-sentence entailment tasks, where both the premise and the hypothesis are no longer than one sentence. Most importantly, none of these are directly aimed at inference on mechanisms in biomedical literature.</p><p>Our work is also related to NLI tasks that go beyond sentence-level entailments. For example, <ref type="bibr" target="#b30">(Yin et al., 2021)</ref> include premises longer than a sentence, but only use three simple rule-based methods to create negative samples. <ref type="bibr" target="#b28">(Yan et al., 2021;</ref><ref type="bibr" target="#b20">Nie et al., 2019)</ref> use larger contexts as premises for the NLI task but only on general purpose domains like news, fiction, and Wiki. On the other hand, the BioNLI dataset is an inference problem with large contexts as premises but in the biomedical domain which often requires handling more complex texts and domain knowledge.</p><p>There is also a growing body of research into exploring factual inconsistency in text generation models <ref type="bibr" target="#b17">(Maynez et al., 2020;</ref><ref type="bibr" target="#b32">Zhu et al., 2021;</ref><ref type="bibr" target="#b24">Utama et al., 2022)</ref>. We take advantage of the known weakness of generation models for hallucination and also employ a constraint based neurological decoding from recently introduced decoding methods <ref type="bibr">(Lu et al., 2021b,a;</ref><ref type="bibr" target="#b13">Kumar et al., 2021)</ref> to generate adversarial examples for BioNLI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BioNLI Creation</head><p>We model the task of understanding if a high-level mechanistic statement is supported by lower-level experimental evidence as natural language inference (NLI). The goal of NLI is to understand whether the given hypothesis can be entailed from the premise or not <ref type="bibr" target="#b5">(Dagan et al., 2005)</ref>. This is typically modeled with three labels (entailed or not, plus a neutral class if the two texts are unrelated). In our case, the premise contains the experimental evidence, while the hypothesis summarizes the higher-level mechanistic information. Both of these texts are extracted from abstracts of biomedical publications, where the beginning sentences (the supporting set) describe experimental evidence, and a conclusion sentence summarizes the mechanistic information that is entailed by these experiments.</p><p>In this work, we introduce the BioNLI dataset, an NLI dataset automatically created from a set of abstracts of PubMed open-access publications. We collected all the abstracts which contain a conclusion sentence with mechanistic information at the end of the abstract, and filter out the rest. Following previous work in mechanism generation <ref type="bibr">(Bastan et al., 2022)</ref>, we focus on conclusion sentences that discuss binary biochemical interactions between a regulator and a regulated entity (both of which are proteins or chemicals). We then generate negative examples by manipulating the structure of the conclusion sentences.</p><p>In the following subsections we describe in detail the generation of both positive and negative examples in BioNLI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Identifying Abstracts with Mechanistic Information</head><p>To identify abstracts that contain conclusion sentences with such binary biochemical interactions, we followed the same procedure and dataset 3 as <ref type="bibr">(Bastan et al., 2022)</ref> . That is, we used a series of patterns (e.g., finding words that start with conclud all patterns are described in Appendix A) to identify conclusion sentences at the end of abstracts, and consider the previous ones as the supporting set. We analyzed the SuMe dataset and found that 91% of the abstracts end with conclusion sentences, which indicates that the filtering heuristic is robust. Further, we take advantage of the structured text in the biomedical domain, by focusing on abstracts that describe some mechanism between two biochemical entities. One of the main entities is called regulator entity and is marked with &lt;re&gt; entity &lt;re&gt; inside the text; the other main entity is called regulated entity and is marked with &lt;el&gt; entity &lt;le&gt; inside the text. We will use this structure to generate negative examples by modifying it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Positive Instances</head><p>For positive examples, we simply use the original conclusion sentence from the abstract as the hypothesis and the supporting set as the premise. These sentences are likely to be accurate as they are written by domain experts, and also peer-reviewed by other scientists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Adversarial Instances</head><p>The key contribution of this paper is on the automatic creation of meaningful, yet difficult negative examples without the use of experts. We introduce multiple strategies for creating negative examples. We group these strategies into two groups: rulebased and neural-based counterfactuals, both of which are detailed below. We show examples of these strategies in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Rule-Based Counterfactuals</head><p>This category consists of rule-based methods that convert a correct conclusion sentence (i.e., the hypothesis) into an instance that is not entailed by the given supporting set by perturbing parts of its semantic structure. Most of them are used in general-domain factual consistency evaluating systems <ref type="bibr" target="#b12">(Kry?ci?ski et al., 2019;</ref><ref type="bibr">Zhu et al., 2020)</ref>:</p><p>Swap Entity Names (SEN): Swapping the entity names. This flips the roles of the entities in the interaction, i.e., the regulator becomes the regulated and vice versa, which contradicts the original evidence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swap Entity Positions (SEP):</head><p>In this perturbation we swap the positions of the two entities in text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swap Random Entity (SRE):</head><p>In this perturbation one of the main entities is randomly swapped with a different entity that occurs in the supporting set and has the same entity type. We use SciSpaCy <ref type="bibr" target="#b19">(Neumann et al., 2019)</ref> and the built in en_ner_bionlp13cg_md model to detect entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swap Random Entity with Out of text entity (SREO):</head><p>In this perturbation we swap one of the two entities in the interaction with a random entity from out of the context which was not available in the supporting set but has the same type as the main entity. Similarly, we use SciSpaCy with the same model to detect entity types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verb Negation (VNeg):</head><p>We randomly select one of the predicates in the original conclusion and change its polarity, e.g., from positive to negative or vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Swap Numbers (SN):</head><p>If the conclusion contains a number, it is swapped with a different number, randomly chosen from the supporting set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lexical Polarity Reversal (LPR):</head><p>We collected a list of terms describing mechanistic interactions (e.g., inhibition and promotion), and swapped them with their antonyms when encountered in the hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Neural-based Counterfactuals</head><p>The above methods are relatively simple perturbations, which might be easily detected by transformer-based classifiers. To counteract this potential limitation, we take advantage of transformerbased generation methods to create more complex and diverse set of negative examples. In this category we have two main approaches:</p><p>Mechanism Generation (GEN): We use a model pretrained on a mechanism generation task in the same context <ref type="bibr">(Bastan et al., 2022)</ref> to generate mechanism sentences (and the relation between main entities) for each abstract in our dataset. As our dataset overlaps with the one from <ref type="bibr">(Bastan et al., 2022)</ref>, we implemented a 5-fold cross validation, and retrained the model with the corresponding training set in each fold. That is, for each split, we train with 4 folds and generate the output for the other fold. The generated texts which get a BLEURT score lower than ?, and predict the relation between two main entities incorrectly are selected as counterfactuals. Here, we set ? = 0.45. Neurologic Decoding (GEN-ND): Neurologic decoding is a decoding algorithm that enables neural language models to generate text while satisfying complex lexical constraints <ref type="bibr" target="#b16">(Lu et al., 2021b)</ref>. We take advantage of this decoding method to impose different structure-aware constraints. For generation, we use the same model as the one described in GEN approach. For decoding this model, we define the following constraints which result in generating negative examples. (we combine all three categories in our results table, naming the entire group GEN-ND):</p><p>(1) Neurologic Decoding with SEN Constraints (GEN-ND-SEN): We imposed as positive constraints (i.e., constraints that should be satisfied during decoding) that the two entities be present in the output, but we swapped their names. That is, the regulator and regulated entities are swapped; if both of them are satisfied in the generated text, the instance is used as a negative example. For example if the original conclusion has the following pattern:</p><p>... &lt; re &gt; entity1 &lt; er &gt; ... &lt; el &gt; entity2 &lt; le &gt; We add the following constraints to the neurological decoding:</p><formula xml:id="formula_0">[[&lt; re &gt; entity2 &lt; er &gt;], [&lt; el &gt; entity1 &lt; le &gt;]]</formula><p>Compared with the general SEN introduced in section 3.3.1, by using these constraints we force the generation model to generate a natural yet negative and completely new sentence.</p><p>(2) Neurologic Decoding with SRE Constraints (GEN-ND-SRE): Similarly, we swapped one of the main entities in the positive constraints with a random entity from the supporting text. To make sure the generated sentence is not too similar to the original conclusion sentence, we used the generated sentence only if both constraints are satisfied and the semantic similarity between the text and the original text is smaller than ?. To compute the semantic similarity we use BioLinkBERT <ref type="bibr" target="#b29">(Yasunaga et al., 2022)</ref>. We set ? to 0.9.</p><p>(3) Neurologic Decoding with Negative Constraints (GEN-ND-NG): The third and last method  we tried uses negative constraints, i.e., its lexical artifacts should not be present in the generated text.</p><p>In particular, the negative constraints we defined contained the original entities. By using the original entities as negative constraints the generated text receives a higher score if the main entities are not shown in their own roles (i.e., neither regulator nor regulated entities are not enclosed with specific markers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dataset Statistics</head><p>The resulting dataset is summarized in two tables. <ref type="table" target="#tab_1">Table 2</ref> shows the maximum number of possible perturbations on each instance. For example, all instances can be perturbed with SEP and SEN approaches, while only the ones that have a number in both conclusion and supporting set can be perturbed with the SN approach. The distribution of the possible perturbations over the dev set is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. As we see in this figure, all sentences can get at least two perturbation SEN and SEP. 5.7% of the instances can not get any other perturbations based on their structure. 35.1% of the data get 3 different adversarial examples. Most of the data, which is about 39% of them, can be perturbed with 4 different approaches. The lowest category is 8 perturbations which are only about 0.1% of the data. We don't have any instance which can be perturbed with all 9 possible methods explained in section 3.3, which shows the diversity and variety on the adversarial instance generation   Note that while our goal is to produce a dataset with as many high quality examples as possible for each category, downstream applications can adjust the distribution of the training categories to be uniform or biased towards specific categories as needed based on their requirements. To study the impact of a balanced distribution of adversarial examples, we sampled the positive and negative classes to produce a balanced dataset. <ref type="table" target="#tab_3">Table 3</ref> shows the distribution of the adversarial categories in this balanced dataset. We evaluate with the original collection as well as with this balanced dataset (see <ref type="table" target="#tab_5">Table 5</ref>). <ref type="table">Table 4</ref> shows a set of rule-based and neural-based adversarial examples. The main entities are enclosed with specific markers and are swapped with different methods. We also see a completely new and negative generated text based on the supporting set with the generation approach (GEN-SEN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Quality Control</head><p>To ensure the quality of the collected dataset, we asked two experts (graduate students in the biomedical domain) to inspect 50 randomly selected adversarial examples generated by the neural-based counterfactual methods (section 3.3.2). We sampled 25 examples each from GEN and GEN-ND methods. Given the abstract and the generated sentence, the experts assessed whether the generated sentence is indeed inconsistent with the given supporting set, meaning that the sentence cannot be concluded given the supporting set. The expert analysis shows that the neural-based counterfactuals are of high quality. They find that 88% of adversarial examples from the GEN method and 84% from the GEN-ND method are correct negative examples, averaging to 86% overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In this section we benchmark the performance of state-of-the-art (SOTA) biomedical language models on the BioNLI dataset. Our evaluation is aimed at assessing the following aspects of the BioNLI:</p><p>1. How difficult is the inference task captured by the dataset? 2. What kinds of perturbations are difficult for the models? 3. How consistent are the models on adversarial instances?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>We fine-tune two state-of-the-art models in biomedical domain: (i) PubMedBERT <ref type="bibr">(Gu et al., 2020)</ref> was pretrained from scratch with texts from the biomedical domain and shown to be effective for a wide variety of biomedical NLP tasks including NER, QA, and sentence similarity.</p><p>(ii) BioLinkBERT <ref type="bibr" target="#b29">(Yasunaga et al., 2022)</ref> augments PubMedBERT by pre-training jointly on linked biomedical articles. We fine-tune the top 3 layers of the base-sized pretrained models from Hugging Face <ref type="bibr" target="#b26">(Wolf et al., 2019)</ref> using PyTorch <ref type="bibr" target="#b21">(Paszke et al., 2019)</ref>. We use AdamW <ref type="bibr" target="#b14">(Loshchilov and Hutter, 2017)</ref> with a learning rate of 1e ? 4 by manually tuning 5 different values. We use the original hyper-parameters of the models and the NLI label prediction is done via binary classification using CLS token. The sequence length we use here is 512 and the beam size is 16. We train each model for 20 epochs and choose the best one based on the performance (macro F1) over the dev set. The performance of both these models is listed in <ref type="table" target="#tab_5">Table 5</ref>. The table reports positive, negative, and overall F1 scores, as well performance for the various types of negative examples (recall). At the time of prediction, we use a binary classifier. Hence, we don't have a fine-grained negative category predictions, therefore, we can't calculate precision. Instead, we report recall for fine-grained negative categories and F1 score for positive and overall negative prediction. We also report macro-F1 for positive and negative classes. In addition, the table includes an ablation experiment, where the performance of the classifiers trained only on the hypothesis ("hypo-only") is contrasted with the classifier trained on the entire data ("premise+hyp"). We also include the performance of the balanced distribution in parentheses, to compare with the overall distribution. <ref type="table" target="#tab_5">Table 5</ref> indicates that the overall performance of the best model on the positive class is 77%, and 79% on all negative examples (macro-average). If we only consider the difficult negative classes (SRE, LPR, GEN-ND, GEN), the best model's performance on the negative categories drop considerably to 55.4%, i.e., only slightly better than a random classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Difficulty</head><p>This table also highlights the difficulty of the Abstract:We investigated whether intracellular pH (pH(i)) is a causal mediator in abscisic acid (ABA)-induced gene expression. We measured the change in pH(i) by a "null-point" method during stimulation of barley (Hordeum vulgare cv Himalaya) aleurone protoplasts with ABA and found that ABA induces an increase in pH(i) from 7.11 to 7.30 within 45 min after stimulation. This increase is inhibited by plasma membrane H(+)-ATPase inhibitors, which induce a decrease in pH(i), both in the presence and absence of ABA. This ABA-induced pH(i) increase precedes the expression of RAB-16 mRNA, as measured by northern analysis. ABA-induced pH(i) changes can be bypassed or clamped by addition of either the weak acids 5,5-dimethyl-2,4-oxazolidinedione and propionic acid, which decrease the pH(i), or the weak bases methylamine and ammonia, which increase the pH(i). Artificial pH(i) increases or decreases induced by weak bases or weak acids, respectively, do not induce RAB-16 mRNA expression. Clamping of the pH(i) at a high value with methylamine or ammonia treatment affected the ABA-induced increase of RAB-16 mRNA only slightly. However, inhibition of the ABA-induced pH(i) increase with weak acid or proton pump inhibitor treatments strongly inhibited the ABA-induced RAB-16 mRNA expression.</p><p>Conclusion:We conclude that, although the &lt;el&gt; ABA &lt;le&gt;-induced the &lt;re&gt; pH &lt;er&gt;(i) increase is correlated with and even precedes the induction of RAB-16 mRNA expression and is an essential component of the transduction pathway leading from the hormone to gene expression, it is not sufficient to cause such expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEN</head><p>We conclude that, although the &lt;el&gt; pH &lt;le&gt;-induced the &lt;re&gt; ABA &lt;er&gt;(i) increase is correlated with and even precedes the induction of RAB-16 mRNA expression and is an essential component of the transduction pathway leading from the hormone to gene expression, it is not sufficient to cause such expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SEP</head><p>We conclude that, although the &lt;re&gt; pH &lt;er&gt;-induced the &lt;el&gt; ABA &lt;le&gt;(i) increase is correlated with and even precedes the induction of RAB-16 mRNA expression and is an essential component of the transduction pathway leading from the hormone to gene expression, it is not sufficient to cause such expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SREO</head><p>We conclude that, although the &lt;el&gt; integrin &lt;le&gt;-induced the &lt;re&gt; pH &lt;er&gt;(i) increase is correlated with and even precedes the induction of RAB-16 mRNA expression and is an essential component of the transduction pathway leading from the hormone to gene expression, it is not sufficient to cause such expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VNeg</head><p>We conclude that, although the &lt;el&gt; ABA &lt;le&gt;-induced the &lt;re&gt; pH &lt;er&gt;(i) increase is not correlated with and even precedes the induction of RAB-16 mRNA expression and is an essential component of the transduction pathway leading from the hormone to gene expression, it is not sufficient to cause such expression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LPR</head><p>We conclude that, although the &lt;el&gt; ABA &lt;le&gt;-induced the &lt;re&gt; pH &lt;er&gt;(i) decrease is correlated with and even precedes the induction of RAB-16 mRNA expression and is an essential component of the transduction pathway leading from the hormone to gene expression, it is not sufficient to cause such expression.</p><p>Generation We conclude that the &lt;re&gt; ABA &lt;er&gt; -induced increase in &lt;el&gt; pH &lt;le&gt; (i) precedes the expression of RAB-16 mRNA. <ref type="table">Table 4</ref>: Example of the generated adversarial instance for the BioNLI dataset using lexico semantic constraints. Regulator entities are enclosed in &lt;re&gt; &lt;er&gt; tags and regulated entities are enclosed in &lt;el&gt; &lt;le&gt; tags. The red texts show the negated phrases. generated categories. While traditional approaches of the adversarial example creation, (i.e., SEN and SEP), are solvable with large transformer-based models, the more complex negative examples produced using generation are considerably more difficult to be classified correctly. <ref type="table" target="#tab_5">Table 5</ref> calls attention to another feature of the BioNLI dataset: classifiers trained with the balanced training data (shown in parentheticals) perform better on minority categories, while the models produced after training on the larger distribution perform better on other categories. This highlights the versatility of the dataset, as well as the importance of customizing the data distribution (including that of negative examples!) for each use case. <ref type="table" target="#tab_5">Table 5</ref> indicates that some categories of the adversarial examples are more difficult than the rest. This is mostly seen in rule-based categories. In average, the neural-based methods generate more difficult sentences than the rule-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Difficulty of Adversarial Instances</head><p>For instance, SEP and SEN instances are easier due to the structure (markers) in the dataset. Even without inspecting the supporting set, the model learns that the entity with &lt;re&gt;&lt;er&gt; markers should be the subject of the text while the entity marked with &lt;el&gt;&lt;le&gt; should be the object of the mechanism.</p><p>Some categories are easier to recognize with context. For instance SREO and SN approaches are not easily detectable with hypothesis only baselines. But, when the model is trained with both premise and hypothesis, these become easier because the contradiction can be recognized using information in the premise (i.e. abstract).</p><p>We have four difficult categories of adversarial examples (SRE, LPR, GEN-ND, and GEN) where the models perform only slightly better than a random classifier. These are the least-frequent classes;  when we train the model under the balanced distribution the performance improves, somewhat. However, performance remains low, which underscores the need for further research on handling these difficult adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Consistency on Adversarial Instances</head><p>In addition to the per-perturbation evaluation, we also merged all available positive and negative instance for each entry of the dataset, and  gesting avenues for further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Error Analysis</head><p>We analyzed a set of 50 instances within the BioNLI dataset which are classified incorrectly. <ref type="table">Ta</ref>  behind the entities which is explained in the conclusion sentence needs multi hop reasoning which makes it difficult to classify correctly. Abbreviations can cause difficulties, when one of the main entities is mentioned in its full form in one sentence but abbreviated in the rest of the sentences. Distant entities also make inference harder. The model fails when the two main entities are in different sentences with many unrelated fragments of text occur between them. Sometimes the abstracts can talk about two related experiments on the same entities, but the mechanism is only related to one of the experiment making it harder to assess entailment. A '-' (dash) between two words, can change the subject and object of a sentence. The entailment is very difficult when there is a subtle difference here. Finally, the similarity between entity names, or name overlaps, specifically in case of SRE and GEN-ND-SRE confuses the model about the correct entailment class. There are cases where the entity is swapped with another entity with similar name or with partially overlapped name, these cases seem to be difficult for the model to classify correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduced a novel semisupervised procedure for the creation of biomedical NLI datasets that include mechanistic information.</p><p>Our key contribution is automating the creation of negative examples that are informative without being simplistic. We achieve this by manipulating the lexico-semantic constraints in the mechanism structures captured in the hypotheses, which we implement both with rules and with neuro-logic decoding. To our knowledge, this is the first paper that employs neuro-logic decoding for the generation of adversarial examples. All in all, we implemented nine different strategies for the creation of adversarial examples.</p><p>We used this procedure to create the BioNLI dataset, which addresses NLI for mechanistic texts in the biomedical domain. An evaluation of a sample of these negative examples by human biomedical experts indicated that 86% of these examples are indeed true negatives. We trained two state-ofthe-art neural NLI classifiers on this dataset, and showed that the overall performance remains relatively low, which indicates that this NLI task is not solved. Critically, we observe that the performance on the different classes of negative examples varies widely, from 97% accuracy on the simple negative examples that change the role of the entities in the hypothesis, to 55% (i.e., barely better than chance) on the negative examples generated using neuro-logic decoding. We hope that this openaccess dataset 4 will enable further research both on biomedical NLI, and on language understanding in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgement</head><p>This material is based on research that is supported in part by the National Science Foundation under the award IIS #2007290. The authors would like to thank the anonymous reviewers and the area chair for their feedback on this work. We would also like to thank the biomedical experts who assessed the quality of the adversarial examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>Unlike many scientific NLI datasets <ref type="bibr" target="#b22">(Romanov and Shivade, 2018;</ref><ref type="bibr" target="#b23">Shivade et al., 2015)</ref> no instance in the BioNLI dataset was directly annotated by human domain experts. Instead, following the trend of machine-generated datasets <ref type="bibr" target="#b8">(Hartvigsen et al., 2022)</ref>, we build upon recent developments in text generation and generate BioNLI automatically.</p><p>The only human annotation in this effort was performed by one expert on a sample of 50 sentences, to check the quality of automatically created negative examples. This minimal effort was justified by previous work in the biomedical space (citation hidden for review), in which we observed that experts had high inter-annotator agreement on the interpretations of scientific information in abstracts.</p><p>The premise-free experiments show the presence of artifacts in some categories of the BioNLI dataset, similar to several other NLI datasets <ref type="bibr" target="#b22">(Romanov and Shivade, 2018;</ref><ref type="bibr" target="#b4">Bowman et al., 2015;</ref><ref type="bibr"></ref> Our data is collected solely from open-access publications in PubMed. We do not include any meta data (authors, publication venue, etc.) in the dataset. The created dataset is also open-access.</p><p>We believe our released dataset and software will contribute to society by promoting further NLI research and applications in the biomedical domain. Long term, we envision that this research will enable novel machine reading applications that automatically discover potential explanations and treatments for diseases that are still misunderstood today.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Extraction Patterns for Identifying Conclusion Sentences</head><p>To extract the conclusion sentences from the abstract, we follow the recipe from <ref type="bibr">Bastan et al. (2022)</ref>. We filtered the abstracts from PubMed dataset which have a form of conclusion sentence at the end. In particular we filtered out all the abstracts that do not have any of the phrases in <ref type="table" target="#tab_9">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyper-parameter Selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Generation Lambda</head><p>One of the strategies to generate negative examples is the generation method (GEN). We trained a t5-large model on the SuMe dataset using 5-fold cross validation. Each time we trained on 4 folds and generated the output for the 5th fold. From the generated texts, we selected the ones which have lowest quality. That is, we selected generated sentences that contain both entities, the predicted relation labels are incorrect, and the Bleurt score of the generated sentence against the true mechanism Used Phrase we conclude that it is concluded that it was concluded that we concluded that we have concluded that it has been concluded that it may be concluded that it was therefore concluded that we therefore conclude that we conclude we thus conclude that it is therefore concluded that we further conclude that  sentence is lower than a threshold ?. In our preliminary experiments we found that ? = 0.45 yields a good compromise between quality and yield. Analyzing the output of this hyper-parameter showed that 90% of the sentences selected with this method are indeed true negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Neurological Decoding Hyper-parameters</head><p>One of the strategies to generate negative examples is the generation method with the neurological decoding (GEN-ND) <ref type="bibr" target="#b16">(Lu et al., 2021b)</ref>. We used the source code introduced in their GitHub page 5 . The hyper-parameter details are shown in <ref type="table" target="#tab_10">Table 8</ref>. We also allowed for the use of negative or positive constraints, another choice that we use as a hyper-parameter.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Distribution of possible perturbations over the dev set approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Percentage of correctly classified perturbations in the dev set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Dataset statistics of the larger distribution. Each instance is perturbed as many times as possible for the dev and test sets and once for the training set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell>40</cell><cell></cell><cell cols="2">39.0% Possible Perturbations in Dev Set</cell></row><row><cell></cell><cell>35</cell><cell></cell><cell cols="2">35.1%</cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell></row><row><cell>Percent</cell><cell>15 20 25</cell><cell></cell><cell></cell><cell>16.3%</cell></row><row><cell></cell><cell>0 5 10</cell><cell>2 5.7%</cell><cell>3</cell><cell>4 Number of Possible Perturbations 5 6 3.3% 0.4% 0.1% 7 8</cell></row></table><note>Dataset statistics of the balanced distribution. We sampled over perturbed classes to create a balance dataset so that no rule-based category have more than 500 instances in the train set. Test set is same as Table 2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Overall performance of two state-of-the-art models in the biomedical domain (PubMedBERT, Bi-oLinkBERT) on both distributions. The models are fine-tuned using the data with premise (p+h) and without premise (h-only) on the BioNLI dataset. The metric used here is recall for fine-grained negative classes and F1 for positive and all negative categories. The different rows indicate the performance for the various kinds of positive and negative examples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Distribution of different error categories in 50</cell></row><row><cell>incorrect classified samples</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Used phrases to filter the abstracts</figDesc><table><row><cell>Parameter</cell><cell>Value</cell><cell>Details</cell></row><row><cell cols="2">min_tgt_length 15</cell><cell>min target length</cell></row><row><cell cols="2">max_tgt_length 256</cell><cell>max target length</cell></row><row><cell>bs</cell><cell>4</cell><cell>batch size</cell></row><row><cell>beam_size</cell><cell>50</cell><cell>beam size</cell></row><row><cell cols="3">length_penalty 0.1 length penalty for beam</cell></row><row><cell>ngram_size</cell><cell>10</cell><cell>ngrams occur once</cell></row><row><cell>prune_factor</cell><cell>50</cell><cell>candidates to keep</cell></row><row><cell>sat_tolerance</cell><cell>2</cell><cell>min satisfied constraints</cell></row><row><cell>beta</cell><cell>2</cell><cell>reward factor</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Neurological decoding hyper parameters</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and data is available at https://github.com/ StonyBrookNLP/BioNLI 2 https://pubmed.ncbi.nlm.nih.gov</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This paper works at a higher level of abstraction, which contains causal semantic relations (or "activations"), which are always directed and not necessarily asymmetric.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Code and data is available at https://github.com/ StonyBrookNLP/BioNLI</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://github.com/GXimingLu/ neurologic_decoding</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recognizing question entailment for medical question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2016</biblScope>
			<biblScope unit="page">310</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic analysis and automatic corpus construction for entailment recognition in medical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asma</forename><surname>Ben Abacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duy</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yassine</forename><surname>Mrabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Artificial Intelligence in Medicine in Europe</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="238" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entailment-based text exploration with application to the health-care domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meni</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL 2012 System Demonstrations</title>
		<meeting>the ACL 2012 System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohaddeseh</forename><surname>Bastan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.04652</idno>
		<title level="m">Mihai Surdeanu, and Niranjan Balasubramanian. 2022. Sume: A dataset towards summarizing biomedical mechanisms</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A large annotated corpus for learning natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine learning challenges workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">2020. Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing</title>
		<editor>Dina Demner-Fushman, Kevin Bretonnel Cohen, Sophia Ananiadou, and Junichi Tsujii</editor>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tinn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoto</forename><surname>Usuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<title level="m">Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hartvigsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipankar</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09509</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning the difference that makes a difference with counterfactually-augmented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12434</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scitail: A textual entailment dataset from science question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The biocreative v. 5 evaluation workshop: tasks, organization, sessions and topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Krallinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>P?rez-P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gael</forename><surname>P?rez-Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Blanco-M?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentino</forename><surname>Fdez-Riverola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvador</forename><surname>Capella-Gutierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An?lia</forename><surname>Louren?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfonso</forename><surname>Valencia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The biocreative v. 5 evaluation workshop: tasks, organization, sessions and topics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Evaluating the factual consistency of abstractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Kry?ci?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.12840</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Controlled text generation as continuous optimization with multiple constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Malmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="14542" to="14554" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungo</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianhui</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zellers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08726</idno>
		<title level="m">Neurologic a* esque decoding: Constrained text generation with lookahead heuristics</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neu-roLogic decoding: (un)supervised neural text generation with predicate logic constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.339</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4288" to="4299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On faithfulness and factuality in abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Maynez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Bohnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.173</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1906" to="1919" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08172</idno>
		<title level="m">The repeval 2017 shared task: Multi-genre natural language inference with sentence representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ScispaCy: Fast and robust models for biomedical natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Ammar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-5034</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th BioNLP Workshop and Shared Task</title>
		<meeting>the 18th BioNLP Workshop and Shared Task<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="319" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14599</idno>
		<title level="m">Adversarial nli: A new benchmark for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06752</idno>
		<title level="m">Lessons from natural language inference in the clinical domain</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Textual inference for eligibility criteria resolution in clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Shivade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Courtney</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Lopetegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Fosler-Lussier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert M</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="211" to="218" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Falsesum: Generating document-level nli examples for recognizing factual inconsistency in summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Prasetya Ajie Utama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bambrick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.06009</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Nafise Sadat Moosavi, and Iryna Gurevych</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Large-scale automated machine reading discovers new cancer driving mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozgun</forename><surname>Valenzuela-Escarcega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gus</forename><surname>Babur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dane</forename><surname>Hahn-Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Hicks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Noriega-Atala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emek</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><forename type="middle">T</forename><surname>Demir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morrison</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/bay098</idno>
	</analytic>
	<monogr>
		<title level="j">Database: The Journal of Biological Databases and Curation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongshuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><forename type="middle">Tulio</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00288</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Control image captioning spatially and temporally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Ma</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.157</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2014" to="2025" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15827</idno>
		<title level="m">Linkbert: Pretraining language models with document links</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DocNLI: A large-scale dataset for documentlevel natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.435</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4913" to="4922" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Hinthorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingkai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08612</idno>
		<title level="m">Xuedong Huang, and Meng Jiang. 2020. Enhancing factual consistency of abstractive summarization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faithfully explainable recommendation via neural logic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikun</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuohui</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfeng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3083" to="3090" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
