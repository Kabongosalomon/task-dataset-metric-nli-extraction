<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised multi-object segmentation using attention and soft-argmax</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Sauvalle</surname></persName>
							<email>bruno.sauvalle@mines-paristech.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">La Fortelle Centre de Robotique Mines ParisTech PSL University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised multi-object segmentation using attention and soft-argmax</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new architecture for unsupervised objectcentric representation learning and multi-object detection and segmentation, which uses a translation-equivariant attention mechanism to predict the coordinates of the objects present in the scene and to associate a feature vector to each object. A transformer encoder handles occlusions and redundant detections, and a convolutional autoencoder is in charge of background reconstruction. We show that this architecture significantly outperforms the state of the art on complex synthetic benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We consider in this paper the tasks of object-centric representation learning and unsupervised object detection and segmentation: Starting from a dataset of images showing various scenes cluttered with objects, our goal is to build a structured object-centric representation of these scenes, i.e. to map each object present in a scene to a vector representing this object and allowing to recover its appearance and segmentation mask. This task is very challenging because the objects appearing in the images may have different shapes, locations, colors or textures, can occlude each other, and we do not assume that the images share the same background. However the rewards of object-centric representations could be significant since they allow to perform complex reasoning on images or videos <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">42]</ref> and to learn better policies on downstream tasks involving object manipulation or localization <ref type="bibr" target="#b46">[46,</ref><ref type="bibr" target="#b50">50]</ref>. The main issue with objectrepresentation learning today is however that existing models are able to process synthetic toy scenes with simple textures and backgrounds but fail to handle more complex or real-world scenes <ref type="bibr" target="#b29">[29]</ref>.</p><p>We propose to improve upon this situation by introducing a translation-equivariant and attention-based approach for unsupervised object detection, so that a translation of the input image leads to a similar translation of the coordinates of the detected objects, thanks to an attention map which is used not only to associate a feature vector to each object present in the scene, but also to predict the coordinates of these objects.</p><p>The main contributions of this paper are the following:</p><p>? We propose a theoretical justification for the use of attention maps and soft-argmax for object localization.</p><p>? We introduce a new translation-equivariant and attention-based object detection and segmentation architecture which does not rely on any spatial prior.</p><p>? We show that the proposed model substantially improves upon the state of the art on unsupervised object segmentation on complex synthetic benchmarks.</p><p>The paper is organized as follows: In section 2, we provide some theoretical motivation for using attention maps and soft-argmax for object localization. In section 3, we review related work on unsupervised object instance segmentation. In section 4 we describe the proposed model. Experimental results are then provided in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivation for using attention maps and soft-argmax for object localization</head><p>It is widely recognized that the success of convolutional neural networks is associated with the fact that convolution layers are equivariant with respect to the action of the group of translations, which makes these layers efficient for detecting features which naturally have this property. It is also easy to show that linear convolution operators are the only linear operators which are equivariant with respect to the natural action of the translation group on feature maps.</p><p>We introduce the following notations to describe the action of the translation group: We consider a grayscale image as a scalar-valued function ?(i, j) defined on Z 2 and an element of the group of translations as a vector (u, v) in Z 2 . The natural action T of the group of translations on an image can be described by the formula A model layer L is called equivariant with respect to translations if it satisfies</p><formula xml:id="formula_0">L(T u,v ?) = T u,v (L(?)).<label>(2)</label></formula><p>Let's now consider a localization model M which takes as input an image ?(i, j) showing one object and produces as output the coordinates of the object present in this image. Such a model does not produce a feature map, so that the previous definition of translation equivariance can not be used for this model. We remark however that the group of translations acts naturally on Z 2 by the action T u,v (i, j) = i + u, j + v, and that the model M should have the equivariance property</p><formula xml:id="formula_1">M (T u,v ?) = T u,v (M (?)).<label>(3)</label></formula><p>Indeed, if the complete image is translated by a vector (u, v), then the object present in this image is also translated, so that the associated coordinates have to be shifted according to the vector (u, v). It is not difficult to see that in the same way that convolutional operators are the only linear operators equivariant with respect to translations, it is also possible to fully describe which elementary operators follow this specific equivariance property. We first remark however that we have to restrict the space of possible input maps ?: if ? is a constant function, it does not change under the action of the translation group, so that the equivariance property 3 cannot be satisfied with such a function. We then suppose that ? satisfies p ?(p) = 1 and consider that the domain of the operator M is the corresponding affine space A. We also replace the linearity condition by an the following affinity condition:</p><p>For all ? i ? R, ? i ? A so that i ? i = 1, we have</p><formula xml:id="formula_2">M ( i ? i ? i ) = i ? i M (? i ).</formula><p>We then have the following proposition:</p><p>Proposition 2.1 An affine operator M which satisfies the equivariance property 3 has to be of the form</p><formula xml:id="formula_3">M (?) = C + p?Z 2 ?(p)p<label>(4)</label></formula><p>for some constant C in R 2 .</p><p>Proof: We write the input map ? as a sum of spatially shifted version of the function ? ? A satisfying ?(p) = 1 for p = (0, 0) and ?(p) = 0 for p = (0, 0):</p><formula xml:id="formula_4">?(p) = q?Z 2 ?(q)?(p ? q)<label>(5)</label></formula><p>We then use the the affine property of M and equivariance property 3:</p><formula xml:id="formula_5">M (?) = M ( q ?(q)?(p ? q)) (6) = q ?(q)M (?(p ? q)) = q ?(q)(M (?) + q) (7) = ( q ?(q))M (?) + q ?(q)q (8) = M (?) + q ?(q)q,<label>(9)</label></formula><p>which proves the proposition since M (?) is a constant. The proposition 2.1 can be interpreted as stating that in order to get an equivariant localization operator, the most straightforward method is to build a normalized attention map ? from the input image and compute the coordinates of the detected object using an attention mechanism with ? as attention map and pixel coordinates as target values. One remarks that it is precisely what the soft-argmax operator is doing: It takes an unnormalized scalar map ? as input, normalizes it using a softmax operator, and then perform localization using the same formula as in 2.1:</p><formula xml:id="formula_6">soft-argmax(?) = p?Z 2 softmax(?)(p)p = p?Z 2 e ?(p) q?Z 2 e ?(q) p<label>(10)</label></formula><p>This operation is called soft-argmax because it allows to compute in a differentiable way an estimate of the coordinates of the maximum of the input map ?. Using softargmax then appears to be the most natural way to get an equivariant localization operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>Unsupervised object detection and segmentation Unsupervised object detection and segmentation models are generally reconstruction models: They try to reconstruct the input image using a specific image rendering process which induces the required object-centric structure. In order to ensure that objects are properly detected, various objectness priors have been defined and implemented:</p><p>? pixel similarity priors. Some models consider the task of object segmentation as a clustering problem, which can be addressed using deterministic <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b33">33]</ref> or probabilistic <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b44">44]</ref> methods: If the feature vectors associated to two different pixels of an image are very similar, then it is considered that these pixels should both belong to the same object or to the background.</p><p>? independence priors. Some models assume that the images are sampled from a distribution which follows a probabilistic model featuring some independence priors between objects and the background, and use variational <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b15">16]</ref> or adversarial <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref> methods to learn these distributions.</p><p>? disentanglement of appearance and location. Foreground objects appearing in the scenes of a given dataset can have similar shapes and appearances but very different scales and locations. Object discovery is performed by disentangling the object appearance generation process, which is performed by a convolutional glimpse generator <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25]</ref> or a learned dictionary <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b39">39]</ref>, from the translation and scaling of the objects appearing in a scene, which is usually done by including a spatial transformer network <ref type="bibr" target="#b23">[24]</ref> in the model. The model described in this paper belongs to this category and uses an convolutional glimpse generator.</p><p>Object detection and segmentation without spatial prior State of the art supervised detection and segmentation models usually rely on predefined reference anchors or center points which are spatially organized according to a periodic grid structure. The use of periodic grids has also been proposed for unsupervised object detection <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b39">39]</ref>. Alternative detection methods relying on heatmaps produced by a U-net <ref type="bibr" target="#b37">[37]</ref> or stacked U-nets <ref type="bibr" target="#b36">[36]</ref> networks, which predict for each pixel the probability of presence of one object on this pixel have been implemented in the supervised setting <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>For some specific applications such as human pose estimation or anatomical landmark localization <ref type="bibr" target="#b43">[43]</ref>, some supervised models predict one heatmap per object. The use of soft-argmax for converting heatmaps to object coordinates has been implemented in the supervised <ref type="bibr" target="#b41">[41,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b5">6]</ref>, semisupervised <ref type="bibr" target="#b21">[22]</ref> and unsupervised settings <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref> but has never been proposed for unsupervised object detection or segmentation.</p><p>More recently, transformer-based <ref type="bibr" target="#b45">[45]</ref> models using object <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b11">12]</ref> or mask <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> queries have been proposed which not not rely explicitly on a spatial grid. These models show that transformers are efficient in the supervised setting to avoid multiple detections of the same object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Description of proposed model 4.1. Model architecture</head><p>The overall architecture of the model is described in <ref type="figure">Fig  1.</ref> The proposed model is composed of a a foreground model and a background model. The background model is a deterministic convolutional autoencoder: We rely on the classical assumption <ref type="bibr" target="#b47">[47]</ref> that background images lie on a low-dimensional manifold, and use the autoencoder to learn this manifold.</p><p>The foreground model is also deterministic and associates to each object in the scene an appearance vector z what which is used to produce a glimpse of the object, which is then scaled and translated at the right position on the image using a spatial transformer network.</p><p>The foreground encoding and reconstruction process can be described as follows: First, a high resolution feature map generator takes a color image of size h ? w as input and produces a high resolution feature map ? of dimension d ? and several scalar attention logit maps A 1 , ..., A K . We will use in this paper the transformer-based Segformer model <ref type="bibr" target="#b49">[49]</ref>, which produces feature maps of size h * ?w * = h/4? w/4. The hyperparameter K is set to the maximum number of objects on a scene in the dataset. The scalar attention logit maps A 1 , ..., A K are transformed into a normalized attention maps A 1 , ..., A K using a softmax operator:</p><formula xml:id="formula_7">A k (i, j) = e A k (i,j) i ,j e A k (i ,j )<label>(11)</label></formula><p>We normalize the pixel indices (i, j) from the range [1, .., w * ] and [1, .., h * ] to the range [?1, 1] required by spatial transformer networks using the formulas</p><formula xml:id="formula_8">x(i) = 2 i ? 1 w * ? 1 ? 1 (12) y(j) = 2 j ? 1 h * ? 1 ? 1,<label>(13)</label></formula><p>and predict initial estimates x 0 k , y 0 k of the coordinates of the detected objects as the the center of mass of the attention maps A k :</p><formula xml:id="formula_9">x 0 k = w * ,h * i=1,j=1 A k (i, j)x(i)<label>(14)</label></formula><formula xml:id="formula_10">y 0 k = w * ,h * i=1,j=1 A k (i, j)y(j)<label>(15)</label></formula><p>We also build K object query feature vectors ? 0 1 , ..., ? 0 K of dimension d ? using the same attention maps A 1 , .., A K as weights and the feature map ? as target values:</p><formula xml:id="formula_11">? 0 k = w * ,h * i=1,j=1 A k (i, j)?(i, j)<label>(16)</label></formula><p>A transformer encoder then takes the K triplets (? 0 k , x 0 k , y 0 k ) 1?k?K as inputs and produces a refined version (? k , x k , y k ) 1?k?K taking into account possible detection redundancies and object occlusions. More precisely, we use  <ref type="figure">Figure 1</ref>. Overview of proposed model. A High resolution feature map generator (Segformer model) is trained to produce a high resolution feature map ? and K scalar attention maps (one per object query). These maps are used to predict the coordinates and scales of the detected objects and the associated feature vectors, which are refined by a transformer encoder and then used as inputs to a glimpse generator and a spatial transformer network to produce K object image layers and masks. A convolutional autoencoder is in charge of background reconstruction. a learned linear embedding to increase the dimension of the triplets (? 0 k , x 0 k , y 0 k ) from d ? + 2 to the input dimension d T of the transformer encoder, and a learned linear projection to reduce the dimension of the outputs of the transformer encoder from d T back to d ? + 2. The transformer encoder does not take any positional encoding as input, considering that the transformation which has to be performed should not depend on the ordering of the detections.</p><p>We force the final values of x k and y k to stay in the range [?1, 1] using clamping. Each transformed feature vector ? k is then split in three terms:</p><formula xml:id="formula_12">? k = (s k , ? k , z what k ).</formula><p>? The first term s k is an inverse scaling factor. It is a scalar if objects in the dataset have widths and heights which are similar (isotropic scaling), or a pair of scalars s x k , x y k if this is not the case (anisotropic scaling). We force the values of s k to stay within a fixed range using a sigmoid function. The maximum value of this range ensures that a non-zero gradient will be available. The minimum value is set higher than 1 to make sure that the glimpse generator will not try to generate a full image layer.</p><p>? The second term is a scalar which is assumed to predict the activation level ? k of the object, which will be used to predict whether it is visible or not. We force this activation value to be positive using an exponential map.</p><p>? The remaining coordinates form a vector z what k which codes for the appearance of the object.</p><p>We then use a convolutional glimpse generator to build a color image o k of the associated object together with the associated scalar mask m k , using z what k as input. These images and masks are translated to the positions (x k , y k ) and scaled according to the inverse scaling factor s k using a spatial transformer network. We note L k and M k for k ? {1, .., K} the corresponding object image layers and masks, and L 0 the background image produced by the background model, so that we have a total of K + 1 image layers.</p><p>We now have to decide for each pixel whether this pixel should show the background layer or one of the K object layers. In order to do this in a differentiable way, we multiply the predicted object masks M k with the associated object activation levels ? k , and normalize the results to get one normalized weights distribution (w k ) 0?k?K per pixel:</p><formula xml:id="formula_13">w k (i, j) = ? k M k (i, j) k ?0..K ? k M k (i, j) ,<label>(17)</label></formula><p>considering that the mask M 0 associated to the background is set to 1 everywhere and that it has a fixed learned activation factor ? 0 . The final reconstructed imageX is then equal to the weighted sum of the various image layers using the weights w k :X</p><formula xml:id="formula_14">(i, j) = K k=0 w k (i, j)L k (i, j)<label>(18)</label></formula><p>During inference the segmentation map is built by assigning to each pixel the layer index k ? {0, .., K} for which w k (i, j) is the maximum. The background model is not needed to get the segmentation maps during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model training 4.2.1 loss function</head><p>In order to train the proposed model, we use a main reconstruction loss function and an auxiliary loss:</p><p>reconstruction loss The local L 1 reconstruction error associated to the pixel (i, j) is</p><formula xml:id="formula_15">l i,j = 3 c=1 |x c,i,j ? x c,i,j |,<label>(19)</label></formula><p>where x c,i,j andx c,i,j are the values of the color channel c at the position (i, j) in the input image and reconstructed image.</p><p>The reconstruction loss is defined as the mean square of this reconstruction error.</p><formula xml:id="formula_16">L rec = 1 hw w,h i=1,j=1 l 2 i,j<label>(20)</label></formula><p>pixel entropy loss For a given pixel (i, j), we expect the distribution of the weights w 0 (i, j), .., w K (i, j) to be onehot, because we assume that the objects are opaque. We observe that a discrete distribution is one-hot if and only if it has a zero entropy, so that minimizing the entropy of this distribution would be a reasonable way to enforce a stick-breaking process. Considering however that the entropy function has a singular gradient near one-hot distributions, we use the square of the entropy function to build the loss function. We then define the pixel entropy loss as</p><formula xml:id="formula_17">L pixel = 1 hw w,h i=1,j=1 ( K k=0 w k (i, j) log(w k (i, j) + )) 2 ,<label>(21)</label></formula><p>where = 10 ?20 is introduced to avoid any numerical issue with the logarithm function.</p><p>This auxiliary loss is weighted using the weight ? pixel before being added to the reconstruction loss.</p><p>During our experiments, we observed that the pixel entropy loss could prevent a successful initialization of the localization process during the beginning of the training. As a consequence, we smoothly activate this auxiliary loss during initialization using a quadratic warmup of the weight.</p><p>The full loss function is then equal to</p><formula xml:id="formula_18">L = L rec + min(1, step N pixel ) 2 ? pixel L pixel ,<label>(22)</label></formula><p>where step is the current training iteration index and N pixel is a fixed hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">curriculum training</head><p>The interaction between the background reconstruction model and the foreground model during training is a very challenging issue, because of the competition between them to reconstruct the image. We handle this problem as in <ref type="bibr" target="#b24">[25]</ref> by implementing curriculum training. We will then evaluate two methods to train the proposed model:</p><p>? baseline training (BT) : The background and foreground models are initialized randomly and trained simultaneously.</p><p>? curriculum training (CT): The training of the model is split in three phases :</p><p>1. The background model is pretrained alone, using the methodology and robust loss function described in <ref type="bibr" target="#b38">[38]</ref>.</p><p>2. The weights of the background model are then frozen and the foreground model is trained using the frozen background model.</p><p>3. The background and foreground models are then fine-tuned simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation on public benchmarks</head><p>We perform a quantitative evaluation of the proposed model on the following datasets: CLEVRTEX <ref type="bibr" target="#b29">[29]</ref>, CLEVR <ref type="bibr" target="#b26">[27]</ref>, ShapeStacks <ref type="bibr" target="#b20">[21]</ref> and ObjectsRoom <ref type="bibr" target="#b27">[28]</ref>.</p><p>We implement on ShapeStacks, ObjectsRoom and CLEVR the same preprocessing as in <ref type="bibr" target="#b14">[15]</ref>.</p><p>We use the same hyperparameter values on these datasets, except for the hyperparameter K related to the number of object queries, which is set to the maximum number of objects in each dataset (i.e. 3 on ObjectsRoom, 6 on ShapeStacks and 10 on CLEVRTEX and CLEVR). We use isotropic scaling on CLEVR and ShapeStacks and anisotropic scaling on the other datasets.</p><p>We use the versions B3 of the Segformer model, and rely on the Hugging Face implementation of this model, with pretrained weights on ImageNet-1k for the hierarchical transformer backbone, but random initialization for the MLP decoder which is used as a feature map generator. We use the standard Pytorch implementation of the transformer encoder. The architecture of the backgroud model autoencoder is the same as in <ref type="bibr" target="#b38">[38]</ref>. The glimpse generator is a sequence of transpose convolution layers, group normalization <ref type="bibr" target="#b48">[48]</ref> layers and CELU <ref type="bibr" target="#b1">[2]</ref> non-linearities, and is described in the supplementary material.</p><p>We use Adam as optimizer. The training process includes a quadratic warmup of the learning rate since the model contains a transformer encoder. We also decrease the learning rate by a factor of 10 when the number of training steps reaches 90% of the total number of training steps. The total number of training steps of the baseline training (BT) scenario is 125 000. In the curriculum training (CT) scenario, the number of training steps for background model pretraining (phase 1) is 500 000 on CLEVRTEX, ShapeStacks and ObjectsRoom, but 2500 on CLEVR, which shows a fixed background, as recommended in <ref type="bibr" target="#b38">[38]</ref>. The number of training steps of phase 2 (training with frozen pretrained background model) is 30 000, and the number of training steps of the final fine-tuning phase (phase 3) is 95 000.</p><p>Full implementation details and hyperparameter values are provided in the supplementary material, and the model code will be made available on the Github platform.</p><p>In order to compare our results with published models, we compute the following evaluation metrics: mean intersection over union (mIoU) and adjusted rand index restricted to foreground objects (ARI-FG). We also provide the mean square error (MSE) between the reconstructed image and the input image, which provides an estimate of the accuracy of the learnt representation. We use the same definitions and methodology as <ref type="bibr" target="#b29">[29]</ref> for these metrics. We provide the mean segmentation covering (defined in <ref type="bibr" target="#b15">[16]</ref>) restricted to foreground objects (MSC-FG) on ObjectsRoom and ShapeStacks where mIoU baseline values are not available.</p><p>We call AST-Seg (Attention and Soft-argmax with Transformer using Segformer) the proposed model, and AST-Seg-B3-BT, AST-Seg-B3-CT respectively the models using a Segformer B3 feature map generator trained under the baseline training or curriculum training scenarios. <ref type="table" target="#tab_0">Table  1</ref> and 2 provide the results obtained on these datasets with a comparison with published results.</p><p>The proposed model trained under the baseline training scenario gets better average results than existing models on the CLEVR and CLEVRTEX dataset, but shows a very high variance. For example, on the CLEVR dataset, the model may fall during training in a bad minimum where the background model tries to predict the foreground objects. Using curriculum training allows to avoid this issue, get stable results on all datasets, and obtain a very significant mIoU improvement on the most complex datasets CLEVR and CLEVRTEX.</p><p>Following the methodology proposed in <ref type="bibr" target="#b29">[29]</ref>, we also evaluated the generalization capability of a model trained on CLEVRTEX when applied to datasets containing out of distribution images showing unseen textures and shapes or camouflaged objects (OOD and CAMO datasets <ref type="bibr" target="#b29">[29]</ref>). The results of this evaluation are provided in <ref type="table" target="#tab_2">Table 3</ref> and show that the proposed model generalizes well, although it is deterministic and does not use any specific regularization scheme.</p><p>Some segmentation prediction samples are provided in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation study and additional experiments</head><p>We provide in <ref type="table" target="#tab_3">Table 4</ref> results obtained using various ablations or modifications on the model architecture or loss function, which show that:</p><p>? The model remains competitive if the transformer encoder is removed by setting</p><formula xml:id="formula_19">(? k , x k , y k ) 1?k?K = (? 0 k , x 0 k , y 0 k ) 1?k?K .</formula><p>The results on the ShapeStacks and ObjectsRoom datasets are even improved with this simplified architecture, with a surprisingly strong improvement on the Shapestacks dataset, which shows the efficiency of the attention and soft-argmax mechanism. The transformer encoder is however necessary on the more complex CLEVR and CLEVRTEX datasets.</p><p>? Training with a number of slots slightly higher than the maximum number of objects does not lead to significant changes in the results. A more substantial increase of the number of slots however leads to poor results on scenes with complex textures due to the increasing fragmentation the objects. This is very different from the situation observed on query-based supervised detection models like DETR, where the number of queries has to be very high compared to the number of objects.</p><p>? It is possible to replace the Segformer high resolution feature map generator with any other generator. The proposed model was originally designed with a custom Unet feature map generator, which gets similar results as the Segformer model on CLEVR, ShapeStacks and ObjectsRoom, but underperforms on the more complex CLEVRTEX dataset. The architecture of this Unet is described in the supplementary material.</p><p>? Using a pretrained backbone is necessary to get good performances with a Segformer feature map generator.</p><p>? We tested an alternative training scenario where the background model remains frozen during the complete training of the foreground model (125 000 iterations).</p><p>The main advantage of this scenario is that it is significantly faster and requires less memory, since the backgrounds of the training images can be pre-computed and memorized. The accuracy of the results is however lower than the curriculum training scenario proposed in this paper, except for the ObjectsRoom dataset.  <ref type="table">Table 2</ref>. Benchmark results on ObjectsRoom and ShapeStacks. Source: <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_20">Model ObjectsRoom ShapeStacks ?ARI-FG (%) ?MSC-FG (%) ?mIoU (%) ?MSE ?ARI-FG (%) ?MSC-FG (%) ?mIoU (%) ?MSE</formula><p>MONet-g <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15]</ref> 54 ? 0 33 ? 1 n/a n/a 70 ? 4 57 ? 12 n/a n/a Gen-v2 <ref type="bibr" target="#b14">[15]</ref> 84 ? 1 58 ? 3 n/a n/a 81 ? 0 68 ? 1 n/a n/a SA <ref type="bibr" target="#b33">[33]</ref> 79 ? 2 64 ? 13 n/a n/a 76 ? 1 70 ? 5 n/a n/a    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have described in this paper a new architecture for unsupervised object-centric representation learning and object detection and segmentation, which relies on attention and soft-argmax, and shown that this new architecture substantially improves upon the state of the art on existing benchmarks showing synthetic scenes with complex shapes and textures. We hope this work may help to extend the scope of structured object-centric representation learning from research to practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Hyperparameter values</head><p>The hyperparameter values used for the proposed model are listed in <ref type="table" target="#tab_5">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Pseudo-code for objects encoder and decoder</head><p>The full encoding and rendering process is described in Algorithms 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Additional implementation details</head><p>The glimpse convolutional generator is described in <ref type="table">Table 7</ref>.</p><p>Synthetic datasets and preprocessing codes were downloaded from the following public repositories:</p><p>? https://www.robots.ox.ac.uk/?vgg/ data/clevrtex/ ? https://ogroth.github.io/ shapestacks/ The Segformer pretrained weights were downloaded from the following link:</p><p>https://huggingface.co/nvidia/mit-b3</p><p>The architecture of the U-net implemented for the ablation study is described in <ref type="table">Table 8</ref>. It contains a sequence of downsample blocks which output feature maps of decreasing sizes, a center block which takes as input the feature map produced by the last downsample block, and upsample blocks, which take as input both the output of the previous upsample or center block and the feature map of the same size produced by corresponding downsample block.</p><p>? A downsample block is composed of a convolutional layer with stride 2 and kernel size 4, with batch normalization and CELU, followed by a residual convolutional layer with stride 1 and kernel size 3 with batch normalization and CELU.</p><p>? The center block is composed of a convolutional layer with stride 1 and kernel size 3 with batch normalization and CELU.</p><p>? An upsample block is composed of a residual convolutional layer with stride 1 and kernel size 3 with batch normalization and CELU, followed by a transpose convolutional layer with stride 2 and kernel size 4, with batch normalization and CELU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Additional image samples</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig 2 .</head><label>2</label><figDesc>Other image samples are available in the supplementary material. The main limitation of the proposed model is the management of shadows, which may be considered by the model as separate objects or integrated to object segmentations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Additional image samples are provided in Figures 3 Figure 3 .Figure 4 .Figure 5 .Figure 6 .Figure 7 .Figure 8 .</head><label>3345678</label><figDesc>Examples of segmentation predictions on CLEVR test dataset Examples of segmentation predictions on CLEVRTEX test dataset Examples of segmentation predictions on ObjectsRoom test dataset Examples of segmentation predictions on ShapeStacks test dataset (using a model without transformer) Examples of segmentation predictions on CAMO test dataset using a model trained on CLEVRTEX only Examples of segmentation predictions on OOD test dataset using a model trained on CLEVRTEX only</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Benchmark results on CLEVR and CLEVRTEX. Results are shown (??) calculated over 3 runs. Source: [29] ?ARI-FG (%) ?MSE ?mIoU (%) ?ARI-FG (%) ? MSE SPAIR [10] 65.95 ? 4.02 77.13 ? 1.92 55? 10 0.00 ? 0.00 0.00 ? 0.00 1101 ? 2 SPACE [32] 26.31 ? 12.93 22.75 ? 14.04 63? 3 9.14 ? 3.46 17.53 ? 4.13 298 ? 80 GNM [25] 59.92 ? 3.72 65.05 ? 4.19 43? 3 42.25 ? 0.18 53.37 ? 0.67 383 ? 2</figDesc><table><row><cell cols="2">CLEVR 56.81 ? 0.40 72.12 ? 0.64 ?mIoU (%) MN [39] Model</cell><cell>CLEVRTEX 75? 1 10.46 ? 0.10 38.31 ? 0.70 335 ? 1</cell></row><row><cell>DTI [35]</cell><cell>48.74 ? 2.17 89.54 ? 1.44</cell><cell>77? 12 33.79 ? 1.30 79.90 ? 1.37 438 ? 22</cell></row><row><cell>Gen-V2 [15]</cell><cell cols="2">9.48 ? 0.55 57.90 ? 20.38 158? 2 7.93 ? 1.53 31.19 ? 12.41 315 ? 106</cell></row><row><cell>eMORL [14]</cell><cell>50.19 ? 22.56 93.25 ? 3.24</cell><cell>33? 8 12.58 ? 2.39 45.00 ? 7.77 318 ? 43</cell></row><row><cell>MONet [4]</cell><cell>30.66 ? 14.87 54.47 ? 11.41</cell><cell>58? 12 19.78 ? 1.02 36.66 ? 0.87 146 ? 7</cell></row><row><cell>SA [33]</cell><cell>36.61 ? 24.83 95.89 ? 2.37</cell><cell>23? 3 22.58 ? 2.07 62.40 ? 2.23 254 ? 8</cell></row><row><cell>IODINE [19]</cell><cell>45.14 ? 17.85 93.81 ? 0.76</cell><cell>44? 9 29.17 ? 0.75 59.52 ? 2.20 340 ? 3</cell></row><row><cell cols="2">AST-Seg-B3-BT 71.92 ? 32.94 76.05 ? 36.13</cell><cell>51? 63 57.30 ? 15.72 71.79 ? 22.88 152 ? 39</cell></row><row><cell cols="2">AST-Seg-B3-CT 90.27 ? 0.20 98.26 ? 0.07</cell><cell>16? 1 79.58 ? 0.54 94.77 ? 0.51 139 ? 7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Benchmark generalization results on CAMO, and OOD for a model trained on CLEVRTEX. Results are shown (??) calculated over 3 runs. Source: [29] ?ARI-FG (%) ?MSE ?mIoU (%) ?ARI-FG (%) ?MSE SPAIR [10] 0.00 ? 0.00 0.00 ? 0.00 1166? 5 0.00 ? 0.00 0.00 ? 0.00 668? 3 SPACE [32] 6.87 ? 3.32 12.71 ? 3.44 387? 66 8.67 ? 3.50 10.55 ? 2.</figDesc><table><row><cell>Model</cell><cell>OOD ?mIoU (%) 09 251? 61 CAMO</cell></row><row><cell>GNM [25]</cell><cell>40.84 ? 0.30 48.43 ? 0.86 626? 5 17.56 ? 0.74 15.73 ? 0.89 353? 1</cell></row><row><cell>MN [39]</cell><cell>12.13 ? 0.19 37.29 ? 1.04 409? 3 8.79 ? 0.15 31.52 ? 0.87 265? 1</cell></row><row><cell>DTI [35]</cell><cell>32.55 ? 1.08 73.67 ? 0.98 590? 4 27.54 ? 1.55 72.90 ? 1.89 377? 17</cell></row><row><cell>Gen-V2 [15]</cell><cell>8.74 ? 1.64 29.04 ? 11.23 539?147 7.49 ? 1.67 29.60 ? 12.84 278? 75</cell></row><row><cell>eMORL [14]</cell><cell>13.17 ? 2.58 43.13 ? 9.28 471? 51 11.56 ? 2.09 42.34 ? 7.19 269? 31</cell></row><row><cell>MONet [4]</cell><cell>19.30 ? 0.37 32.97 ? 1.00 231? 7 10.52 ? 0.38 12.44 ? 0.73 112? 7</cell></row><row><cell>SA [33]</cell><cell>20.98 ? 1.59 58.45 ? 1.87 487? 16 19.83 ? 1.41 57.54 ? 1.01 215? 7</cell></row><row><cell>IODINE [19]</cell><cell>26.28 ? 0.85 53.20 ? 2.55 504? 3 17.52 ? 0.75 36.31 ? 2.57 315? 3</cell></row><row><cell cols="2">AST-Seg-B3-CT 67.50 ? 0.75 83.14 ? 0.75 832? 24 73.07 ? 0.65 87.27 ? 3.78 145? 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results of ablation study and additional experiments (results over 1 run, except for starred values, which are averages over 3 runs)</figDesc><table><row><cell>Dataset</cell><cell cols="2">CLEVRTEX</cell><cell cols="2">CLEVR</cell><cell cols="2">ShapeStacks</cell><cell cols="2">ObjectsRoom</cell></row><row><cell></cell><cell cols="2">mIoU ARI-FG</cell><cell cols="2">mIoU ARI-FG</cell><cell cols="2">mIoU ARI-FG</cell><cell cols="2">mIoU ARI-FG</cell></row><row><cell>full model AST-Seg-B3-CT (reference)</cell><cell>79.58*</cell><cell>94.77*</cell><cell>90.27*</cell><cell>98.26*</cell><cell>78.84*</cell><cell>79.34*</cell><cell>85.02*</cell><cell>87.23*</cell></row><row><cell>model without transformer encoder</cell><cell>75.69</cell><cell>94.41</cell><cell>77.16</cell><cell>93.09</cell><cell>82.99*</cell><cell>82.29*</cell><cell>85.51*</cell><cell>88.49*</cell></row><row><cell>K = 1 + maximum number of objects</cell><cell>79.11*</cell><cell>94.78*</cell><cell>91.03*</cell><cell>98.17*</cell><cell>78.87</cell><cell>80.05</cell><cell>82.90</cell><cell>86.45</cell></row><row><cell>K = 2 ? maximum number of objects</cell><cell>62.10</cell><cell>89.96</cell><cell>90.56</cell><cell>98.29</cell><cell>54.88</cell><cell>65.16</cell><cell>66.78</cell><cell>78.58</cell></row><row><cell cols="2">using a Unet instead of Segformer feature generator 66.82</cell><cell>88.25</cell><cell>90.70</cell><cell>98.17</cell><cell>75.51</cell><cell>77.78</cell><cell>85.59</cell><cell>87.93</cell></row><row><cell>random initialization of Segformer backbone</cell><cell>61.74</cell><cell>80.22</cell><cell>88.94</cell><cell>97.77</cell><cell>62.73</cell><cell>68.40</cell><cell>77.71</cell><cell>79.23</cell></row><row><cell>training without pixel entropy loss</cell><cell>70.18</cell><cell>91.81</cell><cell>85.54</cell><cell>96.09</cell><cell>52.17</cell><cell>60.08</cell><cell>84.21</cell><cell>86.19</cell></row><row><cell>training using frozen pretrained background model</cell><cell>75.30</cell><cell>95.31</cell><cell>81.46</cell><cell>98.29</cell><cell>55.06</cell><cell>66.24</cell><cell>85.82</cell><cell>87.78</cell></row><row><cell>isotropic scaling</cell><cell>78.68</cell><cell>94.78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>84.91</cell><cell>87.20</cell></row><row><cell>anisotropic scaling</cell><cell></cell><cell></cell><cell>87.21</cell><cell>98.53</cell><cell>45.47</cell><cell>36.43</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Training computation time with one Nvidia RTX 3090 GPU (curriculum training)</figDesc><table><row><cell>Dataset</cell><cell>image size</cell><cell cols="2">background model pretraining (phase 1)</cell><cell cols="2">full model training (phase 2 &amp; 3)</cell></row><row><cell></cell><cell></cell><cell>number of iterations</cell><cell>training time</cell><cell cols="2">number of iterations training time</cell></row><row><cell>CLEVRTEX</cell><cell>128 ? 128</cell><cell>500000</cell><cell>57 h 47 mn</cell><cell>125000</cell><cell>16 h 00 mn</cell></row><row><cell>CLEVR</cell><cell>128 ? 128</cell><cell>2500</cell><cell>20 mn</cell><cell>125000</cell><cell>12 h 03 mn</cell></row><row><cell>ObjectsRoom</cell><cell>64 ? 64</cell><cell>500000</cell><cell>14 h 57 mn</cell><cell>125000</cell><cell>6 h 31 mn</cell></row><row><cell>ShapeStacks</cell><cell>64 ? 64</cell><cell>500000</cell><cell>14 h 20 mn</cell><cell>125000</cell><cell>6 h 22 mn</cell></row><row><cell>5.3. Computation time</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">All experiments have been performed using a Nvidia</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">RTX 3090 GPU and a AMD 7402 EPYC CPU.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Some training durations are provided in Table 5.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table><row><cell>Hyperparameter values</cell><cell></cell><cell></cell></row><row><cell>hyperparameter description</cell><cell>notation</cell><cell>value</cell></row><row><cell>Background model pretraining:</cell><cell></cell><cell></cell></row><row><cell>batch size</cell><cell></cell><cell>128</cell></row><row><cell>learning rate</cell><cell></cell><cell>2.10 ?3</cell></row><row><cell>number of background model training iterations:</cell><cell></cell><cell></cell></row><row><cell>-datasets with fixed backgrounds (CLEVR)</cell><cell></cell><cell>2500</cell></row><row><cell>-datasets with complex backgrounds (CLEVRTEX, ShapeStacks,ObjectsRoom)</cell><cell></cell><cell>500000</cell></row><row><cell>Foreground model training:</cell><cell></cell><cell></cell></row><row><cell>batch size</cell><cell></cell><cell>64</cell></row><row><cell>learning rate</cell><cell></cell><cell>4.10 ?5</cell></row><row><cell>Adam ? 1</cell><cell></cell><cell>0.90</cell></row><row><cell>Adam ? 2</cell><cell></cell><cell>0.98</cell></row><row><cell>Adam</cell><cell></cell><cell>10 ?9</cell></row><row><cell>number of foreground model training iterations</cell><cell></cell><cell>125000</cell></row><row><cell>number of steps of phase 2 (CT scenario)</cell><cell></cell><cell>30000</cell></row><row><cell>number of steps of learning rate warmup phase</cell><cell></cell><cell>5000</cell></row><row><cell>number of steps of pixel entropy loss weight warmup phase</cell><cell>N pixel</cell><cell>10000</cell></row><row><cell>initial value of background activation before training</cell><cell>? 0</cell><cell>e 11</cell></row><row><cell>dimension of z what</cell><cell>d zwhat</cell><cell>32</cell></row><row><cell>pixel entropy loss weight</cell><cell cols="2">? pixel 1.10 ?2</cell></row><row><cell>minimum value of inverse scaling factor</cell><cell>s min</cell><cell>1.3</cell></row><row><cell>maximum value of inverse scaling factor</cell><cell>s max</cell><cell>24</cell></row><row><cell>dimension of inputs and outputs of transformer encoder</cell><cell>d T</cell><cell>256</cell></row><row><cell>number of heads of transformer encoder layer</cell><cell></cell><cell>8</cell></row><row><cell>dimension of feedforward transformer layer</cell><cell></cell><cell>512</cell></row><row><cell>number of layers of transformer encoder</cell><cell></cell><cell>6</cell></row><row><cell>? https://github.com/deepmind/multi_</cell><cell></cell><cell></cell></row><row><cell>object_datasets</cell><cell></cell><cell></cell></row><row><cell>? https://github.com/applied-ai-lab/</cell><cell></cell><cell></cell></row><row><cell>genesis.</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Encoding</head><p>Input: input image X Output: object latents {z what k , x k , y k , s k , ? k } 1?k?K // feature and attention maps generation (?, A 1 , .., A K ) = Segformer(X)</p><p>end // computation of positions and feature vectors before transformer refinement   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016-03" />
			<biblScope unit="page" from="3233" to="3241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<title level="m">Continuously Differentiable Exponential Linear Units. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="2" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Emergence of object segmentation in perturbed generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bielski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">MONet: Unsupervised Scene Decomposition and Representation. arxiv preprint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-End Object Detection with Transformers. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="12346" to="213" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention-driven cropping for very high resolution facial landmark detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashanth</forename><surname>Chandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thabo</forename><surname>Beeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5860" to="5869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised object segmentation by redrawing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka?l</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Arti?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Masked-attention Mask Transformer for Universal Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Per-Pixel Classification is Not All You Need for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="17" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Spatially Invariant Unsupervised Object Detection with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Crawford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3412" to="3420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention over learned object embeddings enables complex visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Botvinick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="9112" to="9124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">SOLQ: Segmenting Objects by Learning Queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CenterNet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6568" to="6577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Emami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<title level="m">Sanjay Ranka, and Anand Rangarajan. Efficient Iterative Amortized Inference for Learning Symmetric and Disentangled Multi-Object Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parker</forename><surname>Oiwi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Posner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">GENESIS: Generative Scene Inference and Sampling with Object-Centric Latent Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oiwi</forename><forename type="middle">Parker</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep spatial autoencoders for visuomotor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -IEEE International Conference on Robotics and Automation</title>
		<meeting>-IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2016-06-512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to linearize under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1234" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-object representation learning with iterative variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loie</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019-03" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="4317" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tagger: Deep unsupervised perceptual grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tele Hotloo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4491" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ShapeStacks: Learning Vision-Based Physical Intuition for Generalised Object Stacking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><forename type="middle">B</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="11205" to="724" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improving Landmark Localization with Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1546" to="1555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SegSort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien</forename><forename type="middle">Ju</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7333" to="7343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative neurosymbolic machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 2020-Decem(NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">SCALOR: Generative World Models with Scalable Object Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Janghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -30th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>-30th IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Kabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Lopez Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malcolm</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/multi-object-datasets/" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Multi-Object Datasets</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurynas</forename><surname>Karazija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequential attend, infer, repeat: Generative modelling of moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, volume 2018-Decem</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8606" to="8616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CornerNet: Detecting Objects as Paired Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="642" to="656" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">SPACE: Unsupervised Object-Oriented Scene Representation via Spatial Attention and Decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skand</forename><surname>Vishwanath Peri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Objectcentric learning with slot attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Human pose regression by combining indirect part detection and contextual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Graphics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="15" to="22" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Unsupervised Layered Image Decomposition into Object Prototypes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Monnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="9912" to="483" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015</title>
		<editor>Nassir Navab, Joachim Hornegger, William M Wells, and Alejandro F Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Autoencoderbased background reconstruction and foreground segmentation with background noise estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Sauvalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Fortelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.08001</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arxiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MarioNette: Self-Supervised Sprite Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitriy</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<publisher>NeurIPS</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Faster Attend-Infer-Repeat with Tractable Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Stelzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019-05" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="5966" to="5975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics</title>
		<imprint>
			<biblScope unit="page" from="11210" to="536" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Object Dynamics Distillation for Scene Decomposition and Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">KNEEL: Knee anatomical landmark localization using hourglass networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksei</forename><surname>Tiulpin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iaroslav</forename><surname>Melekhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simo</forename><surname>Saarakkala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2019 International Conference on Computer Vision Workshop, ICCVW 2019</title>
		<meeting>-2019 International Conference on Computer Vision Workshop, ICCVW 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Relational neural expectation maximization: Unsupervised discovery of objects and their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>ICLR 2018 -Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Nips</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5999" to="6009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Entity Abstraction in Visual Model-Based Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Veerapaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Co-Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Janner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="21" />
			<pubPlace>CoRL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Robust principal component analysis: Exact recovery of corrupted low-rank matrices by convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shankar</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22 -Proceedings of the 2009 Conference</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2080" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Group Normalization. International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="742" to="755" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="12077" to="12090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-supervised Visual Reinforcement Learning with Object-centric Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrii</forename><surname>Zadaianchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
