<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Vocoder is All You Need for Speech Super-resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohe</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosung</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Centre for Digital Music</orgName>
								<orgName type="institution">Mary University of London</orgName>
								<address>
									<region>Queen</region>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubo</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Centre for Vision, Speech and Signal Processing (CVSSP)</orgName>
								<orgName type="institution">University of Surrey</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Speech, Audio, and Music Intelligence (SAMI) Group</orgName>
								<address>
									<settlement>ByteDance</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Tian</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Speech, Audio, and Music Intelligence (SAMI) Group</orgName>
								<address>
									<settlement>ByteDance</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
							<email>dwang@cse.ohio-state.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">The Ohio State University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Vocoder is All You Need for Speech Super-resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: neural vocoder</term>
					<term>speech super resolution</term>
					<term>band- width extension</term>
					<term>deep learning</term>
					<term>flexible resolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech super-resolution (SR) is a task to increase speech sampling rate by generating high-frequency components. Existing speech SR methods are trained in constrained experimental settings, such as a fixed upsampling ratio. These strong constraints can potentially lead to poor generalization ability in mismatched real-world cases. In this paper, we propose a neural vocoder based speech super-resolution method (NVSR) that can handle a variety of input resolution and upsampling ratios. NVSR consists of a mel-bandwidth extension module, a neural vocoder module, and a post-processing module. Our proposed system achieves state-of-the-art results on VCTK multi-speaker benchmark. On 44.1 kHz target resolution, NVSR outperforms WSRGlow and Nu-wave by 8% and 37% respectively on log-spectral-distance and achieves a significantly better perceptual quality. We also demonstrate that prior knowledge in the pre-trained vocoder is crucial for speech SR by performing mel-bandwidth extension with a simple replication-padding method. Samples can be found in https://haoheliu.github.io/nvsr.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Speech super-resolution (SR) aims to improve speech fidelity by predicting the high-resolution (HR) speech given the lowresolution (LR) speech. Low-resolution speech is common for reasons such as compression and low sampling rate. From a spectrogram point of view, speech SR is also equally referred to as bandwidth extension (BWE) <ref type="bibr" target="#b0">[1]</ref>. SR is an important technique for real-world applications, including speech quality enhancement <ref type="bibr" target="#b1">[2]</ref> and text-to-speech synthesis <ref type="bibr" target="#b2">[3]</ref>.</p><p>A lot of early studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> break SR into spectral envelop estimation and excitation generation from LR. At that time, the direct mapping from LR to HR is not widely explored since the dimension of HR is relatively high. Later, SR methods based on deep neural network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> show better subjective quality than traditional methods. Most neural network based methods study the 8 kHz to 16 kHz or 4 kHz to 16 kHz sampling rate upsample problems, such as AECNN <ref type="bibr" target="#b9">[10]</ref> and TFNet <ref type="bibr" target="#b10">[11]</ref>. Recently, Nu-wave <ref type="bibr" target="#b11">[12]</ref> and WSRGlow <ref type="bibr" target="#b12">[13]</ref> have explored the higher 48 kHz target resolution, where the input sampling rate is usually 12 kHz, 16 kHz, or 24 kHz.</p><p>Existing speech SR studies are usually performed under controlled experimental settings. For example, the input resolution and bandwidth in data simulations is always fixed value during training and evaluation. However, in real-world scenarios, the speech SR applications generally require the capability of handling diverse settings such as different input resolutions and bandwidths. These mismatches potentially lead to degraded speech SR performance. Also, the demand for high upsamplingratio (UPR) SR is common on low-quality historical recordings, where UPR stands for target bandwidth divided by input bandwidth. Nevertheless, less attention has been given to the high UPR cases. High UPR speech SR is challenging because it has more low-frequency harmonic structures to be predicted. To our knowledge, most speech SR works only experiment on the UPR around 3, 4, and 6 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>To alleviate the difficulties in addressing flexible input resolution and high UPR mentioned above, we propose a neuralvocoder-based approach (NVSR) for speech SR. We train NVSR in two separate stages: (1) an HR mel spectrogram prediction stage; (2) a vocoder waveform synthesis and post-processing stage. Instead of predicting HR in the linear frequency scale as shown in previous studies <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref>, HR prediction on a lowdimensional mel frequency scale is more tractable, especially in high UPR cases. Neural Vocoder <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> is the model that upsample a low dimensional feature (e.g., mel spectrogram) to high-resolution waveforms. This coarse to fine generation process is similar to speech SR. Thus the prior knowledge of speech provided by the neural vocoder, such as phoneme structures, would be useful for speech SR.</p><p>To our knowledge, we are the first attempt that tackles the flexible input resolution and high UPR problems for speech SR. Our proposed NVSR can achieve a UPR up to 22.05 (from 2 kHz to 44.1 kHz). NVSR achieves state-of-the-art result on the VCTK Multi-speaker benchmark under multiple experimental settings, outperforming previous methods by a large margin. We observe that even replacing neural network methods with a simple replication-padding method without learning, NVSR can still significantly outperform existing speech SR methods. This demonstrates the superior performance offered by the neural vocoder for speech SR. Unlike most previous studies, we mainly focus on the multi-speaker setup, which is more challenging than the single-speaker setting studied in other literature <ref type="bibr" target="#b9">[10]</ref>. Our code and pre-trained models are open-sourced 1 to facilitate reproducibility.</p><p>The paper is organized as follows. Section 2 introduces the problem formulation of speech SR. Section 3 introduces the pipeline of NVSR. Section 4 discusses the experimental settings and analysis the results. Section 5 draw a final conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Problem Formulation</head><p>Given a discrete signal x l = [xi] i=1,2,...T ?l sampled at sampling rate l, speech SR system estimates signal y h = [yi] i=1,2,...T ?h , where T is the length in seconds and h &gt; l. According to the Nyquist theory, the highest bandwidth of x l and y h are l/2 (Hz) and h/2 (Hz), respectively. So, x l does not contain the frequency information between h/2 ? l/2 (Hz), which can be considered as the generation target of the SR task.</p><p>Most frequency domain methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref> perform SR by predicting HR spectorgram from LR spectrogram, and transformed to waveform using the HR spectrogram prediction. This process can be formulated as</p><formula xml:id="formula_0">y h = F ?1 (G(F (x h ))),<label>(1)</label></formula><p>where F and F ?1 are the time to frequency spectrogram transformation and it's reverse transformation respectively. x h is the upsampled version of x l , and function G(?) takes band-limited spectrogram as input and output full band spectrogram. Here x l is upsampled to ensure the system input and output have the same shape and the model design is not restricted by the ratio h/l. In most studies, F and F ?1 are short-time-fouriertransform (STFT) and inverse STFT (iSTFT), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Neural Vocoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>STFT Mel FB Resample High Resolution Mel Spectrogram Prediction</head><p>Replication Padding ResUNet or ! " <ref type="figure">Figure 1</ref>: Overview of the NVSR pipline. <ref type="figure">Figure 1</ref> shows the overall pipline of NVSR including mel transform Fmel, high resolution mel spectrogram prediction with G, and inverse mel transform F ?1 mel . Similar to Equation 1, this process can be written a?</p><formula xml:id="formula_1">| " | " # $ " # &amp; " Post Processing '</formula><formula xml:id="formula_2">y h = F ?1 mel (G(Fmel(x h ))) ? V ? (G(Fmel(x h ))).<label>(2)</label></formula><p>Here G aims to predict the HR mel spectrogram with the LR input, as shown in <ref type="bibr">Equation 3</ref>.</p><formula xml:id="formula_3">G : Xmel ? Ymel,<label>(3)</label></formula><p>where Xmel and Ymel stands for the mel spectrogram of x and y. Xmel is calculated by Fmel(x) = |X| W , in which X is the STFT of x and W is a set of mel filter banks. Note that the inverse mel transform is not mathematically solvable. Thus, F ?1 mel in NVSR is modeled by a neural vocoder V ? . We show that V ? achieves almost the same perceptual quality as the ideal inverse transform. For mel spectrogram prediction, we first introduce a ResUNet based method. Then we introduce a non-parametric replication-padding method, which mainly exploits the prior knowledge learned by the vocoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mel Spectrogram Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">ResUNet-based Method</head><p>We use ResUNet <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> to model the G(?) in Equation 3 and estimate Ymel. This process can be written a? where means element-wise multiplication, and is a small constant to avoid the zero values in Xmel. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, ResUNet consists of six encoder and six decoder blocks. There are skip connections between encoder and decoder blocks at the same level. Both encoder and decoder blocks share the same structure, which is four convolutional blocks (ConvBlock). Each ConvBlock consists of two convolution operations with batch normalization <ref type="bibr" target="#b19">[20]</ref>, and leakyReLU <ref type="bibr" target="#b20">[21]</ref> activation. The encoder blocks apply average pooling for downsampling. The decoder blocks apply transpose convolution for upsampling. After the last decoder block, we apply one more ConvBlock to estimate the output residual, which is added with the input to form the final estimation. Note that we use addition in <ref type="figure" target="#fig_0">Figure 2</ref> instead of multiply in Equation 4 because we use log scale during our implementation. ResUNet is optimized using the mean absolute error (MAE) loss between the estimated mel spectrogram?mel and the target mel spectrogram Ymel.</p><formula xml:id="formula_4">Ymel = ResUNet(Xmel) (Xmel + ),<label>(4)</label></formula><formula xml:id="formula_5">loss = 1 T F T F |Ymel ??mel|<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Replication padding-based Method</head><p>Replication padding-based methods do not use any training data for mel spectrogram prediction. Instead, it directly copies the energy of the cutoff frequency to the higher frequency bands. We use this method to demonstrate the importance of the neural vocoder. Specifically, we first search the cutoff frequency c based on the energy of Xmel. Then we construct the frequency cutoff mask MT ?F with binary values, where the frequency indexs greater than c are all zeros and others are all ones. MT ?F is used later to select the higher/lower frequencies bands. Finally, we repeat the energy of cutoff frequency for F ? c times and add it with LR mel spectrogram. The output is the HR mel spectrogram estimation?mel. This process can be written as Equation 6</p><formula xml:id="formula_6">Ymel = M Xmel + |1 ? M| (Xmel[:, c] ? 11?F ) ,<label>(6)</label></formula><p>where 11?F is the all-one matrix, and Xmel[:, c] is the energy distribution across time at the cutoff frequency c.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Neural Vocoder</head><p>We choose TFGAN <ref type="bibr" target="#b16">[17]</ref> as the neural vocoder V ? . TFGAN can directly upsample mel spectrogram into waveform with transpose convolution and one-dimensional convolutional neural networks. By incorporating multi-resolution losses and discriminators in both the time and frequency domains, TFGAN can achieve state-of-the-art performance on vocoding. In order to achieve speaker-independent, TFGAN is trained with a large corpus of more than one thousand speakers. As reported in <ref type="bibr" target="#b21">[22]</ref>, the mean opinion score (MOS) of the open-sourced TFGAN 2 is 3.74, with the ground truth MOS score at 3.95. Even though the output of TFGAN can achieve good perceptual quality, its output still has trivial differences from the target, which makes it achieve poor results in the objective evaluation. For example, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>, the vocoder output looks the same as the target in the waveform, but their absolute difference is still substantial. This is a common evaluation problem in generative model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. To alleviate the problem in evaluation, we propose to perform a lower-frequencies replacement (LFR) operation on the vocoder output as post-processing. Since the original lower-frequencies information in the input usually does not need to change, we replace the low-frequency bands in the vocoder output with the original input. Experimental result shows this can improve the metrics score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Post Processing</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We build the train and test sets using VCTK (version 0.92) <ref type="bibr" target="#b25">[26]</ref>, a multi-speaker English corpus that contains 110 speakers with different accents. We split it into a part for train (VCTK-Train) and a part for test (VCTK-Test). Following the data preparation strategy of <ref type="bibr" target="#b11">[12]</ref>, only the mic1 microphone data is used for experiments, and p280 and p315 are omitted for the technical issues. For the remaining 108 speakers, the last eight speakers 3 constitute the VCTK-Test. The remaining 100 speakers are defined as VCTK-Train. For the training of NVSR, all the utterances are resampled at the 44.1 kHz sample rate.</p><p>We follow the LR simulation process in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10]</ref>. Given a target speech y h in VCTK-Train, to obtain the low-resolution x l , we first convolve y h with an order eight Chebyshev type I lowpass filter with cutoff frequency l/2. Then we subsample the signal to l sample rate using polyphase filtering. We evaluate the performance of each system on different low sampling rates l.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>We use Log-spectral distance (LSD) as the evaluation metrics following <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>. For target signal y h and output estimation  </p><formula xml:id="formula_7">LSD(Y,?) = 1 T ? T t=1 1 F ? F f =1 log 10 ( Y(f, t) 2 Y(f, t) 2 ) 2 (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Baselines</head><p>We include several state-of-the-art methods as baselines. Since NVSR uses a target sampling rate at 44.1 kHz, the baseline with 48 kHz output is downsampled to 44.1 kHz for a fair comparison. Similarly, the output of NVSR can also be downsampled to compare with other baselines with a smaller target sampling rate (e.g., 16 kHz). For the 48 kHz SR model, we reproduced Nu-wave <ref type="bibr" target="#b3">4</ref> and WSRGlow <ref type="bibr" target="#b4">5</ref> with their open-sourced code and default settings. We train each Nu-wave for 200 epochs and each WSRGlow for 100k iterations following <ref type="bibr" target="#b12">[13]</ref>. For 16 kHz target sampling rate, we compared NVSR with the result reported in AECNN <ref type="bibr" target="#b9">[10]</ref>, TFNet <ref type="bibr" target="#b10">[11]</ref>, and AudioUNet <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training Details</head><p>The training high-resolution and low-resolution data pairs are built on the fly. we uniformly randomize the cutoff frequency l/2 of the training data in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16]</ref> kHz. We use an Adam optimizer <ref type="bibr" target="#b26">[27]</ref> with ?1 = 0.5, ?2 = 0.999 and a 3 ? 10 ?4 learning rate to optimize the ResUNet. We apply the first 1000 steps as the warmup phase, during which the learning rate grows linearly from 0 to 3 ? 10 ?4 . Then the learning rate is decayed by 0.85 every epoch. We stop the training after 60 epochs. For all the STFT and iSTFT, we use the Hanning window with a window length of 2048 and a hop length of 441. We use 128 mel filterbanks to calculate the mel spectrogram. We use eight Nvidia-V100-32GB GPUs to train the ResUNet, which takes about 3.9 hours.  <ref type="figure">Figure 4</ref>: Robustness test with a low-resolution audio (a) from an old movie <ref type="bibr" target="#b5">6</ref> . No ground truth is available.  <ref type="figure">Figure 5</ref>: Visualization of <ref type="table">Table.</ref> 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Result and Discussion</head><p>use * to denote a model is trained with a fixed input resolution. For example, we report four different sampling rate settings in <ref type="table" target="#tab_0">Table 1</ref> after training four different WSRGlows. Meanwhile, a single NVSR here can handle four different settings.</p><p>In <ref type="table" target="#tab_0">Table 1</ref>, NVSR achieves an average LSD of 0.86, outperforming WSRGlow's 0.94 and Nu-wave's 1.37 by 0.08 and 0.51. The total parameter number of NVSR is 99.0 million (M), of which 33.9M comes from the vocoder. While WSRGlow is the largest model with 229.9M parameters on each setting. This demonstrates the state-of-the-art performance and the efficiency of NVSR. NVSR-Pad is the replication padding-based NVSR. NVSR-Pad can already achieve an average LSD of 1.27, outperforming Nu-wave on 12 kHz and 24 kHz. This proves the prior knowledge in vocoder can map the constant energy in the padded higher-frequency bands into meaningful energy distribution on the spectrogram. In <ref type="table" target="#tab_1">Table 2</ref>, NVSR achieves the best performance on the 8 kHz input sampling rate. At 4 kHz, NVSR has a similar LSD score as AECNN. NVSR is the first to try 2 kHz to 16 kHz SR and achieve an LSD of 1.07. Note that the NVSR in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_0">Table 1</ref> is the same model.</p><p>In <ref type="figure">Figure 4</ref>, we visualize the outputs of different methods on an old movie recording. The original speech is in low resolution, with the highest frequency around 4 kHz. As shown in the yellow boxes, the output of NVSR <ref type="figure">(Figure 4e</ref>) contains properly shaped harmonic components, while other methods <ref type="figure">(Figure 4b</ref>, and c) mainly fill in the high frequencies with stochastic components. WSRGlow trained with 24 kHz input resolution <ref type="figure">(Figure 4d</ref>) fails to predict because of the mismatch between the input resolution (8 kHz) and the resolution of its training data (24 kHz). Meanwhile, we found NVSR is the only model that can repair the distribution in lower-frequency bands (green boxes). Note that here we do not use post-processing in NVSR because the original input is of low quality.</p><p>In <ref type="table" target="#tab_3">Table 3</ref>, Groundtruth-mel stands for the system using ground truth mel spectrogram directly as the input to the vocoder. This equals the model performance when the mel spectrogram prediction module works flawlessly. This experiment marks the ideally best performance of the NVSR system, with an average LSD of 0.76. We also tried to remove the post-processing LFR operation in NVSR, which degrade the average performance from 0.84 to 0.96. If we do not perform mel spectrogram prediction on NVSR, the average LSD is 1.89, which is still better than the unprocessed audio with a 4.65 LSD. This result means even without the mel prediction stage, the vocoder can still improve the metrics performance on the evaluation set.</p><p>To better understand the result, we visualize <ref type="table" target="#tab_3">Table 3</ref> in <ref type="figure">Figure 5</ref>. The red line is the theoretical best performance of our proposed system using pre-trained TFGAN. The result of NVSR is close to red line, meaning our mel spectrogram prediction module works well. <ref type="figure">Figure 5</ref> also shows the performance of NVSR will degrade without post-processing, especially in high input sampling rate cases. On 24 kHz input sampling rate, the performance of the NVSR-Pad is even on par with the NVSR without post-processing. Note that the proposed post-processing operation can also be applied to other methods like Nu-wave and is very likely to increase the model performance on the test set.</p><p>There are certain limitations of our proposed method. The performance of NVSR largely relies on the neural vocoder, which may become the bottleneck of NVSR. A possible future solution can be fine-tuning two stages in an end-to-end manner or designing an additional post-processing model for refinements. Besides, when extending NVSR to other kinds of sound, like music, vocoder may not as easy to train as the speech vocoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presents a novel and powerful neural vocoder-based speech super-resolution model NVSR. It shows strong performance across a wide range of input sampling rates between 2 kHz to 32 kHz. On the VCTK Multi-Speaker SR benchmark, NVSR outperforms the state-of-the-art models trained with different input resolution settings both on 16 kHz and 44.1 kHz evaluation sets. We also demonstrate that prior knowledge in vocoder is crucial to speech SR using a simple replication padding-based mel spectrogram prediction method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Predicting the higher frequnecies of mel spectrogram with ResUNet. The numbers in each block means input and output channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of the difference between the vocoder output and target waveforms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>LSD on VCTK-Test with 44.1 kHz target sampling rate. Model with * means it is trained with a fixed input resolution. Nu-wave [12] (3.0M?4) 1.42 1.42 1.40 1.22 WSRGlow [13] (229.9M?4) 1.12 0.98 0.87 0.79</figDesc><table><row><cell>Input sampling rate (kHz)</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>24</cell></row><row><cell>NVSR-Pad (33.9M)</cell><cell cols="4">1.54 1.46 1.18 0.91</cell></row><row><cell>NVSR (99.0M)</cell><cell cols="4">0.98 0.91 0.85 0.70</cell></row></table><note>**</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>LSD on VCTK-Test with 16 kHz target sampling rate.</figDesc><table><row><cell>Input sampling rate (kHz)</cell><cell>2</cell><cell>4</cell><cell>8</cell></row><row><cell cols="4">*  AudioUNet [7] (70.9M?3) N/A 1.40 1.32</cell></row><row><cell>*  TFNet [11] (58.8M?3)</cell><cell cols="3">N/A N/A 1.36</cell></row><row><cell>*  AECNN [10] (10.2M?3)</cell><cell cols="3">N/A 0.95 0.88</cell></row><row><cell>NVSR-Pad (33.9M)</cell><cell cols="3">2.51 2.24 1.89</cell></row><row><cell>NVSR (99.0M)</cell><cell cols="3">1.07 0.95 0.78</cell></row><row><cell>y</cell><cell></cell><cell></cell><cell></cell></row></table><note>h , LSD can be computed as Equation 7, where Y and? stand for the magnitude spectrogram of y h and? h . A lower LSD value indicates better SR performance. We report the mean LSD of the VCTK-Test as the final score of a system.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 and</head><label>1</label><figDesc>Table 2compare NVSR with other state-of-the-art models on both 44.1 kHz and 16 kHz target sampling rates. We</figDesc><table><row><cell>Hz</cell><cell></cell><cell></cell></row><row><cell>(a) Original</cell><cell>(b) Nu-wave (8kHz?48kHz) (c) WSRGlow (8kHz?48kHz) (d) WSRGlow (24kHz?48kHz)</cell><cell>(e) Proposed NVSR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Evaluation results on VCTK-Test with different input sampling rate settings. Note that NVSR can also work on other input sampling-rates between 2 kHz to 32 kHz.</figDesc><table><row><cell>Input sampling rate (kHz)</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell><cell>24</cell><cell>32</cell><cell>AVG</cell></row><row><cell>Unprocessed</cell><cell cols="8">5.69 5.50 5.15 4.85 4.54 3.84 2.95 4.65</cell></row><row><cell>Groundtruth-mel</cell><cell cols="8">0.87 0.85 0.81 0.78 0.74 0.66 0.59 0.76</cell></row><row><cell>NVSR-Pad</cell><cell cols="8">1.55 1.54 1.46 1.18 1.11 0.91 0.76 1.21</cell></row><row><cell>NVSR</cell><cell cols="8">1.04 0.98 0.91 0.85 0.79 0.70 0.60 0.84</cell></row><row><cell>w/o post proc.</cell><cell cols="8">1.06 1.02 0.98 0.96 0.93 0.91 0.89 0.96</cell></row><row><cell>w/o mel pred.</cell><cell cols="8">2.37 2.37 2.16 1.95 1.77 1.48 1.13 1.89</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/haoheliu/ssr_eval arXiv:2203.14941v1 [eess.AS] 28 Mar 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/haoheliu/voicefixer 3 p360, p361, p362, p363, p364, p374, p376, s5</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/mindslab-ai/nuwave 5 https://github.com/zkx06111/WSRGlow 6 https://www.youtube.com/watch?v=A7Q7fmIuKdM</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bandwidth extension of audio signals by spectral band replication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekstrand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Benelux Workshop on Model Based Processing and Coding of Audio. Citeseer</title>
		<meeting>the IEEE Benelux Workshop on Model Based Processing and Coding of Audio. Citeseer</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Speech enhancement via frequency bandwidth extension using line spectral frequencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chennoukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gerrits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Miet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sluijter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="665" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A mel-cepstral analysis technique restoring high frequency components from low-sampling-rate speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nankaku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tokuda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generation of broadband speech from narrowband speech based on linear mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nakatoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tsushima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Norimatsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics and Communications in Japan</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural network-based artificial bandwidth expansion of speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kontio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Laaksonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Alku</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep neural network approach to speech bandwidth expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4395" to="4399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Audio super resolution using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kuleshov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Enam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00853</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Speech bandwidth extension with wavenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Assael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Walters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="205" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speech super resolution generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Eskimez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3717" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards robust speech super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2058" to="2066" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Time-frequency networks for audio super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Acoustics, Speech, and Signal Processing</title>
		<meeting>the IEEE Conference on Acoustics, Speech, and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">NU-wave: A diffusion probabilistic model for neural audio upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02321</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">WSRGlow: A glowbased waveform generative model for audio super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08507</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning continuous representation of audio for arbitrary scale super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ok</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00195</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Phase-aware music super-resolution using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04506</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05646</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">TFGAN: Time and frequency domain based generative adversarial network for high-fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.12206</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupling magnitude and phase estimation with deep resunet for music source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Society for Music Information Retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">CWS-PResUNet: Music source separation with channel-wise subband phase-aware resunet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04685</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">VoiceFixer: Toward general speech restoration with neural vocoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.13731</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11362</idno>
		<title level="m">NU-GAN: High resolution neural upsampling with GAN</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A speech synthesis approach for high quality speech separation and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1872" to="1876" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Conditional sound generation using neural discrete time-frequency representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
