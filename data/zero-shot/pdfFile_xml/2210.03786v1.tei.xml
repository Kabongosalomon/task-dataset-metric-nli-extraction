<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating the Performance of StyleGAN2-ADA on Medical Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-07">7 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mckell</forename><surname>Woodland</surname></persName>
							<email>mewoodland@mdanderson.org</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>77005</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wood</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego, La Jolla</addrLine>
									<postCode>92093</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suprateek</forename><surname>Kundu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Koay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Odisio</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunseon</forename><forename type="middle">Christine</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aradhana</forename><forename type="middle">M</forename><surname>Venkatesan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sireesha</forename><surname>Yedururi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Mao</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><forename type="middle">B</forename><surname>Patel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Rice University</orgName>
								<address>
									<postCode>77005</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Baylor College of Medicine</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristy</forename><forename type="middle">K</forename><surname>Brock</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas MD Anderson Cancer Center</orgName>
								<address>
									<postCode>77030</postCode>
									<settlement>Houston</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating the Performance of StyleGAN2-ADA on Medical Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-07">7 Oct 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>StyleGAN2-ADA ? Fr?chet Inception Distance ? Visual Tur- ing Test ? Data Augmentation ? Transfer Learning 2 Woodland et al</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although generative adversarial networks (GANs) have shown promise in medical imaging, they have four main limitations that impede their utility: computational cost, data requirements, reliable evaluation measures, and training complexity. Our work investigates each of these obstacles in a novel application of StyleGAN2-ADA to high-resolution medical imaging datasets. Our dataset is comprised of liver-containing axial slices from non-contrast and contrast-enhanced computed tomography (CT) scans. Additionally, we utilized four public datasets composed of various imaging modalities. We trained a StyleGAN2 network with transfer learning (from the Flickr-Faces-HQ dataset) and data augmentation (horizontal flipping and adaptive discriminator augmentation). The network's generative quality was measured quantitatively with the Fr?chet Inception Distance (FID) and qualitatively with a visual Turing test given to seven radiologists and radiation oncologists.</p><p>The StyleGAN2-ADA network achieved a FID of 5.22 (? 0.17) on our liver CT dataset. It also set new record FIDs of 10.78, 3.52, 21.17, and 5.39 on the publicly available SLIVER07, ChestX-ray14, ACDC, and Medical Segmentation Decathlon (brain tumors) datasets. In the visual Turing test, the clinicians rated generated images as real 42% of the time, approaching random guessing. Our computational ablation study revealed that transfer learning and data augmentation stabilize training and improve the perceptual quality of the generated images. We observed the FID to be consistent with human perceptual evaluation of medical images. Finally, our work found that StyleGAN2-ADA consistently produces high-quality results without hyperparameter searches or retraining.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recently, generative adversarial networks (GANs) have shown promise in many medical imaging tasks, including data augmentation in computer-aided diagnosis <ref type="bibr" target="#b22">[21]</ref>, image segmentation <ref type="bibr" target="#b33">[29]</ref>, image reconstruction <ref type="bibr" target="#b18">[17]</ref>, treatment planning <ref type="bibr" target="#b0">[1]</ref>, image translation <ref type="bibr" target="#b10">[10]</ref>, and anomaly detection <ref type="bibr" target="#b25">[23]</ref>. Despite their potential in medical imaging, GANs have several drawbacks that impede both their capabilities and utilization in the medical field. These obstacles include computational cost, data requirements, flawed measures of assessment, and training complexity.</p><p>GANs are computationally expensive. The original StyleGAN2 project took 51.06 GPU years to create, 0.23 of which were used for training the Flickr-Faces-HQ (FFHQ) weights used in our paper <ref type="bibr" target="#b16">[15]</ref>. Despite being the state-ofthe-art generative model for high-resolution images, StyleGAN2 is often not used in medical imaging literature due to its expense <ref type="bibr" target="#b26">[24]</ref>. If it is used, images are brought to lower resolutions to offset the cost <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b24">22]</ref>. While StyleGAN <ref type="bibr" target="#b15">[14]</ref> (the predecessor to StyleGAN2) has been applied to high-resolution medical images <ref type="bibr" target="#b6">[7]</ref>, we believe our paper is the first rigorous evaluation of StyleGAN2 on multiple high-resolution medical imaging datasets.</p><p>At high-resolutions, GANs require hundreds of thousands of images to effectively train, a requirement that is extremely challenging to satisfy in the medical field. With limited data, the GAN's discriminator overfits on the training examples, obstructing the GAN's ability to converge. Adaptive discriminator augmentation (ADA) was designed to reduce discriminator overfitting through a wide range of data augmentations that do not "leak" to the generated distribution. When applied to a histopathology dataset, ADA improved the FID by 84% <ref type="bibr" target="#b13">[12]</ref>. In our paper, we perform a computational ablation study that examines how ADA and transfer learning affects performance on medical images.</p><p>One of the greatest challenges in GANs is constructing robust quantitative evaluation measures <ref type="bibr" target="#b19">[18]</ref>. The Fr?chet Inception Distance (FID) <ref type="bibr" target="#b9">[9]</ref> is the standard for state of the art evaluation for generative modeling in natural imaging. It relies on an Inception network that was trained on ImageNet, which does not contain medical images <ref type="bibr" target="#b5">[6]</ref>, for its calculation. As such, a common assumption in related literature is that the FID is not applicable to medical images. We revisit this assumption by testing the correlation between the FID and human perceptual evaluation on medical images.</p><p>GANs are notoriously challenging to train. They have numerous hyperparameters and suffer from training instability. In a large empirical evaluation of various GANs, Lu?i? et al. <ref type="bibr" target="#b19">[18]</ref> found that GAN training is extremely sensitive to hyperparameter settings. A separate study illustrated this sensitivity by performing 1,500 hyperparameter searches on three unique medical imaging datasets with various GAN architectures. The authors found that few models produced meaningful images; even fewer models achieved reasonable metric evaluations <ref type="bibr" target="#b28">[26]</ref>. Neither of these studies examined StyleGAN2. Our work is unique in that we test the stability of StyleGAN2, along with its ability to generate quality images without a hyperparameter search.</p><p>The main contributions of our research are as follows:</p><p>-We apply StyleGAN2 to a variety of high-resolution medical imaging datasets.</p><p>-We perform a computational ablation study on the effect of transfer learning and data augmentation on a limited-data medical imaging dataset. -We provide empirical evidence that the FID is consistent with human perceptual evaluation of medical images. -We evaluate StyleGAN2's stability and ability to produce quality results without a hyperparameter search. -We achieve state-of-the-art FIDs on four public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data</head><p>We used the 97 non-contrast and 108 contrast enhanced abdominal computed tomography (CT) scans presented in <ref type="bibr" target="#b1">[2]</ref>. To accentuate the liver, the data was windowed to a level 50 and a width 350, consistent with the preset values for viewing the liver in a commercial treatment planning system (RayStation v10, RaySearch Laboratories, Stockholm, Sweden). All axial slices that contained no liver information were discarded. Voxel values were mapped to the range [0, 255] and converted each axial slice to a PNG image. In all, our training dataset contained 10,600 512x512 images. Three randomly sampled images from our training dataset are shown in the first row of <ref type="figure">Figure 1</ref>. We used an additional 143,345 512x512 images for one experiment in our ablation study. These images were obtained by applying the above mentioned preprocessing steps to 3,029 abdominal CT scans (301 patients) that were retrospectively acquired under an IRB approved protocol.</p><p>Separately, our methods were applied to several publicly available datasets. For the "Segmentation of the Liver Competition 2007" (SLIVER07) dataset 5 <ref type="bibr" target="#b8">[8]</ref>, we used the 20 scans available in the training dataset and converted each slice to a PNG image without any further preprocessing. In total, this dataset consisted of 4,159 512x512 images. To our knowledge, the previous best FID (29.06) on this dataset was achieved by Skandarani et al. using the StyleGAN network.</p><p>The ChestX-ray14 dataset 6 <ref type="bibr" target="#b32">[28]</ref> consists of 112,120 1024x1024 Chest X-ray images in PNG format. The previous best FID on the ChestX-ray14 dataset of 8.02 was achieved using a Progressive Growing GAN <ref type="bibr" target="#b26">[24]</ref>. No preprocessing on this dataset was performed. The Automated Cardiac Diagnosis Challenge (ACDC) dataset 7 <ref type="bibr" target="#b2">[3]</ref> consists of 150 cardiac cine-magnetic resonance imaging (MRI) exams. We used the training dataset, which consists of 100 exams. The images were rescaled to the range [0, 255] using SimpleITK <ref type="bibr" target="#b17">[16]</ref> and padded with zeros. Each slice was then converted to a 2D PNG image. In total, this dataset consisted of 1,902 512x512 images. The previous best FID on the ACDC training dataset (24.74) was achieved with StyleGAN <ref type="bibr" target="#b28">[26]</ref>.</p><p>Additionally, we applied StyleGAN2-ADA to a dataset whose FID had not been previously evaluated: the brain tumor data from the Medical Segmentation Decathlon 8 <ref type="bibr" target="#b27">[25]</ref>, which contains 750 4D MRI volumes. The gadolinium-enhanced T1-weighted 3D images were extracted and windowed to the range [0, 255] using SimpleITK. Slices were converted to 2D PNG images. This dataset consists of 103,030 256x256 PNG images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Generative Modeling</head><p>Due to its state-of-the-art performance on high-resolution images, we used a StyleGAN2 network as our generative model <ref type="bibr" target="#b16">[15]</ref>. For our experiments, we utilized the StyleGAN2 configuration of the official StyleGAN3 repository 9 <ref type="bibr" target="#b14">[13]</ref>. We used the default parameters provided by the implementation, with the exception of changing ? 0 to 0.9 in the Adam optimizer and disabling mixed precision. We did not perform a hyperparameter search. We explored the effects of transfer learning and data augmentation in an ablation study with the following experimental designs: Each of these experiments was performed on our liver CT training dataset. A variation of Experiment 1 was also performed where 143,345 liver images were added to the training dataset. Furthermore, Experiment 4 was performed on the four public datasets. Each experiment was performed on a DGX with eight 40GB A100 GPUs. DGXs were accessed using the XNAT platform <ref type="bibr" target="#b20">[19]</ref>. Experiments ran for 6,250 ticks with metrics calculated and weights saved every 50 ticks. Each experiment took approximately 1.5, 4, and 7 days to complete for 256x256, 512x512, and 1024x1024 sized datasets, respectively. We repeated each experiment five times to test algorithm stability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluation Measures</head><p>Fr?chet Inception Distance The FID is the standard for state of the art GAN evaluation in natural imaging. It is the Fr?chet distance between two multivariate Gaussians constructed from representations extracted from the coding layer of an Inception network that was pretrained on ImageNet <ref type="bibr" target="#b9">[9]</ref>. Several advantages of the FID include its ability to distinguish generated from real samples, agreement with human perceptual judgements, sensitivity to distortions, and computational and sample efficiency <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">9]</ref>. As such, we used the FID as our quantitative metric. For each run, we reported the best FID achieved during training. We used the model weights associated with each best FID for further qualitative analysis. For statistical testing, we used permutation tests with ? = 0.05. Because ImageNet does not contain medical images, prior publications have argued that the FID is not applicable to medical imaging <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">11,</ref><ref type="bibr" target="#b30">27]</ref>. As such, they substitute the Inception network with their own encoding networks. This trend has several limitations. First, the FID is only consistent as a metric inasmuch as the same encoding model is used. By using a new model, the reported distance can no longer be considered in the context of prior work that utilizes the FID. Second, the algorithm designer is formulating their own evaluation metric, which will likely introduce unquantified bias into the presented results. Due to these limitations, we use the original definition of the FID for our calculations.</p><p>Visual Turing Tests Because the applicability of the FID to medical imaging is not well understood, our first visual Turing test evaluated the correlation between the FID and human perception on medical images. The test was administered in a Google Form with four sections (created in random order), one per experiment. Each section contained 40 randomly shuffled images, 20 real and 20 generated. All images were randomly selected and only appeared once in the test. The test was given to five participants with a medical physics background who were not familiar with the images. We evaluated the test with the false positive rate (FPR) and false negative rate (FNR).</p><p>The purpose of the second visual Turing test was to rigorously validate the perceptual quality of the images generated by the pretrained StyleGAN2-ADA model on our dataset. This test consisted of 50 real and 50 generated images randomly sampled and shuffled. Each section contained one image, a question asking the participant if the image was real or fake, and a Likert scale assessing how realistic the image was. The Likert scale was between 1 (fake) and 5 (real). The test was given to seven radiologists or radiation oncologists with an average of 10 years of radiological experience. The results of the Turing test were evaluated with precision, recall, accuracy, FPR, and FNR metrics. Additionally, we computed the average Likert values for both real and generated images. For statistical testing, we used permutation tests with ? = 0.10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>On our dataset, the average (?SD) FIDs, n=5, achieved were 10.70 (?0.72), 7.62 (?0.35), 7.51 (?0.89), and 5.22 (?0.17), for Experiments 1-4, respectively. Both transfer learning and data augmentation were effective tools in mitigating overfitting on limited medical data. Individually, they improved upon the baseline FID by about 30% (95% confidence). Even greater improvements were achieved (50% decrease) in the FID when transfer learning and augmentations were used in tandem (95% confidence). Data augmentation significantly decreased the generator's loss and stabilized training, as shown in <ref type="figure">Figure 3</ref> in the Appendix (95%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real Baseline StyleGAN2</head><p>Pretrained StyleGAN2-ADA <ref type="figure">Fig. 1</ref>. The first row contains images from our training dataset. The second and third rows contain images generated by the baseline StyleGAN2 model. The fourth and fifth rows contain images generated by the pretrained StyleGAN2-ADA model with augmentations. All images were randomly selected. The images generated by the pretrained StyleGAN2-ADA model demonstrate reduced noise artifacts, enhanced detail, and superior anatomical accuracy confidence). Our results show that transfer learning does not need to be performed from a medical imaging dataset to be effective. When Experiment 1 was repeated with the additional 143,345 images, the average (? SD) FID, n=5, attained was <ref type="bibr">8.45 (?0.20)</ref>. This demonstrates that transfer learning and data augmentation, both in conjunction and independently, outperformed a fifteenfold increase in the dataset size.</p><p>On the SLIVER07, ChestX-ray14, and ACDC datasets, we lowered the record FIDs from 29.06 to 10.78 (mean 11.99 ? 1.57), 8.02 to 3.52 (mean 3.63 ? 0.07), and 24.74 to 21.17 (mean 21.43 ? 0.32), respectively. For the Medical Segmentation Decathalon (brain tumors) data, we set a new record FID of 5.39 (mean 5.53 ? 0.01). These state-of-the-art results indicate that StyleGAN2 has stable performance and can generate quality medical images without a hyperparameter search. <ref type="table" target="#tab_0">Table 1</ref> shows the results of the multi-model visual Turing test. This table provides empirical evidence that the FID is consistent with human perceptual judgement on medical images: the lower the FID, the higher the average FPR (Pearson correlation of -0.91, 90% confidence). This suggests that as the FID decreases, it becomes increasingly difficult for humans to distinguish between real and generated images. In addition, the FPRs demonstrate that augmentations improved the perceptual quality of the generated images (90% confidence). When data augmentation was combined with transfer learning, the average participant was more likely to say a generated image was real than fake (55% FPR).  <ref type="figure">Figure 1</ref> displays randomly selected real and generated images from the baseline StyleGAN2 (10.43 FID) and the pretrained StyleGAN2-ADA (5.06 FID) models. Many of the images generated by the baseline StyleGAN2 model contain noise artifacts, especially in the liver. Images generated by the pretrained StyleGAN2-ADA model show reduced noise, enhanced detail, and superior anatomical accuracy. This perceptual improvement substantiates the claim that the FID is applicable to medical images. The Appendix contains auxiliary pretrained StyleGAN2-ADA generated images ( <ref type="figure" target="#fig_0">Figure 4</ref>) and a larger image demonstrating noise artifacts in the baseline StyleGAN2 model <ref type="figure">(Figure 2)</ref>.</p><p>The results of the Turing test given to clinicians, shown in <ref type="table" target="#tab_2">Table 2</ref>, further confirm the high-quality nature of the generated images. Overall, the clinicians classified generated images as real 42% of the time, approaching the equivalent of random guessing. Those that had low FPRs typically had higher FNRs and vice versa (Pearson correlation of -0.71, 90% confidence), indicating a tendency of clinicians to favor either "real" or "fake" when they were unsure. This tendency was likely a factor in the high interobserver variability among the FPRs. Another likely factor was the experience of the clinicians. For the Likert scale, we found that real images achieved an average score of 3.99 (? 1.00) and generated images a score of 3.23 (? 1.21). The overlapping 95% confidence intervals further demonstrate both the challenging nature of the task and the high-quality nature of the generated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We applied StyleGAN2 to multiple high-resolution medical image datasets. Combined with transfer learning and data augmentation, the architecture achieved state-of-the-art results consistently, without any hyperparameter searches or retraining. The generated images were of sufficient quality that an expert's ability to tell whether or not an image was generated approached random guessing. Additionally, we found that the "realness" score, based on a 5-point Likert scale, differed between the generated and real images by less than the standard deviation between clinicians. Across a variety of medical imaging modalities, we were able to set new record FID scores on four publicly-available datasets. Furthermore, our research provided empirical evidence that the FID is consistent with human perceptual judgement on medical images. A multi-model visual Turing test revealed that as the FID improved, the participants perceived artificially generated images as real more frequently. Qualitatively, we saw an appreciable improvement in the fidelity of the generated images as the FID improved from 10.43 to 5.06. From these results, we concluded that the FID is indeed an appropriate metric for medical images. <ref type="figure">Fig. 2</ref>. This image was generated by the baseline StyleGAN2 model (10.43 FID). It was chosen to demonstrate the noise artifacts contained in many of the images generated by the model. <ref type="figure">Fig. 3</ref>. The average generator loss (with standard deviation bars) across training. We see that augmentation not only significantly decreases the loss, but also leads to more stable convergence. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 .</head><label>4</label><figDesc>Randomly selected images generated by the pretrained StyleGAN2-ADA model (5.06 FID).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1 .</head><label>1</label><figDesc>Baseline Disable all StyleGAN2 augmentations and train from scratch. 2. Pretrained Disable all augmentations and begin training with pretrained weights from StyleGAN2 trained on the FFHQ dataset. 3. Augmented Enable mirroring (horizontal flipping) and ADA and train from scratch. 4. Pretrained and Augmented Enable mirroring and ADA and begin training with the official FFHQ StyleGAN2 weights.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Average (?SD) results, n=5, of the multi-model visual Turing test. FIDs are associated with the model used to generate the images in the Turing tests.</figDesc><table><row><cell cols="2">Multi-Model Visual Turing Test Results</cell></row><row><cell>Experiment</cell><cell>FID FPR[%] FNR[%]</cell></row><row><cell>1. Baseline</cell><cell>10.43 29 (?27) 32 (?21)</cell></row><row><cell>2. Pretrained</cell><cell>7.78 34 (?19) 32 (?18)</cell></row><row><cell>3. Augmented</cell><cell>7.15 49 (?11) 34 (?18)</cell></row><row><cell cols="2">4. Pretrained and Augmented 5.06 55 (?9) 41 (?11)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results of the visual Turing test given to clinicians.</figDesc><table><row><cell></cell><cell cols="4">Clinician Visual Turing Test Results</cell><cell></cell></row><row><cell>Clinician</cell><cell cols="5">Precision [%] Recall [%] Accuracy [%] FPR [%] FNR [%]</cell></row><row><cell>1</cell><cell>80</cell><cell>86</cell><cell>82</cell><cell>22</cell><cell>14</cell></row><row><cell>2</cell><cell>76</cell><cell>44</cell><cell>65</cell><cell>14</cell><cell>56</cell></row><row><cell>3</cell><cell>56</cell><cell>80</cell><cell>58</cell><cell>64</cell><cell>20</cell></row><row><cell>4</cell><cell>79</cell><cell>62</cell><cell>73</cell><cell>16</cell><cell>38</cell></row><row><cell>5</cell><cell>58</cell><cell>98</cell><cell>64</cell><cell>70</cell><cell>2</cell></row><row><cell>6</cell><cell>54</cell><cell>88</cell><cell>56</cell><cell>76</cell><cell>12</cell></row><row><cell>7</cell><cell>59</cell><cell>48</cell><cell>57</cell><cell>34</cell><cell>52</cell></row><row><cell>Average (?SD)</cell><cell>66 (?12)</cell><cell>72 (?21)</cell><cell>65 (?10)</cell><cell cols="2">42 (?27) 28 (?21)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://sliver07.grand-challenge.org/ 6 https://nihcc.app.box.com/v/ChestXray-NIHCC 7 https://acdc.creatis.insa-lyon.fr/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">http://medicaldecathlon.com/ 9 https://github.com/NVlabs/stylegan3</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by the Tumor Measurement Initiative through the MD Anderson Strategic Initiative Development Program (STRIDE). We thank the NIH Clinical Center for the ChestX-ray14 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>StyleGAN2-ADA on Medical Images</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rapid treatment planning for low-dose-rate prostate brachytherapy with tp-gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Aleef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">T</forename><surname>Spadinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Peacock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Salcudean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87202-1_56</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87202-156" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>de Bruijne, M., et al.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="581" to="590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated contouring of contrast and noncontrast computed tomography liver images with fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Anderson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.adro.2020.04.023</idno>
		<ptr target="https://doi.org/10.1016/j.adro.2020.04.023" />
	</analytic>
	<monogr>
		<title level="j">Adv Radiat Oncol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">100464</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning techniques for automatic mri cardiac multistructures segmentation and diagnosis: Is the problem solved?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bernard</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2837502</idno>
		<ptr target="https://doi.org/10.1109/TMI.2018.2837502" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2514" to="2525" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cviu.2018.10.009</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2018.10.009" />
	</analytic>
	<monogr>
		<title level="j">Comput Vis Image Underst</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="page" from="41" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Targan: Target-aware generative adversarial networks for multi-modality medical image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_3</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-13" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>de Bruijne, M., et al.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="24" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2009.5206848" />
	</analytic>
	<monogr>
		<title level="m">CVPR 2009</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent space manipulation for high-resolution medical image synthesis via the stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Z Med Phys</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="305" to="314" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.zemedi.2020.05.001</idno>
		<ptr target="https://doi.org/10.1016/j.zemedi.2020.05.001" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Comparison and evaluation of methods for liver segmentation from ct datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Heimann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2009.2013851</idno>
		<ptr target="https://doi.org/10.1109/TMI.2009.2013851" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1251" to="1265" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Curran Associates Inc</publisher>
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthesis of contrast-enhanced spectral mammograms from low-energy mammograms using cgan-based synthesis network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>de Bruijne, M., et al.</editor>
		<imprint>
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="68" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-030-87234-2_7</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87234-27" />
		<imprint>
			<date type="published" when="2021" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional gan with an attention-based generator and a 3d discriminator for 3d medical image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_31</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-131" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>de Bruijne, M., et al.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="318" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balcan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020</title>
		<editor>Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12104" to="12114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Alias-free generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2021</title>
		<editor>J.W.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="852" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00453</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2019.00453" />
	</analytic>
	<monogr>
		<title level="m">CVPR 2019</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4396" to="4405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00813</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00813" />
	</analytic>
	<monogr>
		<title level="m">CVPR 2020</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8107" to="8116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The design of simpleitk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lowekamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ibanez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blezek</surname></persName>
		</author>
		<idno type="DOI">10.3389/fninf.2013.00045</idno>
		<ptr target="https://doi.org/10.3389/fninf.2013.00045" />
	</analytic>
	<monogr>
		<title level="j">Front Neuroinform</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d transformer-gan for high-quality pet reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-87231-1_27</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-87231-127" />
	</analytic>
	<monogr>
		<title level="m">MICCAI 2021</title>
		<editor>de Bruijne, M., et al.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">12904</biblScope>
			<biblScope unit="page" from="276" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Are gans created equal? a large-scale study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<editor>Bengio, S., et al.</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The extensible neuroimaging archive toolkit: an informatic platform for managing, exploring, and sharing neuroimaging data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramaratnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Buckner</surname></persName>
		</author>
		<idno type="DOI">10.1385/ni:5:1:11</idno>
		<ptr target="https://doi.org/10.1385/ni:5:1:11" />
	</analytic>
	<monogr>
		<title level="j">Neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generative adversarial networks to improve fetal brain fine-grained plane classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bonet-Carne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">P</forename><surname>Burgos-Artizzu</surname></persName>
		</author>
		<idno type="DOI">10.3390/s21237975</idno>
		<ptr target="https://doi.org/10.3390/s21237975" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised ganbased radiomics model for data augmentation in breast ultrasound mass classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H D</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Methods Programs Biomed</title>
		<imprint>
			<biblScope unit="volume">203</biblScope>
			<biblScope unit="page">106018</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.cmpb.2021.106018</idno>
		<ptr target="https://doi.org/10.1016/j.cmpb.2021.106018" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection in digital pathology using gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pocevi?i?t?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lundstr?m</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI48211.2021.9434141</idno>
		<ptr target="https://doi.org/10.1109/ISBI48211.2021.9434141" />
	</analytic>
	<monogr>
		<title level="m">ISBI 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1878" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-59050-9_12</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-59050-912" />
		<editor>Niethammer, M., et al.</editor>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="146" to="157" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluating the clinical realism of synthetic chest x-rays generated using progressively growing gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pantanowitz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42979-021-00720-7</idno>
		<ptr target="https://doi.org/10.1007/s42979-021-00720-7" />
	</analytic>
	<monogr>
		<title level="j">SN Comput Sci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">321</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A large annotated medical image dataset for the development and evaluation of segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Simpson</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1902.09063</idno>
		<idno>abs/1902.09063</idno>
		<ptr target="https://doi.org/10.48550/arXiv.1902.09063" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Gans for medical image synthesis: An empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Skandarani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lalande</surname></persName>
		</author>
		<idno>abs/2105.05318</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<idno type="DOI">10.48550/arXiv.2105.05318</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2105.05318" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Evaluating gans in medical imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tronchin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicilia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cordelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Soda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DGM4MICCAI 2021 and DALI 2021</title>
		<editor>Engelhardt, S., et al.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="112" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-030-88210-5_10</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-88210-510" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Summers</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.369</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.369" />
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3462" to="3471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative adversarial networks in medical image segmentation: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Biol Med</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page">105063</biblScope>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.compbiomed.2021.105063</idno>
		<ptr target="https://doi.org/10.1016/j.compbiomed.2021.105063" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
