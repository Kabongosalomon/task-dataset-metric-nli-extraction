<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointManifold: Using Manifold Learning for Point Cloud Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Electronic and Computer Engineering</orgName>
								<orgName type="institution">Peking University PKU Campus</orgName>
								<address>
									<addrLine>No. 2199, Lishui Road</addrLine>
									<settlement>Xili Lake</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Nanshan District</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PointManifold: Using Manifold Learning for Point Cloud Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a point cloud classification method based on graph neural network and manifold learning. Different from the conventional point cloud analysis methods, this paper uses manifold learning algorithms to embed point cloud features for better considering the geometric continuity on the surface. Then, the nature of point cloud can be acquired in low dimensional space, and after being concatenated with features in the original three-dimensional (3D) space, both the capability of feature representation and the classification network performance can be improved. We propose two manifold learning modules, where one is based on locally linear embedding algorithm, and the other is a nonlinear projection method based on neural network architecture. Both of them can obtain better performances than the state-of-the-art baseline. Afterwards, the graph model is constructed by using the k nearest neighbors algorithm, where the edge features are effectively aggregated for the implementation of point cloud classification. Experiments show that the proposed point cloud classification methods obtain the mean class accuracy (mA) of 90.2% and the overall accuracy (oA) of 93.2%, which reach competitive performances compared with the existing state-of-the-art related methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>With the developments of laser radar and other imaging instruments, three-dimensional (3D) data is becoming much more easily available. Consequently, the effective processing and analysis methods should be investigated for 3Drelated applications. As the representative 3D data, point cloud has been widely adopted in indoor navigation, autopilot, and augmented reality, etc. The effective classification of point clouds can be helpful for the better understanding of intelligent systems to different complicated environments. Therefore, the accurate classification of point clouds plays an important role in related practical applications.</p><p>With the improvement of computing power and the substantial increase of data, deep learning <ref type="bibr" target="#b4">(LeCun, Bengio, and Hinton 2015)</ref> technology has become more and more popular for point cloud classification <ref type="bibr" target="#b10">(Maturana and Scherer 2015;</ref><ref type="bibr" target="#b13">Qi et al. 2016;</ref><ref type="bibr" target="#b17">Su et al. 2015)</ref>. Particularly, graph neural network <ref type="bibr" target="#b27">(Zhang, Cui, and Zhu 2018)</ref> develops rapidly, where many data sources can be effectively represented as * Corresponding author: Wei Gao. graphs for modeling and analysis, such as two-dimensional (2D) image, social networking, 3D point sets. Due to the fact that deep learning has the powerful capability in fitting potential relationship among graph nodes, it can fulfill the complicated modeling tasks for high-dimensional data. For 3D point cloud, the properties of data can be actually suitable for graph representation, and therefore the graph neural networks-based analysis method has emerged as a promising exploration direction <ref type="bibr" target="#b26">(Zhang and Rabbat 2018;</ref><ref type="bibr" target="#b20">Wang et al. 2019;</ref><ref type="bibr" target="#b5">Li et al. 2019;</ref><ref type="bibr" target="#b23">Xu et al. 2020)</ref>.</p><p>Point clouds are mainly used to represent the surface shape of objects. At present, the mainstream methods, e.g. <ref type="bibr" target="#b12">(Qi et al. 2017a;</ref><ref type="bibr" target="#b20">Wang et al. 2019)</ref>, directly use Euclidean distance when analyzing the local feature of point clouds, but Euclidean distance cannot accurately reflect the relationship between points due to the curvatures. Additionally, manifold is an extension of curve and surface in the original Euclidean space. Although manifold topological space can be locally treated as Euclidean space, it can be more powerful to evaluate all the elements and their connections. The core idea of manifold learning algorithm is to map data from high dimension to low dimension, which can remove data redundancy while preserving geometric topological relations. Using manifold learning method on point cloud can consider the continuity of the geometric surface of the object, then improve the description for the nature of the geometric shape.</p><p>To introduce manifold learning for point cloud classification, we formulate a novel neural network architecture, named Manifold Learning Module, which consists of two alternative sub-modules, i.e., Locally Linear Embedding (LLE) Module and Manifold Projection (MP) Module. The functions of these two modules are similar. More specifically, the dimensionality of point cloud features is reduced by manifold learning, and then the new generated features are concatenated with the original features to enrich the input features for neural network-based classification.</p><p>In the Locally Linear Embedding Module, we establish the correlation between high-dimensional and lowdimensional spaces by the local symmetries, and then reconstruct the points by neighborhood-preserving mapping. Hence, we can obtain a low-dimensional point set that maintain the continuity of the geometric surface. Additionally, to implement an end-to-end manifold learning method, we present a novel neural network architecture named Manifold Projection Module, which focuses on fitting nonlinear projection mapping from 3D to 2D. Experimental results on the ModelNet40 dataset demonstrate that our methods have better feature representation capability and can lead to better classification results.</p><p>In summary, the main contributions of the proposed method are as follows:</p><p>? We propose a novel point cloud classification method with manifold learning and graph neural network, namely PointManifold.</p><p>? We introduce two manifold learning methods in point cloud classification task, where one is based on locally linear embedding, and the other is a novel manifold projection method based on the designed neural network.</p><p>? With the feature engineering of manifold learning and feature aggregation of graph neural network, compared with previous PointNet series methods, graph-based methods and other state-of-the-art methods, PointManifold get a competitive performance, and significant improvement from its baseline. Besides, we do some ablation study to explore the relationship between two manifold modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In this section, we briefly review different categories of deep learning-based point cloud analysis methods.</p><p>View-based and volumetric methods. In the task of deep learning-based classification and semantic segmentation of two-dimensional (2D) images, feature extraction and embedding are required. The most direct analysis method is rendering 3D point cloud into 2D images, and then uses conventional 2D image classification neural networks, e.g. <ref type="bibr" target="#b17">(Su et al. 2015;</ref><ref type="bibr" target="#b25">Yavartanoo, Kim, and Lee 2018;</ref><ref type="bibr" target="#b13">Qi et al. 2016)</ref>. Moreover, 3D data is a generalization of 2D data, where voxel can be extended from pixel. Therefore, promoting 2D convolutional neural network to 3D convolution is another solution to deal with this task. However, 3D point clouds are sparse and disorderly, and such methods as VoxelNet (Maturana and Scherer 2015) require a large amount of computations. In addition, due to the sparse and uneven density distribution of point clouds, both of view-based and volumetric methods are not sufficiently effective to obtain satisfactory performances. Specifically, large-scale scenes may lead to incredible computation complexity, and the data type transformation from point cloud to voxel will inevitably cause information loss.</p><p>Point cloud-based methods (PointNet Series). To overcome the defects of view-based and volumetric methods, processing each point independently is feasible. The milestone of deep learning-based point cloud analysis is Point-Net <ref type="bibr" target="#b12">(Qi et al. 2017a)</ref>, which uses the multi-layer perceptron (MLP) to extract point feature with high dimension, then uses max pooling to obtain the representative feature vector, which solves the disorder of point cloud simultaneously. Graph-based methods. With the development of graph neural network, applying graph to point cloud analysis has become an emerging research direction. There are some representative works <ref type="bibr" target="#b26">(Zhang and Rabbat 2018;</ref><ref type="bibr" target="#b20">Wang et al. 2019;</ref><ref type="bibr" target="#b5">Li et al. 2019)</ref>. DGCNN <ref type="bibr" target="#b20">(Wang et al. 2019</ref>) uses graph to express point features and utilizes convolution operation. This work makes a summary for the graph-based point cloud analysis method and expresses the frameworks in formula.</p><p>Geometry-based methods. Geometry-based point cloud analysis network is also developed these years. TangentConv <ref type="bibr" target="#b18">(Tatarchenko et al. 2018</ref>) projects point cloud into tangent planes, then 2D convolution is adopted. FPConv <ref type="bibr" target="#b8">(Lin et al. 2020</ref>) learns a local weight matrix to flatten point cloud to 2D grid. MoNet <ref type="bibr" target="#b11">(Monti et al. 2017)</ref> gives a unified framework for generalizing traditional convolutional neural network to non-Euclidean geometric data in spatial domain. ShapeContextNet <ref type="bibr" target="#b9">(Liu 2018)</ref> applies the shape context description in traditional computer vision field to point cloud representation. SPLATNet <ref type="bibr" target="#b16">(Su et al. 2018</ref>) and SO-Net (Li, Chen, and Hee Lee 2018) also focus on the representation of point cloud. Meanwhile, <ref type="bibr" target="#b2">(Hermosilla et al. 2018;</ref><ref type="bibr" target="#b19">Thomas et al. 2019;</ref><ref type="bibr" target="#b15">Shen et al. 2018</ref>) focus on migrating convolution operation to point cloud with geometric consideration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Proposed Method</head><p>In this section, we introduce PointManifold for point cloud classification. Inspired by DGCNN <ref type="bibr" target="#b20">(Wang et al. 2019)</ref>, PointManifold uses graph neural network to embed the point features, and then uses convolution operation to extract features for classification task. Meanwhile, to get the geometric nature of point cloud, we apply manifold learning to enrich the dimension of point features. The main architecture of our method is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Locally Linear Embedding Module</head><p>In fact, a lot of redundancies still exist in many data representation methods, and each data may even have thousands of features, but they can acutally be well represented by limited parameters. As an efficient method for obtaining these parameters, manifold learning algorithm can give the low dimensional representation for data. There are many methods to solve this problem, such as Isomap, locally linear embedding (LLE) , laplacian eigenmaps, etc. Isomap is a generalization of principal content analysis (PCA) algorithm on manifolds, and it is necessary to calculate the distance between all point pairs. Hence, Isomap requires a large amount of calculation. Since LLE only concerns the equality of distances in local range, it will have less computation overhead. Moreover, LLE can also benefit the local to global neural network-based analysis for point clouds. For the sake of low computation complexity and effectiveness of analysis, we choose locally linear embedding algorithm for point cloud dimension reduction.</p><p>Notation. Let P be a point set, r and d are the dimensions of the original space and the new space, respectively, thus P = {p 1 , ..., p n }, p i ? R r . The aim of locally linear embedding is to get a new points representation in lower dimension space, denoted asP ? R d?n , d &lt; r.</p><p>Calculate distance matrix. The locally linear embedding algorithm requires that the relative distance of the points in the local range should be as unchanged as possible. In other words, one point can be reconstructed by using the features of its adjacent points, which is in fact consistent with the nearest neighbor relationship of point cloud analysis. Let p i be a central point in local range, N i is the n range neighbor of p i , and weighted matrix W = [w 1 , ..., w n ], w i ? R n is the distance matrix of point pairs. The central point can be represented as:</p><formula xml:id="formula_0">p i = j?Ni W ij p j , s.t. j W ij = 1, ?i ? {1, ..., n}, (1)</formula><p>where W ij denoted the distance weight between p i and p j , and the constraint is for normalization. W can be obtained by solving:</p><formula xml:id="formula_1">min W i ||p i ? j?Ni W ij p j || 2 2 .<label>(2)</label></formula><p>Get new point representations. In order to obtain the expression of points in the new feature spaceP , the optimized objective function of locally linear embedding algorithm is:</p><formula xml:id="formula_2">min P i ||p i ? j?Ni W ijpj || 2 2 ,<label>(3)</label></formula><p>wherep i andp j are the points ofP . The detailed steps of locally linear embedding algorithm are shown in Alg. 1. In the overall network, point cloud data is firstly feed to the devised LLE Module, the structure of which is shown in <ref type="figure">Fig. 3</ref>. Using LLE to reduce the standardized coordinates to two-dimensional space, the 2D features are then concatenated with the originals. Let L represent locally linear embedding, F ? R b?n be the input feature vector of point set P , where b is the feature dimension, and b = r + d = 5. We can get the equation of F as:  </p><formula xml:id="formula_3">F LLE = {x, y, z, L x (P ), L y (P ))},<label>(4)</label></formula><formula xml:id="formula_4">N i = knn(p i , K) 3 for Each point p i in P do 4 P i = repeat(p i , K); 5 S i = (P i ? N i ) T (P i ? N i ); 6 w i = solve lagrange multipliers(w T i S i w i + ?(w T i 1 k ? 1)); / *</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manifold Projection Module</head><p>The core idea of manifold learning algorithm is to find a mapping from high-dimensional space to low-dimensional  space. Different from the traditional manifold learning algorithms, which use the distance measurement in different spaces to reduce dimension, we design a novel neural network architecture for this task. Projection in Euclidean space. Let Ax+By+Cz+D = 0 be a plane in 3-dimensional Euclidean space,</p><formula xml:id="formula_5">p i (x i , y i , z i ) is a point, p i (x i , y i , z i )</formula><p>is the projection of point p i on the plane. According to the vertical relation, the parametric equation of the vertical line is:</p><formula xml:id="formula_6">? ? ? x i = x i ? At y i = y i ? Bt z i = z i ? Ct .<label>(5)</label></formula><p>Since p i is on the plane, t can be solved by the plane equation:</p><formula xml:id="formula_7">t = Ax i + By i + Cz i + D A 2 + B 2 + C 2 .<label>(6)</label></formula><p>Nonlinear projection by neural network. We can get the linear projection function S by Eq.5 and Eq.6. As manifold y x z <ref type="figure">Figure 4</ref>: Project point cloud into three plane x = 0, y = 0, z = 0 by MP module.</p><p>learning is a nonlinear dimensionality reduction method, we set a nonlinear function S for this process:</p><formula xml:id="formula_8">S ? (x, y, z) = X ? (x, y, z)S,<label>(7)</label></formula><p>where ? is the projection plane, and X is a nonlinear function defined as follows:</p><formula xml:id="formula_9">X (?) = F(Q ? (?)),<label>(8)</label></formula><p>where F is an activation function, and Q ? is a function to fit the mapping. Here, we implement it by multi-layer perceptron. Then, we can get the full definition of manifold projec-tion S: Finally, since all data of ModelNet40 are standardized, we take three projection planes x = 0, y = 0, z = 0 to get a multi-view of the point cloud, as shown in <ref type="figure">Fig.4</ref>. Similar with the LLE Module, we concatenate the new manifold features with the original features. Then, we can get the input feature vector F as:  The EdgeConv Module is mainly based on DGCNN <ref type="bibr" target="#b20">(Wang et al. 2019)</ref>, and the structure is shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. Let f i be the feature vector of central point i, N i be the neighbor of i, which is selected by k nearest neighbor (kNN). We denote e ij as the edge feature of i and its neighbor j, and the definition is:</p><formula xml:id="formula_10">S ? (x, y, z) = F(Q ? (x, y, z, ?))S ? (x, y, z).<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EdgeConv Module</head><formula xml:id="formula_11">n ? u EdgeConv Module kNN GNN Embedding (Edge Feature) n ? k ? V n ? k MLP{c ? t} n ? k ? c ? t n ? c ? t n ? v1 n ? vk Channel Control Block</formula><formula xml:id="formula_12">e ij = h ? (f i , f j ), j ? N i ,<label>(11)</label></formula><p>where h ? (?, ?) is the edgeconv function. We implement it by multi-layer perceptron. Then, we can get the new feature vector f i :</p><formula xml:id="formula_13">f i = g(e ij ) = g j?Ni (h ? (f i , f j )),<label>(12)</label></formula><p>where g(?) is a symmetric function, and here we use maximum. Finally, we achieve the full definition of edgeconv:</p><formula xml:id="formula_14">f i = max j?Ni (LeakyReLU (? m ? f i + ? m ? (f j ? f i ))),<label>(13)</label></formula><p>where ? = (? 1 , ..., ? M , ? 1 , ..., ? M ) is the parameters to be learnt, and M is the number of convolution kernels.</p><p>Since the dimension of input feature vector is higher than that in <ref type="bibr" target="#b20">Wang et al. (2019)</ref>, we add a dynamic channel control block on the backbone network, which can adjust the channel size of each layer to fit different dimensions of input data. In Experiments section, we will analyse the influence of channel size on classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, to demonstrate the effectiveness of Point-Manifold, we design a set of experiments on ModelNet40 <ref type="bibr" target="#b22">(Wu et al. 2015)</ref>, and compare PointManifold with a series of state-of-the-art methods. Additionally, we do several ablation experiments to deeply explore the operation mechanism of Manifold Learning Module and Channel Control Block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification on ModelNet40</head><p>Data. We select ModelNet40 for the classification task, which contains 12308 models and a total of 40 categories. All the models are sampled from the computer-aided design (CAD) models. Each model provides 2048 sampling points pre-processed with standardization, and each point contains 3D position information. In training process, 9840 models are used as the training set, and the remaining 2468 models are used as the test set. In order to speed up the training process and avoid repeated processing of the same data between epochs, we use locally linear embedding algorithm to pre-process the data. This work can reduce the standardized coordinates to two-dimensional space, and then the new features can be concatenated to the original features.</p><p>Environment. The code implementation of the proposed method is based on Pytorch framework (version 1.1.0) and Python (version 3.6). The experimental computing platform includes one NVIDIA RTX 2070 and four NVIDIA Tesla V100. The operating system is Ubuntu (version 16.04), CUDA (version 10.1) and cuDNN (version 7.4).</p><p>Hyper-parameters. We use SGD as optimizer with a momentum of 0.9, and the learning rate is 0.1 with a cosine annealing scheduler. To enhance the fitting ability of the proposed model, we add dropout layer with 0.5 dropout ratio.</p><p>For the classification activation function, we select softmax. For LLE Module, we set training epochs as 250, batch size as 32, channel time t as 1, and the neighbor range k of edgeconv is 20, while the k of LLE is 12. Specifically, both of the range parameters will be doubled in 2048 points experiments. For MP Module, our model is trained with 300 epochs, and the channel time t are set as 2 and 4 for 1024 and 2048 points experiments, respectively.</p><p>Result. Following <ref type="bibr" target="#b20">Wang et al. (2019)</ref>, we report the mean class accuracy (mA) and overall accuracy (oA) on Mod-elNet40. Results are shown in <ref type="table">Table.</ref>1, as we can see the proposed methods achieve a competitive result. PointManifold is better than the mainstream state-of-the-art point cloud analysis methods in classification task, and have a significant improvement compared with the DGCNN baseline. In addition, we test the class-level result for point cloud classification and give more explorations about the relationship between shape and model performance, which is provided in the Supplementary Material. For fair comparison, the results of DGCNN is tested by ourselves, with the same hyperparameters settings as its open source code <ref type="bibr" target="#b20">(Wang et al. 2019)</ref>, and the same environment as our methods. Besides, because of the limitation of our experiment platform, we cannot train MP Module (2048 points) with the batch size of 32, and we set it as 24 instead. We believe that the classification performance could be better if the batch size is larger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Mean Class Accuracy</p><p>Overall Accuracy </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Manifold Learning Effectiveness</head><p>PCA is a common dimension reduction method, which computes principal components and uses them to perform the changes on the basis of the data. In order to verify the effectiveness of manifold learning in geometric feature ex-traction, we conduct a controlled experiment between PCA and our methods, and the results are shown in <ref type="table">Table.</ref> 2. The PCA algorithm makes no improvement compared with the baseline, while LLE and MP module can take into account the geometric properties when reducing dimensions. Hence, they can perform better than the DGCNN baseline.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>As mentioned above, we add a channel control block, thus we set an ablation experiment to explore the influence of channel control parameter t on the model performance. In addition, we also would like to investigate the relationship between the two manifold learning methods, and therefore we set up several sets of ablation experiments. From <ref type="table">Table.</ref>3, it can be seen that, with the additional input features, an appropriate increase in the channel numbers of the entire model can improve the classification performance. Furthermore, due to the function overlapping, using LLE and MP modules for feature extraction simultaneoulsy cannot fully improve the classification performance, and using either of them can get a similar improvement compared with the baseline. In other words, the ablation study results prove that the proposed neural network architecture can implement the function of manifold learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLE MP Planes t Mean Class Accuracy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this work, we propose a novel point cloud classification method with manifold learning and graph neural network.</p><p>Using the proposed locally linear embedding algorithm or nonlinear neural network projection, we can get low dimension point features according to the geometric correlation. These new features are concatenated with the original 3D features, and we extracted them by using graph neural network. The experiments demonstrate that our methods with different manifold learning strategies can improve the point cloud classification performances when compared with the DGCNN baseline method. It can be seen that, in the proposed method, the effectiveness of extracted features are guaranteed by considering the surface continuity of point cloud during the 3D-to-2D projection. Therefore, we believe that, besides manifold learning, many other feature analysis approaches will also be very helpful for the feature representation learning in the point cloud data analysis and understanding tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>More Results on Classification Task</p><p>In order to further explore the effectiveness of Manifold Learning Module on classification task, we test the classification performance of 40 categories of ModelNet40 in detail, and we report three metrics for each category, i.e., precision, recall and f1-score, the results are shown in <ref type="table">Table.</ref>5, from which we can see:</p><p>? In most categories, compared with the DGCNN <ref type="bibr" target="#b20">(Wang et al. 2019</ref>) baseline, our methods make improvement or achieve the same result. However, in four categories, i.e., bench, curtain, door and night stand, our approaches are not better than the baseline.</p><p>? On some of the categories in which the physical features are prominent, our methods make significant improvement, such as bookshelf, cup, dresser, radio, sink and xbox.</p><p>? The classification performance of flower pot is extremely poor, in spite of 4% improvement by MP Module. We visualize some samples to find out the reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization and Analysis</head><p>We attempted to explain the above results by visualization.</p><p>Shape-similar Categories. The classification of shapesimilar categories is difficult. To explain the bad classification performance of flower pot category, we calculate the confusion matrixes of the baseline and our methods, and the results of specific classes are shown in <ref type="table">Table.</ref>  For all the three methods, more than half of flower pot samples are classified as plant. We visualize the two categories, and select some representative samples to show in <ref type="figure" target="#fig_7">Fig. 7</ref>. We find that some of them are nearly the same, since some plants are with flower pots, and some flower pots are not get rid of the plants. The samples of the two categories are similar, thus the features of them extracted by locally linear embedding in 2D space is also difficult to classify, as shown in <ref type="figure" target="#fig_8">Fig.8</ref>.</p><p>In summary, LLE Module is not suitable for the shapesimilar categories, while MP Module can overcome this defect by multi-view feature concatenation. In addition, the 2D feature are easily confused between the shape-similar categories, especially those whose shapes are regular and concise, thus the classification performance is not improved on bench, curtain, door and night stand.  Feature-prominent Categories. On the contrary, with the Manifold Learning Module, the classification effect of the categories with prominent shape features (e.g. antenna, camber) is improved significantly, which proves our methods can extract more geometric features of 3D models. For example, the radio and xbox make an obvious improvement, and we select some representative samples to show in <ref type="figure" target="#fig_9">Fig.9</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Dimension reduction methods for point cloud data. Left: locally linear embedding. Right: neural network based manifold projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Compared with the previous methods, PointNet avoids the huge computation of 3D voxel convolution and shows excellent classification and segmentation performances at a higher speed. On the basis of PointNet, Qi et al. then propose PointNet++ containing down sampling and up sampling architecture, which enriches the collection of global features and solves the problem of uneven point cloud density. These two methods lay the foundation for the application of deep learning on point cloud analysis in recent years. Additionally, there are derived PointSIFT (Jiang et al. 2018) which focuses on uniform sampling to establish local descriptor, and PointASNL (Yan et al. 2020) which focuses on adaptive sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Solve for a low-dimensional mapping * / 7 W = new matrix(N, N ); 8 for Each point p i in P do 9 for j in range(N ) do 10 if j ? N i then 11 W ji = w ji ; 12 else 13 W ji = 0; / * The minimum distance loss is obtained by using the Lagrange multiplier method, in this case, Y is a matrix composed of the eigenvectors of M * / 14 Y = new matrix(D, N ); 15 for i in range(N ) do 16 M i = (I i ? W i )(I i ? W i ) T ; 17 eigen vec, eigen val = eigens(M i , D); 18 Y i = eigen vec(2 : D + 1); 19 return Y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>The main architecture of the PointManifold. Manifold Learning Module is consisted of two sub-module: Locally Linear Embedding Module and Manifold Projection Module. MLP is the abbreviation of multi-layer perceptron, and t is the parameter come from Channel Control Block which influence the channel size of each layer. LeakyReLU and BatchNorm are used in each layer, and Dropout layer is set at the last two MLP layers. The structure of Locally Linear Embedding (LLE) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The structure of Manifold Projection (MP) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>F</head><label></label><figDesc>M P = {x, y, z, S x=0 (P ), S y=0 (P ), S z=0 (P )}, (10) where S ? R 6?n . here d = 2, thus the dimension of new feature b = r + 3 ? d = 9. The architecture of manifold projection module is shown in Fig.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>The structure of EdgeConv module. The kNN algorithm get k nearest neighbors for each point, the graph neural network establish edge feature v between each point and its neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Samples of ModelNet40: (a) flower pot, (b) plant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>LLE samples. Left: flower pot . Right: plant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Samples of ModelNet40: (a) radio. (b) xbox.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>where x , y represent the new coordinates of points, and L ? R 2?n .</figDesc><table><row><cell>Algorithm 1: Locally Linear Embedding</cell><cell></cell></row><row><cell>Input: (points set P , neighborhood range K, new</cell><cell></cell></row><row><cell>dimension D)</cell><cell></cell></row><row><cell cols="2">Output: new representation Y of points set in new</cell></row><row><cell>space</cell><cell></cell></row><row><cell>/ * Using kNN to get the</cell><cell></cell></row><row><cell>neighborhood</cell><cell>* /</cell></row><row><cell>1 for Each point p i in P do</cell><cell></cell></row><row><cell>2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: ModelNet40 classification result with different</cell></row><row><cell>methods of dimension reduction (1024 points).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Ablation study result on ModelNet40 (1024 points).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Prediction recall on flower pot category (1024 points).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Category-level classification result on ModelNet40 (1024 points).</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">3D Semantic Parsing of Large-Scale Indoor Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Point2Node: Correlation Learning of Dynamic-Node for Point Cloud Feature Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monte carlo convolution for learning on non-uniformly sampled point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hermosilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-P</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Vinacua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ropinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Pointsift: A sift-like network module for 3d point cloud semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00652</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="820" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FPConv: Learning Local Flattening for Point Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attentional ShapeContextNet for Point Cloud Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision &amp; Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Voxnet: A 3d convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5115" to="5124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Point-net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mining Point Cloud Local Structures by Kernel Correlation and Graph Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2530" to="2539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Tangent Convolutions for Dense Prediction in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1807.02443</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-E</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">KPConv: Flexible and Deformable Convolution for Point Clouds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PointConv: Deep Convolutional Networks on 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Grid-GCN for Fast and Scalable Point Cloud Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5661" to="5670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00492</idno>
		<title level="m">PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks with Adaptive Sampling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spnet: Deep 3d object classification and retrieval using stereographic projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yavartanoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="691" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A Graph-CNN for 3D Point Cloud Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
		<idno>abs/1812.01711</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep Learning on Graphs: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno>abs/1812.04202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
