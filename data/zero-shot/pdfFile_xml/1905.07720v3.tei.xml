<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Butterfly: One-step Approach towards Wildly Unsupervised Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20201">JANUARY 2020 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Feng</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Jie</forename><surname>Lu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangquan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
						</author>
						<title level="a" type="main">Butterfly: One-step Approach towards Wildly Unsupervised Domain Adaptation</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<date type="published" when="20201">JANUARY 2020 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-machine learning</term>
					<term>weakly-supervised learning</term>
					<term>transfer learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In unsupervised domain adaptation (UDA), classifiers for the target domain (TD) are trained with clean labeled data from the source domain (SD) and unlabeled data from TD. However, in the wild, it is difficult to acquire a large amount of perfectly clean labeled data in SD given limited budget. Hence, we consider a new, more realistic and more challenging problem setting, where classifiers have to be trained with noisy labeled data from SD and unlabeled data from TD-we name it wildly UDA (WUDA). We show that WUDA ruins all UDA methods if taking no care of label noise in SD, and to this end, we propose a Butterfly framework, a powerful and efficient solution to WUDA. Butterfly maintains four deep networks simultaneously, where two take care of all adaptations (i.e., noisy-to-clean, labeled-to-unlabeled, and SD-to-TD-distributional) and then the other two can focus on classification in TD. As a consequence, Butterfly possesses all the conceptually necessary components for solving WUDA. Experiments demonstrate that, under WUDA, Butterfly significantly outperforms existing baseline methods. The code of Butterfly can be found at github.com/fengliu90/Butterfly. Index Terms-machine learning, weakly-supervised learning, transfer learning ! ? Feng Liu is with the</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D OMAIN adaptation (DA) aims to learn a discriminative classifier in the presence of a shift between training data in source domain and test data in target domain <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>. Currently, DA can be divided into three categories: supervised DA <ref type="bibr" target="#b6">[7]</ref>, semi-supervised DA <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> and unsupervised DA (UDA) <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. When the number of labeled data is few in target domain, supervised DA is also known as few-shot DA <ref type="bibr" target="#b20">[21]</ref>. Since unlabeled data in target domain can be easily obtained, UDA exhibits the greatest potential in the real world <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>.</p><p>UDA methods train with clean labeled data in a source domain (i.e., clean source data) and unlabeled data in a target domain (i.e., unlabeled target data) to obtain classifiers for the target domain (TD), which mainly consist of three orthogonal techniques: integral probability metrics (IPM) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, adversarial training <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref> and pseudo labeling <ref type="bibr" target="#b12">[13]</ref>. Compared to IPMand adversarial-training-based methods, the pseudo-labelingbased method (i.e., asymmetric tri-training domain adaptation (ATDA) <ref type="bibr" target="#b12">[13]</ref>) can construct a high-quality target-specific representation, providing a better classification performance.</p><p>However, in the wild, the data volume of the source <ref type="figure" target="#fig_2">Fig. 1</ref>. Wildly unsupervised domain adaptation (WUDA). The blue line denotes that UDA transfers knowledge from clean source data (Ps) to unlabeled target data (Px t ). However, perfectly clean data is hard to acquire. This brings wildly unsupervised domain adaptation (WUDA), namely transferring knowledge from noisy source data ( Ps) to unlabeled target data (Px t ). Note that label corruption process (black dash line) is unknown in practice. To handle WUDA, a compromise solution is a two-step approach (green line), which sequentially combines label-noise algorithms ( Ps ?Ps, label correction) and existing UDA (Ps ? Px t ). This paper proposes a robust one-step approach called Butterfly (red line, Ps ? Px t directly), which eliminates noise effects fromPs.</p><p>domain tends to be large <ref type="bibr" target="#b44">[45]</ref>. To avoid the expensive labeling cost, labeled data in the source domain normally come from amateur annotators or the Internet <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. This brings us a new, more realistic and more challenging problem, wildy unsupervised domain adaptation (abbreviated as WUDA, <ref type="figure" target="#fig_2">Figure 1</ref>). This adaptation aims to transfer knowledge from noisy labeled data in the source domain ( P s , i.e., noisy source data) to unlabeled target data (P xt ). Unfortunately, existing UDA methods share an implicit assumption that there are no noisy source data <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b48">[49]</ref>. Namely, these methods focus on transferring knowledge from clean source data (P s ) to unlabeled target data (P xt ). Therefore, these methods cannot well handle WUDA ( <ref type="figure" target="#fig_0">Figure 2</ref>).</p><p>Manuscript received January 28, 2020.  <ref type="bibr" target="#b35">[36]</ref>), domain-adversarial neural network (DANN, an adversarial training based method <ref type="bibr" target="#b22">[23]</ref>), asymmetric tri-training domain adaptation (ATDA, a pseudo-label based method <ref type="bibr" target="#b12">[13]</ref>) and transferable curriculum learning (TCL, a robust UDA method <ref type="bibr" target="#b43">[44]</ref>). B-Net is our proposed WUDA method. We report target-domain accuracy of all methods when the noise rate of source domain changes (a) from 5% to 70% (symmetry-flip noise) and (b) from 5% to 45% (pair-flip noise). Clearly, as the noise rate of source domain increases, the target-domain accuracy of representative UDA methods drops quickly while that of B-Net keeps stable consistently.</p><p>To validate this fact, we empirically reveal the deficiency of existing UDA methods <ref type="figure" target="#fig_0">(Figure 2</ref>, e.g., deep adaptation network (DAN) <ref type="bibr" target="#b35">[36]</ref> and domain-adversarial neural network (DANN) <ref type="bibr" target="#b22">[23]</ref>). To improve these methods, a straightforward solution is a two-step approach. In <ref type="figure" target="#fig_2">Figure 1</ref>, we can first use label-noise algorithms to train a classifier on noisy source data, then leverage this trained classifier to assign pseudo labels for noisy source data. Via UDA methods, we can transfer knowledge from pseudo-labeled source data (P s ) to unlabeled target data (P xt ). Nonetheless, pseudo-labeled source data are still noisy, and such two-step approach may not eliminate noise effects.</p><p>To circumvent the issue of two-step approach, we present a robust one-step approach called Butterfly. In high level, Butterfly directly transfers knowledge from P s to P xt , and uses the transferred knowledge to construct target-specific representations. In low level, Butterfly maintains four networks dividing two branches ( <ref type="figure" target="#fig_4">Figure 3</ref>): Two networks in Branch-I are jointly trained on noisy source data and pseudolabeled target data (data in mixture domain (MD)); while two networks in Branch-II are trained on pseudo-labeled target data. Our ablation study (see Section 9.9) confirms the network design of Butterfly (see <ref type="bibr">Section 7)</ref> is the optimal.</p><p>The reason why Butterfly can be robust takes root in the dual-checking principle (DCP): Butterfly checks highcorrectness data out, from not only the data in MD but also the pseudo-labeled target data. After cross-propagating these high-correctness data, Butterfly can obtain high-quality domain-invariant representations (DIR) and target-specific representations (TSR) simultaneously in an iterative manner. If we only check data in MD (i.e., B-Net-M in Section 9.9), the error existed in pseudo-labeled target data will accumulate, leading to the low-quality DIR and TSR.</p><p>We conduct experiments on simulated WUDA tasks, including 4 MNIST-to-SYND tasks, 4 SYND-to-MNIST tasks and 24 human-sentiment tasks. Besides, we conduct experiments on 3 real-world WUDA tasks. Empirical results demonstrate that Butterfly can robustly transfer knowledge from noisy source data to unlabeled target data. Meanwhile, Butterfly performs much better than existing UDA methods when source domain (SD) suffers the extreme (e.g., 45%) noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LITERATURE REVIEW</head><p>This section reviews the existing UDA methods in detail.</p><p>UDA methods train with clean source data and unlabeled target data to classify target-domain data, which mainly consist of three orthogonal techniques: integral probability metrics (IPM) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, adversarial training <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> and pseudo labeling <ref type="bibr" target="#b12">[13]</ref>.</p><p>IPMs (such as maximum mean discrepancy <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b49">[50]</ref> and Wasserstein distance <ref type="bibr" target="#b34">[35]</ref>) are used to measure the discrepancy between distributions of two domains. By minimizing the IPM between two domains, models trained with clean source data can classify unlabeled target data accurately <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref>. In this line, representative methods include conditional transferable components <ref type="bibr" target="#b13">[14]</ref>, scatter component analysis <ref type="bibr" target="#b32">[33]</ref> and DAN <ref type="bibr" target="#b35">[36]</ref>.</p><p>Another technique is the adversarial training method inspired by the theory of domain adaptation <ref type="bibr" target="#b0">[1]</ref>. This theory suggests that predictions must be based on features, and these features cannot be used to discriminate source and target domains <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref>. For example, DANN considers two deep networks: one is used to construct new features that predict labels in the TD; while the other is to make two domains non-distinguishing based on these new features <ref type="bibr" target="#b22">[23]</ref>. DANN simultaneously trains two deep networks to find domain-invariant representations between two domains.</p><p>The last technique is the pseudo-label method, which regards pseudo labels given by a classifier as true labels <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b50">[51]</ref>. The joint domain adaptation (JDA) matches joint distributions of two domains using these pseudo labels <ref type="bibr" target="#b50">[51]</ref>. The asymmetric tri-training domain adaptation (ATDA) leverages three networks asymmetrically <ref type="bibr" target="#b12">[13]</ref>. Specifically, two networks are used to annotate unlabeled target data, namely generating pseudo labels. The other network can obtain target-specific representations based on the pseudo-labeled data. Since pseudo-label UDA methods can effectively reduce the upper bound of expected risk in the TD <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b51">[52]</ref>, we also consider using the pseudo-label technique to help address the WUDA problem (like ATDA <ref type="bibr" target="#b12">[13]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>In this section, we introduce notations used in this paper and two common label-noise generation processes <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>The following notations are used to demonstrate theoretical results of this paper.</p><p>? a space X ? R d and Y = {1, 2, . . . , K} as a label set;</p><p>? f t (x t ) andf t (x t ) represent the ground-truth and pseudo labeling function of the target domain, where f t ,f t : X ? Y; </p><formula xml:id="formula_0">? A = {x :f t (x) = f t (x)} and B = X /A represent the area wheref t (x) = f t (x) (the set A) and the area wheref t (x) = f t (x) (the set B); ?p s (x s ,? s ), p s (x s ,</formula><formula xml:id="formula_1">? q xt (x) = p xt (x)1 A (x)/P xt (A) represents the proba- bility density of X t restricted in A; ? p xt (x t ) = p xt (x t )1 B (x t )/P xt (B)</formula><p>represents the probability density of X t restricted in B;</p><p>? H is the class of arbitrary decision functions h : X ? R K ;</p><formula xml:id="formula_2">? : R K ? Y ? R + is the loss function.</formula><p>(t, y) means the loss incurred by predicting an output t (e.g., h(x)) when the ground truth is y; ? expected discrepancy (associated with ) between an arbitrary decision function h and a ground-truth or pseudo labeling function f (f could be f t orf t ) under different marginal densities:</p><formula xml:id="formula_3">? L H = { (h(x), y)|h ? H, x ? X ,</formula><formula xml:id="formula_4">R s (h, f ) = Ep xs (xs) [ (h(x s ), f (x s ))], R s (h, f ) = E px s (xs) [ (h(x s ), f (x s ))], R t (h, f ) = E px t (xt) [ (h(x t ), f (x t ))].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generating label-noise via the transition matrix</head><p>We assume that there are clean source data denoted by a m.r.v. (X s , Y s ) defined on X ? Y with the probability density p s (x s , y s ). However, samples of (X s , Y s ) cannot be directly obtained and we can only observe noisy source data (denoted by m.r.v. (X s ,? s )) with the probability densityp s (x s ,? s ) <ref type="bibr" target="#b52">[53]</ref>.p s (x s ,? s ) is generated from p s (x s , y s ) and a transition matrix Q ij = Pr(? s = j|Y s = i). Each element in Q, Pr(? s = j|Y s = i), is a transition probability, i.e., the flip rate from a correct label i to a noisy label j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Generating label-noise via the sample selection</head><p>The transition matrix Q is easily estimated in certain situations <ref type="bibr" target="#b52">[53]</ref>. However, in more complex situations, such as clothing1M dataset <ref type="bibr" target="#b54">[55]</ref>, noisy source data is directly generated by selecting data from a pool, which mixes correct data (data with correct labels) and incorrect data (data with incorrect labels). Namely, how the correct label i is corrupted to j (i = j) is unclear.</p><p>Let</p><formula xml:id="formula_5">(X s , Y s , V s ) be a m.r.v. defined on X ? Y ? V with the probability density p po s (x s , y s , v s ), where V = {0, 1}</formula><p>is the perfect-selection random variable. V s = 1 means "correct" and V s = 0 means "incorrect". Nonetheless, samples of (X s , Y s , V s ) cannot be obtained and we can only observe (X s ,? s ) from a distribution with the following density.</p><formula xml:id="formula_6">p s (x s ,? s ) = 1 vs=0 p po Xs,Ys|Vs (x s , y s |v s )p po Vs (v s ),<label>(1)</label></formula><p>where p po</p><formula xml:id="formula_7">Vs (v s ) = X K ys=1 p po s (x s , y s , v s )dx s . Eq.</formula><p>(1) means that we lose the information regarding V s . If we uniformly draw samples fromp s (x s ,? s ), the noise rate of these samples is p po</p><formula xml:id="formula_8">Vs (0). It is clear that the m.r.v. (X s , Y s |V s = 1) is the m.r.v. (X s , Y s ) mentioned in Section 3.2. Then, q s (x s , y s ) is used to describe the density of incorrect m.r.v. (X s , Y s |V s = 0). Using p s (x s , y s ) and q s (x s , y s ),p s (x s ,? s ) is expressed b? p s (x s ,? s ) = (1 ? ?)p s (x s , y s ) + ?q s (x s , y s ),<label>(2)</label></formula><p>where ? = p po Vs (0). To reduce noise effects from incorrect data, researchers aim to recover the information of V s , i.e., to select the correct data <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">WILDLY UNSUPERVISED DOMAIN ADAPTATION</head><p>In this section, we first define a new, more realistic and more challenging problem setting called wildly unsupervised domain adaptation (WUDA), and explain the nature of WUDA. Then, we empirically show that representative UDA methods cannot handle WUDA well, which motivates us to propose a novel method to address the WUDA problem (Section 7). Problem 1 (Wildly Unsupervised Domain Adaptation). Let X t be a m.r.v. defined on the space X with respect to the probability density p xt , (X s , Y s ) be a m.r.v. defined on the space X ? Y with respect to the probability densityp s , wherep s is the probability density regarding noisy source data (generated in Section 3.2 or 3.3), and Y = {1, . . . , K} is the label set. Let p xs be the marginal density ofp s . Given i.i.d. dataD s = {(x si ,? si )} ns i=1 and D t = {x ti } nt i=1 drawn fromP s and P xt separately, in wildly unsupervised domain adaptation, we aim to train with noisy source dataD s and target data D t to accurately annotate data drawn from P xt , where p xs = p xt . Remark 1. In Problem 1,D s is noisy source data, D t is unlabeled target data, andP s and P xt are two probability measures corresponding to densitiesp s (x s ,? s ) and p xt (x t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Nature of WUDA</head><p>Specifically, there are five distributions involved in WUDA setting: 1) a marginal distribution on source data, i.e., p xs in Problem 1; 2) a marginal distribution on target data, i.e., p xt in Problem 1; 3) an incorrect conditional distribution of label given x s , q(y s |x s ); 4) a correct conditional distribution of label given x s , p(y s |x s ) and 5) a correct conditional distribution of label given x t , p(y t |x t ).</p><p>Based source dataD s are mixture of correct source data from p xs (x s )p(y s |x s ) and incorrect data from p xs (x s )q(y s |x s ).</p><p>Target data D t are drawn from p xt . In WUDA setting, we aim to train a classifier withD s and D t . This classifier is expected to accurately annotate data from p xt , i.e., to accurately simulate distribution 5). This paper considers WUDA under the common assumption used in the label-noise field, i.e., the i th element in the diagonal of the noise transition matrix is greater than other elements in the i th row or i th column of the noise transition matrix, where i = 1, 2, . . . , K <ref type="bibr" target="#b52">[53]</ref>. Therefore, the proposed approach is able to solve any WUDA problem under the above assumption in principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">WUDA ruins UDA methods</head><p>We take a simple example to illustrate the phenomenon that WUDA ruins representative UDA methods. In Section 5.1, we theoretically analyze the reason of this phenomenon.</p><p>We corrupt source data using symmetry flipping <ref type="bibr" target="#b57">[58]</ref> and pair flipping <ref type="bibr" target="#b55">[56]</ref> that are two representative ways to corrupt true labels. Precise definitions of symmetry flipping (Q S ) and pair flipping (Q P ) are presented below, where ? is the noise rate and K is the number of labels.</p><formula xml:id="formula_9">Q S = ? ? ? ? ? ? ? ? 1 ? ? ? K?1 . . . ? K?1 ? K?1 ? K?1 1 ? ? ? K?1 . . . ? K?1 . . . . . . . . . ? K?1 . . . ? K?1 1 ? ? ? K?1 ? K?1 ? K?1 . . . ? K?1 1 ? ? ? ? ? ? ? ? ? ? K?K ,<label>(3)</label></formula><formula xml:id="formula_10">Q P = ? ? ? ? ? ? ? 1 ? ? ? 0 . . . 0 0 1 ? ? ? 0 . . . . . . . . . . . . 0 1 ? ? ? ? 0 . . . 0 1 ? ? ? ? ? ? ? ? ? K?K .<label>(4)</label></formula><p>For example, if ? = 0.4 and K = 11, for the symmetry flipping, the probability that label "0" is corrupted to label "1" is (1 ? ?)/(K ? 1) = 0.04. For the pair flipping, the probability that label "0" is corrupted to label "1" is ? = 0.4.</p><p>To instantiate noisy source data and target data, we leverage MNIST and SYND (see <ref type="figure" target="#fig_6">Figure 4</ref>), respectively (i.e., K = 10). We first construct two WUDA tasks with symmetryflip noise: corrupted SYND?MNIST (S?M) and corrupted MNIST?SYND (M?S). In <ref type="figure" target="#fig_0">Figure 2</ref>-(a), we report accuracy of representative UDA methods on unlabeled target data, when the noise rate ? of SD changes from 5% to 70%. It is clear that target-domain accuracy of these representative UDA methods drops quickly when ? increases. This means that WUDA ruins representative UDA methods. Then, we construct another two WUDA tasks with pair-flip noise. In <ref type="figure" target="#fig_0">Figure 2</ref>-(b), we report target-domain accuracy, when the noise rate ? of SD changes from 5% to 45%. Again, WUDA still ruins representative UDA methods. Note that, in practice, pair-flip noise is much harder than symmetry-flip noise, the noise rate of pair-flip noise cannot be over 50% <ref type="bibr" target="#b55">[56]</ref>. However, the proposed Butterfly network (abbreviated as B-Net, <ref type="figure" target="#fig_4">Figure 3</ref>) performs robustly when ? increases (blue lines in <ref type="figure" target="#fig_0">Figure 2</ref>).</p><p>In Section 5, we will analyze the WUDA problem in theory and show why WUDA provably ruins all UDA methods and why the two-step approach is a compromise solution. Then, Section 6 presents how to address the WUDA problem in principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ANALYSIS OF WUDA PROBLEM</head><p>In this section, we analyze the WUDA problem from a theoretical view and show the difficulty of the WUDA problem. Completed proofs of lemmas and theorems are demonstrated in the Appendix. In the main content, we provide the main ideas of proving these theoretical results (i.e., Proof (sketch)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">WUDA provably ruins UDA methods</head><p>Theoretically, we show that existing UDA methods cannot directly transfer useful knowledge fromD s to D t . We first present the relation between R s (h) andR s (h). Theorem 1. Ifp s (x s ,? s ) is generated by a transition matrix Q as demonstrated in Section 3.2, we hav?</p><formula xml:id="formula_11">R s (h) = R s (h) + E px s (xs) [? T (x s )(Q ? I) (h(x s ))], (5) where (h(x s )) = [ (h(x s ), 1), ..., (h(x s ), K)] T and ?(x s ) = [p Ys|Xs (1|x s ), ..., p Ys|Xs (K|x s )] T . Ifp s (x s ,? s )</formula><p>is generated by sample selection as described in in Section 3.3, we hav? </p><formula xml:id="formula_12">R s (h) = (1 ? ?)R s (h) + ?E qx s (xs) [? T q (x s ) (h(x s ))], (6) where ? q (x s ) = [q Ys|Xs (1|x s ),</formula><formula xml:id="formula_13">(h) ? E qx s (xs) [? T q (x s ) (h(x s ))]. Specifically, we assume: there is a constant 0 &lt; M s &lt; ? such that E qx s (xs) [? T q (x s ) (h(x s ))] ? R s (h) + M s .</formula><p>Theorem 1 shows thatR s (h) equals R s (h) if only two cases happen: 1) Q = I and ? = 0, or 2) some special combinations (e.g., special p xs , q xs , Q, ? and ) make the second term in Eq. (5) equal zero or make the second term in Eq. (6) equal ?R s (h). Case 1) means that source data are clean, which is not real in the wild. Case 2) rarely happens, since it is difficult to find such special combinations when p xs , q xs , Q and ? are unknown. As a result,R s (h) has an essential difference with R s (h). Then, following the proof skills in <ref type="bibr" target="#b0">[1]</ref>, we derive the upper bound of R t (h) as below.</p><p>Theorem 2. For any h ? H, we have</p><formula xml:id="formula_14">R t (h, f t ) ?R s (h) (i) noisy-data risk + |R t (h,f t ) ?R s (h,f t )| (ii) discrepancy between distributions + |R s (h,f t ) ? R s (h)| (iii) domain dissimilarity + |R s (h) ? R s (h)| + |R s (h,f t ) ? R s (h,f t )| (iv) noise effects from source ?s + |R t (h, f t ) ? R t (h,f t )| (v) noise effects from target ?t .<label>(7)</label></formula><p>Proof (sketch). For any h ? H, we have</p><formula xml:id="formula_15">R t (h, f t ) = R t (h, f t ) +R s (h) ?R s (h) + R s (h, f t ) ? R s (h, f t ) =R s (h) + R t (h, f t ) ?R s (h, f t ) + R s (h, f t ) ? R s (h) + R s (h) ?R s (h) +R s (h, f t ) ? R s (h, f t ).</formula><p>Since we do not know f t , we substitute the following equations into the above equation,</p><formula xml:id="formula_16">R t (h, f t ) = R t (h,f t ) + R t (h, f t ) ? R t (h,f t ), R s (h, f t ) =R s (h,f t ) +R s (h, f t ) ?R s (h,f t ), R s (h, f t ) = R s (h,f t ) + R s (h, f t ) ? R s (h,f t ),</formula><p>which proves this theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 3.</head><p>To ensure that we can gain useful knowledge fromf t (x t ), we assume: there is a constant 0</p><formula xml:id="formula_17">&lt; M t &lt; ? such that E qx s (x) [ (h(x),f t (x))] ? R s (h,f t ) + M t and E qx t (x) [ (h(x),f t (x))] ? R t (h, f t ) + M t .</formula><p>Since we do not have labels in the target domain, we also assume that there exists an h ? H such that R t (h, f t ) + R s (h) is a small value. This assumption follows common assumption of UDA problem <ref type="bibr" target="#b0">[1]</ref> and ensures that the adaptation is possible.</p><p>It is clear that the upper bound of R t (h, f t ), shown in Eq. (7), has 5 terms. However, existing UDA methods only focus on minimizing (i) + (ii) <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b35">[36]</ref> or (i) + (ii) + (iii) <ref type="bibr" target="#b12">[13]</ref>, which ignores terms (iv) and (v) (i.e., ? = ? s +? t ). Thus, existing UDA methods cannot handle WUDA well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Two-step approach is a compromise solution</head><p>To reduce noise effects, a straightforward solution is twostep approach. For example, in the first step, we can train a classifier with noisy source data using co-teaching <ref type="bibr" target="#b55">[56]</ref> and use this classifier to annotate pseudo labels for source data. In the second step, we use ATDA <ref type="bibr" target="#b12">[13]</ref> to train a target-domain classifier with pseudo-labeled-source data and pseudo-labeled target data.</p><p>Nonetheless, the pseudo-labeled source data are still noisy. Let labels of noisy source data? s be replaced with pseudo labels? s after using co-teaching. Noise effects ? will become pseudo-label effects ? p as follows.</p><formula xml:id="formula_18">? p = |R s (h) ? R s (h)| + |R s (h,f t ) ? R s (h,f t )| pseudo-labeled-source effects ? s +? t , (8) whereR s (h) andR s (h,f t ) correspond toR s (h) and R s (h,f t ) in ? s .</formula><p>It is clear that the difference between ? p and ? is ? s ? ? s . The left term in ? s may be less than that in ? s due to a label-noise algorithm (e.g., co-teaching <ref type="bibr" target="#b55">[56]</ref>), but the right term in ? s may be higher than that in ? s since a label-noise algorithm does not consider minimizing it. Thus, it is hard to say whether ? s &lt; ? s (i.e., ? p &lt; ?). This means that two-step approach may not really reduce noise effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">HOW TO ADDRESS WUDA IN PRINCIPLE</head><p>To eliminate noise effects ?, we aim to select correct data simultaneously from noisy source data and pseudo-labeled target data. In theory, we prove that noise effects will be eliminated if we can select correct data with a high probability. Let ? s 01 represent the probability that incorrect data is selected from noisy source data, and ? t 01 represent the probability that incorrect data is selected from pseudolabeled target data. Theorem 3 shows that ? ? 0 if ? s 01 ? 0 and ? t 01 ? 0 and presents a new upper bound of R t (h, f t ). Before stating Theorem 3, we first present two m.r.v.s below.</p><formula xml:id="formula_19">? (X s , Y s , V s ) defined on X ?Y ?V with the probability density p po s (x s , y s , v s ), where V = {0, 1}; ? (X t , V t ) defined on X ?V with the probability density p po t (x t , v t ), where V = {0, 1}. p po Vt (v t ) is the marginal density of p po t (x t , v t ).</formula><p>The V s has been introduced in Section 3.3. Similar with V s , V t is also a perfect-selection random variable. Data drawn from the distribution of (X t , V t ) can be regarded as a pool that mixes the correct (v t = 1) and incorrect (v t = 0) pseudolabeled target data. Namely,</p><formula xml:id="formula_20">V t = 1 means f t (x t ) =f t (x t ) and V t = 0 means f t (x t ) =f t (x t )</formula><p>. It is clear that, higher value of p po Vt (V t = 1) means thatf t is more like f t . In following, we use ? vt to represent p po Vt (v t = 0). Note that both perfect-selection random variables V s and V t cannot be observed and we can only observe following m.r.v.s.</p><formula xml:id="formula_21">? (X s , Y s , U s ) defined on X ?Y ?V with the probability densityp po s (x s , y s , u s ); ? (X t , U t ) defined on X ? V with the probability densityp po t (x t , u t ).p po Ut (u t ) is the marginal density ofp po t (x t , u t ).</formula><p>The U s and U t are algorithm-selection random variables. Data drawn from the distribution of (X s , Y s , U s ) can be regarded as a pool that mixes the selected (u s = 1) and unselected (u s = 0) noisy source data. Data drawn from the distribution of (X t , U t ) can be regarded as a pool that mixes the selected (u t = 1) and unselected (u t = 0) pseudo-labeled target data. We can obtain observations of (X s , Y s , U s ) and (X t , U t ) using an algorithm that is used to select correct data. After executing the algorithm, we can obtain observations {x si ,? si , u si } ns i=1 and {x ti , u ti } nt i=1 . Based on (X s , Y s , U s ) and (X t , U t ), we can define the following expected risks.</p><formula xml:id="formula_22">R po s (h, u s ) = (1 ? ? us ) ?1 Ep po s (xs,ys,us) [u s (h(x s ), y s )], R po t (h,f t , u t ) = (1 ? ? ut ) ?1 Ep po t (xt,ut) [u t (h(x t ),f t (x t ))], R po s (h,f t , u s ) = (1 ? ? us ) ?1 Ep po s (xs,ys,us) [u s (h(x s ),f t (x s ))]</formula><p>. where ? us =p po Us (u s = 0) and ? ut =p po Ut (u t = 0). Since we can observe (X s , Y s , U s ) and (X t , U t ), the empirical estimators of these three risks can be easily computed. Then, we define following probabilities to describe the relation between perfect-selection random variables and algorithmselection random variables, where i, j = 0, 1.</p><formula xml:id="formula_23">? ? s ji = Pr(V s = j|U s = i) represents the probability of the event: V s = j given U s = i, ? ? t ji = Pr(V t = j|U t = i) represents the probability of the event: V t = j given U t = i.</formula><p>Remark 4. Based on above definitions, we know that 1) ? s 01 is the probability that incorrect data is selected from noisy source data, and 2) ? t 01 is the probability that incorrect data is selected from pseudo-labeled target data.</p><p>Using ? s ji and ? t ji , we can show the relation between probability densities of (X s , Y s |V s ) and (X s , Y s |U s ), and the relation between probability densities of (X t |V t ) as follows.</p><formula xml:id="formula_24">p po Xs,Ys|Us (x s , y s |i) = ? s 0i p po Xs,Ys|Vs (x s , y s |0) + ? s 1i p po Xs,Ys|Vs (x s , y s |1), p po Xt|Ut (x t |i) = ? t 0i p po Xt|Vt (x t |0) + ? t 1i p po Xt|Vt (x t |1). Since p po Xs,Ys|Vs (x s , y s |1) = p s (x s , y s ), p po Xs,Ys|Vs (x s , y s |0) = q s (x s , y s ), p po Xt|Vt (x t |0) = p xt (x t )1 A (x t )/P xt (A) = q xt (x t ), p po Xt|Vt (x t |1) = p xt (x t )1 B (x t )/P xt (B) = p xt (x t ), we hav? p po Xs,Ys|Us (x s , y s |i) = ? s 0i q s (x s , y s ) + ? s 1i p s (x s , y s ),<label>(9)</label></formula><formula xml:id="formula_25">p po Xt|Ut (x t |i) = ? t 0i q xt (x t ) + ? t 1i p xt (x t ).<label>(10)</label></formula><p>Remark 5. Eq. (9) and Eq. <ref type="formula" target="#formula_6">(10)</ref> show that, if ? s 01 ? 0 and ? t 01 ? 0, we have 1)p po</p><formula xml:id="formula_26">Xs,Ys|Us (x s , y s |1) ? p s (x s , y s ) and 2) p po Xt|Ut (x t |1) ? p xt (x t ).</formula><p>However, we cannot prove the main theorem (Theorem 3) using 1) and 2), since we only take care of risks instead of densities (like 1) and 2)).</p><p>Next, we present a lemma to show the relation betwee? R </p><formula xml:id="formula_27">|R po s (h, u s ) ? R s (h)| ?? s 01 max{E qs(xs,ys) [ (h(x s ), y s )], R s (h)}.<label>(11)</label></formula><p>Proof (sketch). Based on definition ofR po s (h, u s ) and the factp po</p><formula xml:id="formula_28">s (x s , y s , u s ) =p po Xs,Ys|Us (x s , y s |1)p po Us (1),R po s (h, u s ) equals X K ys=1 (h(x s ), y s )p po Xs,Ys|Us (x s , y s |1)p po Us (1)dx s 1 ? ? us</formula><p>Then, we can use the definition of ? us and the Eq. (9) to prove this lemma.</p><p>Similar with Lemma 1, we can obtain</p><formula xml:id="formula_29">|R po s (h,f t , u s ) ? R s (h,f t )| ?? s 01 max{E qx s (xs) [ (h(x s ),f t (x s ))], R s (h,f t )}.<label>(12)</label></formula><p>Then, we give another lemma to show relation betwee?</p><formula xml:id="formula_30">R po t (h,f t , u t ) and R t (h,f t ). Lemma 2. Given the m.r.v. (X t , U t ) with the probability den- sityp po s (x t , u t ) and Eq. (10), if E p x t (xt) [ (h(x t ), f t (x t ))] ? R t (h, f t ) + ? s 01 M t , then we have |R po t (h,f t , u t ) ? R t (h, f t )| ?? t 01 max{E qx t (xt) [ (h(x t ),f t (x t ))], R t (h, f t )} + ? t 11 ? s 01 M t .<label>(13)</label></formula><p>Proof (sketch). According to definition ofR po t (h,f t , u t ), we can unfold it to b?</p><formula xml:id="formula_31">R po t (h,f t , u t ) = (1 ? ? ut ) ?1 X (h(x t ),f t (x t ))p po Xt|Ut (x t |1)p po Ut (1)dx t .</formula><p>Then, using the definition of ? us , Eq. (9), the definition of V t (f t (x t ) =f t (x t ) when V t = 1) and the assumption that</p><formula xml:id="formula_32">E p x t (xt) [ (h(x t ), f t (x t ))] ? R t (h, f t ) + ? s 01 M t , we hav? R po t (h,f t , u t ) ? ? t 01 E qx t (xt) [ (h(x t ),f t (x t ))] + ? t 11 (R t (h, f t ) + ? s 01 M t ).</formula><p>Finally, we can upper bound |R po t (h,f t , u t )?R t (h, f t )| using the above inequality, which proves this lemma.</p><formula xml:id="formula_33">Remark 6. In Lemma 2, E p x t (xt) [ (h(x t ), f t (x t ))] ? R t (h, f t ) + ? s 01 M t means that the expected risk restricted in B (i.e., E p x t (xt) [ (h(x t ), f t (x t ))]) can represent the true risk R t (h, f t ) when ? s</formula><p>01 is small. If this assumption fails, we cannot gain useful knowledge fromf t even when we can select correct data from pseudo-labeled target data (? t 01 = 0). Inequalities (11), <ref type="bibr" target="#b11">(12)</ref> and <ref type="bibr" target="#b12">(13)</ref> show that if we can perfectly avoid annotating incorrect data as "correct" (i.e., ? s 01 = 0 and ? t 01 = 0), we haveR po </p><formula xml:id="formula_34">s (h, u s ) = R s (h),R po s (h,f t , u t ) = R s (h,f t ) andR po t (h,f t , u t ) = R t (h, f t ). Nonetheless, ? s 01 and ? t 01 never equal zero, and E qs(xs,ys) [ (h(x), y)], E qx s (xs) [ (h(x s ),f t (x s ))] and E qx t (xt) [ (h(x t ),f t (x t ))]</formula><formula xml:id="formula_35">(X s , Y s , U s ) to represent (X s , Y s |V s = 1).</formula><p>In Theorem 3, we prove that, under assumptions in Remarks 2, 3 and Lemma 2,R po</p><formula xml:id="formula_36">s (h, u s ) ? R s (h), R po s (h,f t , u t ) ? R s (h,f t ) andR po t (h,f t , u t ) ? R t (h, f t ) if ? s 01 ? 0 and ? t 01 ? 0. Moreover, we give a new upper bound of R t (h, f t ).</formula><p>In the new upper bound, we show that: ? ? 0 if ? s 01 ? 0 and ? t 01 ? 0. Theorem 3. Given two m.r.v.s (X s , Y s , U s ) defined on X ?Y ?V and (X t , U t ) defined on X ? V, under the assumptions in Remark 2, Remark 3 and Lemma 2, ? ? (0, 1), there are ? s and ? t , if ? s 01 &lt; ? s and ? t 01 &lt; ? t , for any h ? H, we will have</p><formula xml:id="formula_37">|R po s (h,f t , u s ) ?R s (h,f t )| +|R po s (h, u s ) ?R s (h)| &lt; 2 . (14) Moreover, we will have R t (h, f t ) ?R po s (h, u s ) (i) noisy-data risk + |R po t (h,f t , u t ) ?R po s (h,f t , u s )| (ii) discrepancy between distributions + |R s (h,f t ) ? R s (h)| (iii) domain dissimilarity + 2 (iv) noise effects ?s + 2 (iv) noise effects ?t .<label>(15)</label></formula><p>Proof. We first prove upper bounds of |R po</p><formula xml:id="formula_38">s (h, u s ) ? R s (h)|, |R po s (h,f t , u t ) ? R s (h,f t )| and |R po t (h,f t , u t ) ? R t (h, f t )| under assumptions in Theorem 3. Based on Lemma 1, |R po s (h, u s ) ? R s (h)| = |? s 01 E qs(xs,ys) [ (h(x s ), y s )] ? (1 ? ? s 11 )R s (h)| ? |? s 01 (R s (h) + M s ) ? ? s 01 R s (h)| = ? s 01 M s .<label>(16)</label></formula><p>Similar, we have</p><formula xml:id="formula_39">|R po s (h,f t , u s ) ? R s (h,f t )| ? ? t 01 M t ,<label>(17)</label></formula><formula xml:id="formula_40">|R po t (h,f t , u t ) ? R t (h, f t )| ? ? t 01 M t + ? t 11 ? s 01 M t .<label>(18)</label></formula><p>Since M s and M t are positive constants, it is clear that</p><formula xml:id="formula_41">R po s (h, u s ) ? R s (h),R po s (h,f t , u s ) ? R s (h,f t ) and R po t (h,f t , u t ) ? R t (h, f t ) when ? s 01 ? 0 and ? t 01 ? 0. Specifically, ? ? (0, 1), let ? t = /M t and ? s = / max{M s , ? t 11 M t }. When ? s 01 &lt; ? s and ? t 01 &lt; ? t , we have |R po s (h, u s ) ? R s (h)| + |R po s (h,f t , u s ) ? R s (h,f t )| &lt; 2 (19) |R po t (h,f t , u t ) ? R t (h, f t )| &lt; 2 .<label>(20)</label></formula><p>Hence, we prove the Eq. <ref type="bibr" target="#b13">(14)</ref>. In following, we give a new upper bound of R t (h, f t ). Recall Theorem 2, we replace 1)</p><formula xml:id="formula_42">R s (h) withR po s (h, u s ), 2)R s (h,f t ) withR po s (h,f t , u s ), 3) R t (h,f t ) withR po t (h,f t , u t ). Then, we have R t (h, f t ) ?R po s (h, u s ) + |R po t (h,f t , u t ) ?R po s (h,f t , u t ))| + |R s (h,f t ) ? R s (h)| + |R po s (h, u s ) ? R s (h)| + |R po s (h,f t , u s ) ? R s (h,f t )| + |R t (h, f t ) ?R po t (h,f t , u t )|.<label>(21)</label></formula><p>Let ? s 01 ? ? s and ? t 01 ? ? t , based on Eqs. <ref type="formula" target="#formula_6">(19)</ref> and <ref type="formula" target="#formula_8">(20)</ref>, we have</p><formula xml:id="formula_43">R t (h, f t ) ?R po s (h, u s ) (i) noisy-data risk + |R po t (h,f t , u t ) ?R po s (h,f t , u s )| (ii) discrepancy between distributions + |R s (h,f t ) ? R s (h)| (iii) domain dissimilarity + 2 (iv) noise effects ?s + 2 (iv) noise effects ?t .</formula><p>Hence, we prove this theorem.</p><p>Theorem 3 shows that if selected data have a high probability to be correct ones (? s 01 ? 0 and ? t 01 ? 0), then ? s and ? t approach zero, meaning that noise effects are eliminated. This motivates us to find a reliable way to select correct data from noisy source data and pseudo-labeled target data and propose the butterfly to WUDA problem. Remark 7. Note that, since Theorems 2 and 3 hold for any hypothesis and any data distributions, the bounds in both theorems are loose and pessimistic. However, both theorems are proposed to show which factors we should take care of in the WUDA problem and both theorems point out the major difference between WUDA and UDA. From this perspective, both theorems are very important for positioning and understanding the WUDA problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">BUTTERFLY: TOWARDS ROBUST ONE-STEP AP-PROACH</head><p>This section presents Butterfly to solve the WUDA problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">What is the Principle-guided Solution?</head><p>Guided by Theorem 3, a robust approach should check highcorrectness data out (meaning ? s 01 ? 0 and ? t 01 ? 0). This checking process will make (iv) and (v), 2 + 2 , become 0. Then, we can obtain gradients ofR po</p><formula xml:id="formula_44">s (h, u s ),R po s (h,f t , u s ) andR po t (h,f t , u t ) w.r.t.</formula><p>parameters of h and use these gradients to minimize them, which minimizes (i) and (ii)</p><formula xml:id="formula_45">as (i) + (ii) ?R po s (h, u s ) +R po s (h,f t , u s ) +R po t (h,f t , u t ).</formula><p>Note that (iii) cannot be directly minimized since we cannot pinpoint clean source data. However, following <ref type="bibr" target="#b12">[13]</ref>, we can indirectly</p><formula xml:id="formula_46">minimize (iii) via minimizingR po s (h, u s ) +R po s (h,f t , u s ), as (iii) ? R s (h,f t ) + R s (h) ?R po s (h, u s ) +R po s (h,f t , u s ) + 2 ,</formula><p>where the last inequality follows Eq. <ref type="bibr" target="#b13">(14)</ref>. This means that a robust approach guided by Theorem 3 can minimize all terms in the right side of inequality in Eq. (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Dual-checking principle</head><p>Memorization effects of deep networks. Recently, an interesting observation for deep networks is that they can memorize easy samples first, and gradually adapt to hard samples as increasing training epochs <ref type="bibr" target="#b58">[59]</ref>. Namely, although deep networks can fit everything (e.g., mislabeled data) in the end, they learn patterns first <ref type="bibr" target="#b58">[59]</ref>: this suggests deep networks can gradually memorize the data, moving from regular data to irregular data such as outliers. To utilize this memorization effects, previous studies have shown that we can regard small-loss data as correct ones (also known as the small-loss trick). Then we can obtain a good classifier that is trained with the small-loss data <ref type="bibr" target="#b53">[54]</ref>.</p><p>Co-teaching learning paradigm. However, if we only use small-loss trick to select correct data (like <ref type="bibr" target="#b53">[54]</ref>), we will get accumulated errors caused by sample-selection bias <ref type="bibr" target="#b55">[56]</ref>. Therefore, researchers also consider a new deep learning paradigm called co-teaching, where we train two deep networks simultaneously, and let them teach each other <ref type="bibr" target="#b55">[56]</ref>. Based on this novel learning paradigm, we can effectively reduce the negative effects from the accumulated errors caused by sample-selection bias.</p><p>Dual-checking principle. Motivated by Section 7.1, we propose the dual-checking principle (DCP): we need to check high-correctness data out in the source and target domains simultaneously. According to the memorization effects of deep networks, we realize DCP based on deep networks, small-loss trick and the co-teaching learning paradigm (i.e., the Butterfly introduced below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Principle-guided Butterfly</head><p>To realize the robust approach for addressing the WUDA problem, we propose a Butterfly framework, which trains four networks dividing into two branches <ref type="figure" target="#fig_4">(Figure 3</ref>). By using DCP, Branch-I checks which data is correct in the mixture domain; while Branch-II checks which pseudo-labeled target data is correct. To ensure these checked data highly-correct, we apply the small-loss trick based on memorization effects of deep learning <ref type="bibr" target="#b58">[59]</ref>. After cross-propagating these checked data <ref type="bibr" target="#b59">[60]</ref>, Butterfly can obtain high-quality DIR and TSR simultaneously in an iterative manner. Theoretically, Branch-I minimizes (i)+(ii)+(iii)+(iv); while Branch-II minimizes (ii) + (v). This means that Butterfly can minimize all terms in the right side of inequality in Eq. (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Loss function in Butterfly</head><p>According toR </p><formula xml:id="formula_47">L(?, u; F, D) = 1 n i=1 u i n i=1 u i (F (x i ),y i ),<label>(22)</label></formula><p>where n is the batch size (i.e., n = |D|), and F represents a network (e.g., F 1 , F 2 , F t1 and F t2 ).</p><formula xml:id="formula_48">D = {(x i ,y i )} n i=1</formula><p>is a mini-batch for training a network, where {x i ,y i } n i=1 could be data in MD or TD <ref type="figure" target="#fig_4">(Figure 3</ref>), and ? represents parameters of F and u = [u 1 , ..., u n ] T is an n-by-1 vector whose elements equal 0 or 1. For two networks in Branch-I, following <ref type="bibr" target="#b12">[13]</ref>, we also add a regularizer |? T f 11 ? f 21 | in their loss functions, where ? f 11 and ? f 21 are weights of the first fully-connect layer of F 1 and F 2 . With this regularizer, F 1 and F 2 will learn from different features.</p><p>Nature of the loss L. In the loss function L, we have n samples:</p><formula xml:id="formula_49">{(x i ,y i )} n i=1 .</formula><p>For the i th sample, we will compute its cross-entropy loss (i.e., (F (x i ),y i )), and we will denote this sample as "selected" if u i = 1. Thus, the nature of L is actually the average value of cross-entropy loss of these "selected" samples. Note that, we need to set a constrain to prevent n i=1 u i = 0 in L, which means that we should select at least one sample to compute L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Training procedures of Butterfly</head><p>This subsection will first present the checking process in Butterfly (Algorithm 1). Then, the full training procedure of Butterfly (Algorithm 2) will be introduced in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.1">Checking process in Butterfly (Algorithm 1)</head><p>We first obtain four inputs: 1) networks F 1 and F 2 , and 2) a mini-batch D, and 3) learning rate ? and 4) remember rate ? (line 1). Then, we will obtain the best u 1 by solving a minimization problem (line 2). L represents the loss function defined in Eq. <ref type="bibr" target="#b21">(22)</ref>. ? 1 represents the parameters of the network F 1 . Similarly we will obtain the best u 2 (line 3). ? 2 represents the parameters of the network F 2 . Next, ? 1 and ? 2 are updated using gradient descent, where the gradients are computed using a given optimizer (lines 4-5). Finally, we substitute the updated ? 1 into F 1 and the updated ? 2 into F 2 and output F 1 and F 2 (line 6). Note that, lines 2-3 correspond to the small-loss trick mentioned in Section 7.2, and lines 4-5 corresponds to the co-teaching paradigm in Section 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 8.</head><p>In line 2 or 3 in Algorithm 1, we need to solve a minimization problem: min u :1u &gt;?|D| L(?, u ; F, D) and return the best u as u (u 1 in line 2 and u 2 in line 3). In this paragraph, we will show how to quickly solve this problem using a sorting algorithm. Recall the nature of the loss L, we know L is the average value of cross-entropy losses of "selected" samples, and 1u is the number of these "selected" samples. Therefore, this minimization problem is equivalent to "given a fixed F (F 1 or F 2 ) and n samples in D, how to select at least k samples such that L is minimized", where k = ?|D| . To solve this problem, we first use a sorting algorithm (top_k function in TensorFlow) to sort these n samples according to their cross-entropy losses (F 1 (x i ),y i ). Then, we select k samples with the smallest cross-entropy losses. Finally, let u i of these k samples be 1 and u i of the other samples be 0, and we can get the best u = [u 1 , . . . , u n ]. The average value of cross-entropy losses of these k samples Algorithm 1 Checking(F 1 , F 2 , D, ?, ?) 1: Input networks F1, F2, mini-batch D, learning rate ?, remember rate ?; 2: Obtain u1 = arg min u 1 :1u 1 &gt;?|D| L(?1, u 1 ; F1, D); // Check high-correctness data 3: Obtain u2 = arg min u 2 :1u 2 &gt;?|D| L(?2, u 2 ; F2, D); // Check high-correctness data 4: Update ?1 = ?1 ? ??L(?1, u2; F1, D); // Update ?1 5: Update ?2 = ?2 ? ??L(?2, u1; F2, D); // Update ?2 6: Output F1 and F2 Algorithm 2 Butterfly Framework: quadruple training for WUDA problem 1: InputDs, Dt, learning rate ?, fixed ? , fixed ?t, epoch T k and Tmax, iteration Nmax, # of pseudo-labeled target data ninit, max of ninit n l t,max ; 2: Initial F1, F2, Ft1, Ft2,D l t =Ds,D =Ds, n l t = ninit; for T = 1, 2, . . . , Tmax do 3: Shuffle training setD; // Noisy dataset for N = 1, . . . , Nmax do 4: Fetch mini-batch? fromD; 5: Update Branch-I: F1, F2 = Checking(F1, F2,?, ?, R(T )); // Check data in MD using Algorithm 1 6: Fetch mini-batch?t fromD l t ; 7: Update Branch-II: Ft1, Ft2 = Checking(Ft1, Ft2,?t, ?, Rt(T )); // Check data in TD using Algorithm 1 end 8: ObtainD l t = Labeling(F1, F2, Dt, n l t ); // Label Dt, following [13] 9: ObtainD =Ds ?D l t ; // Update MD 10: Update n l t = min{T /20 * nt, n l t,max };</p><formula xml:id="formula_50">11: Update R(T ) = 1 ? min{ T T k ?, ? }, Rt(T ) = 1 ? min{ T T k ?t, ?t}; end 12: Output Ft1 and Ft2</formula><p>is the minimized value of L(?, u ; F, D) under the constrain 1u &gt; ?|D|. It is clear that this solving process is equivalent to finding small-loss samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2">Training procedures of Butterfly (Algorithm 2)</head><p>Update parameters of networks. First, we initialize training data for two branches (D for Branch-I andD l t for Branch-II), four networks (F 1 , F 2 , F t1 and F t2 ) and the number of pseudo labels (line 2). In the first epoch (T = 1), following <ref type="bibr" target="#b12">[13]</ref>,D l t is the same withD s (i.e., we use noisy source data as pseudo-labeled target data), since we cannot annotate pseudo labels for target data when T = 1. After mini-batch? is fetched fromD (line 4), F 1 and F 2 check high-correctness data out and update their parameters (lines 5) using Algorithm 1. Using similar procedures, F t1 and F t2 also update their parameters using Algorithm 1 (lines 6-7).</p><p>Assign pseudo labels. In each epoch, after N max mini-batch updating, we randomly select n l t unlabeled target data and assign them pseudo labels using the Labeling function <ref type="bibr" target="#b12">[13]</ref>, F 1 and F 2 (lines 8). Following <ref type="bibr" target="#b12">[13]</ref>, the Labeling function in Algorithm 2 (line 8) assigns pseudo labels to unlabeled target data, when predictions of F 1 and F 2 agree and at least one of them is confident about their predictions (probability above 0.9 or 0.95). Using this function, we can obtain the pseudo-labeled target dataD l t for training Branch-II in the next epoch. Then, we mergeD l t andD s to beD for training Branch-I in the next epoch (line 9).</p><p>Update other parameters. Finally, we update n l t , R(T ) and R t (T ) in lines 10-11. Note that R(T ) and R t (T ) are actually piecewise-defined linear functions:</p><formula xml:id="formula_51">R(T ) = 1 ? ?, T ? T k , 1 ? T /T k ? ?, T ? T k , R t (T ) = 1 ? ? t , T ? T k , 1 ? T /T k ? ? t , T ? T k .</formula><p>In Algorithm 2, we use ? to represent the noise rate (i.e., the ratio of data with incorrect labels) in MD and use ? t to represent the noise rate in TD. However, in WUDA, we cannot obtain the ground-truth ? and ? t . Thus, we regard ? and ? t as hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.6">Can we realize DCP using other models?</head><p>Based on Theorem 3, if we check high-correctness source data and pseudo-labeled target data out, we can reduce the negative effects of noisy source data significantly. Thus, we propose the DCP to check correct data out, which is introduced in Section 7.2. In Butterfly, we realize DCP using deep networks, since the memorization effects of deep networks ensures that we can check correct data out. For nonnetwork models, if they also have memorization effects like deep networks, they can also be used into our approach. We also tried other models. Unfortunately, these models cannot fit the pattern first (like what deep networks did when fitting training data), meaning that, currently, we can only realize our approach using deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.7">A Generalization Bound for WUDA</head><p>In this subsection, we prove a generalization bound for WUDA problem using the loss function Eq. (22) and Theorem 3 <ref type="bibr" target="#b0">1</ref> . Practitioner may safely skip it. First, we introduce the Rademacher complexity of a class of vector-valued functions <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>, <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, which measures the degree to <ref type="bibr" target="#b0">1</ref>. Please note that this is a generalization bound for WUDA problem rather than Butterfly. In Butterfly, we essentially have four classifiers (F 1 , F 2 , F t1 , F t2 ), which is very difficult to analyze it. We will develop a generalization and estimation error bound for Butterfly in the future. which a class can fit random noise. Rademacher Complexity of H is defined as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1 (Rademacher Complexity of H). Given a sample</head><formula xml:id="formula_52">S = {(x i )} n i=1</formula><p>, the empirical Rademacher complexity of the set H is defined as follows.</p><formula xml:id="formula_53">S (H) = 2 n E ? sup h?H n i=1 K k=1 ? ik h k (x i ) ,</formula><p>where h k (?) is the k th component of function h ? H and the ? ik are n ? K matrix of independent Rademacher variables <ref type="bibr" target="#b62">[63]</ref>. The Rademacher complexity of the set H is defined as the expectation of? H (H) over all samples of size n:</p><formula xml:id="formula_54">n (H) = E S ? S (H) |S| = n .</formula><p>Then, using the Rademacher complexity, we can prove an upper bound ofR po s (h, u s ) to show the relation be-tweenR po s (h, u s ) and the loss function Eq. <ref type="bibr" target="#b21">(22)</ref>. As a common practice <ref type="bibr" target="#b66">[67]</ref>, <ref type="bibr" target="#b67">[68]</ref>, we assume that, 1) there are C h &gt; 0 and C L &gt; 0 such that sup h?H h ? ? C h and sup t ??Ch max y (t, y) ? C L , and 2) (t, y) is Lipschitz continuous in t ? ? C h with a Lipschitz constant L . </p><formula xml:id="formula_55">R po s (h, u s ) ? L(?, h; u s , D xy s ) + ? 2L ? D x s (H) 1 ? ? s + 3C L 1 ? ? s ln ? 2 2n ,<label>(23)</label></formula><p>where L is defined in Eq. Then</p><formula xml:id="formula_56">, let ?(S s ) = sup ?L H R po s ( , h) ? L Ss ( , h)</formula><p>. Changing a point of S s affects ?(S s ) at most C L /(n(1 ? ? s )). Thus, by McDiarmid's inequality applied to ?(S s ), for any ? &gt; 0, with probability of at least 1 ? ?/2, the following inequality holds.</p><formula xml:id="formula_57">?(S s ) ? E Ss [?(S s )] + C L 1 ? ? s ln(?/2)</formula><p>2n .</p><p>Then, we have</p><formula xml:id="formula_58">E Ss [?(S s )] = E Ss sup ?L H R po s (h, u s ) ? L Ss (h) ? 2 n(1 ? ? s ) E ?,Ss sup ?L H n i=1 ? i u si (h(x si ), y si ) .<label>(24)</label></formula><p>Because of the existence of u si , Eq. <ref type="formula" target="#formula_8">(24)</ref> is not the Rademacher complexity of L H (i.e., (L H )). However, we can prove that Eq. (24) can be bounded by (L H )/(1 ? ? s ) using the property of sup.</p><p>Since changing a point of S s affects n (L H ) at most 2C L /n, by McDiarmid's inequality, for any ? &gt; 0, with probability of at least 1 ? ?/2, the following inequality holds.</p><formula xml:id="formula_59">n (L H ) ?? Ss (L H ) + 2C L ln(?/2) 2n .</formula><p>Since is Lipschitz continuous, according to <ref type="bibr" target="#b62">[63]</ref>, we hav?</p><formula xml:id="formula_60">Ss (L H ) ? ? 2L ? D x s (H)</formula><p>, which proves this lemma.</p><p>Finally, we prove the generalization bound for WUDA problem as follows. </p><formula xml:id="formula_61">R t (h, f t ) ?2 L(?, h; u s , D xy s ) + L(?, h; u s , D x? s ) + L(?, h; u t , D x? t ) + 4 ? 2L ? D x s (H) 1 ? ? s + ? 2L ? D x t (H) 1 ? ? t + 12C L 1 ? ? s ln ? 2 2n s + 3C L 1 ? ? t ln ? 2 2n t + 6 ,<label>(25)</label></formula><p>where L is defined in Eq.</p><formula xml:id="formula_62">(22), D xy s = {x si , y si } ns i=1 , D x? s = {x si ,f t (x si )} ns i=1 , D x? t = {x ti ,f t (x ti )} nt i=1 D x s = {x si } ns i=1 , D x t = {x ti } nt i=1 , u s = [u s1 , . . . , u sns ] T , ? s = ? us = 1 ? ns i=1 u si /n s , u t = [u t1 , . . . , u tnt ] T and ? t = ? ut = 1 ? nt i=1 u ti /n t .</formula><p>Proof. We prove this theorem (i.e., Inequality (25)) according to Inequality <ref type="formula" target="#formula_6">(21)</ref>, where (25) has 7 terms in the right side and (21) have 6 terms in the right side. 1) For last 3 terms in <ref type="formula" target="#formula_6">(21)</ref>, according to <ref type="bibr" target="#b15">(16)</ref>, <ref type="formula" target="#formula_6">(17)</ref> and <ref type="formula" target="#formula_6">(18)</ref>, we know the sum of last three terms of (21) is less than or equal to 4 .</p><p>2) For first 3 terms in <ref type="formula" target="#formula_6">(21)</ref>, we have shown that (in Section 7.1) the sum of the first 3 terms in (21) is less than or equal to ( * ):</p><formula xml:id="formula_63">2R po s (h, u s ) + 2R po s (h,f t , u s ) +R po t (h,f t , u t ) + 2 .</formula><p>Then, we can prove that (similar with Lemma 3), with probability of at least 1 ? ?, for any h ? H,</p><formula xml:id="formula_64">R po s (h,f t , u s ) ? L(?, h; u s , D x? s ) + ? 2L ? D x s (H) 1 ? ? s + 3C L 1 ? ? s ln ? 2 2n s ,<label>(26)</label></formula><formula xml:id="formula_65">R po t (h,f t , u t ) ? L(?, h; u t , D x? t ) + ? 2L ? D x t (H) 1 ? ? s + 3C L 1 ? ? t ln ? 2 2n t .<label>(27)</label></formula><p>Combining <ref type="formula" target="#formula_8">(23)</ref>, <ref type="bibr" target="#b25">(26)</ref>, <ref type="bibr" target="#b26">(27)</ref> with ( * ), based on 1), we prove this theorem. Note that, 6 equals 4 (in 1)) + 2 (in ( * )). ? n t T , then, with the probability of at least 1 ? 3?, for any h ? H, the following inequality holds.</p><formula xml:id="formula_66">R t (h, f t ) ? 2 L(?, h; u s , D xy s ) + L(?, h; u s , D x? s ) + L(?, h; u t , D x? t ) + 4 ? 2L ? D x s (H) 1 ? ? s + ? 2L ? D x t (H) 1 ? ? t + 12C L 1 ? ? s ln ? 2 2n s + 3C L 1 ? ? t ln ? 2 2n t + C s ? (2M s + M t ) ? n s T + 3C t ? M t ? n t T ,<label>(28)</label></formula><p>where L, D xy s , D x? s , D x? t , D x t , u s , ? s , u t , ? t are defined in Theorem 4, T is the number of training epochs, and C s ? and C t ? are two finite constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 9.</head><p>In Corollary 4, we assume that ? s 01 and ? t 01 will go to zero with the convergence speed of O(1/ ? n s T ) and O(1/ ? n t T ), respectively. In Section 9.8, we verify this assumption through our experiments.</p><p>Corollary 4 shows the empirical upper bound of the target risk (i.e., R t (h, f t )). Based on this bound, we can obtain the estimation error bound of R t (h, f t ) as follows. First, let</p><formula xml:id="formula_67">R L t (h, S s , S t ) =2 L(?, h; u s , D xy s ) + L(?, h; u s , D x? s ) + L(?, h; u t , D x? t ),<label>(29)</label></formula><p>where D xy s , D x? s and D x? t are defined in Theorem 4, and h = arg min h?HR</p><formula xml:id="formula_68">L t (h, S s , S t ) means the empirical min- imizer ofR L t (h, S s , S t ), and h * = arg min h?H R t (h, f t ) means the true risk minimizer of R t (h, f t ), and H = {h|R L t (h, S s , S t ) ? }. Then, we have R t ( h, f t ) ? R t (h * , f t ) = R t ( h, f t ) ?R L t ( h, S s , S t ) +R L t ( h, S s , S t ) ? R t (h * , f t ) +R L t (h * , S s , S t ) ?R L t (h * , S s , S t ) = R t ( h, f t ) ?R L t ( h, S s , S t ) +R L t (h * , S s , S t ) ? R t (h * , f t ) +R L t ( h, S s , S t ) ?R L t (h * , S s , S t ) ? sup h?H (R t (h, f t ) ?R L t (h, S s , S t )) + + 0,<label>(30)</label></formula><p>whereR L t ( h, S s , S t ) ?R L t (h * , S s , S t ) due to the definition of h. If all conditions in Theorem 4 are satisfied, with the probability of at least 1 ? 3?, for any h ? H, we have</p><formula xml:id="formula_69">R t ( h, f t ) ? R t (h * , f t ) ? 4 ? 2L ? D x s (H) 1 ? ? s + ? 2L ? D x t (H) 1 ? ? t + 12C L 1 ? ? s ln ? 2 2n s + 3C L 1 ? ? t ln ? 2 2n t + C s ? (2M s + M t ) ? n s T + 3C t ? M t ? n t T + . (31) Eq. (31) ensures that learning withR L t ( h, S s , S t ) is consistent: as n s , n t ? ? and ? 0, R t ( h, f t ) ? R t (h * , f t ). For linear-in-parameter model with a bounded norm,? D x s (H) = O(1/ ? n s ) and? D x t (H) = O(1/ ? n t ) and thus R t ( h, f t ) ? R t (h * , f t ) in O(1/ ? n s + 1/ ? n t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">COMPARISON TO RELATED WORKS</head><p>In this section, we compare Butterfly with related works and show why related works cannot handle WUDA problem. Relations to co-teaching. As Butterfly is related to coteaching, we discuss their major differences here. Although co-teaching applies the small-loss trick and the cross-update technique to train deep networks against noisy data, it can only deal with one-domain problem instead cross-domain problem. Besides, we argue that Butterfly is not a simple mixtrue of co-teaching and ATDA for two reasons. First, network structure of Butterfly is different with that of ATDA and co-teaching: Butterfly maintains four networks; while ATDA maintains three and co-teaching maintains two. We cannot simply combine ADTA and co-teaching to derive Butterfly. Second, we have justified that the sequential mixture of co-teaching and ATDA (i.e., two-step method) cannot eliminate noise effects caused by noisy source data (see Section 5.2). Specifically, two-step methods only take care of part of noise effects but Butterfly takes care of the whole noise effects. Thus, Butterfly is the first method to eliminate noise effects rather than alleviate it. Relations to TCL. Recently, transferable curriculum learning (TCL) is a robust UDA method to handle noise <ref type="bibr" target="#b43">[44]</ref>. TCL uses small-loss trick to train DANN <ref type="bibr" target="#b22">[23]</ref>. However, TCL can only minimize (i) + (ii) + (iv), while Butterfly can minimize all terms in the right side of Eq. (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">EXPERIMENTS</head><p>We conduct experiments on 32 simulated WUDA tasks and 3 real-world WUDA tasks to verify the efficacy of Butterfly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Simulated WUDA tasks</head><p>We verify the effectiveness of our approach on three benchmark datasets (vision and text), including MNIST, SYN-DIGITS (SYND) 2 and human-sentiment analysis (i.e., Amazon products reviews on book, dvd, electronics and kitchen) <ref type="bibr" target="#b2">3</ref> . They are used to construct 14 basic tasks: MNIST?SYND (M?S), SYND?MNIST (S?M), book?dvd (B?D), book?electronics 2. Digit datasets (MNIST and SYN Digit) can be downloaded from official code of ATDA. The link is https://github.com/ksaito-ut/atda. <ref type="bibr" target="#b2">3</ref>. Sentiment datasets (Amazon products reviews) can be downloaded from the official code of marginalized Stacked Denoising Autoencoder. The link is https://www.cse.wustl.edu/~mchen/code/mSDA.tar. (a) Bing provided by <ref type="bibr" target="#b68">[69]</ref> (b) Caltech256 provided by <ref type="bibr" target="#b69">[70]</ref> (c) ImageNet provided by <ref type="bibr" target="#b70">[71]</ref> (d) SUN provided by <ref type="bibr" target="#b71">[72]</ref>  (B?E), . . . , and kitchen ? electronics (K?E). These tasks are often used for evaluation of UDA methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b24">[25]</ref>. <ref type="figure" target="#fig_6">Figure 4</ref> shows datasets MNIST and SYND.</p><p>Since all source datasets are clean, we corrupt source data using symmetry flipping <ref type="bibr" target="#b57">[58]</ref> and pair flipping <ref type="bibr" target="#b55">[56]</ref> with noise rate ? chosen from {0.2, 0.45}. Note that, there are other ways to generate the noisy source data, such as asymmetry flipping. However, since the asymmetry flipping can be regarded as the combination of symmetry flipping and pair flipping, we only use symmetry flipping and pair flipping to generate simulated WUDA tasks. In real-world WUDA tasks, we have more complex noisy source data, where the noisy type in the source domain is unknown.</p><p>Therefore, for each basic task, we have four kinds of noisy source data: Pair-45% (P45), Pair-20% (P20), Symmetry-45% (S45), Symmetry-20% (S20). Following <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, we can corrupt clean-label datasets manually using the noise transition matrix Q S and Q P . Namely, we evaluate the performance of each method using 32 simulated WUDA tasks: 8 digit tasks and 24 human-sentiment tasks. Since the human-sentiment task is a binary classification problem, pair flipping is equal to symmetry flipping, meaning that we have 24 human-sentiment tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Real-world WUDA tasks</head><p>We also verify the efficacy of our approach on "cross-dataset benchmark" including Bing, Caltech256, Imagenet and SUN <ref type="bibr" target="#b47">[48]</ref>  <ref type="bibr" target="#b3">4</ref> . In this benchmark, Bing, Caltech256, Imagenet and SUN contain common 40 classes. Since Bing dataset was formed by <ref type="bibr" target="#b3">4</ref>. Real-world datasets (BCIS) can be downloaded from the website of the project "A Testbed for Cross-Dataset Analysis": https://sites.google. com/site/crossdataset/home/files ("setup DENSE decaf7", 1.3GB, de-caf7 features). collecting images retrieved by Bing image search, it contains rich noisy data, with presence of multiple objects in the same image and caricaturization <ref type="bibr" target="#b47">[48]</ref>. We use Bing as noisy source data, and Caltech256, Imagenet and SUN as unlabeled target data, which can form three real-world WUDA tasks. <ref type="figure" target="#fig_9">Figure 5</ref> shows datasets Bing, Caltech256, Imagenet and SUN (taking "horse" as the common class).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Baselines</head><p>We realize Butterfly using four networks (B-Net) and compare B-Net with following baselines: 1) ATDA: representative pseudo-labeling-based UDA method <ref type="bibr" target="#b12">[13]</ref>; 2) DAN: representative IPM-based UDA method <ref type="bibr" target="#b35">[36]</ref>; 3) DANN: representative adversarial-training-based UDA method <ref type="bibr" target="#b22">[23]</ref>; 4) Manifold embedded distribution alignment (MEDA): a representative non-deep UDA method <ref type="bibr" target="#b72">[73]</ref>; 5) TCL: an existing robust UDA method; 6) co-teaching+ATDA (Co+ATDA): a two-step method (see Section 5.2); and 7) co-teaching+TCL (Co+TCL). Since MEDA cannot extract features from images, we only compare with MEDA on human-sentiment tasks, where features are already given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Network structure and optimizer</head><p>We implement all methods on Python 3.6 with a NIVIDIA P100 GPU. We use MomentumSGD for optimization in digit and real-world tasks, and set the momentum as 0.9. We use Adagrad for optimization in human-sentiment tasks because of sparsity of review data <ref type="bibr" target="#b12">[13]</ref>. F 1 , F 2 , F t1 and F t2 are 6-layer CNN (3 convolutional and 3 fully-connected layers) for digit tasks; and are 3-layer neural networks (3 fully-connected layers) for human-sentiment tasks; and are 4-layer neural networks (4 fully-connected layers) for real-world tasks. The ReLU active function is used as activation function of these  <ref type="figure">Fig. 6</ref>. The architecture of B-Net for digit WUDA tasks SYND ? MNIST. We added BN layer in the last convolution layer in CNN and FC layers in F 1 and F 2 . We also used dropout in the last convolution layer in CNN and FC layers in F 1 , F 2 , F t1 and F t2 (dropout probability is set to 0.5).  We added BN layer in the first FC layers in F 1 and F 2 . We also used dropout in the first FC layers in F 1 , F 2 , F t1 and F t2 (dropout probability is set to 0.5).</p><p>networks. Besides, dropout and batch normalization are also used. The network topology is shown in <ref type="figure" target="#fig_10">Figures 6 and 7</ref>. As deep networks are highly nonconvex, even with the same network and optimization method, different initializations can lead to different local optimal. Thus, following <ref type="bibr" target="#b56">[57]</ref>, we take four networks with the same architecture but different initialization as four classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Experimental setup</head><p>Since this paper deals with the challenging situation where no labeled data are available in the target domain, we follow the common protocol to set hyperparameters that the similar tasks have the same hyperparameters <ref type="bibr" target="#b36">[37]</ref>. For example, we set the same hyperparameters for all WUDA tasks regarding digit datasets (there are 8 WUDA tasks regarding digit datasets). The selected hyperparameters are robust to many tasks rather than a specific task. Details can be found below. For all 35 WUDA tasks, T k is set to 5, and T max is set to 30, and (?, ?) is the cross-entropy loss function. Learning rate is set to 0.01 for simulated tasks and 0.05 for real-world WUDA tasks, ? t is set to 0.05 for simulated tasks and 0.02 for real-world WUDA tasks. Confidence level of labeling function in line 8 of Algorithm 2 is set to 0.95 for 8 digit tasks, and 0.9 for 24 human-sentiment tasks and 0.8 for real-world WUDA tasks. ? is set to 0.4 for digit tasks, 0.1 for humansentiment tasks, 0.2 for real-world WUDA tasks. n l t,max is set to 15, 000 for digit tasks, 500 for human-sentiment tasks and 4000 for real-world WUDA tasks. N max is set to 1000 for digit tasks, and 200 for human-sentiment and real-world tasks. Batch size is set to 128 for digit, real-world WUDA tasks, and 24 for human-sentiment tasks. Penalty parameter is set to 0.01 for digit, real-world WUDA tasks, and 0.001 for human-sentiment tasks.</p><p>To fairly compare all methods, they have the same network structure. Namely, ATDA, DAN, DANN, TCL and B-Net adopt the same network structure for each dataset. Note that DANN and TCL use the same structure for their discriminate networks. All experiments are repeated 10 times and we report the average accuracy values and standard deviation (STD) of accuracy values of 10 experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6">Results on simulated WUDA tasks</head><p>This subsection presents accuracy on unlabled target data (i.e., target-domain accuracy) in 32 simulated WUDA tasks. <ref type="table" target="#tab_6">Table 1</ref> reports the target-domain accuracy in 8 digit tasks. As can be seen, average target-domain accuracy of B-Net is higher than those of all baselines. On S20 case (the easiest case), most methods work well. ATDA has a satisfactory performance although it does not consider the noise effects explicitly. Then, when facing harder cases (i.e., P20 and P45),   ATDA fails to transfer useful knowledge from noisy source data to unlabeled target data. When facing the hardest cases (i.e., M?S with P45 and S45), DANN has higher accuracy than DAN and ATDA have. However, when facing the easiest cases (i.e., S?M with P20 and S20), the performance of DANN is worse than that of DAN and ATDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6.1">Results on digits WUDA tasks</head><p>Although two-step method Co+ATDA (or Co+TCL) outperforms ATDA (or TCL) in all 8 tasks, it cannot beat one-step method: B-Net in terms of average target-domain accuracy. This result is an evidence for the claim in Section 5.2. In the task S?M with P20, Co+ATDA outperforms all methods (slightly higher than B-Net), since pseudo-labeled source data are almost correct. <ref type="figure" target="#fig_11">Figures 8 and 9</ref> show the target-domain accuracy vs. number of epochs among ATDA, Co+ATDA and B-Net. Besides, we show the accuracy of ATDA trained with clean source data (ATDA-TCS) as a reference point. When accuracy of one method is close to that of ATDA-TCS (red dash line), this method successfully eliminates noise effects. From our observations, it is clear that B-Net is very close to ATDA-TCS in 7 out of 8 tasks (except for S?M task with P45, <ref type="figure" target="#fig_11">Figure 8-(d)</ref>), which is an evidence that Butterfly can eliminate noise effects. Since P45 case is the hardest one and we only have finite samples, it is reasonable that B-Net cannot perfectly eliminate noise effects. An interesting phenomenon is that, B-Net outperforms ATDA-TCS in 2 M?S tasks <ref type="figure" target="#fig_12">(Figure 9-(a)</ref>, (c)). This means that B-Net transfers more useful knowledge (from noisy source data to unlabeled target data) even than ATDA-TCS (from clean source data to unlabeled target data). 9.6.2 Results on human sentiment WUDA tasks <ref type="table" target="#tab_8">Tables 2 and 3</ref> report the target-domain accuracy of each method in 24 human-sentiment WUDA tasks. For these tasks, B-Net has the highest average target-domain accuracy. It should be noted that two-step method does not always perform better than existing UDA methods, such as for 20%noise situation. The reason is that co-teaching performs poorly when pinpointing clean source data from noisy source data. Another observation is that noise effects is not eliminated like target-domain accuracy in 8 digit WUDA tasks. The reason mainly includes that 1) these datasets only provide predefined features (i.e., we cannot extract better features from original contents in the training process), and 2) we only have finite samples and the number of samples in these datasets is smaller than those of digit datasets.   that, in Bing?Caltech256 and Bing?ImageNet tasks, ATDA is slightly worse than B-Net. However, in Bing?SUN task, ATDA is much worse than B-Net. The reason is that the DIR between Bing and SUN are more affected by noisy source data. This is also observed when comparing DANN and TCL. Compared to Co+ATDA, ATDA is slightly better than Co+ATDA. This abnormal phenomenon can be explained using ? (see Section 5.2), after using co-teaching to assign pseudo labels to noisy source data, the second term in ? s may increase, which results in that ? increases, i.e., noise effects actually increase. This phenomenon is an evidence that a two-step method may not really reduce noise effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.7">Results on real-world WUDA tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.8">Can we check correct data out?</head><p>This subsection verifies that ? s 01 and ? t 01 will go to zero with the convergence speed of O(1/ ? n s T ) and O(1/ ? n t T ), respectively. <ref type="figure" target="#fig_2">Figure 10</ref> shows the values of ? s 01 and ? t 01 . It can be seen that ? s 01 and ? t 01 will go to zero when increasing the training epochs. ? s 01 is always lower than ? t 01 because that n s is much larger than n t , indicating that we can check more correct data out when more samples are available. <ref type="figure" target="#fig_2">Figure 10</ref>-(b) shows that we can always find two finite C s ? such that ? s 01 goes to the zero with the convergence speed of O(1/ ? n s T ). So do C t ? and ? t 01 in <ref type="figure" target="#fig_2">Figure 10</ref>-(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.9">Ablation study</head><p>Finally, we conduct thorough experiments to show the contribution of individual components in B-Net. We report average target-domain accuracy on 32 simulated WUDA tasks (8 digit and 24 human-sentiment WUDA tasks) and 3 real-world WUDA tasks. We consider following baselines:    <ref type="figure" target="#fig_2">Fig. 10</ref>. The values of ? s 01 and ? t 01 on the S?M task (P20), where ns = 494, 000 is much larger than nt = 10, 000. Since we do not have pseudo-labeled target data in the first epoch, we illustrate ? t 01 from the second epoch.    <ref type="bibr" target="#b56">[57]</ref> to check data in MD and TD.</p><p>? DCP-M: realize DCP via MentorNet <ref type="bibr" target="#b53">[54]</ref> to check data in MD and TD.  <ref type="table" target="#tab_11">Table 5</ref> reports average target-domain accuracy of above baselines and B-Net. As can be seen, 1) maintaining 4 networks (like B-Net) is better than maintaining 6 networks (like Tri-C-Net) since B-Net outperforms Tri-C-Net in terms of average target-domain accuracy; 2) B-Net benefits from adding the constraint to the loss function L; 3) realizing DCP by co-teaching is better than using Decoupling or MentorNet; and 4) DCP is necessary since accuracy of B-Net is higher than those of B-Net-S, B-Net-T, B-Net-ST and B-Net-M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">CONCLUSIONS</head><p>This paper opens a new problem called wildly unsupervised domain adaptation (WUDA). However, existing UDA methods cannot handle WUDA well. To address this problem, we propose a robust one-step approach called Butterfly. Butterfly maintains four deep networks simultaneously: Two take care of all adaptations; while the other two can focus on classification in target domain. We compare Butterfly with existing UDA methods on 32 simulated and 3 real-world WUDA tasks. Empirical results demonstrate that Butterfly can robustly transfer knowledge from noisy source data to unlabeled target data. In the future, we will extend our Butterfly framework to address open-set WUDA, where label space of target domain is larger than that of source domain. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A PROOFS</head><p>This section presents the completed proofs for theoretical results obtained in this paper. Since we have provided completed proofs regarding Theorems 3 and 4, we do not repeat them here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of Theorem 1</head><p>Proof. We will fist prove Eq. (5) (Case 1) and then prove Eq. <ref type="formula">(6)</ref>  </p><p>where (h(xs)) = [ (h(xs), 1), ..., (h(xs), K)] T and?(xs) = [p? s |Xs (1|xs), . . . ,p? s|Xs (K|xs)] T . According to definition of the transition matrix Q, we know that?  </p><formula xml:id="formula_71">T (xs) = ? T (xs)Q,<label>(33)</label></formula><formula xml:id="formula_72">ES s 1 n i=1 usi n i=1 usi (h(xsi), ysi) = X 1 n i=1 usi n i=1 1 u si =0</formula><p>which means that LS s ( , h) is an unbiased estimator ofR po s ( , h). Then, let ?(Ss) = sup ?L H R po s ( , h) ? LS s ( , h) . Changing a point of Ss affects ?(Ss) at most CL/(n(1 ? ?s)). Thus, by McDiarmid's inequality applied to ?(Ss), for any ? &gt; 0, with probability of at least 1 ? ?/2, the following inequality holds.  where Inequality <ref type="formula" target="#formula_10">(44)</ref> is based on the fact that there are always , ? LH such that (h(xs1), ys1) ? (h(xs1), ys1) &gt; 0. Repeat above procedures n ? 1 times, we have </p><p>Changing a point of Ss affects n(LH) at most 2CL/n. Thus, by McDiarmid's inequality, for any ? &gt; 0, with probability of at least 1 ? ?/2, the following inequality holds. </p><p>Since is Lipschitz continuous, according to <ref type="bibr" target="#b62">[63]</ref>, we hav?</p><formula xml:id="formula_77">Ss (LH) ? ? 2L ? D x s (H).<label>(47)</label></formula><p>Combining <ref type="formula" target="#formula_9">(39)</ref>, <ref type="formula" target="#formula_9">(43)</ref>, <ref type="formula" target="#formula_10">(45)</ref>, <ref type="formula" target="#formula_10">(46)</ref> and <ref type="formula" target="#formula_10">(47)</ref>, we prove this lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Proof of Corollary 1</head><p>We prove this corollary (i.e., Inequality (28)) according to Inequality <ref type="formula" target="#formula_6">(21)</ref>, where (28) has 8 terms in the right side and (21) have 6 terms in the right side. 1) For last 3 terms in <ref type="formula" target="#formula_6">(21)</ref>, since ? s 01 &lt; C s ? / ? nsT and ? t 01 &lt; C t ? / ? ntT , according to <ref type="bibr" target="#b15">(16)</ref>, <ref type="formula" target="#formula_6">(17)</ref> and <ref type="formula" target="#formula_6">(18)</ref>, we know the sum of last three terms of (21) is less than or equal to C s ? (Ms + Mt)/ ? nsT + 2C t ? Mt/ ? ntT (i.e., the last 2 terms in <ref type="formula" target="#formula_8">(28)</ref>). 2) For first 3 terms in (21), we have shown that (in Section 7.1) the sum of the first 3 terms in <ref type="formula" target="#formula_6">(21)</ref>  </p><p>Combining <ref type="formula" target="#formula_8">(23)</ref>, <ref type="bibr" target="#b47">(48)</ref>, <ref type="bibr" target="#b48">(49)</ref> with ( * ), we get the first 6 terms in <ref type="bibr" target="#b24">(25)</ref>. Hence we obtain all 6 terms in <ref type="bibr" target="#b24">(25)</ref> and prove this corollary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ADDITIONAL EXPERIMENTAL RESULTS</head><p>In this section, we present the standard deviation of target-domain accuracy of all methods on WUDA tasks.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>arXiv:1905.07720v3 [cs.LG] 18 Feb 2021 (a) Symmetry-flip noise: S?M (left), M?S (right) (b) Pair-flip noise: S?M (left), M?S (right) WUDA ruins representative UDA methods. Representative UDA methods includes deep adaptation network (DAN, an IPM based method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>y ? Y} is the class of loss functions associated with H; ? expected risks on the noisy m.r.v. and correct m.r.v.: R s (h) = Ep s(xs,?s) [ (h(x s ),? s )], R s (h) = E ps(xs,ys) [ (h(x s ), y s )];</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Lemma 1 .</head><label>1</label><figDesc>po s (h, u s ) and R s (h). Given the m.r.v. (X s , Y s , U s ) with the probability densityp po s (x s , y s , u s ) and Eq. (9), we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>po s (h, u s ),R po t (h,f t , u t ) andR po s (h,f t , u s ) defined in Section 6, four networks trained by Butterfly share the same loss function but with different inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 3 .</head><label>3</label><figDesc>Given a sample S s = {(x si , y si , u si )} n i=1 drawn from the probability densityp po s (x s , y s , u s ), with the probability of at least 1 ? ? over samples S s of size n drawn fromp po s (x s , y s , u s ), the following inequality holds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(22), D xy s = {x si , y si } n i=1 , D x s = {x si } n i=1 , u s = [u s1 , . . . , u sn ] T and ? s = ? us = 1 ? n i=1 u si /n. Proof (sketch). For simplicity, in this proof, we let L Ss ( , h) = L(?, h; u s , D xy s ),R po s ( , h) =R po s (h, u s ), and E Ss [?] = E Ss?(P po s ) n [?], whereP po s is the probability measure corresponding to the densityp po s . We first prove that L Ss ( , h) is an unbiased estimator ofR po s ( , h) based on the definition of R po s (h, u s ) in Section 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Theorem 4 .</head><label>4</label><figDesc>Given a sample S s = {(x si , y si , u si )} ns i=1 drawn from the probability densityp po s (x s , y s , u s ) and a sample S t= {(x ti , u ti )} nt i=1 drawn from the probability densityp po t (x t , u t ), under the assumptions in Remark 2, Remark 3 and Lemma 2, ? ? (0, 1), there are ? s and ? t , if ? s 01 &lt; ? s and ? t 01 &lt; ? t , then, with the probability of at least 1 ? 3?, for any h ? H, the following inequality holds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Corollary 1 (</head><label>1</label><figDesc>Generalization Bound for WUDA). Given a sample S s and a sample S t defined in Theorem 4, under the assumptions in Remark 2, Remark 3 and Lemma 2, if ? s 01 &lt; C s ? / ? n s T and ? t 01 &lt; C t ? /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of MNIST and SYND.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of Bing, Caltech256, ImageNet and SUN (taking "horse" as the common class).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .</head><label>7</label><figDesc>The architecture of B-Net for (a) human-sentiment WUDA tasks and (b) real-world WUDA tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Target-domain accuracy vs. number of epochs on four SYND?MNIST WUDA tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Target-domain accuracy vs. number of epochs on four MNIST?SYND WUDA tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>?</head><label></label><figDesc>Tri-C-Net: triply check data in SD, MD and TD. Compared to B-Net, Tri-C-Net has another branch (denoted by Branch-III) to check data in SD. Namely, Tri-C-Net has three branches (i.e., six networks). Parameters of CNN of the Branch-III are the same with that of Branch-I and Branch-II.? B w/o C: train B-Net by Algorithm 2, without adding |? T f 11 ? f 21 | into the loss function of B-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>DatasetsTri-C-Net B w/o C DCP-D DCP-M B-Net-S B-Net-T B-Net-ST B-Net-M B-Net</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>?B-</head><label></label><figDesc>Net-S: train B-Net where the check is turned on for Source data in MD. ? B-Net-T: train B-Net where the check is turned on for Target data in TD. ? B-Net-ST: train B-Net where the checks are turned on for Source data in MD and Target data in TD. ? B-Net-M: train B-Net where the check is turned on for all data in MD. Note that in the full B-Net, the checks are turned on for all data in MD and TD. Comparing B-Net with Tri-C-Net shows whether two branches (i.e., four networks) are the optimal design. Comparing B-Net with B w/o C reveals if the constraint |? T f 11 ? f 21 | takes effects. Comparing B-Net with DCP-D and DCP-M shows whether realizing DCP via coteaching is the optimal way. Comparing B-Net with B-Net-S, B-Net-T, B-Net-ST and B-Net-M reveals if DCP is necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>= ( 1 ?</head><label>1</label><figDesc>usi (h(xsi), ysi)dP po s ?u s ) usi (h(xsi), ysi)dP po s = (1 ? ?u s ) ?1 1 n n i=1Eppo s (xs,ys,us) [us (h(xs), ys)] =R po s (h, us) =R po s ( , h),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>xsi), ysi) := n(LH).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>n(LH) ?? Ss (LH) + 2CL ln(?/2) 2n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>y s ) and q s (x s , y s ) represent probability densities of noisy, correct and incorrect multivariate random variable (m.r.v.) defined on X ? Y, respectively, andp xs (x s ), p xs (x s ) and q xs (x s ) are their marginal densities on X ;</figDesc><table /><note>? p xt (x t ) represents the probability density of m.r.v. X t defined on X ;</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>..., q Ys|Xs (K|x s )] T .</figDesc><table><row><cell cols="2">Proof (sketch). For Eq. (5), we can prove it using the definition</cell></row><row><cell cols="2">of the transition matrix defined in Section 3.2 and the fact</cell></row><row><cell>p s (x s ,? s ) =p?</cell><cell>s|Xs (? s |x s )p xs (x s ). For Eq. (6), we can prove</cell></row><row><cell cols="2">it using Eq. (2) and the definition of R s (h).</cell></row><row><cell cols="2">Remark 2. In Eq. (6), E qx s (xs) [? T q (x s ) (h(x s ))] represents</cell></row><row><cell cols="2">the expected risk on the incorrect m.r.v.. To ensure to obtain</cell></row><row><cell cols="2">useful knowledge fromP</cell></row></table><note>s , we need to avoidR s</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>may equal +? for some h ? H. Namely, even when ? s 01 and ? t 01 are very small,R po s (h, u s ) is probably far away from R s (h). Thus, without proper assumptions,it is useless to use</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 1</head><label>1</label><figDesc>Target-domain accuracy on 8 digit WUDA tasks (SYND?MNIST ). Bold value represents the highest accuracy in each row.</figDesc><table><row><cell>Tasks</cell><cell cols="2">Type DAN</cell><cell cols="2">DANN ATDA</cell><cell>TCL</cell><cell>Co+TCL</cell><cell cols="2">Co+ATDA B-Net</cell></row><row><cell></cell><cell>P20</cell><cell cols="2">90.17% 79.06%</cell><cell>55.95%</cell><cell>80.81%</cell><cell>88.56%</cell><cell>95.37%</cell><cell>95.29%</cell></row><row><cell>S?M</cell><cell>P45 S20</cell><cell cols="2">67.00% 55.34% 90.74% 75.19%</cell><cell>53.66% 89.87%</cell><cell>55.97% 80.23%</cell><cell>73.27% 85.88%</cell><cell>75.43% 95.22%</cell><cell>90.21% 95.88%</cell></row><row><cell></cell><cell>S45</cell><cell cols="2">89.31% 65.87%</cell><cell>87.53%</cell><cell>68.54%</cell><cell>75.69%</cell><cell>92.03%</cell><cell>94.97%</cell></row><row><cell></cell><cell>P20</cell><cell cols="2">40.82% 58.78%</cell><cell>33.74%</cell><cell>58.88%</cell><cell>59.08%</cell><cell>58.02%</cell><cell>60.36%</cell></row><row><cell>M?S</cell><cell>P45 S20</cell><cell cols="2">28.41% 43.70% 30.62% 53.52%</cell><cell>19.50% 49.80%</cell><cell>45.31% 56.74%</cell><cell>47.15% 56.91%</cell><cell>46.80% 56.64%</cell><cell>56.62% 57.05%</cell></row><row><cell></cell><cell>S45</cell><cell cols="2">28.21% 43.76%</cell><cell>17.20%</cell><cell>49.91%</cell><cell>51.22%</cell><cell>54.29%</cell><cell>56.18%</cell></row><row><cell cols="2">Average</cell><cell cols="2">58.16% 58.01%</cell><cell>50.91%</cell><cell>62.05%</cell><cell>67.22%</cell><cell>71.73%</cell><cell>75.82%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 reports</head><label>4</label><figDesc>the target-domain accuracy in 3 tasks. B-Net enjoys the best performance on all tasks. It should be noted</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 2</head><label>2</label><figDesc>Target-domain accuracy on 12 human-sentiment WUDA tasks with the 20% noise rate. Bold values mean the highest values in each row.</figDesc><table><row><cell>Tasks</cell><cell>DAN</cell><cell>DANN</cell><cell>ATDA</cell><cell>TCL</cell><cell>MEDA</cell><cell>Co+TCL</cell><cell>Co+ATDA B-Net</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 3</head><label>3</label><figDesc>Target-domain accuracy on 12 human-sentiment WUDA tasks with the 45% noise rate. Bold values mean the highest values in each row. Target-domain accuracy on 3 real-world WUDA tasks. The source domain is the Bing dataset that contains noisy information from the Internet. Bold value represents the highest accuracy in each row.</figDesc><table><row><cell>Tasks</cell><cell>DAN</cell><cell>DANN</cell><cell>ATDA</cell><cell></cell><cell>TCL</cell><cell>MEDA</cell><cell cols="2">Co+TCL</cell><cell>Co+ATDA B-Net</cell></row><row><cell>B?D</cell><cell>52.43%</cell><cell>52.98%</cell><cell>53.56%</cell><cell></cell><cell>54.44%</cell><cell>54.50%</cell><cell>53.21%</cell><cell></cell><cell>54.32%</cell><cell>56.59%</cell></row><row><cell>B?E</cell><cell>52.17%</cell><cell>53.50%</cell><cell>55.14%</cell><cell></cell><cell>54.14%</cell><cell>54.29%</cell><cell>53.98%</cell><cell></cell><cell>57.34%</cell><cell>55.74%</cell></row><row><cell>B?K</cell><cell>52.89%</cell><cell>51.84%</cell><cell>51.14%</cell><cell></cell><cell>53.32%</cell><cell>53.68%</cell><cell>51.77%</cell><cell></cell><cell>53.28%</cell><cell>57.00%</cell></row><row><cell>D?B</cell><cell>53.11%</cell><cell>53.04%</cell><cell>54.48%</cell><cell></cell><cell>53.27%</cell><cell>53.66%</cell><cell>54.85%</cell><cell></cell><cell>55.95%</cell><cell>55.15%</cell></row><row><cell>D?E</cell><cell>51.30%</cell><cell>53.04%</cell><cell>54.21%</cell><cell></cell><cell>53.77%</cell><cell>54.11%</cell><cell>55.63%</cell><cell></cell><cell>56.08%</cell><cell>58.91%</cell></row><row><cell>D?K</cell><cell>52.15%</cell><cell>53.17%</cell><cell>57.99%</cell><cell></cell><cell>52.45%</cell><cell>52.45%</cell><cell>58.10%</cell><cell></cell><cell>59.94%</cell><cell>66.20%</cell></row><row><cell>E?B</cell><cell>51.38%</cell><cell>51.08%</cell><cell>52.54%</cell><cell></cell><cell>52.14%</cell><cell>52.56%</cell><cell>54.88%</cell><cell></cell><cell>53.30%</cell><cell>54.93%</cell></row><row><cell>E?D</cell><cell>52.83%</cell><cell>51.24%</cell><cell>49.02%</cell><cell></cell><cell>52.57%</cell><cell>53.03%</cell><cell>50.03%</cell><cell></cell><cell>49.62%</cell><cell>52.88%</cell></row><row><cell>E?K</cell><cell>54.21%</cell><cell>53.58%</cell><cell>51.66%</cell><cell></cell><cell>55.04%</cell><cell>55.42%</cell><cell>56.15%</cell><cell></cell><cell>52.10%</cell><cell>56.12%</cell></row><row><cell>K?B</cell><cell>50.44%</cell><cell>51.77%</cell><cell cols="2">51.96%</cell><cell>51.50%</cell><cell>51.52%</cell><cell>53.81%</cell><cell></cell><cell>52.59%</cell><cell>51.39%</cell></row><row><cell>K?D</cell><cell>52.20%</cell><cell>51.45%</cell><cell>52.86%</cell><cell></cell><cell>53.19%</cell><cell>53.38%</cell><cell>55.69%</cell><cell></cell><cell>54.52%</cell><cell>53.53%</cell></row><row><cell>K?E</cell><cell>54.72%</cell><cell>53.33%</cell><cell>52.11%</cell><cell></cell><cell>53.46%</cell><cell>53.81%</cell><cell>51.26%</cell><cell></cell><cell>52.62%</cell><cell>53.71%</cell></row><row><cell cols="2">Average 52.49%</cell><cell>52.50%</cell><cell>53.65%</cell><cell></cell><cell>53.27%</cell><cell>53.54%</cell><cell>54.11%</cell><cell></cell><cell>54.31%</cell><cell>56.01%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE 4</cell><cell></cell><cell></cell></row><row><cell cols="2">Target</cell><cell>DAN</cell><cell cols="3">DANN ATDA</cell><cell>TCL</cell><cell>Co+TCL</cell><cell cols="2">Co+ATDA B-Net</cell></row><row><cell cols="3">Caltech256 77.83%</cell><cell>78.00%</cell><cell cols="2">80.84%</cell><cell>79.35%</cell><cell>79.27%</cell><cell cols="2">79.89%</cell><cell>81.71%</cell></row><row><cell cols="2">Imagenet</cell><cell>70.29%</cell><cell>72.16%</cell><cell cols="2">74.89%</cell><cell>72.53%</cell><cell>72.33%</cell><cell cols="2">74.73%</cell><cell>75.00%</cell></row><row><cell>SUN</cell><cell></cell><cell>24.56%</cell><cell>26.80%</cell><cell cols="2">26.26%</cell><cell>28.80%</cell><cell>29.15%</cell><cell cols="2">26.31%</cell><cell>30.54%</cell></row><row><cell cols="2">Average</cell><cell>57.56%</cell><cell>58.99%</cell><cell cols="2">60.66%</cell><cell>60.23%</cell><cell>60.25%</cell><cell cols="2">60.31%</cell><cell>62.42%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 5</head><label>5</label><figDesc>Results of ablation study. Average target-domain accuracy on 8 simulated digit WUDA tasks (Digit), 24 simulated human-sentiment WUDA tasks (Sentiment) and 3 real-world WUDA tasks (Real-world). Bold value represents the highest accuracy in each row.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>Jie Lu (F'18) is a Distinguished Professor and the Director of the Centre for Artificial Intelligence at the University of Technology Sydney, Australia. She received the Ph.D. degree from Curtin University of Technology, Australia, in 2000. Her main research expertise is in fuzzy transfer learning, decision support systems, concept drift, and recommender systems. She has published six research books and 400 papers in Artificial Intelligence, IEEE transactions on Fuzzy Systems and other refereed journals and conference proceedings. She has won over 20 Australian Research Council (ARC) discovery grants and other research grants for over $7 million. She serves as Editor-In-Chief for Knowledge-Based Systems (Elsevier) and Editor-In-Chief for International Journal on Computational Intelligence Systems (Atlantis), has delivered 20 keynote speeches at international conferences, and has chaired 10 international conferences. She is a Fellow of IEEE and Fellow of IFSA. Han is currently an Assistant Professor of Computer Science at Hong Kong Baptist University and a Visiting Scientist at RIKEN Center for Advanced Intelligence Project (RIKEN AIP), hosted by Masashi Sugiyama. He was a Postdoc Fellow at RIKEN AIP (2019-2020), advised by Masashi Sugiyama. He received his Ph.D. degree in Computer Science from University of Technology Sydney (2015-2019), advised by Ivor W. Tsang and Ling Chen. During 2018-2019, he was a Research Intern with the AI Residency Program at RIKEN AIP, working on robust deep learning projects with Masashi Sugiyama, Gang Niu and Mingyuan Zhou. His current research interests lie in machine learning, deep learning and artificial intelligence. His long-term goal is to develop trustworthy intelligent systems, which can learn from a massive volume of complex (e.g., weakly-supervised, adversarial, and private) data (e.g, single-/multi-label, ranking, domain, similarity, graph and demonstration) automatically. He has served as program committes of NeurIPS, ICML, ICLR, AISTATS, UAI, AAAI, IJCAI, ACML and ICDM. He received the National Scholarship (2013), UTS International Research Scholarship (2014) and UTS Research Publication Award (2017 and 2018). Niu is a research scientist at RIKEN Center for Advanced Intelligence Project. He received the PhD degree in computer science from Tokyo Institute of Technology in 2013. His research interests include mainly weakly-supervised learning and its applications. He has published 10 NeurIPS (including 1 oral and 1 spotlight) and 10 ICML papers and also served as an area chair for ICML 2019, NeurIPS 2019 and ICML 2020. He has authored five monographs, five textbooks, and 460 papers including 220 refereed international journal papers. Dr. Zhang has won seven Australian Research Council (ARC) Discovery Projects grants and many other research grants. He was awarded an ARC QEII fellowship in 2005. He has served as a member of the editorial boards of several international journals, as a guest editor of eight special issues for IEEE transactions and other international journals, and co-chaired several international conferences and workshops in the area of fuzzy decision-making and knowledge engineering. Sugiyama is Director of RIKEN Center for Advanced Intelligence Project and Professor at the University of Tokyo. He received the PhD degree in computer science from Tokyo Institute of Technology in 2001. His research interests include theories and algorithms of machine learning. He was awarded the Japan Society for the Promotion of Science Award and the Japan Academy Medal in 2017.</figDesc><table><row><cell>Bo Gang Guangquan Zhang is an Associate Professor</cell></row><row><cell>and Director of the Decision Systems and e-</cell></row><row><cell>Service Intelligent (DeSI) Research Laboratory</cell></row><row><cell>at the University of Technology Sydney, Australia.</cell></row><row><cell>He received the Ph.D. degree in applied math-</cell></row><row><cell>ematics from Curtin University of Technology,</cell></row><row><cell>Australia, in 2001.</cell></row><row><cell>His research interests include fuzzy machine</cell></row><row><cell>learning, fuzzy optimization, and machine learn-</cell></row><row><cell>ing. Masashi</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>(Case 2).</figDesc><table><row><cell cols="4">Case 1. According to definition ofRs(h), we hav?</cell></row><row><cell cols="4">Rs(h) = Ep s(xs,?s) [ (h(xs),?s)]</cell></row><row><cell></cell><cell></cell><cell>K</cell><cell></cell></row><row><cell>=</cell><cell></cell><cell></cell><cell>(h(xs),?s)ps(xs,?s)dxs</cell></row><row><cell></cell><cell>X</cell><cell>?s=1</cell><cell></cell></row><row><cell></cell><cell></cell><cell>K</cell><cell></cell></row><row><cell>=</cell><cell>X</cell><cell>?s=1</cell><cell>(h(xs),?s)p? s|Xs (?s|xs)px s (xs)dxs</cell></row><row><cell>=</cell><cell></cell><cell cols="2">T (xs) (h(xs))px s (xs)dxs,</cell></row><row><cell></cell><cell cols="2">X?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Ys|Xs (1|xs), ... , p Ys|Xs (K|xs)] T . Substituting Eq. (33) into Eq. (32), we hav? According to definition ofRs(h) and Eq. (2), we hav? Ys|Xs (ys|xs)qx s (xs)dxs. Ys|Xs (1|xs), ..., q Ys|Xs (K|xs)] T , we hav? Proof. For any h ? H, we have For simplicity, in this proof, we let LS s ( , h) = L(?, h; us, D xy s ),R po s ( , h) =R po s (h, us), and ES s [?] = E Ss?(P po s ) n [?], whereP po s is the probability measure corresponding to the densityp po s . We first show that LS s ( , h) is an unbiased estimator ofR po s ( , h) based on the definition ofR po s (h, us) in Section 6. Since Ss = {(xsi, ysi, usi)} n i=1 are i.i.d samples fromP po s , ES s [LS s ( , h)] can be expressed as follows.</figDesc><table><row><cell>A.5 Proof of Lemma 3</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rs(h) =</cell><cell cols="3">? T (xs)Q (h(xs))px s (xs)dxs</cell></row><row><cell></cell><cell>X</cell><cell></cell><cell></cell></row><row><cell>=</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Rs(h) = Ep s (xs,?s) [ (h(xs),?s)]</cell></row><row><cell></cell><cell>K</cell><cell></cell><cell></cell></row><row><cell>=</cell><cell cols="3">(h(xs),?s)ps(xs,?s)dxs</cell></row><row><cell>X</cell><cell>?s=1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>K</cell><cell></cell><cell></cell></row><row><cell>=</cell><cell cols="4">(h(xs), ys) (1 ? ?)ps(xs, ys) + ?qs(xs, ys) dxs</cell></row><row><cell>X</cell><cell>ys=1</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>K</cell><cell></cell><cell>K</cell></row><row><cell cols="2">= (1 ? ?)</cell><cell cols="2">(h(xs), ys)ps(xs, ys)dxs + ?</cell><cell>(h(xs), ys)qs(xs, ys)dxs</cell></row><row><cell></cell><cell>X</cell><cell>ys=1</cell><cell>X</cell><cell>ys=1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>K</cell></row><row><cell cols="3">= (1 ? ?)Rs(h) + ?</cell><cell cols="2">(h(xs), ys)q (34)</cell></row><row><cell></cell><cell></cell><cell>X</cell><cell>ys=1</cell></row><row><cell cols="5">Let ?q(xs) = [q Rs(h) = (1 ? ?)Rs(h) + ?E qx s (xs) [? T q (xs) (h(xs))].</cell></row><row><cell>Hence, Case 2 is proved.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.2 Proof of Theorem 2</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Rt(h, ft) = Rt(h, ft) +Rs(h) ?Rs(h) + Rs(h, ft) ? Rs(h, ft)</cell></row></table><note>where ?(xs) = [pX ?T (xs)I (h(xs))px s (xs)dxs +X ?T (xs)(Q ? I) (h(xs))px s (xs)dxs = Rs(h) + E px s (xs) [? T (xs)(Q ? I) (h(xs))].Hence, Case 1 is proved. Case 2.=Rs(h) + Rt(h, ft) ?Rs(h, ft) + Rs(h, ft) ? Rs(h) + Rs(h) ?Rs(h) +Rs(h, ft) ? Rs(h, ft).(35) Since we do not know ft, we substitute following equations into Eq. (35), Rt(h, ft) = Rt(h,ft) + Rt(h, ft) ? Rt(h,ft), Rs(h, ft) =Rs(h,ft) +Rs(h, ft) ?Rs(h,ft), Rs(h, ft) = Rs(h,ft) + Rs(h, ft) ? Rs(h,ft).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head></head><label></label><figDesc>where Eq. (40) is based on Eq. (38), Inequalities (41) and (42) are based on Jensen's Inequality. Because of existence of usi, Eq. (43) is not the Rademacher complexity of LH (i.e., (LH)). However, in following, we prove that Eq. (43) can be bounded by (LH)/(1 ? ?s). E?,S s sup , ?L H (h(xs1), ys1) ? (h(xs1), ys1) + E?,S s sup</figDesc><table><row><cell></cell><cell>=</cell><cell>1 2</cell><cell cols="2">, ?L H E?,S s sup</cell><cell cols="3">us1( (h(xs1), ys1) ? (h(xs1), ys1)) +</cell><cell>n i=2</cell><cell>?iusi (h(xsi), ysi) +</cell><cell>n i=2</cell><cell>?iusi (h(xsi), ysi)</cell></row><row><cell></cell><cell>?</cell><cell>1 2</cell><cell></cell><cell></cell><cell></cell><cell>n i=2</cell><cell cols="2">?iusi (h(xsi), ysi) +</cell><cell>n i=2</cell><cell>?iusi (h(xsi), ysi)</cell><cell>(44)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">?1 (h(xs1), ys1) +</cell><cell cols="3">?iusi (h(xsi), ysi) ,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">?L H</cell><cell>i=2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>n</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">E?,S s sup</cell><cell cols="2">?iusi (h(xsi), ysi)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>?L H</cell><cell>i=1</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>n</cell><cell></cell><cell></cell></row><row><cell cols="4">= E?,S s sup</cell><cell cols="2">?1us1 (h(xs1), ys1) +</cell><cell cols="2">?iusi (h(xsi), ysi)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?L H</cell><cell></cell><cell cols="2">i=2</cell><cell></cell></row><row><cell>=</cell><cell>1 2</cell><cell cols="3">E?,S s sup</cell><cell></cell><cell></cell><cell></cell><cell>n i=2</cell><cell>?iusi (h(xsi), ysi)</cell></row></table><note>, ?L H us1 (h(xs1), ys1) +n i=2 ?iusi (h(xsi), ysi) + (?us1) (h(xs1), ys1) +=</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>is less than or equal to ( * ):2R po Then, we can prove that (similar with Lemma 3), with probability of at least 1 ? ?, for any h ? H,</figDesc><table><row><cell></cell><cell></cell><cell cols="3">C s ? Ms ? nsT</cell><cell>+</cell><cell>C t ? Mt ? ntT</cell><cell>.</cell></row><row><cell>R po s (h,ft, us) ? L(?, h; us, D x? s ) +</cell><cell>?</cell><cell>2L ? D x s (H) 1 ? ?s</cell><cell>+</cell><cell cols="2">3CL 1 ? ?s</cell><cell cols="2">ln ? 2 2ns</cell><cell>,</cell><cell>(48)</cell></row><row><cell>R t (h,ft, ut) ? L(?, h; ut, D x? po t ) +</cell><cell cols="2">? 2L ? D x t (H) 1 ? ?s</cell><cell>+</cell><cell cols="2">3CL 1 ? ?t</cell><cell cols="2">ln ? 2 2nt</cell><cell>.</cell></row></table><note>s (h, us) + 2R pos (h,ft, us) +R pot (h,ft, ut) +</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>TABLE 6</head><label>6</label><figDesc>The standard deviation of target-domain accuracy on 8 digit WUDA tasks (SYND?MNIST ). Bold value represents the highest accuracy in each row.</figDesc><table><row><cell>Tasks</cell><cell cols="2">Type DAN</cell><cell cols="2">DANN ATDA</cell><cell>TCL</cell><cell>Co+TCL</cell><cell cols="2">Co+ATDA B-Net</cell></row><row><cell></cell><cell>P20</cell><cell cols="2">0.23% 1.12%</cell><cell>31.26%</cell><cell>3.88%</cell><cell>3.26%</cell><cell>0.66%</cell><cell>0.50%</cell></row><row><cell>S?M</cell><cell>P45 S20</cell><cell cols="2">6.43% 6.88% 1.18% 1.29%</cell><cell>6.45% 1.32%</cell><cell>7.08% 1.17%</cell><cell>6.45% 1.32%</cell><cell>4.02% 0.38%</cell><cell>5.43% 0.23%</cell></row><row><cell></cell><cell>S45</cell><cell cols="2">1.38% 1.59%</cell><cell>1.64%</cell><cell>1.62%</cell><cell>1.64%</cell><cell>1.29%</cell><cell>1.13%</cell></row><row><cell></cell><cell>P20</cell><cell cols="2">4.30% 4.59%</cell><cell>4.62%</cell><cell>4.54%</cell><cell>4.62%</cell><cell>2.73%</cell><cell>4.31%</cell></row><row><cell>M?S</cell><cell>P45 S20</cell><cell cols="2">2.01% 2.05% 4.82% 4.84%</cell><cell>2.06% 4.88%</cell><cell>1.87% 4.70%</cell><cell>2.06% 4.88%</cell><cell>6.81% 3.20%</cell><cell>4.06% 2.66%</cell></row><row><cell></cell><cell>S45</cell><cell cols="2">2.02% 2.25%</cell><cell>2.25%</cell><cell>2.22%</cell><cell>2.25%</cell><cell>1.68%</cell><cell>2.79%</cell></row><row><cell cols="2">Average</cell><cell cols="2">2.80% 3.08%</cell><cell>6.81%</cell><cell>3.39%</cell><cell>3.31%</cell><cell>2.60%</cell><cell>2.64%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>TABLE 7</head><label>7</label><figDesc>The standard deviation of target-domain accuracy on 12 human-sentiment WUDA tasks with the 20% noise rate. Bold values mean the highest values in each row.</figDesc><table><row><cell>Tasks</cell><cell>DAN</cell><cell>DANN</cell><cell>ATDA</cell><cell>TCL</cell><cell>MEDA</cell><cell>Co+TCL</cell><cell cols="2">Co+ATDA B-Net</cell></row><row><cell>B?D</cell><cell>1.48%</cell><cell>1.37%</cell><cell>1.45%</cell><cell>1.41%</cell><cell>1.40%</cell><cell>1.38%</cell><cell>1.47%</cell><cell>1.47%</cell></row><row><cell>B?E</cell><cell>1.82%</cell><cell>1.67%</cell><cell>1.81%</cell><cell>1.77%</cell><cell>1.76%</cell><cell>1.77%</cell><cell>1.68%</cell><cell>1.81%</cell></row><row><cell>B?K</cell><cell>1.34%</cell><cell>1.33%</cell><cell>1.33%</cell><cell>0.97%</cell><cell>1.31%</cell><cell>1.21%</cell><cell>1.24%</cell><cell>1.33%</cell></row><row><cell>D?B</cell><cell>1.84%</cell><cell>1.50%</cell><cell>1.83%</cell><cell>1.78%</cell><cell>1.83%</cell><cell>1.68%</cell><cell>1.79%</cell><cell>1.63%</cell></row><row><cell>D?E</cell><cell>1.78%</cell><cell>1.72%</cell><cell>1.75%</cell><cell>1.77%</cell><cell>1.74%</cell><cell>1.66%</cell><cell>1.72%</cell><cell>1.77%</cell></row><row><cell>D?K</cell><cell>2.02%</cell><cell>1.98%</cell><cell>2.00%</cell><cell>1.81%</cell><cell>1.97%</cell><cell>1.96%</cell><cell>1.88%</cell><cell>1.90%</cell></row><row><cell>E?B</cell><cell>1.54%</cell><cell>1.42%</cell><cell>1.52%</cell><cell>1.22%</cell><cell>1.53%</cell><cell>1.45%</cell><cell>1.51%</cell><cell>1.53%</cell></row><row><cell>E?D</cell><cell>1.72%</cell><cell>1.65%</cell><cell>1.71%</cell><cell>1.48%</cell><cell>1.67%</cell><cell>1.79%</cell><cell>1.53%</cell><cell>1.70%</cell></row><row><cell>E?K</cell><cell>1.29%</cell><cell>1.12%</cell><cell>1.27%</cell><cell>1.22%</cell><cell>1.27%</cell><cell>1.15%</cell><cell>1.14%</cell><cell>1.28%</cell></row><row><cell>K?B</cell><cell>1.86%</cell><cell>1.74%</cell><cell>1.84%</cell><cell>1.72%</cell><cell>1.82%</cell><cell>1.72%</cell><cell>1.63%</cell><cell>1.85%</cell></row><row><cell>K?D</cell><cell>0.44%</cell><cell>0.11%</cell><cell>0.42%</cell><cell>0.27%</cell><cell>0.39%</cell><cell>0.31%</cell><cell>0.21%</cell><cell>0.43%</cell></row><row><cell>K?E</cell><cell>1.00%</cell><cell>0.68%</cell><cell>0.98%</cell><cell>0.64%</cell><cell>0.98%</cell><cell>0.96%</cell><cell>0.79%</cell><cell>0.99%</cell></row><row><cell cols="2">Average 1.51%</cell><cell>1.36%</cell><cell>1.49%</cell><cell>1.34%</cell><cell>1.47%</cell><cell>1.42%</cell><cell>1.38%</cell><cell>1.48%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>TABLE 8</head><label>8</label><figDesc>The standard deviation of target-domain accuracy on 12 human-sentiment WUDA tasks with the 45% noise rate. Bold values mean the highest values in each row.</figDesc><table><row><cell>Tasks</cell><cell>DAN</cell><cell>DANN</cell><cell>ATDA</cell><cell>TCL</cell><cell>MEDA</cell><cell>Co+TCL</cell><cell cols="2">Co+ATDA B-Net</cell></row><row><cell>B?D</cell><cell>1.11%</cell><cell>0.83%</cell><cell>0.92%</cell><cell>1.11%</cell><cell>1.11%</cell><cell>1.07%</cell><cell>0.97%</cell><cell>0.88%</cell></row><row><cell>B?E</cell><cell>2.92%</cell><cell>2.86%</cell><cell>2.37%</cell><cell>2.57%</cell><cell>2.90%</cell><cell>2.90%</cell><cell>2.85%</cell><cell>2.69%</cell></row><row><cell>B?K</cell><cell>2.12%</cell><cell>1.95%</cell><cell>1.89%</cell><cell>1.90%</cell><cell>2.11%</cell><cell>2.03%</cell><cell>1.76%</cell><cell>1.91%</cell></row><row><cell>D?B</cell><cell>1.81%</cell><cell>1.71%</cell><cell>1.26%</cell><cell>1.28%</cell><cell>1.81%</cell><cell>1.70%</cell><cell>1.54%</cell><cell>1.52%</cell></row><row><cell>D?E</cell><cell>1.71%</cell><cell>1.52%</cell><cell>1.14%</cell><cell>1.71%</cell><cell>1.70%</cell><cell>1.62%</cell><cell>1.43%</cell><cell>1.55%</cell></row><row><cell>D?K</cell><cell>1.91%</cell><cell>1.62%</cell><cell>1.86%</cell><cell>1.65%</cell><cell>1.90%</cell><cell>1.85%</cell><cell>1.51%</cell><cell>1.74%</cell></row><row><cell>E?B</cell><cell>1.37%</cell><cell>1.02%</cell><cell>1.16%</cell><cell>1.12%</cell><cell>1.36%</cell><cell>1.26%</cell><cell>0.90%</cell><cell>1.24%</cell></row><row><cell>E?D</cell><cell>1.53%</cell><cell>1.23%</cell><cell>1.35%</cell><cell>1.27%</cell><cell>1.51%</cell><cell>1.43%</cell><cell>1.32%</cell><cell>1.23%</cell></row><row><cell>E?K</cell><cell>1.29%</cell><cell>0.71%</cell><cell>0.75%</cell><cell>0.85%</cell><cell>1.28%</cell><cell>1.18%</cell><cell>0.89%</cell><cell>1.05%</cell></row><row><cell>K?B</cell><cell>2.26%</cell><cell>1.92%</cell><cell>1.58%</cell><cell>2.08%</cell><cell>2.24%</cell><cell>2.16%</cell><cell>2.01%</cell><cell>2.06%</cell></row><row><cell>K?D</cell><cell>2.86%</cell><cell>2.23%</cell><cell>2.58%</cell><cell>2.41%</cell><cell>2.85%</cell><cell>2.70%</cell><cell>2.35%</cell><cell>2.62%</cell></row><row><cell>K?E</cell><cell>1.89%</cell><cell>1.46%</cell><cell>1.25%</cell><cell>1.62%</cell><cell>1.86%</cell><cell>1.85%</cell><cell>1.38%</cell><cell>1.67%</cell></row><row><cell cols="2">Average 1.90%</cell><cell>1.59%</cell><cell>1.51%</cell><cell>1.63%</cell><cell>1.88%</cell><cell>1.81%</cell><cell>1.58%</cell><cell>1.68%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>TABLE 9</head><label>9</label><figDesc>The standard deviation of target-domain accuracy on 3 real-world WUDA tasks. The source domain is the Bing dataset that contains noisy information from the Internet. Bold value represents the highest accuracy in each row.</figDesc><table><row><cell>Target</cell><cell>DAN</cell><cell cols="2">DANN ATDA</cell><cell>TCL</cell><cell>Co+TCL</cell><cell cols="2">Co+ATDA B-Net</cell></row><row><cell cols="2">Caltech256 0.65%</cell><cell>0.52%</cell><cell>0.60%</cell><cell>0.48%</cell><cell>0.61%</cell><cell>0.34%</cell><cell>0.36%</cell></row><row><cell>Imagenet</cell><cell>0.32%</cell><cell>0.24%</cell><cell>0.29%</cell><cell>0.21%</cell><cell>0.26%</cell><cell>0.34%</cell><cell>0.51%</cell></row><row><cell>SUN</cell><cell>1.61%</cell><cell>1.51%</cell><cell>1.61%</cell><cell>1.55%</cell><cell>1.59%</cell><cell>1.87%</cell><cell>1.46%</cell></row><row><cell>Average</cell><cell>0.86%</cell><cell>0.75%</cell><cell>0.83%</cell><cell>0.75%</cell><cell>0.82%</cell><cell>0.85%</cell><cell>0.78%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">This lemma is proved.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where (a) is based on the definition of ?u s and Eq. (9). Thus, we have</p><p>This lemma is proved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Lemma 2</head><p>According to definition ofR po t (h,ft, ut) in Section 6, we hav? </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Feature space independent semi-supervised domain adaptation via kernel matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="66" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-source domain adaptation: A causal view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3150" to="3157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="819" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Data-driven approach to multiple-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="3487" to="3496" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Cross language text classification via subspace co-regularized multi-view learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning with augmented features for supervised and semi-supervised heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1134" to="1148" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning with augmented features for heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<meeting><address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="711" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature space independent semi-supervised domain adaptation via kernel matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="66" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning discriminative correlation subspace for heterogeneous domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="3252" to="3258" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Asymmetric tri-training for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2988" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Domain adaptation with conditional transferable components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2839" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transferable representation learning with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2018" />
			<publisher>Early Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation across domain shifts by generating intermediate data representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2288" to="2302" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-paced collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2019" />
			<publisher>Early Access</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised heterogeneous domain adaptation via shared fuzzy relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Fuzzy Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3555" to="3568" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cluster alignment with a teacher for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9944" to="9953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Few-shot adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motiian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Iranmanesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6673" to="6683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual recognition in rgb images and videos by learning from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2030" to="2036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="3723" to="3732" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A DIRT-T approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4893" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">GCAN: graph convolutional adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8266" to="8276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Task refinement learning for improved accuracy and stability of unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ziser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Reichart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5895" to="5906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Domain adaptation of deformable part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2367" to="2380" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dlow: Domain flow for adaptation and generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2477" to="2486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for face anti-spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1794" to="1809" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scatter component analysis : A unified framework for domain adaptation and domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1414" to="1430" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Minimax statistical learning with wasserstein distances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raginsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2692" to="2701" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Transfer learning with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/1707.09724</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Causal generative domain adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<idno>abs/1804.04333</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="1994" to="2003" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep domain generalization via conditional invariant adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="647" to="663" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Collaborative and adversarial network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3801" to="3809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Transferable curriculum for weakly-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4951" to="4958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards ultrahigh dimensional feature selection for big data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1371" to="1429" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Harvesting image databases from the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="754" to="766" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A testbed for cross-dataset analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV TASK-CV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="18" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Transfer learning with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09724</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep kernels for non-parametric two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6316" to="6326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transfer joint matching for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1410" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Classification with noisy labels by importance reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="447" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICML</title>
		<imprint>
			<biblScope unit="page" from="2309" to="2318" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="8527" to="8537" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Decoupling &quot;when to update&quot; from &quot;how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="961" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2233" to="2241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A closer look at memorization in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Evolving culture versus local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Growing Adaptive Machines</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
		<respStmt>
			<orgName>COLT</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">A vector-contraction inequality for rademacher complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ALT</publisher>
			<biblScope unit="page" from="3" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Multiclass learning: From theory to algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="1593" to="1602" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-class learning using unlabeled samples: Theory and algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2880" to="2886" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7404" to="7413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Foundations of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Positiveunlabeled learning with non-negative risk estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiryo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Du Plessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Exploiting weakly-labeled web images to improve object classification: a domain adaptation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="181" to="189" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">SUN database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Visual domain adaptation with manifold embedded distribution alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<editor>ACM MM, S. Boll, K. M. Lee, J. Luo, W. Zhu, H. Byun, C. W. Chen, R. Lienhart, and T. Mei</editor>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="402" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">His research interests include domain adaptation and twosample test. He has served as a senior program committee member for ECAI and program committee members for NeurIPS</title>
	</analytic>
	<monogr>
		<title level="m">ICML, IJCAI, CIKM, ECAI, FUZZ-IEEE and ISKE. He also served as reviewers for TPAMI, TNNLS, TFS and TCYB. He has received the UTS-FEIT HDR Research Excellence Award</title>
		<meeting><address><addrLine>Lanzhou University, China</addrLine></address></meeting>
		<imprint>
			<publisher>UTS Research Publication Award</publisher>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>Faculty of Engineering and Information Technology, University of Technology Sydney, Australia</orgName>
		</respStmt>
	</monogr>
	<note>He received an M.Sc. degree in probability and statistics and a B.Sc. degree in pure mathematics from the School of Mathematics and Statistics. Best Student Paper Award of FUZZ-IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
