<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date type="published" when="2021-10-20">2021. October 20-24, 2021. October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<email>wuwenhao17@mails.ucas.edu.cn.</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingde</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Shenzhen Institute of Advanced Technology</orgName>
								<orgName type="institution" key="instit2">CAS</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwu</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Zou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingde</forename><surname>Yao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">ACM Reference Format</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DSANet: Dynamic Segment Aggregation Network for Video-Level Representation Learning</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21)</title>
						<meeting>the 29th ACM International Conference on Multimedia (MM &apos;21) <address><addrLine>China</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published" when="2021-10-20">2021. October 20-24, 2021. October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475344</idno>
					<note>ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3474085.3475344 * Equal contribution. ? Corresponding author (</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neural networks</term>
					<term>action recognition</term>
					<term>video representation learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Long-range and short-range temporal modeling are two complementary and crucial aspects of video recognition. Most of the stateof-the-arts focus on short-range spatio-temporal modeling and then average multiple snippet-level predictions to yield the final videolevel prediction. Thus, their video-level prediction does not consider spatio-temporal features of how video evolves along the temporal dimension. In this paper, we introduce a novel Dynamic Segment Aggregation (DSA) module to capture relationship among snippets. To be more specific, we attempt to generate a dynamic kernel for a convolutional operation to aggregate long-range temporal information among adjacent snippets adaptively. The DSA module is an efficient plug-and-play module and can be combined with the off-the-shelf clip-based models (i.e., TSM, I3D) to perform powerful long-range modeling with minimal overhead. The final video architecture, coined as DSANet. We conduct extensive experiments on several video recognition benchmarks (i.e., Mini-Kinetics-200, Kinetics-400, Something-Something V1 and ActivityNet) to show its superiority. Our proposed DSA module is shown to benefit various video recognition models significantly. For example, equipped with DSA modules, the top-1 accuracy of I3D ResNet-50 is improved from 74.9% to 78.2% on Kinetics-400. Codes are available at https://github.com/whwu95/DSANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>? Computing methodologies ? Activity recognition and understanding. <ref type="figure">Figure 1</ref>: The pipeline of our method for video recognition. Input video is divided into several segments and a short snippet is randomly sampled from each segment. Green trapezoids denote the snippet-level spatio-temporal networks (e.g., I3D, TSM) which share parameters on all snippets. DSA module can be easily inserted into the networks to capture long-range temporal structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the rapid development of the Internet and mobile devices, video data has exploded over the past years. Video action recognition, as a fundamental problem in video analytics, has become one of the most active research topics. Previous methods have obtained noticeable improvements for video recognition. 3D CNNs are intuitive spatio-temporal networks and natural extension from their 2D counterparts, which directly tackle 3D volumetric video data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. However, the heavy computational cost of 3D CNNs limits its application. Recent studies have shown that factorizing 3D convolutional kernel into spatial (e.g., 1?3?3) and temporal components (e.g., 3?1?1) is preferable to reduce complexity as well as boost accuracy, e.g., P3D <ref type="bibr" target="#b21">[22]</ref>, R(2+1)D <ref type="bibr" target="#b27">[28]</ref>, S3D-G <ref type="bibr" target="#b36">[37]</ref>, etc. In practice, however, compared with their 2D counterparts, the increased computational cost cannot be ignored. Thus, 3D channel-wise convolution are also applied to video recognition in CSN <ref type="bibr" target="#b26">[27]</ref> and X3D <ref type="bibr" target="#b6">[7]</ref>. To capture temporal information with reasonable training resources, the other alternative 2D CNN based architectures are developed i.e., TSM <ref type="bibr" target="#b16">[17]</ref>, TEI <ref type="bibr" target="#b18">[19]</ref>, TEA <ref type="bibr" target="#b15">[16]</ref>, MVFNet <ref type="bibr" target="#b33">[34]</ref>, etc. There methods aim to design efficient temporal module based on existing 2D CNNs to perform efficient temporal modeling. Temporal Shift Module (TSM) <ref type="bibr" target="#b16">[17]</ref> is the representative 2D-based model, which achieves a good trade-off between performance and complexity meanwhile introduces zero FLOPs for the 2D CNN backbone. However, the above architectures, both based on 2D CNNs and 3D CNNs, are mainly designed to learn short-range spatio-temporal representation (snippet-level) instead of long-range representation (video-level). Specifically, during training, these snippet-level methods sample sub-frames (4 frames, 8 frames or 16 frames) from a random snippet of the original video. While for inference, they uniformly sample multiple short snippets from a video and compute the classification scores for all snippets individually and the final prediction is just the average voting from the snippet-level predictions. Thus, during training, these clip-based models take no account of modeling long-range temporal structure due to the limited observation of only a single snippet. Also, simply averaging the prediction scores of all snippets could yield a sub-optimal solution during inference.</p><p>To alleviate the problem mentioned above, Temporal Segment Network <ref type="bibr" target="#b29">[30]</ref> provides a simple framework to perform video-level learning, which just simply fuses snippet-level predictions. Lately, V4D <ref type="bibr" target="#b38">[39]</ref> presents an intuitive 4D convolutional operation to capture inter-clip interaction, which is a natural extension of enhancing the representation of the original clip-based 3D CNNs. However, TSN fails on capturing the finer temporal structure, (e.g., the temporal pattern of the ordering of features), and V4D causes high computational cost compared with the TSN-equipped counterpart and might lack the ability to describe the dynamics by simply employing a fixed number of video invariant kernels. This inspires us to focus on designing a temporal module, which keeps both advantages of high efficiency and strong ability of capturing long-range temporal structure.</p><p>In this paper, we creatively introduce a module to adaptively aggregate the features of multiple snippets, namely Dynamic Segment Aggregation Module (DSA Module). <ref type="figure">Figure 1</ref> illustrates the pipeline of our solution, DSA module is a novel generic module that can be easily plugged into the existing clip-based networks (e.g., TSM <ref type="bibr" target="#b16">[17]</ref>, I3D <ref type="bibr" target="#b2">[3]</ref>) to perform long-range modeling with the ignorable cost. Briefly, the DSA module firstly gets global context information from features of all snippets, then it utilizes the contextual view to learns the weight of aggregation in a channel-wise convolution manner. Thus, DSA module can capture relation-ship among snippets via dynamic fusion along snippet dimension. To make this module more efficient, we separate the features across the channel dimension. One keeps the original activation and the other performs dynamic segment aggregation separately followed by a concatenation operation. In this way, DSA only introduces a bit more FLOPs which is close to zero, while bringing remarkable improvement. In practice, we integrate the DSA module into the existing spatio-temporal block (e.g., TSM block, I3D block) to form our DSANet.</p><p>We identify the effectiveness of the proposed DSA module via comprehensive ablations. Extensive experimental results on multiple well-known datasets, including Mini-Kinetics-200 <ref type="bibr" target="#b36">[37]</ref>, Kinetics-400 <ref type="bibr" target="#b13">[14]</ref>, Something-Something V1 <ref type="bibr" target="#b9">[10]</ref> and ActivityNet-v1.3 <ref type="bibr" target="#b1">[2]</ref> show the superiority of our solution. All of these prove DSA boosts the performance over its snippet-level counterpart. Overall, our major contributions are summarized as follows:</p><p>? Instead of snippet-level temporal modeling, we propose to exploit an effective and efficient video-level framework for learning video representation. To tackle this, the proposed DSA module provides a novel mechanism to adaptively aggregate snippet-level features. ? The DSA module works in a plug-and-play way and can be easily integrated into existing snippet-level methods. Without any bells and whistles, the DSA module brings consistent improvements when combined with both 2D CNN-based and 3D CNN-based networks (e.g., TSM, I3D, etc). ? Extensive experiments on four public benchmark datasets demonstrate that the proposed DSA obtain an evident improvement over previous long-range temporal modeling methods with a minimal computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 CNNs for Video Recognition.</head><p>Using cutting edge 2D CNN architecture is desirable to address the video recognition task. Two-stream <ref type="bibr" target="#b23">[24]</ref> proposes to jointly learn spatial and temporal context through two parallel branch structure. 3D CNN expands a new dimension of 2D convolution whereby intuitively apply to assemble spatio-temporal feature. C3D <ref type="bibr" target="#b25">[26]</ref> is the first one to using VGG of 3D type in video recognition. I3D <ref type="bibr" target="#b2">[3]</ref> is a milestone of 3D convolution which used pre-trained parameters of 2D CNNs onto the 3D CNNs by inflating the parameter on a new axis, boost the performance by a large margin. Along with significant study on separable convolution, some methods (e.g., S3D-G <ref type="bibr" target="#b36">[37]</ref>, R(2+1)D <ref type="bibr" target="#b27">[28]</ref>, P3D <ref type="bibr" target="#b21">[22]</ref>) decompose the full 3D convolution into a 2D convolution and a 1D temporal convolution to reduce the burdensome parameters and accelerate the training.</p><p>To alleviate the high computational cost of video classification networks, the other alternative 2D CNN based architectures (e.g., STM <ref type="bibr" target="#b12">[13]</ref>, TSM <ref type="bibr" target="#b16">[17]</ref>, ECO <ref type="bibr" target="#b40">[41]</ref>, TEI <ref type="bibr" target="#b18">[19]</ref>, TEA <ref type="bibr" target="#b15">[16]</ref>, GSM <ref type="bibr" target="#b24">[25]</ref>, MVFNet <ref type="bibr" target="#b33">[34]</ref>) and channel-wise 3D CNN based architectures (e.g., CSN <ref type="bibr" target="#b26">[27]</ref>, X3D <ref type="bibr" target="#b6">[7]</ref>) are developed. Also, there is active research on dynamic inference <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref>. These methods can get good computation/accuracy trade-offs.</p><p>Besides, recently we notice that the concurrent work to ours is TANet <ref type="bibr" target="#b19">[20]</ref>, which composed of two branches, the local branch aims to learn a location sensitive importance map to enhance discriminative features and the global branch produces the location invariant weights to aggregate temporal information. Both the global branch of TANet and our DSANet are inspired by the previous dynamic convolution mechanisms <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref>. However, the TANet aims to capture clip-level temporal information and our DSANet targets on video-level temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Long-Term Modeling for Video Recognition</head><p>The obvious difference between long-term modeling method and short-term modeling method is whether the method can capture the relation of different video segments for video-level representation learning. Various long-term modeling frameworks have been developed for capturing more complex temporal structure in video action recognition fields. In <ref type="bibr" target="#b5">[6]</ref>, CNN features of video frames are fed into the LSTM network for video recognition. ShuttleNet <ref type="bibr" target="#b22">[23]</ref> introduces biologically-inspired feedback connections to model long-term dependencies of spatial CNN descriptors. NetVlad <ref type="bibr" target="#b0">[1]</ref>, ActionVlad <ref type="bibr" target="#b8">[9]</ref>, AttentionClusters <ref type="bibr" target="#b20">[21]</ref> are proposed for better local feature integration instead of directly average pooling as used.</p><p>MARL <ref type="bibr" target="#b34">[35]</ref> uses multiple agents as frame selectors instead of the general uniform sampler from the entire video for better global temporal modeling. Each agent learns a flexibly moving policy through the temporal axis to get a vital representation frame and other agents' behavior as well. <ref type="bibr" target="#b14">[15]</ref> proposes a new solution as dynamic sampler. Ranking the high score clips generated by a small classifier network, a top-k pooling method is then applied to feed the most related parts into a large network.</p><p>Closely related works are TSN <ref type="bibr" target="#b29">[30]</ref>, T-C3D <ref type="bibr" target="#b17">[18]</ref>, StNet <ref type="bibr" target="#b10">[11]</ref> and V4D <ref type="bibr" target="#b38">[39]</ref>. <ref type="bibr" target="#b29">[30]</ref> operates on a sequence of short snippets sparsely sampled from the entire video and the parameter is updated according to segmental consensus derived from all snippet-level prediction. T-C3D replaces 2D backbones with C3D. So these models only capture coarse co-occurrence rather than the finer temporal structures. StNet applies 2D ConvBlocks on "superimage" for local spatial-temporal modeling and 1D temporal convolutions for global temporal dependency. V4D <ref type="bibr" target="#b38">[39]</ref> tends to focus on the intuitive 4D convolutional operation to capture inter-clip interaction, which is a natural extension to enhance the representation power of the original snippet-level 3D CNNs, however, causes high computational cost. Therefore we present a flexible long-range modeling module that shares the capacity of learning video-level representations yet still keeps the efficiency of the snippet-level counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DYNAMIC SEGMENT AGGREGATION 3.1 Preliminary: 4D Convolution</head><p>To aggregate the snippet-level features, V4D cast 3D convolution into 4D convolution. Formally, following the notations in V4D <ref type="bibr" target="#b38">[39]</ref>, the whole video formed by a series of short snippets of ? R ? ? ? , where is the number of the input channel, ? denotes the number of output channel, is the dimension of sampled frames from each snippet, and , represent height and width of the frames respectively. Further, video can then be represented as a 5-D tensor := { 1 , 2 , . . . , }, where is the number of snippets as a new dimension. Then, the 4D convolutional operation can be formulated as:</p><formula xml:id="formula_0">? , , ,?, = ?? , , , , ? , , , , , , +?, +?,?+?, +?,<label>(1)</label></formula><p>where is the output tensor, is the 4D convolution kernel, , , , index along the , , , dimensions of kernel and ? represents the number of kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic Kernel Generation</head><p>Due to the flowing dynamics of videos, we learn an adaptive, context-aware kernel to handle video variations along the temporal dimension. To be simplified, we denote the input to the DSA module as a tensor ? R ? ? ? ? . The overall DSA module is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>(c). We can formalize this dynamic kernel generation as follows:</p><formula xml:id="formula_1">= (F ( , ( ))),<label>(2)</label></formula><p>where aggregates video feature maps for global view by using global average pooling, which produces?? R ? . F is a mapping function that could generate kernel with learnable parameters of </p><formula xml:id="formula_2">? ? ? ? ? ? ?3 4?56 2 res 3 1?3 2 , 128 1?3 2 , 128 ?2 ? ? ? ? ? ? 1?1 2 , 128 1?3 2 , 128 1?1 2 , 512 ? ? ? ? ? ? ?4 4?28 2 res 4 3?3 2 , 256 3?3 2 , 256 ?2 ? ? ? ? ? ? 3?1 2 , 256 1?3 2 , 256 1?1 2 , 1024 ? ? ? ? ? ? ?6 4?14 2 res 5 3?3 2 , 512 3?3 2 , 512 ?2 ? ? ? ? ? ? 3?1 2 , 512 1?3 2 , 512 1?1 2 , 2048 ? ? ? ? ? ? ?3 4?7 2 global average pool, fc # classes</formula><p>. And ? R ? is generated dynamic convolutional kernel for each channel, where is the kernel size. In this paper, we set to 3. To capture context information effectively, we use multi-layer perceptron (MLP) network with batch normalization and activation function (e.g., ReLU) to instantiate F in this paper. is a superparameters of fully connected layer in MLP to control the size of feature. To conduct better capability, we constraint with a Softmax function to generate normalized kernel weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DSA as a Specific Case of 4D Convolution</head><p>In this section, we formulate our proposed DSA as a specific case of 4D convolution. While capturing the long-term perception structure, the learned kernel above can be viewed as aggregation weights to perform dynamic weighted fusion along dimensions via channel-wise convolutional operation , , ,?, =</p><formula xml:id="formula_3">?? , , + , ,?, .<label>(3)</label></formula><p>At this point, we can see that DSA has a reduced kernel size along , , dimensions, which is also a 4D convolutional kernel.</p><p>To make this module more efficient, we decompose the feature maps into two parts as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c). Let 1 ? R ? ? ? ? and 2 ? R (1? ) ? ? ? ? be the two splits of along the channel dimension, where ? [0, 1] denoted the proportion of input channels for dynamic aggregation. Finally, we get the DSA module output ? R ? ? ? ? as:</p><formula xml:id="formula_4">, , ,?, = Concat ( ?? , 1 , + , ,?, , 2 ),<label>(4)</label></formula><p>? ? ? ?</p><p>(a) DSA Block (for TSM) where Concat represents concatenation operation along dimension of channel.</p><formula xml:id="formula_5">(b) DSA Block (for I3D) (c) DSA Module (d) DSA Network</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DSA Network Architecture</head><p>The proposed DSA module is a flexible module that can be easily plugged into the existing clip-based networks to capture long-range temporal structure. Here we illustrate how to adapt it into a residual style block, and then build up the Dynamic Segment Aggregation Network (DSANet). To evaluate the generality of the DSA module, we build up the DSANet on both 2D and 3D backbone networks (i.e., TSM, I3D). TSM <ref type="bibr" target="#b16">[17]</ref> and I3D <ref type="bibr" target="#b7">[8]</ref>, the two widely used networks, are respectively the representatives of the 2D CNN-based model and 3D CNN-based model. To avoid confusion, as shown in <ref type="table" target="#tab_0">Table 1</ref>, I3D refer to the slow-only branch of SlowFast <ref type="bibr" target="#b7">[8]</ref> which has no temporal downsampling operations. For a fair comparison with many recent state-of-the-art video recognition methods, we follow the same practices and instantiate the DSANet using ResNet-style backbone in this instantiation. Specifically, we integrate the proposed module into the I3D ResNet block or TSM ResNet block to construct our DSA block. The overall design of the DSA block is shown in <ref type="figure" target="#fig_0">Figure 2</ref>(a) and <ref type="figure" target="#fig_0">Figure 2(b)</ref>. And there are four optional positions (denoted as I, II, III, IV) to insert DSA module and we will perform ablation studies in Sec. 4.3. Then following <ref type="figure" target="#fig_0">Figure 2(d)</ref>, we replace some residual blocks with DSA blocks to form our DSANet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>We have noticed that our DSA module gets global context information from features of all snippets by global average pooling, which is same with the squeeze operation in SENet <ref type="bibr" target="#b11">[12]</ref>. However, there are essential differences between SENet and DSA as follows:</p><p>1) We propose to aggregate the snippet-level features by utilizing an efficient convolutional operation.</p><p>2) In corresponding to the dynamics of video data, we further build a method to learn an adaptive convolutional kernel with ignorable computation cost for the convolutional operation. Essentially, our method is different from SENet, which applies a self-attention mechanism to generate a channel-wise attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and Evaluation Metrics</head><p>We evaluate our method on several standard video recognition benchmarks, including Mini-Kinetics-200 <ref type="bibr" target="#b36">[37]</ref>, Kinetics-400 <ref type="bibr" target="#b13">[14]</ref>, Something-Something V1 <ref type="bibr" target="#b9">[10]</ref> and ActivityNet-v1.3 <ref type="bibr" target="#b1">[2]</ref>. Kinetics-400 contains 400 human action categories and provides around 240k training video clips and 20k validation video clips. Each example is temporally trimmed to be around 10 seconds. Mini-Kinetics-200 consists of the 200 categories with the most training examples and is a subset of the Kinetics-400 dataset. Since the full Kinetics-400 dataset is quite large, we use Mini-Kinetics-200 for our ablation experiments. For Something-Something V1 datasets, the actions therein mainly include object-object and human-object interactions, which require strong temporal relation to well categorizing them. Something-Something V1 includes about 110k videos for 174 fine-grained classes. To elaborately study the effectiveness and generalization ability of our solution, we also conduct experiments for untrimmed video recognition. We choose ActivityNet-v1.3 which is a large-scale untrimmed video benchmark, contains 19,994 untrimmed videos of 5 to 10 minutes from 200 activity categories.</p><p>We report top-1 and top-5 accuracy (%) for Mini-Kinetics-200, Kinetics-400 and Something-Something V1. For ActivityNet-v1.3, we follow the original official evaluation scheme using mean average precision (mAP). Also, we report the computational cost (in FLOPs) as well as the number of model parameters to depict model complexity. In this paper, we only use the RGB frames of these datasets and don't use extra modalities (e.g., optical flow, audio).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Here we report the implementation details.</p><p>Training. We evaluate the DSA module on both 2D and 3D backbone networks. Specifically, the slow-only branch of SlowFast <ref type="bibr" target="#b7">[8]</ref> is applied as our 3D backbone baseline (denoted as I3D) and TSM <ref type="bibr" target="#b16">[17]</ref> is applied as our 2D backbone baseline. We use random initialization for our DSA module and train the model in an end-to-end manner. Following the similar practice in V4D <ref type="bibr" target="#b38">[39]</ref> and TSN <ref type="bibr" target="#b29">[30]</ref>, we uniformly divide the whole video into segments and we set = 4 in this paper, then randomly select a snippet of consecutive 32 or 64 frames from each segment as temporal augmentation. For each snippet, we uniformly sample 4 or 8 frames with a fixed stride of 8. The size of the short side of these frames is fixed to 256 and then random scaling is utilized for data augmentation. Finally, we resize the cropped regions to 224?224 for network training. On the Mini-Kinetics-200 dataset, the learning rate is 0.01 and will be reduced by a factor of 10 at 40 and 60 epochs (70 epochs in total) respectively. On the Kinetics-400 dataset, our model is trained for 100 epochs starting with a learning rate of 0.01 and reducing it by a factor of 10 at 50, 75, and 90 epochs. For Something-Something V1 dataset, our model is trained for 50 epochs starting with a learning rate 0.01 and reducing it by a factor of 10 at 30, 40 and 45 epochs. For these three datasets, our models are initialized by pre-trained models on ImageNet <ref type="bibr" target="#b4">[5]</ref>. As for ActivityNet-v1.3, we followed the common practice to fine-tune from Kinetics-400 pre-trained weights and start training with a learning rate of 0.01 for 200 epochs. The learning rate is decayed by a factor of 10 at 80 and 160 epochs.</p><p>Inference. We follow the common setting in <ref type="bibr" target="#b7">[8]</ref> to just uniformly sample multiple clips from a video along its temporal axis. We sample 8 clips unless otherwise specified. For each clip, we resize the short side to 256 pixels and take 3 spatial crops in each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>This section provides ablation studies on Mini-Kinetics-200 to investigate different aspects of the DSA module as shown in <ref type="table" target="#tab_1">Table 2</ref>. Accordingly, concrete analysis is as follows.</p><p>Effectiveness of DSA. We first investigate the performance of our DSA module with a toy backbone I3D ResNet-18 (R18). Specifically, we follow the Slow path in SlowFast <ref type="bibr" target="#b7">[8]</ref> to build the I3D R18 backbone which is the same as the structure in V4D <ref type="bibr" target="#b38">[39]</ref>. As it can be seen in <ref type="table" target="#tab_1">Table 2a</ref>, long-term modeling is complementary to clip-based method I3D R18. Specifically, when equipped with TSN, V4D, and our DSA, I3D R18 obtains 0.8%, 3.4%, and 5.1% gain respectively. In comparison to these video-level methods, DSA significantly outperforms TSN and V4D by 4.3% and 1.7% accuracy with the same training protocols and even using less number of snippets for inference. In the following, unless stated otherwise, we apply I3D ResNet-50 (R50) as the backbone.</p><p>Inserted Position. As in <ref type="figure" target="#fig_0">Figure 2(b)</ref>, there are four optional positions to integrate our DSA module into the standard I3D ResNet block. <ref type="table" target="#tab_1">Table 2b</ref> shows the results of DSA module in different position. We find that position II gets slightly higher than position I and IV. Hence, the DSA module is inserted before the second convolution of the I3D ResNet Block by default in the following ablations. Parameter choices of . We experiment with different to figure out the optimal super-parameters of fully connected layers in the DSA module as shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c). And in <ref type="table" target="#tab_1">Table 2c</ref>, our module with = 2 get better performance thus we choose it in our experiments.</p><p>Which stage to insert DSA blocks. We denote the conv2_x to conv5_x of ResNet architecture as res{2} to res{5}. To evaluate which stage may be important for DSA design, we insert DSA blocks into a different stage of I3D R50 and only replace one with DSA block for every two I3D ResNet blocks. <ref type="table" target="#tab_1">Table 2d</ref> shows that the performance of DSA blocks on res{2}, res{3}, res{4} or res{5} is similar, which suggests that DSA can perform effective adaptive fusion on both low-level features and high-level features of snippets.</p><p>Parameter choices of . As shown in <ref type="table" target="#tab_1">Table 2e</ref>, we compare networks with different proportion of input channels ( = 1/8, 1/4, 1/2, 1) for long-term temporal modeling. Here we add DSA blocks into res{5} and we observe that the change in appeared to have little impact on performance thus we adopt = 1/8 for efficiency in the following.</p><p>The number of DSA Blocks. We here expect to figure out if more DSA blocks can further improve performance. <ref type="table" target="#tab_1">Table 2f</ref> compares DSA blocks added to multiple stages of I3D ResNet. The res{3,4} achieves the highest performance and will be used in the following experiments.</p><p>Generality of DSA. Being a general module, DSA could be combined with both 2D and 3D clip-based networks. To show this, we add DSA to TSM (DSA+TSM R50) which is the recent state-ofthe-art model based on 2D CNN and train such a combination in an end-to-end manner. For the 3D backbone, the slow-only branch of SlowFast is applied as our backbone network (denoted as I3D R50) due to its promising performance on various datasets. As shown in <ref type="table" target="#tab_1">Table 2g</ref>, adding DSA to TSM R50 could boost the top-1 accuracy by 3.0%. A consistent improvement is observed as in I3D R50, which has an increase of 3.8%. Different Backbone. In <ref type="table" target="#tab_1">Table 2h</ref> we compare various instantiations of DSA networks on Mini-Kinetics-200. As expected, using deeper backbones is complementary to our method. Comparing with the I3D R18 counterparts, our DSA gets additional improvement on I3D R50.</p><p>Comparison with SENet. To avoid misunderstanding, here we clarify the essential difference between SENet and DSANet is that SENet is a kind of self-attention mechanism by using its global feature to calibrate the features in channel-level, while our DSA generates dynamic convolution kernel to adaptively aggregate the snippet-level features with a global view. To prove the effectiveness of DSA, we conduct the comparative experiments in <ref type="table" target="#tab_1">Table 2h</ref>. Under the same setting, our DSA outperforms the SE module with a clear margin (77.3% vs. 73.8%, 81.8% vs. 78.5%) on I3D R18 and I3D R50.</p><p>Training Cost. Here we make a comparison with the state-ofthe-art video-level method V4D+I3D under the same setting of backbone and inputs. We list the number of FLOPs for all models in <ref type="table" target="#tab_1">Table 2i</ref>, our DSA+I3D is more lightweight (83.8G vs. 143.0G) than V4D+I3D <ref type="bibr" target="#b38">[39]</ref> and achieve a similar cost with the baseline TSN+I3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-arts</head><p>In this section, we compare our approach with the state-of-theart methods on trimmed video datasets (i.e., Mini-Kinetics-200 <ref type="bibr" target="#b36">[37]</ref>, Kinetics-400 <ref type="bibr" target="#b13">[14]</ref> and Something-Something V1 <ref type="bibr" target="#b9">[10]</ref>) and untrimmed video dataset (i.e., ActivityNet <ref type="bibr" target="#b1">[2]</ref>).</p><p>Results on Mini-Kinetics-200. We evaluate the DSA-equipped networks on Mini-Kinetics-200. As shown in <ref type="table" target="#tab_2">Table 3</ref>, DSA with less input already outperforms the previous methods. Notably, DSA-R18 (77.3%) has similar accuracy with I3D R101 (77.4%) and I3D R50-NL (77.5%). Compared with V4D, DSA achieves better accuracy on R18 (77.3% vs. 75.6%) and R50 (81.8% vs. 80.7%) with fewer clips for prediction (8 vs. 10) which shows the superiority of our method.</p><p>Results on Kinetics-400. We make a thorough comparison in <ref type="table">Table 4</ref>, where our DSA outperforms the recent SOTA approaches on Kinetics-400. Here we only list the models using RGB as inputs for fair comparisons. We provide results of our DSA network trained with two different inputs (i.e., 4 frames ( = 4) or 8 frames ( = 8) sampled from each snippet). The upper part of the <ref type="table">Table 4</ref> shows the results of clip-based models. Compared with the 2D CNN-based model, during inference, when utilizing less frames as input (4?8 vs. 8?10), our DSA still outperforms these current stateof-the-art efficient methods (e.g., TSM, TEINet, TEA and MVFNet) <ref type="table">Table 4</ref>: Comparison with the state-of-the-art models on Kinetics-400 dataset. We report the inference cost by computing the GFLOPs. * indicates the result of our calculation using the official model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone ? ? #crop GFLOPs Top-1 Top-5 TSM <ref type="bibr" target="#b16">[17]</ref> ResNet-50 8?10?3 33?30=990 74.1% 91.2% TEINet <ref type="bibr" target="#b18">[19]</ref> ResNet-50 8?10?3 33?30=990 74.9% 91.8% TEA <ref type="bibr" target="#b15">[16]</ref> ResNet-50 8?10?3 35?30=1050 75.0% 91.8% TANet <ref type="bibr" target="#b19">[20]</ref> ResNet-50 8?10?3 43?30=1290 76.1% 92.3% MVFNet <ref type="bibr" target="#b33">[34]</ref> ResNet-50 8?10?3 33?30=990 76.0% 92.4% NL+I3D <ref type="bibr" target="#b30">[31]</ref> 3D when using much less GFLOPs (4.3?). Furthermore, our DSA has better results with longer inputs, and exceed V4D by 0.8%. For readers' reference, here we also report the results of ensemble the two models. As shown in <ref type="figure" target="#fig_1">Figure 3</ref>, comparison with prior works (Slow-Fast <ref type="bibr" target="#b7">[8]</ref>, X3D <ref type="bibr" target="#b6">[7]</ref>, TSM <ref type="bibr" target="#b16">[17]</ref>, TEA <ref type="bibr" target="#b15">[16]</ref>, MVFNet <ref type="bibr" target="#b33">[34]</ref>, V4D <ref type="bibr" target="#b38">[39]</ref>), our DSANet achieves state-of-the-art performance on Kinetics-400 and get better accuracy-computation trade-off than other methods. Model Backbone mAP TSN <ref type="bibr" target="#b29">[30]</ref> BN-Inception 79.7% TSN <ref type="bibr" target="#b29">[30]</ref> Inception V3 83.3% TSN-Top3 <ref type="bibr" target="#b29">[30]</ref> Inception V3 84.5% V4D+I3D <ref type="bibr" target="#b38">[39]</ref> 3D ResNet50 88.9% DSA+I3D (Ours) 3D ResNet50 90.5% Results on ActivityNet-v1.3. To verify the generalization ability of our DSANet, we further evaluate the performance of our method on a large-scale untrimmed video recognition benchmark, ActivityNet <ref type="bibr" target="#b1">[2]</ref>. We finetuned the model pre-trained on Kinetics-400 on the Activitynet-v1.3 dataset and report the mean average precision (mAP) follow the official evaluation metrics. As shown in <ref type="table" target="#tab_4">Table 5</ref>, our method outperforms the previous methods (i.e., V4D, TSN-Top3, TSN) with a clear margin (90.5% vs. 88.9%, 84.5%, 83.3%).</p><p>Results on Something-Something V1. We also evaluate our method on Something-Something V1 dataset which focuses on  <ref type="table">Table 6</ref>: Comparison with the state-of-the-art models on Something-Something V1 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone Top-1 MultiScale TRN <ref type="bibr" target="#b39">[40]</ref> BN-Inception 34.4% ECO <ref type="bibr" target="#b40">[41]</ref> BN-Inception+3D ResNet 18 46.4% S3D-G <ref type="bibr" target="#b36">[37]</ref> S3D Inception 45.8% Nonlocal+GCN <ref type="bibr" target="#b31">[32]</ref> 3D ResNet50 46.1% TSM <ref type="bibr" target="#b16">[17]</ref> ResNet50 47.2% I3D (our impl.) 3D ResNet50 48.7% V4D+I3D <ref type="bibr" target="#b38">[39]</ref> 3D ResNet50 50.4% DSA+I3D (Ours) 3D ResNet50 51.8%</p><p>modeling temporal relation and is more complicated than Kinetics dataset. As shown in <ref type="table">Table 6</ref>, comparing with prior works, our DSA obtains comparable or better performance. To verify the effectiveness of DSA, we report the baseline result of I3D ResNet-50 in our implementation for fair comparison. Remarkably, equipped with the DSA module, the top-1 accuracy of I3D ResNet-50 improves from 48.7% to 51.8% on Something-Something V1. In this way, we believe our DSA can achieve higher accuracy when using better snippet-level counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">VISUALIZATION</head><p>We also visualize some examples of predictions generated by DSANet and I3D model on the validation data of Kinetics-400 in <ref type="figure" target="#fig_2">Figure 4</ref>. Compared with I3D which simply averaging the prediction scores of all snippets to yield a video-level prediction, we see that our DSANet can learn global context information and adaptively capture relation-ship among snippets via dynamic aggregation, then provide more accurate results with contextual perspective.</p><p>In this paper, we presented the Dynamic Segment Aggregation (DSA) module to better exploit long-range spatiotemporal representation in videos. The DSA module can be easily applied to the existing snippet-level methods in a plug-and-play manner with a negligible computational cost, bringing consistent improvements on both 2D/3D CNN backbone. A series of empirical studies verify that our DSA network is an accurate and efficient model for large scale video classification. The experimental results show that our method has achieved state-of-the-art performance on four public benchmarks with a high inference efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our framework for video action recognition. (a) shows the TSM block equipped with a DSA module. (b) shows the I3D block equipped with a DSA module. (c) depicts the architecture of the DSA module. (d) shows our DSA block which integrates the DSA module into the ResNet-Style block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy-computation trade-off on Kinetics-400 for different methods in the inference phase. The horizontal axis shows the inference cost per video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of predictions with DSANet and I3D. (Green: correct predictions, Red: wrong predictions.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Two backbone instantiations of the I3D network. The dimensions of kernels are denoted by { ? 2 , } for temporal, spatial, and channel sizes. Strides are denoted as {temporal stride, spatial stride 2 }.</figDesc><table><row><cell>stage</cell><cell cols="2">I3D ResNet18</cell><cell></cell><cell>I3D ResNet50</cell><cell>output sizes</cell></row><row><cell>raw clip</cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell><cell>32?224 2</cell></row><row><cell>data layer</cell><cell cols="2">stride 8, 1 2</cell><cell></cell><cell>stride 8, 1 2</cell><cell>4?224 2</cell></row><row><cell>conv 1</cell><cell cols="2">1?7 2 , 64 stride 1, 2 2</cell><cell></cell><cell>1?7 2 , 64 stride 1, 2 2</cell><cell>4?112 2</cell></row><row><cell>pool 1</cell><cell cols="2">1?3 2 max stride 1, 2 2</cell><cell></cell><cell>1?3 2 max stride 1, 2 2</cell><cell>4?56 2</cell></row><row><cell>res 2</cell><cell>1?3 2 , 64 1?3 2 , 64</cell><cell>?2</cell><cell>? ? ? ? ? ?</cell><cell>1?1 2 , 64 1?3 2 , 64 1?1 2 , 256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on Mini-Kinetics-200. We show top-1 and top-5 classification accuracy (%), as well as computational complexity measured in FLOPs (floating-point operations).(a) Study on the effectiveness of DSA module. denotes the number of frames sampled from each video snippet, denotes the number of snippets. Backbone: I3D R18.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) Study on different position to</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">insert DSA module. Setting: I3D</cell></row><row><cell></cell><cell>Model</cell><cell>?</cell><cell></cell><cell>?</cell><cell cols="4">?#crop Top-1 Top-5 Params</cell><cell></cell><cell cols="2">R50, =2, =1, stage: res 5 .</cell></row><row><cell></cell><cell>I3D R18</cell><cell cols="2">4 ? 1</cell><cell cols="2">4 ? 10 ? 3</cell><cell>72.2</cell><cell>91.2</cell><cell>32.3M</cell><cell></cell><cell cols="2">Position Top-1 Top-5</cell></row><row><cell></cell><cell>I3D R18</cell><cell cols="2">16 ? 1</cell><cell cols="2">16 ? 10 ? 3</cell><cell>73.4</cell><cell>91.1</cell><cell>32.3M</cell><cell></cell><cell>I</cell><cell>81.4</cell><cell>95.4</cell></row><row><cell></cell><cell>TSN+I3D R18</cell><cell cols="2">4 ? 4</cell><cell cols="2">4 ? 10 ? 3</cell><cell>73.0</cell><cell>91.3</cell><cell>32.3M</cell><cell></cell><cell>II</cell><cell>81.5</cell><cell>95.2</cell></row><row><cell></cell><cell>V4D+I3D R18</cell><cell cols="2">4 ? 4</cell><cell cols="2">4 ? 10 ? 3</cell><cell>75.6</cell><cell>92.7</cell><cell>33.1M</cell><cell></cell><cell>III</cell><cell>80.8</cell><cell>95.2</cell></row><row><cell></cell><cell>DSA+I3D R18</cell><cell cols="2">4 ? 4</cell><cell cols="2">4 ? 8 ? 3</cell><cell>77.3</cell><cell>93.9</cell><cell>32.3M</cell><cell></cell><cell>IV</cell><cell>81.4</cell><cell>95.1</cell></row><row><cell cols="3">(c) Parameter choices of . Set-</cell><cell cols="3">(d) The DSA blocks in different</cell><cell cols="4">(e) Parameter choices of . Set-</cell><cell cols="2">(f) The number of DSA block in-</cell></row><row><cell cols="3">ting: I3D R50, Position II, =1,</cell><cell cols="3">stage of I3D R50. Setting: Posi-</cell><cell cols="4">ting: I3D R50, Position II, =2, in-</cell><cell cols="2">serted into I3D R50. Setting: Posi-</cell></row><row><cell cols="2">inserted stage: res 5 .</cell><cell></cell><cell cols="2">tion II, =2, =1.</cell><cell></cell><cell cols="3">serted stage: res 5 .</cell><cell></cell><cell cols="2">tion II, =2, =1/8.</cell></row><row><cell cols="3">Setting Top-1 Top-5</cell><cell cols="3">Stage Top-1 Top-5</cell><cell></cell><cell cols="3">Setting Top-1 Top-5</cell><cell cols="2">Stages Blocks Top-1 Top-5</cell></row><row><cell>=1</cell><cell>81.0</cell><cell>95.1</cell><cell cols="2">res{2} 81.4</cell><cell>94.7</cell><cell></cell><cell>=1</cell><cell>81.5</cell><cell>95.2</cell><cell>res{5}</cell><cell>1</cell><cell>81.5 95.0</cell></row><row><cell>=2</cell><cell>81.5</cell><cell>95.2</cell><cell cols="2">res{3} 81.3</cell><cell>95.1</cell><cell></cell><cell>=1/2</cell><cell>81.7</cell><cell>95.4</cell><cell>res{4,5}</cell><cell>4</cell><cell>81.5 95.3</cell></row><row><cell>=4</cell><cell>81.2</cell><cell>95.0</cell><cell cols="2">res{4} 81.3</cell><cell>95.3</cell><cell></cell><cell>=1/4</cell><cell>81.6</cell><cell>95.0</cell><cell>res{3,4}</cell><cell>5</cell><cell>81.8 95.4</cell></row><row><cell>=8</cell><cell>81.3</cell><cell>95.0</cell><cell cols="2">res{5} 81.5</cell><cell>95.2</cell><cell></cell><cell>=1/8</cell><cell>81.5</cell><cell>95.0</cell><cell>res{2,3}</cell><cell>3</cell><cell>81.4 95.1</cell></row><row><cell cols="4">(g) Different short-term temporal struc-</cell><cell cols="5">(h) Study on the effectiveness of DSA module</cell><cell cols="3">(i) Training FLOPs. Comparison with V4D,</cell></row><row><cell cols="2">ture for DSA module.</cell><cell></cell><cell></cell><cell cols="5">with different backbones (I3D R18, I3D R50).</cell><cell cols="3">the extra computation cost brought by the</cell></row><row><cell cols="4">Model TSM R50 DSA+TSM R50 80.4 Top-1 Top-5 77.4 93.4 95.0 I3D R50 78.0 93.9 DSA+I3D R50 81.8 95.4</cell><cell cols="5">SENet+I3D uses SE module to replace the DSA module in DSANet. Arch. I3D SENet+I3D DSA+I3D ResNet18 72.2 73.8 77.3 ResNet50 78.0 78.5 81.8</cell><cell cols="3">DSA module is close to zero. Model Input size TSN+I3D R50 4 ? 4 ? 224 2 ? 3 83.8G FLOPs V4D+I3D R50 4 ? 4 ? 224 2 ? 3 143.0G DSA+I3D R50 4 ? 4 ? 224 2 ? 3 83.8G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art models on Mini-Kinetics-200 dataset. denoted the temporal length of video snippet, denoted the number of video snippets.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>?</cell><cell>?</cell><cell cols="3">? #crop Top-1 Top-5</cell></row><row><cell>S3D [37]</cell><cell>S3D Inception</cell><cell>64 ? 1</cell><cell>N/A</cell><cell></cell><cell>78.9%</cell><cell>-</cell></row><row><cell>I3D [38]</cell><cell>3D ResNet50</cell><cell>32 ? 1</cell><cell cols="2">32 ? 10 ? 3</cell><cell cols="2">75.5% 92.2%</cell></row><row><cell>I3D [38]</cell><cell>3D ResNet101</cell><cell>32 ? 1</cell><cell cols="2">32 ? 10 ? 3</cell><cell cols="2">77.4% 93.2%</cell></row><row><cell>I3D+NL [38]</cell><cell>3D ResNet50</cell><cell>32 ? 1</cell><cell cols="2">32 ? 10 ? 3</cell><cell cols="2">77.5% 94.0%</cell></row><row><cell>I3D+CGNL [38]</cell><cell>3D ResNet50</cell><cell>32 ? 1</cell><cell cols="2">32 ? 10 ? 3</cell><cell cols="2">78.8% 94.4%</cell></row><row><cell>I3D+NL [38]</cell><cell>3D ResNet101</cell><cell>32 ? 1</cell><cell cols="2">32 ? 10 ? 3</cell><cell cols="2">79.2% 93.2%</cell></row><row><cell cols="2">I3D+CGNL [38] 3D ResNet101</cell><cell>32 ? 1</cell><cell cols="2">32 ? 10 ? 3</cell><cell cols="2">79.9% 93.4%</cell></row><row><cell>V4D+I3D [39]</cell><cell>3D ResNet18</cell><cell>4 ? 4</cell><cell cols="2">4 ? 10 ? 3</cell><cell cols="2">75.6% 92.7%</cell></row><row><cell>V4D+I3D [39]</cell><cell>3D ResNet50</cell><cell>4 ? 4</cell><cell cols="2">4 ? 10 ? 3</cell><cell cols="2">80.7% 95.3%</cell></row><row><cell cols="2">DSA+I3D (Ours) 3D ResNet18</cell><cell>4 ? 4</cell><cell cols="2">4 ? 8 ? 3</cell><cell cols="2">77.3% 93.9%</cell></row><row><cell cols="2">DSA+I3D (Ours) 3D ResNet50</cell><cell>4 ? 4</cell><cell cols="2">4 ? 8 ? 3</cell><cell cols="2">81.8% 95.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>7% vs. 74.1%/74.9%/75.0%/76.0%) meanwhile with (2?) less calculation cost. Compared with computationally expensive models of 3D architecture, DSA with 4?8 frames input outperforms NL I3D-R50 with 32?10 frames (77.7% vs. 74.9%) but only uses 4.2? less GFLOPs, and uses 16.8? less GFLOPs than NL I3D-R50 with 128?10 frames meanwhile achieves a better accuracy (77.7% vs. 76.5%). Moreover, DSA with 8?8 frames input achieves a better accuracy than Slowfast-R50 and Slowfast-R101 (78.2% vs. 77.9%/77.0%) when using 3.2?, 2? less GFLOPs respectively. Compared with X3D<ref type="bibr" target="#b6">[7]</ref>, our DSA achieves better performance (77.7% vs. 77.5%) while using less inference cost (503G vs. 744G). Remarkably, equipped with the DSA module, the top-1 accuracy of Slowonly (namely I3D R50 above) improves from 74.9% to 78.2% on Kinetics-400. Then compared with video-level method, our DSA with 4?8 frames as input outperform V4D with 8?10 frames (78.2% vs. 77.4%)</figDesc><table><row><cell></cell><cell>ResNet-50</cell><cell>32?10?3</cell><cell>70.5?30=2115</cell><cell>74.9% 91.6%</cell></row><row><cell>NL+I3D [31]</cell><cell>3D ResNet-50</cell><cell>128?10?3</cell><cell>282?30=8460</cell><cell>76.5% 92.6%</cell></row><row><cell>X3D-L [7]</cell><cell>-</cell><cell>16?10?3</cell><cell>24.8?30=744</cell><cell>77.5% 92.9%</cell></row><row><cell>Slowfast [8]</cell><cell>3D R50+3D R50</cell><cell>(4+32)?10?3</cell><cell>36.1?30=1083</cell><cell>75.6% 92.1%</cell></row><row><cell>Slowfast [8]</cell><cell>3D R50+3D R50</cell><cell>(8+32)?3?10</cell><cell>65.7?30=1971</cell><cell>77.0% 92.6%</cell></row><row><cell>Slowfast [8]</cell><cell>3D R101+3D R101</cell><cell>(8+32)?3?10</cell><cell>106?30=3180</cell><cell>77.9% 93.2%</cell></row><row><cell>Slowonly [8]</cell><cell>3D ResNet-50</cell><cell>8?10?3</cell><cell>41.9?30=1257</cell><cell>74.9% 91.5%</cell></row><row><cell>V4D+I3D [39]</cell><cell>3D ResNet-50</cell><cell>8?10?3</cell><cell cols="2">286.1?2.5?3=2146  *  77.4% 93.1%</cell></row><row><cell>DSA+I3D (Ours)</cell><cell>3D ResNet-50</cell><cell>4?8?3</cell><cell>83.8?2?3=503</cell><cell>77.7% 93.1%</cell></row><row><cell>DSA+I3D (Ours)</cell><cell>3D ResNet-50</cell><cell>8?8?3</cell><cell>167.7?2?3=1006</cell><cell>78.2% 93.2%</cell></row><row><cell>DSA+I3D (Ours)</cell><cell>3D ResNet-50</cell><cell>(4+8)?8?3</cell><cell>251.5?2?3=1509</cell><cell>79.0% 93.7%</cell></row><row><cell>with a clear margin (77.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison with state-of-the-art on ActivityNet v1.3. The results using Multi-scale Temporal Window Integration [30] (abbreviated as M-TWI) with only RGB frames.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11030" to="11039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">X3D: Expanding Architectures for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning Spatio-Temporal Aggregation for Action Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The&quot; Something Something&quot; Video Database for Learning and Evaluating Visual Common Sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stnet: Local and global spatial-temporal modeling for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scsampler: Sampling salient clips from video for efficient action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6232" to="6242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TEA: Temporal Excitation and Aggregation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">T-C3D: Temporal convolutional 3D network for real-time action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TEINet: Towards an Efficient Architecture for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11669" to="11676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06803</idno>
		<title level="m">TAM: Temporal Adaptive Module for Video Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies for action recognition with a biologicallyinspired deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yemin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>In Neurips</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gate-Shift Networks for Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swathikiran</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1102" to="1111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5552" to="5561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Carafe: Content-aware reassembly of features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3007" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Non-Local Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adaptive Focus for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haojun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03245</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">MVFNet: Multi-View Fusion Network for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Multi-Agent Reinforcement Learning Based Frame Sampling for Effective Untrimmed Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dynamic Inference: A New Approach Toward Efficient Video Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR Workshops</title>
		<meeting>CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="676" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Compact Generalized Non-local Network. Neurips</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">V4D: 4D Convolutional Neural Networks for Video-level Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal relational reasoning in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ECO: Efficient Convolutional Network for Online Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamaljeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
