<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large Scale Audiovisual Learning of Sounds with Weakly Labeled Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haytham</forename><forename type="middle">M</forename><surname>Fayek</surname></persName>
							<email>haythamfayek@fb.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
							<email>anuragkr@fb.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Facebook Reality Labs</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large Scale Audiovisual Learning of Sounds with Weakly Labeled Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing sounds is a key aspect of computational audio scene analysis and machine perception. In this paper, we advocate that sound recognition is inherently a multi-modal audiovisual task in that it is easier to differentiate sounds using both the audio and visual modalities as opposed to one or the other. We present an audiovisual fusion model that learns to recognize sounds from weakly labeled video recordings. The proposed fusion model utilizes an attention mechanism to dynamically combine the outputs of the individual audio and visual models. Experiments on the large scale sound events dataset, AudioSet, demonstrate the efficacy of the proposed model, which outperforms the single-modal models, and state-of-theart fusion and multi-modal models. We achieve a mean Average Precision (mAP) of 46.16 on Audioset, outperforming prior state of the art by approximately +4.35 mAP (relative</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sound recognition is key for intelligent agents that perceive and interact with the world, similar to how the perception of different sounds is critical for humans to understand and interact with the environment. Sounds are primarily produced by actions on or interactions between objects, and hence we often learn to immediately associate sounds with visual objects and entities. This visual association is often necessary for recognizing and understanding the acoustic activities occurring around us. For example, it is easier to distinguish between sounds produced by a Vacuum Cleaner and sounds produced by a Hair Dryer by listening to and seeing the appliances, as opposed to one or the other.</p><p>Sound recognition is inherently a multi-modal audiovisual task. Thus, we ought to build machine learning models for sound recognition that are multi-modal, inspired by how humans perceive sound. In addition to reducing uncertainties in certain situations as instantiated above, audiovisual learning of sounds can lead to a more holistic understanding of the environment. For example, an alarm clock can be designed * Equal contribution.</p><p>to produce Church Bell sounds; while an audio only sound recognition model can perhaps label the sound as Church Bells, which would not be incorrect, it nevertheless does not represent the actual event. Hence, systems designed to do audiovisual perception of sounds would lead to better and more complete understanding of these phenomena. However, most prior work on sound recognition focuses on learning to recognize sounds only from audio <ref type="bibr">[Virtanen et al., 2017;</ref><ref type="bibr" target="#b5">Kumar, 2018]</ref>.</p><p>Multi-modal learning approaches, which aim to train a single model by using the audio and visual modalities simultaneously are one class of well established audiovisual learning approaches. However, these methods present their own set of challenges. In these methods, it is often necessary to appropriately synchronize the audio and visual components and ensure temporal consistencies for a single model to consume both modalities. Moreover, training a single multi-modal model which uses both modalities is a difficult feat due to different learning dynamics for the audio and visual modalities <ref type="bibr" target="#b6">[Wang et al., 2019a]</ref>. Further, training and inference in multi-modal models is computationally cumbersome, particularly for large datasets <ref type="bibr" target="#b1">[Gao et al., 2019]</ref>.</p><p>Another class of audiovisual learning methods can be described as Fusion approaches. Fusion approaches aim to combine two independent single-modal models. This offers several advantages. We can build strong individual models for each modality separately, with the freedom to design modal-specific models without requiring to factor in the unnecessary complexities arising out of multi-modal models and training. Furthermore, fusion approaches are often more interpretable than multi-modal models as it is easier to qualitatively analyze and understand the contributions of each modality through the individual models. Finally, fusion approaches allow combining multiple models per modality, which can be advantageous in improving performance.</p><p>In this paper, we propose fusion based approaches for audiovisual learning of sounds. We propose methods that learn to combine individual audio and visual models that were respectively trained on each modality separately. We first build state-of-the-art audio and visual sound recognition systems. We then propose attention fusion models to dynamically combine these audio and visual models. Specifically, our fusion models learn to pay attention to the appropriate modality in a sample-specific class-specific manner. Our models are de-signed for weakly supervised learning, where we train models using weakly labeled data <ref type="bibr" target="#b4">[Kumar and Raj, 2016a]</ref>. We analyze our proposed models on the largest dataset for sound events, Audioset <ref type="bibr" target="#b1">[Gemmeke et al., 2017]</ref>, and show that the proposed models outperform state-of-the-art single-modal models, baseline fusion models, and multi-modal models. The results and ensuing analysis attest to the importance of using both the audio and visual modalities for sound recognition and efficacy of our attention fusion models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning Sound Events. Numerous prior works have proposed a variety of supervised methods for detecting and classifying sound events <ref type="bibr">[Virtanen et al., 2017;</ref><ref type="bibr" target="#b5">Kumar, 2018]</ref>. Large-scale sound event detection has been possible primarily through weakly supervised learning <ref type="bibr" target="#b4">[Kumar and Raj, 2016a]</ref> and the release of large-scale weakly labeled sound events datasets, such as Audioset <ref type="bibr" target="#b1">[Gemmeke et al., 2017]</ref>. Most of the recent methods rely on deep neural networks, in particular, deep Convolutional Neural Networks (ConvNets) <ref type="bibr" target="#b4">[Kumar and Ithapu, 2020;</ref><ref type="bibr" target="#b0">Adavanne et al., 2019;</ref><ref type="bibr">Kong et al., 2019;</ref><ref type="bibr" target="#b1">Ford et al., 2019]</ref>. While mean and max pooling have been shown to be effective in handling weakly labeled data <ref type="bibr" target="#b5">[Kumar, 2018]</ref>, more involved methods such as adaptive pooling <ref type="bibr" target="#b5">[McFee et al., 2018]</ref> or attention based pooling <ref type="bibr" target="#b6">[Wang et al., 2019b;</ref><ref type="bibr">Kong et al., 2019]</ref> have also been proposed. Audiovisual Learning. There is a profusion of work on multi-modal audiovisual learning for video recognition and understanding in recent years <ref type="bibr">[Baltru?aitis et al., 2018]</ref>. Progress in multi-modal learning was facilitated by the availability of large-scale video datasets <ref type="bibr" target="#b3">[Kay et al., 2017;</ref><ref type="bibr" target="#b1">Gemmeke et al., 2017;</ref><ref type="bibr" target="#b2">Gu et al., 2018]</ref>, advances in selfsupervised learning of multi-modal representations that exploit cross-modal information <ref type="bibr" target="#b0">[Aytar et al., 2016;</ref><ref type="bibr">Arandjelovic and Zisserman, 2017;</ref><ref type="bibr" target="#b5">Owens and Efros, 2018;</ref><ref type="bibr" target="#b3">Korbar et al., 2018]</ref>, custom loss functions that synchronize learning multiple modalities <ref type="bibr" target="#b6">[Wang et al., 2019a]</ref>, and others. Despite the recent progress, multi-modal audiovisual learning remains a challenging problem. Primary reasons include difficulties in designing multi-modal neural network architectures that work well across various datasets and tasks, jointly learning multiple modalities, training and inference efficiency, etc. Audiovisual Fusion. Late fusion approaches are a popular to combine audio and visual models . Unweighted late fusion such as averaging the outputs of the audio and visual models can demonstrate competitive performance in addition to not requiring any additional parameters and training beyond the audio and visual singlemodal models <ref type="bibr" target="#b5">[Lan et al., 2012]</ref>. Nevertheless, weighted late or mid-level fusion has been extensively studied as it provides avenues to adaptively combine information between the different modalities <ref type="bibr" target="#b5">[Lan et al., 2012;</ref><ref type="bibr" target="#b7">Wu et al., 2016;</ref><ref type="bibr" target="#b3">Kazakos et al., 2019]</ref>. Further, attention mechanisms <ref type="bibr">[Bahdanau et al., 2014]</ref> were used to dynamically combine the outputs of the single-modal models <ref type="bibr" target="#b5">[Long et al., 2018;</ref><ref type="bibr" target="#b5">Sterpu et al., 2018;</ref><ref type="bibr" target="#b3">Hori et al., 2018;</ref><ref type="bibr" target="#b7">Zhou et al., 2019;</ref><ref type="bibr" target="#b5">Lin et al., 2019]</ref>.</p><p>Audiovisual Learning of Sounds Prior work on audiovisual learning of sounds, whether through single-model multimodal learning or through fusion of individual models independently trained on the respective modalities, is scarce. <ref type="bibr" target="#b6">[Wang et al., 2019a</ref>] describes a multi-modal learning approach in which a single model is trained jointly for both the audio and visual modalities and applies this to sound recognition. Such methods are susceptible to several difficulties as outlined in Section 1. We, on the other hand, propose fusion models for audiovisual learning of sounds and show in Section 2 ( <ref type="table" target="#tab_4">Table 3</ref>) the effectiveness of our fusion models compared with the multi-modal models in <ref type="bibr" target="#b6">[Wang et al., 2019a]</ref>. Another work related to ours is <ref type="bibr" target="#b5">[Parekh et al., 2019]</ref>, which attempts to learn representations from weakly labeled audiovisual data and use these representations in different tasks including sound event recognition; therein, the audio and visual models are only combined through average fusion. Perhaps worth mentioning are other works such as as <ref type="bibr" target="#b0">[Aytar et al., 2016;</ref><ref type="bibr">Arandjelovic and Zisserman, 2017;</ref>; they attempted to use the visual modality to learn representations for the audio modality that can be used for downstream sound event classification tasks. However, in all prior work, the downstream sound classification tasks are done on small-scale datasets, which does not provide in-depth understanding of audiovisual learning for sounds at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Audiovisual Models for Sounds</head><p>We learn sound events in the weakly supervised setting, where only recording-level labels are provided for each training recording, with no temporal information. The fundamentals of weakly supervised learning of sound events are based on Multiple Instance Learning (MIL) <ref type="bibr" target="#b4">[Kumar and Raj, 2016a]</ref>. In MIL, the learning problem is formulated in terms of bags and respective labels, (B, Y ); each bag is a collection of instances <ref type="bibr" target="#b0">[Andrews et al., 2003]</ref>. A bag is labeled as positive for a class, Y = 1, if at least one instance within the bag is positive. On the other hand, if all instances within the bag are negative for a given class, then the label is negative, Y = 0. For weakly labeled sound recognition, recordings which are tagged with the presence of a sound class become a positive bag for that class and negative otherwise. The audio and visual models in Sections 3.1 and 3.2 follow this formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Audio Model</head><p>In the MIL setting for weakly labeled sound event recognition, each recording becomes a bag as required in the MIL framework. The instances in the bag are segments of the recording. Assuming that each instance is represented by a feature vector x; the training data takes the form of</p><formula xml:id="formula_0">(B i , Y i ), i = 1 to n, where the i th bag consists of m i in- stances B i = {x i1 ? ? ? x imi }.</formula><p>In the audio model, the key idea is that the learner can be trained by first mapping the instance-level predictions to the corresponding bag-level predictions, and then these bag-level predictions can be used to compute a loss function to be minimized, akin to the classic MI-SVM approach for MIL <ref type="bibr">[An-drews et al., 2003]</ref>. Herein, the max function was used to aggregate the instance-level predictions.</p><p>Let f be the function we wish to learn; then the training involves minimizing the following loss function.</p><formula xml:id="formula_1">L(?, ?) = n i=1 l(g ? (f ? (x i1 ), ? ? ? , f ? (x imi )), Y i ) (1)</formula><p>where, l is the loss function measuring divergence between predictions and targets, and ? represents learnable parameters of f . Note that the function g, used to aggregate the instance-level predictions from f to the bag-level predictions, can itself contain learnable parameters ?.</p><p>The audio model is a ConvNet that maps Log-scaled Melfilter-bank feature representations of the entire audio recording to (multiple) labels, trained by minimizing the loss function defined in Equation 1. The sampling rate of all audio recordings is 16 kHz; 64 Mel-filter-bank representations are obtained for 16 ms windows in the audio recording, shifted by 10 ms, which amounts to 100 frames per second of audio.</p><p>The overall architecture of the network is detailed in Table 1. The network produces instance and bag representations for any given input at Block B5. The bag consists of 2048-dimensional instance representations. The network is designed for a receptive field of~1 second (96 frames). Hence, it outputs 2048-dimensional features for~1 second of audio, every~0.33 seconds (32 frames). Three 1 ? 1 convolutional layers (B6 to B8) are then used to predict outputs for each segment, which are then aggregated via the layer G that uses global average pooling. The number of segments depends on the size of the input. As shown in <ref type="table" target="#tab_1">Table 1</ref>, for an input with 1024 frames, 30 segments are produced.</p><p>The loss function for the i th training audio recording is defined as:</p><formula xml:id="formula_2">l(B i , Y i ) = 1 C C c=1 h(Y i c , p i c )<label>(2)</label></formula><p>where, p i c is the output for the c th class and h(</p><formula xml:id="formula_3">Y i c , p i c ) = ?Y i c * log(p i c ) ? (1 ? Y i c ) * log(1 ? p i c )</formula><p>is the binary cross entropy loss function. Furthermore, our audio model training also incorporates the sequential co-supervision method described in <ref type="bibr" target="#b4">[Kumar and Ithapu, 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual Model</head><p>Similar to the audio model, the visual model for sounds is based on the MIL framework. Herein, the entire video is the bag and instances of the bag are the frames of the video. Specifically, we sample 64 frames from the videos to form the bags. Each frame (instance) is then represented by 2048feature representations obtained from a <ref type="bibr">ResNet-152 [He et al., 2016]</ref>, pre-trained on Imagenet <ref type="bibr">[Deng et al., 2009]</ref>. This yields the video bag representations for each recording. We obtain a single 2048-dimensional vector representation for each bag by averaging the feature representations for each instance. This idea of mapping bags to a single vector representations was previously employed in <ref type="bibr">[Kumar and Raj, 2016b;</ref><ref type="bibr" target="#b6">Wei et al., 2014]</ref> to make MIL algorithms scalable.</p><p>We then train a 4 hidden layer Deep Fully Connected Neural Network (DNN) to recognize sound events. The number </p><formula xml:id="formula_4">Block B8 Conv: C, 1 ? 1 C ? 30 ? 1 G (g())</formula><p>Global average pooling C ? 1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Baseline Audiovisual Fusion</head><p>Our audiovisual learning of sounds focuses on developing fusion models that combine outputs from models trained individually on the audio and visual modalities. The following is a description of a number of baseline fusion methods. Average Fusion. We simply compute the arithmetic mean of the outputs of the audio and visual models. Despite its simplicity, average fusion serves as a strong baseline. <ref type="bibr" target="#b5">[Parekh et al., 2019]</ref> used average fusion to combine audio and visual predictions.</p><p>Regression. We train a linear layer with a sigmoid activation function that ingests the (mean-standard-deviation) normalized outputs of the audio and visual models, and predicts the probability of the sound classes. The model is L2 regularized with the regularization parameter set to ? = 10 ?5 .</p><p>Multi-Layer Perceptron (MLP). We train a fully connected neural network with a single hidden layer that ingests concatenated outputs of the audio and visual models, and predicts the probability of the sound classes. The hidden layer comprises 512 units with batch normalization and ReLU activation. Furthermore, a dropout layer with a dropout rate of 0.5 is used. The output layer is a fully connected layer with C units and a sigmoid activation function.  <ref type="figure">Figure 1</ref>: Audiovisual fusion for sound recognition. The audio and visual models map the respective input to segment-level representations which are then used to obtain single-modal predictions, ha and hv, respectively. The attention fusion function, nattn, ingests the singlemodal predictions, ha and hv, to produce weights for each modality, ?a and ?v. The single-modal audio and visual predictions, ha and hv, are mapped toha andhv via functions, na and nv, respectively, and fused using the attention weights, ?a and ?v. The fused output,hav, is then mapped to multi-modal multi-label predictions via function nav.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Attention Audiovisual Fusion</head><p>The audio and visual models have been separately trained to map the audio and visual modalities, respectively, to the various sound classes. The performance of each class varies between modalities. This variation can be sample-specific as well. For a given sound class, in some videos, the audio modality might be sufficient to correctly recognize the sound, whereas in other videos, the visual modality might be more suited. Thus, we ought to fuse the outputs from the audio and visual models in a sample-specific and class-specific manner.</p><p>To this end, we design an attention mechanism to combine the audio and visual models. <ref type="figure">Figure 1</ref> shows the general schema of the attention fusion mechanism. Let h a and h v represent the outputs of the audio and visual models for a given video recording respectively. We learn an attention function, a(?), that generates weights ? a and ? v for the audio and visual modalities respectively. We constrain the attention weights to sum to 1 for each class, i.e., ? a + ? v = 1, where 1 is a vector of ones.</p><p>The attention function is implemented as a neural network, n attn : ? a = n attn ([h a , h v ], W attn ) (3) where, W attn denotes the learnable parameters of network n attn , and [h a , h v ] denotes the concatenation of h a and h v . The output layer in n attn has a sigmoid activation function.</p><p>The final output that combines the audio and visual models is obtained as follows:</p><formula xml:id="formula_5">h a = n a (h a , W n a ) ,h v = n v (h v , W n v ) (4) h av = ? a h a + (1 ? ? a ) h v (5) o av = n av (h av , W n av )<label>(6)</label></formula><p>where, n a and n v are neural networks that process inputs h a and h v intoh a andh v , respectively, which are then combined through attention weights (? a and ? v = 1 ? ? a ), as in Equation 5. W n a and W n v are the learnable parameters of n a and n v respectively. The combined outputs,h av , are passed through another neural network n av with a sigmoid activation function to produce the final outputs o av .</p><p>Note that n a and n v are generic functions representing all possible transforms: linear, non-linear, or even the identity of h a and h v respectively. For the identity case, n(h) = h, and n does not contain any learnable parameters. For the linear transform case, n does not contain any non-linear activation functions. Similarly, n av might just be a sigmoid function, i.e., n av (h av ) = 1/(1 + exp ?hav ), and hence, containing no learnable parameters. Otherwise, it can be a neural network with one or more layers with a sigmoid activation at the output layer. We also attempted to learn the attention weights with a single modality, either h a or h v instead of the combined [h a , h v ]. In this case, ? a is either obtained through self-attention ? a = n attn (h a , W attn ) or through cross-modal attention ? a = n attn (h v , W attn ). Similarly, ? v is obtained, and both ? a and ? v are normalized to sum to 1 for each class. However, attention weights learned from single modality was found to be inferior to learning them through both modalities as in Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experiments are conducted on AudioSet which is described in Section 4.1. The experiments and results are presented in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Audioset Dataset</head><p>Audioset <ref type="bibr" target="#b1">[Gemmeke et al., 2017]</ref> is the largest dataset for sound events. The dataset provides YouTube videos for 527 sound events. Each video clip is approximately 10 s in length annotated by a human with multiple labels denoting the sound events present in the video clip. The average number of labels per video clip is 2.7. The dataset is weakly labeled as the labels of each video clip denote the absence or presence of sound events but do not contain any temporal information. The distribution of sound event classes in the training set is severely unbalanced, ranging from around 1 million videos for the most represented class, Music, to approximately 120 videos for the least represented class, Screech.</p><p>We use the predefined Unbalanced set for training and Eval set for performance evaluation. The training set comprises   approximately 2 million videos, whereas the evaluation set comprises approximately 20, 000 videos. The evaluation set contains at least 59 videos for each class. In practice, the total video count is slightly less than the numbers listed above due to the unavailability of links when this work was carried out.</p><p>We sample approximately 25, 000 videos from the training set to use as the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Evaluation of Fusion Methods</head><p>Experimental Setup. The outputs (predictions) of the audio and visual models are used as input to all fusion models. These outputs are normalized to zero mean and unit standard deviation prior to feeding them into the fusion models (except for average fusion). The mean and standard deviation are computed using the training set only. The details of the audio model training can be found in <ref type="bibr" target="#b4">[Kumar and Ithapu, 2020]</ref>. The network in the video model is trained for 20 epochs using the Adam optimizer [Kingma and Ba, 2014] with a mini-batch size of 144. Hyperparameters such as the learning rate are selected using the validation set, which is also used to select the best model during training.</p><p>In the fusion experiments, all neural networks are trained using Adam for 100 epochs. The mini-batch size is set to 256. The validation set is used to select the best hyperparameters, such as the learning rate for each model, and the best model during training. Similar to prior work on Audioset, we use Average Precision (AP) and Area Under Receiver Operating Characteristic (ROC) Curve (AUC) to measure the performance per class. Mean Average Precision (mAP) and mean Area Under ROC Curve (mAUC) computed over all 527 classes are used for comparison. Comparison of Fusion Methods.  Our attention fusion model achieves 46.16 mAP, which is an absolute improvement of 7.81 mAP (relative: 20.4%) over the audio model. It also outperforms all baseline fusion methods: an absolute improvement of 3.32 mAP (relative: 7.7%) over average fusion. In this case, n attn takes in the concatenated normalized outputs from the audio (h a ) and visual (h v ) models. n attn is a neural network with a single hidden layer of 512 units; batch normalization and ReLU activation is applied on this layer which is followed by a dropout layer with a dropout rate of 0.5. The output layer has 527 units and a sigmoid activation function. n a , n v , and n av are all single layer networks with 512 units and sigmoid activations.</p><p>We also study different variations of the proposed fusion model. The unimodal approaches uses only a single modality in the attention network, either h a or h v as opposed to using h a and h v combined through concatenation. When ? a is obtained by using h a and ? v using h v , we denote it as self unimodal attention. However, if ? a uses h v and vice versa, we call it cross unimodal attention. In these cases, there are two attention networks, n a attn and n v attn but other aspects of the architecture remains identical to the one described above.</p><p>We also consider another variation where n a and n v are simply identity functions that do not contain learnable parameters. Similarly, in another variation, we make n av an identity mapping. The performance of these fusion models are listed in <ref type="table" target="#tab_6">Table 4</ref>. The multi-modal attention model is slightly better than the unimodal attention models. However, removing neural networks n a , n v , and n av leads to significant deterioration in performance.</p><p>Comparison with State-of-the-Art. <ref type="table" target="#tab_4">Table 3</ref> shows comparison with state-of-the-art models on Audioset. Our audio model is slightly better than the state-of-the-art performance on Audioset. Note that <ref type="bibr" target="#b1">[Ford et al., 2019]</ref> also shows a performance of 39.2 mAP. However, this is obtained by a ensembeling outputs from multiple models and the best single model performance (which is fairer to compare with) is 38.0. To the best of our knowledge, <ref type="bibr" target="#b6">[Wang et al., 2019a]</ref> is the only prior work that reported visual and audiovisual models for sound events on Audioset. Our visual model is +6.93 mAP (relative: 36.8%) better than <ref type="bibr" target="#b6">[Wang et al., 2019a]</ref>. More importantly, our audiovisual model is +4.35 mAP (relative: 10.4%) compared with <ref type="bibr" target="#b6">[Wang et al., 2019a]</ref> and sets a new state-of-the-art on Audioset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Class Specific Analysis</head><p>Figure 2 provides a per class depiction of the audio, visual, and audiovisual models. As expected, the audio model outperforms the video model for the majority of sound events. For sounds such as Heartbeat, Jingle Bell, and Whispering, audio recognition of sounds is considerably better than visual recognition of sounds. However, for classes such as Patter, Shuffling Cards, and Bus, the visual model outperforms the audio model by a large margin. Overall, the visual model outperforms the audio model for 117 sound classes by a small or large margin. For some classes, both audio and visual models perform well (e.g., Turkey, Train, and Music Accordion), or both fail to achieve good performance (e.g., Squish and Hum).</p><p>For the audiovisual fusion model, we see that for most classes, audiovisual learning leads to an improvement over the better of the audio or visual models, as observable by mostly blue bubbles in <ref type="figure" target="#fig_1">Figure 2</ref>. <ref type="figure">Figure 3</ref> shows a histogram of the absolute deterioration and improvement in performance through audiovisual learning compared to the better of the audio or visual models. For over 83% of classes (439 of 527), we see an improvement, whereas for the remaining  classes, audiovisual learning leads to deterioration in performance, compared with the better of the audio or visual models. Furthermore, for over 17% of classes (91 out of 527), audiovisual learning achieves an absolute improvement of more than +10 mAP. On the other hand, for only 15 classes audiovisual learning leads to an absolute deterioration of more than 5 mAP, with only 3 classes with more than 10 deterioration in mAP. For most cases, the deterioration is minor. From <ref type="figure" target="#fig_1">Figure 2</ref>, we can see some examples of classes with large improvements in the audiovisual model, e.g., Explosion, Ringtone Clicking, Vacuum Cleaner, and Bathtub. Several of these classes with large improvement have similar AP using audio and visual models.</p><p>For several sound classes, one of the modalities is much more superior. One may argue for using only the appropriate single-modal models. However, we see that in several of these cases the other modality, despite being inferior on its own, can provide complimentary information, which can lead to improved performance through audiovisual learning. Consider for example, classes such as Angry Music and Plop, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. While the audio modality is much more useful for these classes compared to the visual modality, audiovisual learning still leads to considerable improvement as the information from the visual modality compliments the audio modality. The same inference applies for classes such as Bus and Motorboat, where the visual modality is much more superior compared to the audio modality. However, there are exceptions to the above observations. On the audio side, Whispering and Police Siren, and on the video side, Shuffling Cards and Writing, are examples of such exceptions.</p><p>One final observation we would like to point out from <ref type="figure" target="#fig_1">Figure 2</ref> is that, in general for classes with low performance for the audio and visual models (less than 10 AP), audiovisual fusion also does not lead to any improvement. We believe that a multi-modal framework in which the audio and visual modalities are jointly used to train a single model might be more suitable in these situations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Analysis of Attention</head><p>We analyze the attention weights obtained for each sample for each class. For each class, we compute the mean attention weight (? c a ) by averaging over all evaluation examples which contains the c th class. This gives us an estimate of the importance of the audio modality for the c th class (and consequently for the visual modality as well). <ref type="figure">Figure 4</ref> is a histogram plot of the mean attention weights. The right side of the plot with higher ? a represents dominance of the audio modality, whereas on the left side the visual modality is the dominant modality. The distribution is clearly skewed towards the audio modality, which is expected since the audio model is better than the visual model for more than 75% of the sound classes. There are more than 110 sounds for which on average, the audio modality gets all the weight (? c a ? 1). Interestingly, there are around 38 sound classes where the visual modality has complete dominance (? c a ? 0). Furthermore, for more than 70 sound classes the visual modality on average gets more than 90% weight (? c a ? 0.1). This signifies that for several sounds the visual modality can play a vital role in their recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose audiovisual models for learning sounds. Instead of jointly using audio and visual modalities in a single model, we propose to combine the individual models that were trained separately on each modality. To this end, we use a state-of-the-art audio based sound event recognition model and propose a novel vision based model for recognizing sound events. On the Audioset dataset of sound events, it outperforms a prior vision model for sounds by +6.93 mAP (relative: 36.9%). We then propose an attention mechanism for audiovisual fusion that dynamically fuses the outputs from the audio and visual models in a sample-specific and class-specific manner. Our proposed audiovisual learning model improves state-of-the-art performance on Audioset by approximately +4.35 mAP (relative: 10.4%). We also provide thorough analysis of the role of the audio and visual modalities for various sound classes. The outcomes of this analysis can play a crucial role in the development of future models for audiovisual learning of sounds.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Audio, visual, and audiovisual models Average Precision (AP). The horizontal and vertical axes denote the audio and visual models APs respectively for all classes. The diameter of each bubble denotes the absolute change in AP over the single-modal models (audio or visual, whichever is better). Blue denotes improvement over the single-modal model, whereas red indicates deterioration over the single-modal model, as depicted in the color bar. Several sound classes of interest are annotated accordingly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Histogram of change in AP for our audiovisual attention fusion model with respect to single modality models (audio or visual whichever is better). Mean of attention weights for all classes in AudioSet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Audio Convolutional Neural Network (ConvNet) architecture. The number of filters and size of each filter varies between convolutional layers; e.g., Conv: 64, 3 ? 3 denotes a convolutional layer of 64 filters, each of size 3 ? 3. C denotes the number of classes. All convolutional layers except in B8 are followed by batch normalization and Rectified Linear Units (ReLU); the convolutional layer in B8 is followed by a sigmoid function.</figDesc><table /><note>of units in each layer are 2048, 2048, 1024, 1024, C respec- tively, where C is the number of classes. The first and second hidden layers are followed by a dropout layer with a dropout rate of 0.3. Rectified Linear Units (ReLU) activation is used in all layers except for the last layer, where sigmoid activation is used. Similar to the audio model, the network is trained with the binary cross entropy loss function.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of mAP and mAUC for different fusion methods for combining audio and visual models.</figDesc><table><row><cell>Model</cell><cell cols="2">mAP mAUC</cell></row><row><cell>Audio (ResNet-50) [Ford et al., 2019]</cell><cell>38.0</cell><cell>97.10</cell></row><row><cell>Audio (Ours)</cell><cell>38.35</cell><cell>97.12</cell></row><row><cell>Visual (R(2+1)D-101) [Wang et al., 2019a]</cell><cell>18.80</cell><cell>91.80</cell></row><row><cell>Visual (Ours)</cell><cell>25.73</cell><cell>91.30</cell></row><row><cell cols="2">AudioVisual (G-Blend) [Wang et al., 2019a] 41.80</cell><cell>97.50</cell></row><row><cell>AudioVisual (Ours)</cell><cell>46.16</cell><cell>97.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>mAP and mAUC for state-of-the-art audio, visual, and au- diovisual sound recognition models on AudioSet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>summarizes the</cell></row><row><cell>results. The audio model achieves 38.35 mAP and 97.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation of attention fusion models. mAUC, whereas the visual model achieves 25.73 mAP and 91.30 mAUC. The superiority of the audio model over the visual model is expected due to the nature of the task.</figDesc><table><row><cell>Average fusion of the audio and visual outputs achieves</cell></row><row><cell>42.84 mAP, an absolute improvement of 4.49 mAP (relative:</cell></row><row><cell>11.7%) over the audio model, and an absolute improvement</cell></row><row><cell>of 17.11 mAP (relative: 66.5%) over the visual model. The</cell></row><row><cell>regression fusion model leads to a minor improvement over</cell></row><row><cell>average fusion: +0.26 mAP. The MLP fusion model leads to</cell></row><row><cell>considerable improvement over average fusion: +2.76 mAP.</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sound event classification and detection with weakly labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>References [adavanne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12667</idno>
		<idno>arXiv:1409.0473</idno>
	</analytic>
	<monogr>
		<title level="m">Tadas Baltru?aitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: A survey and taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<editor>Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei</editor>
		<meeting><address><addrLine>New York, USA; Stuart Andrews</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<idno type="arXiv">arXiv:1912.04487</idno>
		<idno>arXiv:1808.03766</idno>
	</analytic>
	<monogr>
		<title level="m">The activitynet large-scale activity recognition challenge 2018 summary</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal attention for fusion of audio and spatiotemporal features for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<idno>arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<editor>Kong et al., 2019] Qiuqiang Kong, Changsong Yu, Yong Xu, Turab Iqbal, Wenwu Wang, and Mark D Plumbley</editor>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops<address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="7763" to="7774" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kumar and Raj, 2016b] Anurag Kumar and Bhiksha Raj. Weakly supervised scalable audio content analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi Krishna Ithapu ;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhiksha</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>ACM Multimedia</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 20th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="111" to="115" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>International Conference on Multimedia Modeling. Virtanen et al., 2017] Tuomas Virtanen, Mark D Plumbley, and Dan Ellis. Computational analysis of sound scenes and events</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<idno type="arXiv">arXiv:1905.12681</idno>
	</analytic>
	<monogr>
		<title level="m">What makes training multi-modal networks hard? arXiv preprint</title>
		<editor>Xiu-Shen Wei, Jianxin Wu, and Zhi-Hua Zhou</editor>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1037" to="1042" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Data Mining</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-stream multi-class fusion of deep networks for video classification</title>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="6565" to="6569" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
