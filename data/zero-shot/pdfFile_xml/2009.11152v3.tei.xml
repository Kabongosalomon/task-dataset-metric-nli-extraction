<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Pre-training for Sequence Labelling in Spoken Dialog</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emile</forename><surname>Chapuis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Telecom Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Colombo</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Telecom Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM GBS France</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Manica</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">IBM Research Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Labeau</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Telecom Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Clavel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Telecom Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Pre-training for Sequence Labelling in Spoken Dialog</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sequence labelling tasks like Dialog Act and Emotion/Sentiment identification are a key component of spoken dialog systems. In this work, we propose a new approach to learn generic representations adapted to spoken dialog, which we evaluate on a new benchmark we call Sequence labellIng evaLuatIon benChmark fOr spoken laN-guagE benchmark (SILICONE). SILICONE 1 is model-agnostic and contains 10 different datasets of various sizes. We obtain our representations with a hierarchical encoder based on transformer architectures, for which we extend two well-known pretraining objectives. Pre-training is performed on OpenSubtitles: a large corpus of spoken dialog containing over 2.3 billion of tokens. We demonstrate how hierarchical encoders achieve competitive results with consistently fewer parameters compared to state-of-the-art models and we show their importance for both pre-training and finetuning. . 1993. The hcrc map task corpus: natural dialogue for speech recognition. Scott Thornbury and Diana Slade. 2006. Conversation: From description to pedagogy. Cambridge University Press. Quan Hung Tran, Gholamreza Haffari, and Ingrid Zukerman. 2017. A generative attentional neural network model for dialogue act classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The identification of both Dialog Acts (DA) and Emotion/Sentiment (E/S) in spoken language is an important step toward improving model performances on spontaneous dialogue task. Especially, it is essential to avoid the generic response problem, i.e., having an automatic dialog system generate an unspecific response -that can be an answer to a very large number of user utterances <ref type="bibr">(Yi et al., 2019;</ref><ref type="bibr">Colombo et al., 2019)</ref>. DA and emotion identification <ref type="bibr">(Witon et al., 2018;</ref><ref type="bibr">Jalalzai et al., 2020)</ref> are done through sequence labelling systems that are usually trained on * stands for equal contribution 1 Benchmark can be found in the dataset library from HuggingFace <ref type="bibr">(Wolf et al., 2020)</ref> at https:// huggingface.co/datasets/silicone large corpora (with over 100k labelled utterances) such as Switchboard <ref type="bibr">(Godfrey et al., 1992)</ref>, <ref type="bibr">MRDA (Shriberg et al., 2004)</ref> or Daily Dialog Act <ref type="bibr">(Li et al., 2017)</ref>. Even though large corpora enable learning complex models from scratch (e.g., seq2seq <ref type="bibr">(Colombo et al., 2020)</ref>), those models are very specific to the labelling scheme employed. Adapting them to different sets of emotions or dialog acts would require more annotated data. Generic representations <ref type="bibr">(Mikolov et al., 2013;</ref><ref type="bibr">Pennington et al., 2014;</ref><ref type="bibr">Peters et al., 2018;</ref><ref type="bibr">Devlin et al., 2018;</ref><ref type="bibr">Yang et al., 2019;</ref><ref type="bibr">Liu et al., 2019)</ref> have been shown to be an effective way to adapt models across different sets of labels. Those representations are usually trained on large written corpora such as OSCAR <ref type="bibr">(Su?rez et al., 2019)</ref>, Book Corpus <ref type="bibr">(Zhu et al., 2015)</ref> or <ref type="bibr">Wikipedia (Denoyer and Gallinari, 2006)</ref>. Although achieving state-of-the-art (SOTA) results on written benchmarks <ref type="bibr">(Wang et al., 2018)</ref>, they are not tailored to spoken dialog (SD). <ref type="bibr">Indeed, Tran et al. (2019)</ref> have suggested that training a parser on conversational speech data can improve results, due to the discrepancy between spoken and written language (e.g., disfluencies <ref type="bibr">(Stolcke and Shriberg, 1996)</ref>, fillers <ref type="bibr">(Shriberg, 1999;</ref><ref type="bibr">Dinkar et al., 2020)</ref>, different data distribution). Furthermore, capturing discourse-level features, which distinguish dialog from other types of text <ref type="bibr">(Thornbury and Slade, 2006)</ref>, e.g., capturing multi-utterance dependencies, is key to embed dialog that is not explicitly present in pre-training objectives <ref type="bibr">(Devlin et al., 2018;</ref><ref type="bibr">Yang et al., 2019;</ref><ref type="bibr">Liu et al., 2019)</ref>, as they often treat sentences as a simple stream of tokens. The goal of this work is to train on SD data a generic dialog encoder capturing discourse-level features that produce representations adapted arXiv:2009.11152v3 [cs.CL] 8 Feb 2021 to spoken dialog. We evaluate these representations on both DA and E/S labelling through a new benchmark SILICONE (Sequence labellIng evaLuatIon benChmark fOr spoken laNguagE) composed of datasets of varying sizes using different sets of labels. We place ourselves in the general trend of using smaller models to obtain lightweight representations <ref type="bibr">(Jiao et al., 2019;</ref><ref type="bibr">Lan et al., 2019)</ref> that can be trained without a costly computation infrastructure while achieving good performance on several downstream tasks <ref type="bibr">(Henderson et al., 2020)</ref>. Concretely, since hierarchy is an inherent characteristic of dialog <ref type="bibr">(Thornbury and Slade, 2006)</ref>, we propose the first hierarchical generic multi-utterance encoder based on a hierarchy of transformers. This allows us to factorise the model parameters, getting rid of long term dependencies and enabling training on a reduced number of GPUs. Based on this hierarchical structure, we generalise two existing pre-training objectives. As embeddings highly depend on data quality <ref type="bibr">(Le et al., 2019)</ref> and volume <ref type="bibr">(Liu et al., 2019)</ref>, we preprocess OpenSubtitles <ref type="bibr">(Lison et al., 2019)</ref>: a large corpus of spoken dialog from movies. This corpora is an order of magnitude bigger than corpora <ref type="bibr">(Budzianowski et al., 2018b;</ref><ref type="bibr">Lowe et al., 2015;</ref><ref type="bibr">Danescu-Niculescu-Mizil and Lee, 2011)</ref> used in previous works <ref type="bibr">(Mehri et al., 2019;</ref><ref type="bibr">Hazarika et al., 2019)</ref>. Lastly, we evaluate our encoder along with other baselines on SILICONE, which lets us draw finer conclusions of the generalisation capability of our models 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We start by formally defining the Sequence Labelling Problem. At the highest level, we have a set D of conversations composed of utterances, i.e., D = (C 1 , C 2 , . . . , C |D| ) with Y = (Y 1 , Y 2 , . . . , Y |D| ) being the corresponding set of labels (e.g., DA, E/S). At a lower level each conversation C i is composed of utterances u, i.e C i = (u 1 , u 2 , . . . , u |C i | ) with Y i = (y 1 , y 2 , . . . , y |C i | ) being the corresponding sequence of labels: each u i is associated with a unique label y i . At the lowest level, each utterance u i can be seen as a sequence of words, i.e u i = (? i 1 , ? i 2 , . . . , ? i |u i | ). Concrete examples with dialog act can be found in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="bibr">Utterances</ref> DA How long does that take you to get to work? qw Uh, about forty-five, fifty minutes.</p><p>sd How does that work, work out with, uh, storing your bike and showering and all that? qw  MLM Loss: The MLM loss corrupts sequences (or in our case, utterances) by masking a proportion p ? of tokens. The model learns bidirectional representations by predicting the original identities of the masked-out tokens. Formally, for an utterance u i , a random set of indexed positions m u i is selected and the associated tokens are replaced by a masked token <ref type="bibr">[MASK]</ref> to obtain a corrupted utterance u masked i . The set of parameters ? is learnt by maximizing :</p><formula xml:id="formula_0">L u MLM (?, u i ) = E t?m u i log(p ? (? i t |? i ))<label>(1)</label></formula><p>where? i is the corrupted utterance, m u i j ? unif{1, |u i |} ? j ? [1, p ? ] and p ? is the proportion of masked tokens. GAP Loss: the GAP loss consists in computing a classic language modelling loss across different factorisation orders of the tokens. In this way, the model will learn to gather information across all possible positions from both directions. The set of parameters ? is learnt by maximising:</p><formula xml:id="formula_1">L u GAP (?, u i ) = E E z?Z |u i | t log p ? (? i zt |u z&lt;t i )</formula><p>(2) where Z |u i | is the set of permutations of length |u i | and u z&lt;t i represent the first t tokens of u i when permuting the sequence according to z ? Z |u i | .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hierarchical Encoding</head><p>Capturing dependencies at different granularity levels is key for dialog embedding. Thus, we choose a hierarchical encoder <ref type="bibr">(Chen et al., 2018b;</ref><ref type="bibr">Li et al., 2018a)</ref>. It is composed of two functions f u and f c , satisfying:</p><formula xml:id="formula_2">E u i = f u ? (? 1 , . . . , ? |u i | ) (3) E C j = f d ? (E u 1 , . . . , E C j )<label>(4)</label></formula><p>where E u i ? R du is the embedding of u i and E C j ? R d d the embedding of C j . The structure of the hierarchical encoder is depicted in <ref type="figure">Fig</ref> Current self-supervised pre-training objectives such as MLM and GAP are trained at the sequence level, which for us translates to only learning f u ? . In this section, we extend both the MLM and GAP losses at the dialog level in order to pre-train f d ? . Following previous work on both multi-task learning <ref type="bibr">(Argyriou et al., 2007;</ref><ref type="bibr">Ruder, 2017)</ref> and hierarchical supervision <ref type="bibr">(Garcia et al., 2019;</ref><ref type="bibr">Sanh et al., 2019)</ref>, we argue that optimising simultaneously at both levels rather than separately improves the quality of the resulting embeddings. Thus, we write our global hierarchical loss as:</p><formula xml:id="formula_3">L(?) = ? u * L u (?) + ? d * L d (?)<label>(5)</label></formula><p>where L u (?) is either the MLM or GAP loss at the utterance level and L d (?) is its generalisation at the dialog level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">MLM Loss</head><p>The MLM loss at the utterance level is defined in Equation 1. Our generalisation at the dialog level masks a proportion p C of utterances and generates the sequences of masked tokens (a concrete example can be found in Appendix B). Thus, at the dialog level the MLM loss is defined as:</p><formula xml:id="formula_4">L d MLM (?, C k ) = E ? ? j?m C k |u j | i=1 log(p ? (? j i |C k )) ? ? (6) where m C k j ? unif{1, |C k |} ? j ? [1, p C ]</formula><p>is the set of positions of masked utterances in the context C k ,C k is the corrupted context, and p C is the proportion of masked utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3">GAP Loss</head><p>The GAP loss at the utterance level is defined in Equation 2. A possible generalisation of the GAP at the dialog level is to compute the loss of the generated utterance across all factorization orders of the context utterances. Formally, the GAP loss is defined at the dialog level as:</p><formula xml:id="formula_5">L d GAP (?, C k ) = E ? ? E z?Z T |C k | t=1 |uz t | i=1 log p ? (? zt i |C z&lt;t k ) ? ?<label>(7)</label></formula><p>where ? zt i denotes the first i-th tokens of the permuted t-th utterance when permuting the context according to z ? Z T and C z&lt;t k the first t utterances of C k when permuting the context according to z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Architecture</head><p>Commonly, The functions f u ? and f d ? are either modelled with recurrent cells <ref type="bibr">(Serban et al., 2015)</ref> or Transformer blocks <ref type="bibr">(Vaswani et al., 2017)</ref>. Transformer blocks are more parallelizable, offering shorter paths for the forward and backward signals and requiring significantly less time to train compared to recurrent layers. To the best of our knowledge this is the first attempt to pre-train a hierarchical encoder based only on transformers 3 . The structure of the model can be found in <ref type="figure" target="#fig_0">Figure 1</ref>. In order to optimize dialog level losses as described in Equation 5, we generate (through g dec ? ) the sequence with a Transformer Decoder (T dec ). For downstream tasks, the context embedding E C k is fed to a simple MLP (simple classification), or to a CRF/GRU/LSTM (sequential prediction) -see Appendix B for more details. In the rest of the paper, we will name our hierarchical transformer-based encoder HT and the hierarchical RNN-based encoder HR. We use ? x y to refer to the set of model parameters learnt using the pre-training objective y (either MLM or GAP) at the level x 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Pre-training Datasets</head><p>Datasets used to pre-train dialog encoders <ref type="bibr">(Hazarika et al., 2019;</ref><ref type="bibr">Mehri et al., 2019)</ref>   <ref type="formula">(3)</ref> OpenSubtitles is an order of magnitude larger than any other spoken language dataset used in previous work. We segment OpenSubtitles by considering the duration of the silence between two consecutive utterances. Two 3 Although it is possible to relax the fixed size imposed by transformers <ref type="bibr">(Dai et al., 2019)</ref> in this paper we follow <ref type="bibr">(Colombo et al., 2020)</ref> and fix the context size to 5 and the max utterance length to 50 -these choices are made to work with OpenSubtitles, since the number of available dialogs drops when considering a number of utterances greater than 5.</p><p>4 if x = u solely utterance level training is used, if x = d solely dialog level is used and if x = u, d multi level supervision is used (?u, ? d ? {0, 1} 2 according to the case.) 5 http://opus.nlpl.eu/OpenSubtitles-alt-v2018.php consecutive utterances belong to the same conversation if the silence is shorter than ? T 6 . Conversations shorter than the context size T are dropped 7 . After preprocessing, Opensubtitles contains subtitles from 446520 movies or series which represent 54642424 conversations and over 2.3 billion of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Baseline Encoder</head><p>We compare the different methods we presented with two different types of baseline encoders: pre-trained encoders, and hierarchical encoders based on recurrent cells. The latter, achieve current SOTA performance in many sequence labelling tasks <ref type="bibr">(Li et al., 2018a;</ref><ref type="bibr">Colombo et al., 2020;</ref><ref type="bibr">Lin et al., 2017)</ref>. Pre-trained Encoder Models. We use BERT (Devlin et al., 2018) through the pytorch implementation provided by the Hugging Face transformers library <ref type="bibr">(Wolf et al., 2019)</ref>. The pre-trained model is fed with a concatenation of the utterances. Formally given an input</p><formula xml:id="formula_6">context C k = (u 1 , . . . u T ) the concatenation [u 1 , . . . , u T ] is fed to BERT.</formula><p>Hierarchical Recurrent Encoders. In this work we rely on our own implementation of the model based on HR. Hyperparameters are described in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation of Sequence Labelling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Related Work</head><p>Sequence labelling tasks for spoken dialog mainly involve two different types of labels: DA and E/S. Early work has tackled the sequence labelling problem as an independent classification of each utterance. Deep neural network models that currently achieve the best results <ref type="bibr">(Keizer et al., 2002;</ref><ref type="bibr">Surendran and Levow, 2006;</ref><ref type="bibr">Stolcke et al., 2000)</ref> model both contextual dependencies between utterances <ref type="bibr">(Colombo et al., 2020;</ref><ref type="bibr">Li et al., 2018b)</ref> and labels <ref type="bibr">(Chen et al., 2018b;</ref><ref type="bibr">Kumar et al., 2018;</ref><ref type="bibr">Li et al., 2018c)</ref>.</p><p>The aforementioned methods require large corpora to train models from scratch, such as: Switchboard Dialog Act </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Presentation of SILICONE</head><p>Despite the similarity between methods usually employed to tackle DA and E/S sequential classification, studies usually rely on a single type of label. Moreover, despite the variety of small or medium-sized labelled datasets, evaluation is usually done on the largest available corpora (e.g., SwDA, MRDA). We introduce SILICONE, a collection of sequence labelling tasks, gathering both DA and E/S annotated datasets. SILICONE is built upon preexisting datasets which have been considered by the community as challenging and interesting. Any model that is able to process multiple sequences as inputs and predict the corresponding labels can be evaluated on SILICONE. We especially include small-sized datasets, as we believe it will ensure that well-performing models are able to both distil substantial knowledge and adapt to different sets of labels without relying on a large number of examples. The description of the datasets composing the benchmark can be found in the following sections, while corpora statistics are gathered in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">DA Datasets</head><p>Switchboard Dialog Act Corpus (SwDA) is a telephone speech corpus consisting of twosided telephone conversations with provided topics. This dataset includes additional features such as speaker id and topic information. The SOTA model, based on a seq2seq architecture with guided attention, reports an accuracy of 85.5% (Colombo et al., 2020) on the official split. ICSI MRDA Corpus (MRDA) has been introduced by <ref type="bibr">Shriberg et al. (2004)</ref>. It contains transcripts of multi-party meetings handannotated with DA. It is the second biggest dataset with around 110k utterances. The SOTA model reaches an accuracy of 92.2% <ref type="bibr">(Li et al., 2018a)</ref> and uses Bi-LSTMs with attention as encoder as well as additional features, such as the topic of the transcript. DailyDialog Act Corpus (DyDA a ) has been produced by <ref type="bibr">Li et al. (2017)</ref>. It contains multiturn dialogues, supposed to reflect daily communication by covering topics about daily life. The dataset is manually labelled with dialog act and emotions. It is the third biggest corpus of SILICONE with 102k utterances. The SOTA model reports an accuracy of 88.1% (Li et al., 2018a), using Bi-LSTMs with attention as well as additional features. We follow the official split introduced by the authors. HCRC MapTask Corpus (MT) has been introduced by <ref type="bibr">(Thompson et al., 1993)</ref>. To build this corpus, participants were asked to collaborate verbally by describing a route from a first participant's map by using the map of another participant. This corpus is small (27k utterances). As there is no standard train/dev/test split 8 performances depends on the split. Tran et al. (2017) make use of a Hierarchical LSTM encoder with a GRU decoder layer and achieves an accuracy of 65.9%. Bt Oasis Corpus (Oasis) contains the transcripts of live calls made to the BT and operator services. This corpus has been introduced by <ref type="bibr">(Leech and Weisser, 2003)</ref> and is rather small (15k utterances). There is no standard train/dev/test split 9 and few studies use this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">S/E Datasets</head><p>In S/E recognition for spoken language, there is no consensus on the choice the evaluation metric (e.g., Ghosal et al. by reporting the accuracy. Additionally, emotion/sentiment labels are neither merged nor prepossessed 10 . <ref type="bibr">8</ref> We split according to the code in https://github.com/NathanDuran/Maptask-Corpus. <ref type="bibr">9</ref> We use a random split from https://github.com/NathanDuran/BT-Oasis-Corpus.</p><p>10 Comparison with concurrent work is more difficult as system performance heavily depends on the number of classes and label processing varies across studies DailyDialog Emotion Corpus (DyDA e ) has been previously introduced and contains eleven emotional labels. The SOTA model <ref type="bibr">(De Bruyne et al., 2019)</ref> is based on BERT with additional Valence Arousal and Dominance features and reaches an accuracy of 85% on the official split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multimodal</head><p>EmotionLines Dataset (MELD) has been created by enhancing and extending EmotionLines dataset <ref type="bibr">(Chen et al., 2018a)</ref> where multiple speakers participated in the dialogues. There are two types of annotations MELD s and MELD e : three sentiments (positive, negative and neutral) and seven emotions (anger, disgust, fear, joy,neutral, sadness and surprise). The SOTA model with text only is proposed by <ref type="bibr">Zhang et al. (2019b)</ref> and is inspired by quantum physics. On the official split, it is compared with a hierarchical bi-LSTM, which it beats with an accuracy of 61.9% (MELD s ) and 67.9% (MELD e ) against 60.8% and 65.2. IEMOCAP database (IEMO) is a multimodal database of ten speakers. It consists of dyadic sessions where actors perform improvisations or scripted scenarios. Emotion categories are: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise, and other. There is no official split on this dataset. One proposed model is built with bi-LSTMs and achieves 35.1%, with text only <ref type="bibr">(Zhang et al., 2019b)</ref>. SEMAINE database (SEM) comes from the Sustained Emotionally coloured Machine human Interaction using Nonverbal Expression project <ref type="bibr">(Mckeown et al., 2013)</ref>. This dataset has been annotated on three sentiments labels: positive, negative and neutral by <ref type="bibr">Barriere et al. (2018)</ref>. It is built on Multimodal Wizard of Oz experiment where participants held conversations with an operator who adopted various roles designed to evoke emotional reactions. There is no official split on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results on SILICONE</head><p>This section gathers experiments performed on the SILICONE benchmark. We first analyse an appropriate choice for the decoder, which is selected over a set of experiments on our baseline encoders: a pre-trained BERT model and a <ref type="bibr">(Clavel and Callejas, 2015)</ref>.</p><p>hierarchical RNN-based encoder (HR). Since we focus on small-sized pre-trained representations, we limit the sizes of our pre-trained models to TINY and SMALL (see <ref type="table" target="#tab_10">Table 7</ref>). We then study the results of the baselines and our hierarchical transformer encoders (HT ) on SILICONE along three axes: the accuracy of the models, the difference in performance between the E/S and the DA corpora, and the importance of pre-training. As we aim to obtain robust representations, we do not perform an exhaustive grid search on the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decoder Choice</head><p>Current research efforts focus on single label prediction, as it seems to be a natural choice for sequence labelling problems (subsection 2.1). Sequence labelling is usually performed with CRFs <ref type="bibr">(Chen et al., 2018b;</ref><ref type="bibr">Kumar et al., 2018)</ref> and GRU decoding (Colombo et al., 2020), however, it is not clear to what extent inter-label dependencies are already captured by the contextualised encoders, and whether a plain MLP decoder could achieve competitive results. As can be seen in <ref type="table" target="#tab_4">Table 3</ref>, we found that in the case of E/S prediction there is no clear difference between CRFs and MLPs, while GRU decoders exhibit poor performance, probably due to a lack of training data. It is also important to notice, that training a sequential decoder usually requires thorough hyper-parameter fine-tuning. As our goal is to learn and evaluate general representations that are decoder agnostic, in the following, we will use a plain MLP decoder for all the models compared. <ref type="table" target="#tab_6">Table 4</ref> provides an exhaustive comparison of the different encoders over the SILICONE benchmark. As previously discussed, we adopt a plain MLP as a decoder to compare the different encoders. We show that SILICONE covers a set of challenging tasks as the best performing model achieves an average accuracy of 74.3. Moreover, we observe that despite having half the parameters of a BERT model, our proposed model achieves an average result that is 2% higher on the benchmark. SILICONE covers two different sequence labelling tasks: DA and E/S. In <ref type="table" target="#tab_6">Table 4</ref> and <ref type="table" target="#tab_4">Table 3</ref>, we can see that all models exhibit a consistently higher   average accuracy (up to 14%) on DA tagging compared to E/S prediction. This performance drop could be explained by the different sizes of the corpora (see <ref type="table" target="#tab_3">Table 2</ref>). Despite having a larger number of utterances per label (u/l), E/S tasks seem generally harder to tackle for the models. For example, on Oasis, where the u/l is inferior than those of most E/S datasets (MELD s , MELD e , IEMO and SEM), models consistently achieve better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">General Performance Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Importance of Pre-training for SILICONE</head><p>Results reported in <ref type="table" target="#tab_6">Table 4</ref> and <ref type="table" target="#tab_4">Table 3</ref> show that pre-trained transformer-based encoders achieve consistently higher accuracy on SILICONE, even when they are not explicitly considering the hierarchical structure. This difference can be observed both in smallsized datasets (e.g. MELD and SEM) and in medium/large size datasets (e.g SwDA and MRDA). To validate the importance of pretraining in a regime of low data, we train different HT (with random initialisation) on different portions of SEM and MELD s . Results  shown in <ref type="figure" target="#fig_5">Figure 2</ref> illustrate the importance of pre-trained representations.</p><formula xml:id="formula_7">( u MLM ) (TINY) ( d MLM ) (TINY) ( u, d MLM ) (TINY) ( random ) (TINY)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Model Analysis</head><p>In this section, we dissect our hierarchical pretrained models in order to better understand the relative importance of each component. We show how a hierarchical encoder allows us to obtain a light and efficient model. Additional experiments can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pre-training on Spoken vs Written Data</head><p>First, we explore the differences in training representations on spoken and written corpora. Experimentally, we compare the predictions on SILICONE made by HT (? u M LM ) and the one made by HT (? BERT ?2layers ). The latter is a    <ref type="bibr">(Devlin et al., 2018)</ref>) of the second layer of BERT. In both cases, predictions are performed using an MLP 11 . Results in <ref type="table" target="#tab_7">Table 5</ref> show higher accuracy when the pre-training is performed on spoken data. Since SILICONE is a spoken language benchmark, this result might be due to the specific features of colloquial speech (e.g. disfluencies, sentence length, vocabulary, word frequencies).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Hierarchy and Multi-Level Supervision</head><p>We study the relative importance of three aspects of our hierarchical pre-training with multi-level supervision. We first show that accounting for the hierarchy increases the performance of fine-tuned encoders, even without our specific pre-training procedure. We then compare our two proposed hierarchical pretraining procedures based on the GAP or MLM loss. Lastly, we look at the contribution of the possible levels of supervision on reduced training data from SEM. <ref type="bibr">11</ref> We consider the two first layer for a fair comparison based on the number of model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Importance of hierarchical fine-tuning</head><p>We compare the performance of BERT-4layers with the HT (? BERT ?2layer ) previously described. Results reported in <ref type="table" target="#tab_7">Table 5</ref> demonstrate that fine-tuning on downstream tasks with a hierarchical encoder yields to higher accuracy, with fewer parameters, even when using already pre-trained representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">MLM vs GAP</head><p>In this experiment, we compare the different pre-training objectives at utterance and dialog level. As a reminder HT (? u M LM ) and HT (? u GAP ) are respectively trained using the standard MLM loss (Devlin et al., 2018) and the standard GAP loss <ref type="bibr">(Yang et al., 2019)</ref>. In <ref type="table" target="#tab_9">Table 6</ref> we report the different pre-training objective results. We observe that pre-training at the dialog level achieves comparable results to the utterance level pre-training for MLM and slightly worse for GAP. Interestingly, we observe that HT (? u GAP ) compared to HT (? u M LM ) achieves worse results, which is not consistent with the performance observed on other benchmarks, such as GLUE <ref type="bibr">(Wang et al., 2018)</ref>. The lower accuracy of the models trained using a GAP-based loss could be due to several factors (e.g., model size, pre-training using the GAP loss could require a finer choice of hyperparameters). Finally, we see that supervising at both dialog and utterance level helps for MLM 12 .   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Multi level Supervision for pre-training</head><p>In this section, we illustrate the advantages of learning using several levels of supervision on small datasets. We fine-tune different model on SEM using different size of the training set. Results are shown in <ref type="figure" target="#fig_5">Figure 2</ref>. Overall we see that introducing sequence level supervision induces a consistent improvement on SEM. Results on MELD s are provided in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Other advantages of hierarchy</head><p>Introducing a hierarchical design in the encoder allows to break dialog into utterances and to consider inputs of size </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we propose a hierarchical transformer-based encoder tailored for spoken dialog. We extend two well-known pre-training objectives to adapt them to a hierarchical setting and use OpenSubtitles, the largest spoken language dataset available, for encoder pretraining. Additionally, we provide an evaluation benchmark dedicated to comparing sequence labelling systems for the NLP community, SILICONE, on which we compare our models and pre-training procedures with previous approaches. By conducting ablation studies, we demonstrate the importance of using a hierarchical structure for the encoder, both for pre-training and fine-tuning. Finally, we find that our approach is a powerful method to learn generic representations on spoken dialog, with less parameters than state-of-the-art transformer models. These results open new future research directions: (1) to investigate new pre-training objectives leveraging the hierarchical framework in order to achieve better results on SILICONE while keeping light models (2) to provide multilingual models using the whole pre-training corpus (OpenSubtitles) available in 62 languages, (3) investigate robust methods <ref type="bibr">(Staerman et al., 2020a)</ref> and the application of our embedding to different anomaly detection settings <ref type="bibr">(Staerman et al., 2019</ref><ref type="bibr">(Staerman et al., , 2020b</ref>. We hope that the SILICONE benchmark, experimental results, and publicly available code encourage further research to build stronger sequence labelling systems for NLP.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Details on data composing SILICONE</head><p>In this section, we illustrate the diversity of the dataset composing SILICONE. In <ref type="figure" target="#fig_6">Figure 3</ref>, we plot two histograms representing the different utterance lengths for DA and E/S. As expected, for spoken dialog, lengths are shorter than for written benchmarks (e.g., GLUE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Details for Models</head><p>In this section we report model hyperparameters and as well as additional descriptions of our baselines. For all models we use a tokenizer based on WordPiece <ref type="bibr">(Wu et al., 2016)</ref>. We also provide a concrete example of corrupted context for the MLM Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hierarchical pre-training</head><p>We report in <ref type="table" target="#tab_13">Table 8</ref> the main hyperparameters used fo our model pre-training. We used GELU (Hendrycks and Gimpel, 2016) activations and the dropout rate (Srivastava et al., 2014) is set to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 MLM Loss example</head><p>In this section we propose a visual illustration of the corrupted context <ref type="figure">Figure 4</ref> by the MLM Loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Experimental Hyper-parameters for SILICONE</head><p>For all models, we use a batch size of 64 and automatically select the best model on the validation set according to its loss. We do not perform exhaustive grid search either on the learning rate (that is set to 10 ?4 ), nor on other hyper-parameters to perform a fair comparison between all the models. We use ADAMW (Kingma and Ba, 2014; Loshchilov and Hutter, 2017) with a linear scheduler on the learning rate and the number of warm-up steps is set to 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Additional Details on Baselines</head><p>A representation for all the baselines can be found in <ref type="figure">Figure 5</ref>. For all models, both hidden dimension and embedding dimension is set to 768 to ensure fair comparison with the proposed model. The MLP used for decoding contains 3 layers of sizes <ref type="bibr">(768,</ref><ref type="bibr">348,</ref><ref type="bibr">192)</ref>. We use RELU (Agarap, 2018) to introduce non linearity inside our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experimental Results</head><p>In this section we report the detailed results on SILICONE, including the ones presented in Table 4. We report results on two new experiments: importance of pre-training time for both a TINY and SMALL model, we report the convergence time of a TINY model and finally we extend subsubsection 5.2.3 by reporting results on IEMO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Detailed Results on SILICONE</head><p>We show in <ref type="table" target="#tab_16">Table 9</ref> the results on the SILICONE benchmark for all the models mentioned in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Improvement over pre-training</head><p>In this experiment we illustrate how pretraining improves performance on SEM (see <ref type="bibr">Figure 6)</ref>. As expected accuracy improves when pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Multi level Supervision for pre-training MELD</head><p>In this experiment we report results of the experiment mentioned in subsubsection 5.2.3. In this experiment we see that the training process seems to be noisier for fractions lower than 40%. For larger percentages, we observe that including higher supervision (at the dialog level) during pre-training leads to a consistent improvement.  (e) Corrupted context with utterance u4 masked.</p><p>Figure 4: This figure shows an example of corrupted context. Here p C is randmoly set to 2 meaning that two utterances will be corrupted. u 1 and u 4 are randomly picked in 4b, 4d and then masked in 4c, 4e.   In this figure f u ? , f d ? and the sequence label decoder (g dec ? ) are respectively colored in green, blue and red for the hierarchical encoder (see <ref type="figure">Figure 5a</ref> and <ref type="figure">Figure 5d</ref>). For BERT there is no hierarchy and embedding is performed through f u ? colored in grey (see <ref type="figure">Figure 5c</ref>, <ref type="figure">Figure 5d</ref>)   <ref type="figure">Figure 7</ref>: A comparison of different parameters initialisation on MELD s . Training is performed using a different percentage of complete training set. Validation and test set are fixed over all experimentation. Each score is the averaged accuracy over 10 random runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t P h W 6 A 1 X D Q l + O z S E Y z Q G k 8 G c / v k = " &gt; A A</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Negative Results on GAP</head><p>We briefly describe few ideas we tried to make GAP works at both the utterance and dialog level. We hypothesise that:</p><p>? giving the same weight to the utterance level and the dialog level (see <ref type="table" target="#tab_4">Equation 3</ref>) was responsible of the observed plateau. Different combinations lead to fairly poor improvements.</p><p>? the limited model capacity was part of the issue. Larger models does not give the expected results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2. 1</head><label>1</label><figDesc>Pre-training Objectives Our work builds upon existing objectives designed to pre-train encoders: the Masked Language Model (MLM) from Devlin et al. (2018); Liu et al. (2019); Lan et al. (2019); Zhang et al. (2019a) and the Generalized Autoregressive Pre-training (GAP) from Yang et al. (2019).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " b 4 T v 1 + r / 0 N H H 4 6 q A n t U g e Y Q 6 9 2 o = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P Q b 9 c 8 a v + H H i V B D m p o B y N f v m r N 1 A 0 F S A t 5 c S Y b u A n N s y I t o x y m J Z 6 q Y G E 0 D E Z Q t d R S Q S Y M J t f P M V n T h n g W G l X 0 u K 5 + n s i I 8 K Y i Y h c p y B 2 Z J a 9 m f i f 1 0 1 t f B V m T C a p B U k X i + K U Y 6 v w 7 H 0 8 Y B q o 5 R N H C N X M 3 Y r p i G h C r Q u p 5 E I I l l 9 e J a 2 L a l C r 1 u 5 q l f p 1 H k c R n a B T d I 4 C d I n q 6 B Y 1 U B N R J N E z e k V v n v F e v H f v Y 9 F a 8 P K Z Y / Q H 3 u c P 5 M a Q a Q = = &lt; / l a t e x i t &gt; ! 1 L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q g h r 5 a H 4 t Q L M G 7 R V W t 0 F Y f E V O v I = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 4 G v V L Z r / g z o G U S 5 K Q M O e q 9 0 l e 3 r 0 g q q L S E Y 2 M 6 g Z / Y M M P a M s L p p N h N D U 0 w G e E B 7 T g q s a A m z G Y X T 9 C p U / o o V t q V t G i m / p 7 I s D B m L C L X K b A d m k V v K v 7 n d V I b X 4 Y Z k 0 l q q S T z R X H K k V V o + j 7 q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r A u p 6 E I I F l 9 e J s 3 z S l C t V O + q 5 d p V H k c B j u E E z i C A C 6 j B D d S h A Q Q k P M M r v H n G e / H e v Y 9 5 6 4 q X z x z B H 3 i f P w 3 3 k I Q = &lt; / l a t e x i t &gt; ! i L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H f e s P o d r L 2 1 g L 0 R J D e p a N x 9 A Q I o = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 6 y X q n s V / w Z 0 D I J c l K G H P V e 6 a v b V y Q V V F r C s T G d w E 9 s m G F t G e F 0 U u y m h i a Y j P C A d h y V W F A T Z r O L J + j U K X 0 U K + 1 K W j R T f 0 9 k W B g z F p H r F N g O z a I 3 F f / z O q m N L 8 O M y S S 1 V J L 5 o j j l y C o 0 f R / 1 m a b E 8 r E j m G j m b k V k i D U m 1 o V U d C E E i y 8 v k + Z 5 J a h W q n f V c u 0 q j 6 M A x 3 A C Z x D A B d T g B u r Q A A I S n u E V 3 j z j v X j v 3 s e 8 d c X L Z 4 7 g D 7 z P H 2 L X k L w = &lt; / l a t e x i t &gt; ! i 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 8 e I q u 0 Y 6 M Q M u 6 O d 0 N f 4 i B + B / n A = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P r F + u + F V / D r x K g p x U U I 5 G v / z V G y i a C p C W c m J M N / A T G 2 Z E W 0 Y 5 T E u 9 1 E B C 6 J g M o e u o J A J M m M 0 v n u I z p w x w r L Q r a f F c / T 2 R E W H M R E S u U x A 7 M s v e T P z P 6 6 Y 2 v g o z J p P U g q S L R X H K s V V 4 9 j 4 e M A 3 U 8 o k j h G r m b s V 0 R D S h 1 o V U c i E E y y + v k t Z F N a h V a 3 e 1 S v 0 6 j 6 O I T t A p O k c B u k R 1 d I s a q I k o k u g Z v a I 3 z 3 g v 3 r v 3 s W g t e P n M M f o D 7 / M H O b W Q o Q = = &lt; / l a t e x i t &gt; u 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d Y O a v Y 0 F h 1 I 5 0 H 4 8 x m I Y u l Y G o R A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 6 8 y r N b c u r s A W S d e Q W p Q o D W s f g 1 G E U s k K s s E N a b v u b H 1 U 6 o t Z w L n l U F i M K Z s S s f Y z 6 i i E o 2 f L m 6 d k 4 t M G Z E w 0 l k p S x b q 7 4 m U S m N m M s g 6 J b U T s + r l 4 n 9 e P 7 H h j Z 9 y F S c W F V s u C h N B b E T y x 8 m I a 2 R W z D J C m e b Z r Y R N q K b M Z v H k I X i r L 6 + T z l X d a 9 Q b D 4 1 a 8 7 a I o w x n c A 6 X 4 M E 1 N O E e W t A G B h N 4 h l d 4 c 6 T z 4 r w 7 H 8 v W k l P M n M I f O J 8 / P n m N u A = = &lt; / l a t e x i t &gt; u i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 t l / t H V l y j F a x G D 2 b f J l B 4 E Q c 4 4 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 1 4 Z V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j i C U S l W W C G t P 3 3 N j 6 K d W W M 4 H z y i A x G F M 2 p W P s Z 1 R R i c Z P F 7 f O y U W m j E g Y 6 a y U J Q v 1 9 0 R K p T E z G W S d k t q J W f V y 8 T + v n 9 j w x k + 5 i h O L i i 0 X h Y k g N i L 5 4 2 T E N T I r Z h m h T P P s V s I m V F N m s 3 j y E L z V l 9 d J 5 6 r u N e q N h 0 a t e V v E U Y Y z O I d L 8 O A a m n A P L W g D g w k 8 w y u 8 O d J 5 c d 6 d j 2 V r y S l m T u E P n M 8 f k 5 G N 8 A = = &lt; / l a t e x i t &gt; E uc &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r N 3 e 2 E m 7 C 3 c x S 5 T V n H E Z B t v 5 O n 8 = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k U d F k U w W U F + 4 A 2 h M l 0 0 g 6 d T M L M R C w h v + L G h S J u / R F 3 / o 2 T N g t t P T B w O O d e 7 p k T J J w p 7 T j f 1 t r 6 x u b W d m W n u r u 3 f 3 B o H 9 W 6 K k 4 l o R 0 S 8 1 j 2 A 6 w o Z 4 J 2 N N O c 9 h N J c R R w 2 g u m N 4 X f e 6 R S s V g 8 6 F l C v Q i P B Q s Z w d p I v l 0 b R l h P C O b Z b e 5 n q U / y q m / X n Y Y z B 1 o l b k n q U K L t 2 1 / D U U z S i A p N O F Z q 4 D q J 9 j I s N S O c 5 t V h q m i C y R S P 6 c B Q g S O q v G y e P U d n R h m h M J b m C Y 3 m 6 u + N D E d K z a L A T B Z J 1 b J X i P 9 5 g 1 S H V 1 7 G R J J q K s j i U J h y p G N UF I F G T F K i + c w Q T C Q z W R G Z Y I m J N n U V J b j L X 1 4 l 3 Y u G 2 2 w 0 7 5 v 1 1 n V Z R w V O 4 B T O w Y V L a M E d t K E D B J 7 g G V 7 h z c q t F + v d + l i M r l n l z j H 8 g f X 5 A w p g l H E = &lt; / l a t e x i t &gt; E u1&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h l h k s E q P R A H M q u f 5 s S R Y I Q + J z X Q = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k U d F k U w W U F + 4 A 2 h M l 0 0 g 6 d T M L M R C w h v + L G h S J u / R F 3 / o 2 T N g t t P T B w O O d e 7 p k T J J w p 7 T j f 1 t r 6 x u b W d m W n u r u 3 f 3 B o H 9 W 6 K k 4 l o R 0 S 8 1 j 2 A 6 w o Z 4 J 2 N N O c 9 h N J c R R w 2 g u m N 4 X f e 6 R S s V g 8 6 F l C v Q i P B Q s Z w d p I v l 0 b R l h P C O b Z b e 5 n q e / m V d + u O w 1 n D r R K 3 J L U o U T b t 7 + G o 5 i k E R W a c K z U w H U S 7 W V Y a k Y 4 z a v D V N E E k y k e 0 4 G h A k d U e d k 8 e 4 7 O j D J C Y S z N E x r N 1 d 8 b G Y 6 U m k W B m S y S q m W v E P / z B q k O r 7 y M i S T V V J D F o T D l S M e o K A K N m K R E 8 5 k h m E h m s i I y w R I T b e o q S n C X v 7 x K u h c N t 9 l o 3 j f r r e u y j g q c w C m c g w u X 0 I I 7 a E M H C D z B M 7 z C m 5 V b L 9 a 7 9 b E Y X b P K n W P 4 A + v z B 7 4 l l D 8 = &lt; / l a t e x i t &gt; u g &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g R E 4 L z Y Q a 2 f U C E I i + I w 3 S c M 9 7 3 Y = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 i 7 d 3 Y T d j V B C / 4 I X D 4 p 4 9 Q 9 5 8 9 + Y t D l o 6 4 O B x 3 s z z M w L Y s G N d d 1 v Z 2 1 9 Y 3 N r u 7 R T 3 t 3 b P z i s H B 2 3 T Z R o h i 0 W i U h 3 A 2 p Q c I U t y 6 3 A b q y R y k B g J 5 j c 5 n 7 n C b X h k X q w 0 x h 9 S U e K h 5 x R m 0 v J 4 6 g 8 q F T d m j s H W S V e Q a p Q o D m o f P W H E U s k K s s E N a b n u b H 1 U 6 o t Z w J n 5 X 5 i M K Z s Q k f Y y 6 i i E o 2 f z m + d k f N M G Z I w 0 l k p S + b q 7 4 m U S m O m M s g 6 J b V j s + z l 4 n 9 e L 7 H h t Z 9 y F S c W F V s s C h N B b E T y x 8 m Q a 2 R W T D N C m e b Z r Y S N q a b M Z v H k I X j L L 6 + S 9 m X N q 9 f q 9 / V q 4 6 a I o w S n c A Y X 4 M E V N O A O m t A C B m N 4 h l d 4 c 6 T z 4 r w 7 H 4 v W N a e Y O Y E / c D 5 / A I 8 B j e 0 = &lt; / l a t e x i t &gt; L u &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 6 v i q 6 Y j b g v e r G w w 6 p j F B y p c r 7 7 c = " &gt; A A A B 9 H i c b V D L S g M x F L 1 T X 7 W + q i 7 d B I v g q s x I Q Z d F N y 5 c V L A P a M e S S T N t a C Y Z k 0 y h D P 0 O N y 4 U c e v H u P N v z L S z 0 N Y D g c M 5 9 3 J P T h B z p o 3 r f j u F t f W N z a 3 i d m l n d 2 / / o H x 4 1 N I y U Y Q 2 i e R S d Q K s K W e C N g 0 z n H Z i R X E U c N o O x j e Z 3 5 5 Q p Z k U D 2 Y a U z / C Q 8 F C R r C x k t + L s B k R z N O 7 2 W P S L 1 f c q j s H W i V e T i q Q o 9 E v f / U G k i Q R F Y Z w r H X X c 2 P j p 1 g Z R j i d l X q J p j E m Y z y k X U s F j q j 2 0 3 n o G T q z y g C F U t k n D J q r v z d S H G k 9 j Q I 7 m Y X U y 1 4 m / u d 1 E x N e + S k T c W K o I I t D Y c K R k S h r A A 2 Y o s T w q S W Y K G a z I j L C C h N j e y r Z E r z l L 6 + S 1 k X V q 1 V r 9 7 V K / T q v o w g n c A r n 4 M E l 1 O E W G t A E A k / w D K / w 5 k y c F + f d + V i M F p x 8 5 x j + w P n 8 A R Z 5 k l A = &lt; / l a t e x i t &gt; L d &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " c u A Z V E T v n F m x d 0 r S w y V C 0 j W F I e E = " &gt; A A A B 9 H i c b V D L S s N A F L 2 p r 1 p f V Z d u B o v g q i R S 0 G X R j Q s X F e w D 2 l g m k 0 k 7 d D K J M 5 N C C f 0 O N y 4 U c e v H u P N v n K R Z a O u B g c M 5 9 3 L P H C / m T G n b / r Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q K O i R B L a J h G P Z M / D i n I m a F s z z W k v l h S H H q d d b 3 K T + d 0 p l Y p F 4 k H P Y u q G e C R Y w A j W R n I H I d Z j g n l 6 N 3 / 0 h 9 W a X b d z o F X i F K Q G B V r D 6 t f A j 0 g S U q E J x 0 r 1 H T v W b o q l Z o T T e W W Q K B p j M s E j 2 j d U 4 J A q N 8 1 D z 9 G Z U X w U R N I 8 o V G u / t 5 I c a j U L P T M Z B Z S L X u Z + J / X T 3 R w 5 a Z M x I m m g i w O B Q l H O k J Z A 8 h n k h L N Z 4 Z g I p n J i s g Y S 0 y 0 6 a l i S n C W v 7 x K O h d 1 p 1 F v 3 D d q z e u i j j K c w C m c g w O X 0 I R b a E E b C D z B M 7 z C m z W 1 X q x 3 6 2 M x W r K K n W P 4 A + v z B / y m k j 8 = &lt; / l a t e x i t &gt; N u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D i t c P r l / H K t R i T i v S P 6 y / H u 8 O j k = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 g G 8 p m u 2 m X b j Z h d y K U 0 H / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D P z 2 0 9 c G x G r B 5 w k 3 I / o U I l Q M I p W e r z r p 6 S H I u K m X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P S 7 3 U 8 I S y M R 3 y r q W K 2 i V + N r 9 4 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h l Z 8 J l a T I F V s s C l N J M C a z 9 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 L J h u A t v 7 x K W h d V r 1 a t 3 d c q 9 e s 8 j i K c w C m c g w e X U I d b a E A T G C h 4 h l d 4 c 4 z z 4 r w 7 H 4 v W g p P P H M M f O J 8 / P h O Q p Q = = &lt; / l a t e x i t &gt; N d ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d z L K j x l h C N b l c V J 6 Z I V q O / 7 B e 7 4 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 g G 8 p m s 2 m X b j Z h d y K U 0 n / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B a k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l k k y z X i T J T L R n Y A a L o X i T R Q o e S f V n M a B 5 O 1 g d D P z 2 0 9 c G 5 G o B x y n 3 I / p Q I l I M I p W e r z r h 6 S H I u a m X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u q F C c t i r p B J a k z X c 1 P 0 J 1 S j Y J J P S 7 3 M 8 J S y E R 3 w r q W K 2 i X + Z H 7 x l J x Z J S R R o m 0 p J H P 1 9 8 S E x s a M 4 8 B 2 x h S H Z t m b i f 9 5 3 Q y j K 3 8 i V J o h V 2 y x K M o k w Y T M 3 i e h 0 J y h H F t C m R b 2 V s K G V F O G N q S S D c F b f n m V t C 6 q X q 1 a u 6 9 V 6 t d 5 H E U 4 g V M 4 B w 8 u o Q 6 3 0 I A m M F D w D K / w 5 h j n x X l 3 P h a t B S e f O Y Y / c D 5 / A C P Y k J Q = &lt; / l a t e x i t &gt; ! g i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L T o A F H D j 3 / N K N d r Y Q L O 6 s 9 G o c y s = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 0 l s 1 2 k y 7 d 7 I b d j V B C f 4 Y X D 4 p 4 9 d d 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X p p x p 4 7 r f z t r 6 x u b W d m W n u r u 3 f 3 B Y O z r u a p k p Q j t E c q n 6 I d a U M 0 E 7 h h l O + 6 m i O A k 5 7 Y W T 2 8 L v P V G l m R Q P Z p r S I M G x Y B E j 2 F j J H 8 i E x n j I H u P q s F Z 3 G + 4 c a J V 4 J a l D i f a w 9 j U Y S Z I l V B j C s d a + 5 6 Y m y L E y j H A 6 q w 4 y T V N M J j i m v q U C J 1 Q H + f z k G T q 3 y g h F U t k S B s 3 V 3 x M 5 T r S e J q H t T L A Z 6 2 W v E P / z / M x E 1 0 H O R J o Z K s h i U Z R x Z C Q q / k c j p i g x f G o J J o r Z W x E Z Y 4 W J s S k V I X j L L 6 + S 7 m X D a z a a 9 8 1 6 6 6 a M o w K n c A Y X 4 M E V t O A O 2 t A B A h K e 4 R X e H O O 8 O O / O x 6 J 1 z S l n T u A P n M 8 f x A 2 Q 6 w = = &lt; / l a t e x i t &gt; ! g L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F P 7 p j z R W F y P L Y J 5 u 9 + t t 9 w 3 K f k 0 = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F w 8 e K t g P S G P Z b D f p 0 t 1 s 2 J 0 I p f R n e P G g i F d / j T f / j Z s 2 B 2 1 9 M P B 4 b 4 a Z e W E q u A H X / X Z K a + s b m 1 v l 7 c r O 7 t 7 + Q f X w q G N U p i l r U y W U 7 o X E M M E T 1 g Y O g v V S z Y g M B e u G 4 5 v c 7 z 4 x b b h K H m C S s k C S O O E R p w S s 5 P e V Z D E Z 3 D 3 G l U G 1 5 t b d O f A q 8 Q p S Q w V a g + p X f 6 h o J l k C V B B j f M 9 N I Z g S D Z w K N q v 0 M 8 N S Q s c k Z r 6 l C Z H M B N P 5 y T N 8 Z p U h j p S 2 l Q C e q 7 8 n p k Q a M 5 G h 7 Z Q E R m b Z y 8 X / P D + D 6 C q Y 8 i T N g C V 0 s S j K B A a F 8 / / x k G t G Q U w s I V R z e y u m I 6 I J B Z t S H o K 3 / P I q 6 V z U v U a 9 c d + o N a + L O M r o B J 2 i c + S h S 9 R E t 6 i F 2 o g i h Z 7 R K 3 p z w H l x 3 p 2 P R W v J K W a O 0 R 8 4 n z + X w p D O &lt; / l a t e x i t &gt; ! g 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 / 5 J k J F 6 d j V J n d u j h m I L N z 5 N r 3 I = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 0 l s 1 2 k i 7 d 7 I b d j V B C f 4 Y X D 4 p 4 9 d d 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X p p x p 4 7 r f z t r 6 x u b W d m W n u r u 3 f 3 B Y O z r u a p k p C h 0 q u V T 9 k G j g T E D H M M O h n y o g S c i h F 0 5 u C 7 / 3 B E o z K R 7 M N I U g I b F g E a P E W M k f y A R i M v Q e 4 + q w V n c b 7 h x 4 l X g l q a M S 7 W H t a z C S N E t A G M q J 1 r 7 n p i b I i T K M c p h V B 5 m G l N A J i c G 3 V J A E d J D P T 5 7 h c 6 u M c C S V L W H w X P 0 9 k Z N E 6 2 k S 2 s 6 E m L F e 9 g r x P 8 / P T H Q d 5 E y k m Q F B F 4 u i j G M j c f E / H j E F 1 P C p J Y Q q Z m / F d E w U o c a m V I T g L b + 8 S r q X D a / Z a N 4 3 6 6 2 b M o 4 K O k V n 6 A J 5 6 A q 1 0 B 1 q o w 6 i S K J n 9 I r e H O O 8 O O / O x 6 J 1 z S l n T t A f O J 8 / b o W Q s w = = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f d ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T u X T 7 U 7 F t J O E i v D Z c 4 S a F x J Z y W A = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 0 l s 1 m 0 y 7 d 7 I b d i V B C f 4 Y X D 4 p 4 9 d d 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X p o I b c N 1 v Z 2 1 9 Y 3 N r u 7 J T 3 d 3 b P z i s H R 1 3 j c o 0 Z R 2 q h N L 9 k B g m u G Q d 4 C B Y P 9 W M J K F g v X B y W / i 9 J 6 Y N V / I B p i k L E j K S P O a U g J X 8 e D i A M Q P y G F W H t b r b c O f A q 8 Q r S R 2 V a A 9 r X 4 N I 0 S x h E q g g x v i e m 0 K Q E w 2 c C j a r D j L D U k I n Z M R 8 S y V J m A n y + c k z f G 6 V C M d K 2 5 K A 5 + r v i Z w k x k y T 0 H Y m B M Z m 2 S v E / z w / g / g 6 y L l M M 2 C S L h b F m c C g c P E / j r h m F M T U E k I 1 t 7 d i O i a a U L A p F S F 4 y y + v k u 5 l w 2 s 2 m v f N e u u m j K O C T t E Z u k A e u k I t d I f a q I M o U u g Z v a I 3 B 5 w X 5 9 3 5 W L S u O e X M C f o D 5 / M H z r G Q 8 g = = &lt; / l a t e x i t &gt; g dec ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Z P O d L w 8 Z j 6 x Y n 8 K 1 M k i o 1 o + 0 c 4 c = " &gt; A A A B + H i c b V B N S 8 N A E N 3 U r 1 o / G v X o J V g E T y W R g h 6 L X j x W s K 3 Q x r D Z T N q l m 0 3 Y n Q g 1 9 J d 4 8 a C I V 3 + K N / + N S Z u D t j 4 Y e L w 3 w 8 w 8 P x F c o 2 1 / G 5 W 1 9 Y 3 N r e p 2 b W d 3 b 7 9 u H h z 2 d J w q B l 0 W i 1 j d + 1 S D 4 B K 6 y F H A f a K A R r 6 A v j + 5 L v z + I y j N Y 3 m H 0 w T c i I 4 k D z m j m E u e W R 9 5 Q x w D 0 o c s A D a r e W b D b t p z W K v E K U m D l O h 4 5 t c w i F k a g U Q m q N Y D x 0 7 Q z a h C z g T M a s N U Q 0 L Z h I 5 g k F N J I 9 B u N j 9 8 Z p 3 m S m C F s c p L o j V X f 0 9 k N N J 6 G v l 5 Z 0 R x r J e 9 Q v z P G 6 Q Y X r o Z l 0 m K I N l i U Z g K C 2 O r S M E K u A K G Y p o T y h T P b 7 X Y m C r K M M + q C M F Z f n m V 9 M 6 b T q v Z u m 0 1 2 l d l H F V y T E 7 I G X H I B W m T G 9 I h X c J I S p 7 J K 3 k z n o w X 4 9 3 4 W L R W j H L m i P y B 8 f k D l 3 m T D A = = &lt; / l a t e x i t &gt; E Ck &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t k r + X i o v j 6 / p h N T W b 0 D F k b X C g D Y = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k U d F k s g s s K 9 g F t C J P p p B 0 6 m Y S Z i V J i P 8 W N C 0 X c + i X u / B s n b R b a e m D g c M 6 9 3 D M n S D h T 2 n G + r b X 1 j c 2 t 7 d J O e X d v / + D Q r h x 1 V J x K Q t s k 5 r H s B V h R z g R t a 6 Y 5 7 S W S 4 i j g t B t M m r n f f a B S s V j c 6 2 l C v Q i P B A s Z w d p I v l 0 Z R F i P C e b Z z c z P m v 5 k 5 t t V p + b M g V a J W 5 A q F G j 5 9 t d g G J M 0 o k I T j p X q u 0 6 i v Q x L z Q i n s / I g V T T B Z I J H t G + o w B F V X j a P P k N n R h m i M J b m C Y 3 m 6 u + N D E d K T a P A T O Z B 1 b K X i / 9 5 / V S H V 1 7 G R J J q K s j i U J h y p G O U 9 4 C G T F K i + d Q Q T C Q z W R E Z Y 4 m J N m 2 V T Q n u 8 p d X S e e i 5 t Z r 9 b t 6 t X F d 1 F G C E z i F c 3 D h E h p w C y 1 o A 4 F H e I Z X e L O e r B f r 3 f p Y j K 5 Z x c 4 x / I H 1 + Q O O z 5 Q z &lt; / l a t e x i t &gt; C k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J R W g L C n Y I 1 z i T S r U A e u r k I w w I B 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M d i L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M G n O / 8 8 S 1 E b F 6 x G n C / Y i O l A g F o 2 i l h 8 Z g M i h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w x s / E y p J k S u 2 X B S m k m B M 5 n + T o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b S v q l 6 t W r u v V e q 3 e R x F O I N z u A Q P r q E O d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t h a c f O Y U / s D 5 / A E V I I 2 s &lt; / l a t e x i t &gt; u |Ck| &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k A S G Q v e A j w G 0 l F B p A Z T u h j t b a 3 0 = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V g h 6 L v X i s Y D + k X Z Z s m m 1 D k + y S Z I W y 7 a / w 4 k E R r / 4 c b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T R W i L x D x W 3 R B r y p m k L c M M p 9 1 E U S x C T j v h u D H 3 O 0 9 U a R b L B z N J q C / w U L K I E W y s 9 J g G 2 b Q R j K e z o F x x q + 4 C a J 1 4 O a l A j m Z Q / u o P Y p I K K g 3 h W O u e 5 y b G z 7 A y j H A 6 K / V T T R N M x n h I e 5 Z K L K j 2 s 8 X B M 3 R h l Q G K Y m V L G r R Q f 0 9 k W G g 9 E a H t F N i M 9 K o 3 F / / z e q m J b v y M y S Q 1 V J L l o i j l y M R o / j 0 a M E W J 4 R N L M F H M 3 o r I C C t M j M 2 o Z E P w V l 9 e J + 2 r q l e r 1 u 5 r l f p t H k c R z u A c L s G D a 6 j D H T S h B Q Q E P M M r v D n K e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H z I u k K w = &lt; / l a t e x i t &gt; ! |Ck| L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t D 2 J S u P E W a 7 f x / Y u L D P + c 4 B v 3 S c = " &gt; A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I p 6 L L Y j Q s X F e w D 2 h g m 0 0 k 7 d B 5 h Z l I o a f / E j Q t F 3 P o n 7 v w b p 2 0 W 2 n r g w u G c e 7 n 3 n i h h V B v P + 3 Y K G 5 t b 2 z v F 3 d L e / s H h k X t 8 0 t I y V Z g 0 s W R S d S K k C a O C N A 0 1 j H Q S R R C P G G l H o / r c b 4 + J 0 l S K R z N J S M D R Q N C Y Y m S s F L p u T 3 I y Q O H 9 U z a t h 6 P p L H T L X s V b A K 4 T P y d l k K M R u l + 9 v s Q p J 8 J g h r T u + l 5 i g g w p Q z E j s 1 I v 1 S R B e I Q G p G u p Q J z o I F t c P o M X V u n D W C p b w s C F + n s i Q 1 z r C Y 9 s J 0 d m q F e 9 u f i f 1 0 1 N f B N k V C S p I Q I v F 8 U p g 0 b C e Q y w T x X B h k 0 s Q V h R e y v E Q 6 Q Q N j a s k g 3 B X 3 1 5 n b S u K n 6 1 U n 2 o l m u 3 e R x F c A b O w S X w w T W o g T v Q A E 2 A w R g 8 g 1 f w 5 m T O i / P u f C x b C 0 4 + c w r + w P n 8 A b 6 F k 7 0 = &lt; / l a t e x i t &gt; ! |Ck| 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G e h 0 P D 0 + 2 O c 4 u H V 2 V V 6 f p N G B o R Y = " &gt; A A A B + X i c b V B N T w I x E J 3 F L 8 S v V Y 9 e G o m J J 7 J r S P R I 5 O I R E w E T W D f d U q C h 2 2 7 a L g l Z + C d e P G i M V / + J N / + N B f a g 4 E s m e X l v J j P z o o Q z b T z v 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 7 v F J S 8 t U E d o k k k v 1 G G F N O R O 0 a Z j h 9 D F R F M c R p + 1 o V J / 7 7 T F V m k n x Y C Y J D W I 8 E K z P C D Z W C l 2 3 K 2 M 6 w K H / l E 3 r 4 W g 6 C 9 2 y V / E W Q O v E z 0 k Z c j R C 9 6 v b k y S N q T C E Y 6 0 7 v p e Y I M P K M M L p r N R N N U 0 w G e E B 7 V g q c E x 1 k C 0 u n 6 E L q / R Q X y p b w q C F + n s i w 7 H W k z i y n T E 2 Q 7 3 q z c X / v E 5 q + j d B x k S S G i r I c l E / 5 c h I N I 8 B 9 Z i i x P C J J Z g o Z m 9 F Z I g V J s a G V b I h + K s v r 5 P W V c W v V q r 3 1 X L t N o + j C G d w D p f g w z X U 4 A 4 a 0 A Q C Y 3 i G V 3 h z M u f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A l M G T o g = = &lt; / l a t e x i t &gt; General structure of our proposed hierarchical dialog encoder, with a decoder: f u ? , f d ? and the sequence label decoder (g dec ? ) are colored respectively in green, blue and red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(SwDA)(Godfrey et al., 1992), Meeting Recorder Dia-log Act (MRDA)(Shriberg et al., 2004), Daily Dialog Act(Li et al., 2017), HCRC Map Task Corpus (MT)(Thompson et al., 1993). This makes harder their adoption to smaller datasets, such as: Loqui human-human dialogue corpus (Loqui) (Passonneau and Sachar., 2014), BT Oasis Corpus (Oasis)(Leech  and Weisser, 2003), Multimodal Multi-Party Dataset (MELD)(Poria et al., 2018a), Interactive emotional dyadic motion capture database (IEMO), SEMAINE database (SEM)(Mckeown  et al., 2013).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(2019); Poria et al. (2018b) use a weighted F-score while Zhang et al. (2019b) report accuracy). For SILICONE, we choose to stay consistent with the DA research and thus follow Zhang et al. (2019b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>A comparison of pre-trained encoders being fine-tuned on different percentage the training set of SEM. Validation and test set are fixed over all experiments, reported scores are averaged over 10 different random split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Histograms showing the utterance length for each dataset of SILICONE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 L</head><label>1</label><figDesc>A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F w 8 K F e w H t K F s t p t 2 6 e 4 m 7 E 6 E U v o X v H h Q x K t / y J v / x q T N Q V s f D D z e m 2 F m X h B L Y d F 1 v 5 3 C 2 v r G 5 l Z x u 7 S z u 7 d / U D 4 8 a t k o M Y w 3 W S Q j 0 w m o 5 V J o 3 k S B k n d i w 6 k K J G 8 H 4 5 v M b z 9 x Y 0 W k H 3 E S c 1 / R o R a h Y B Q z 6 f 6 u U e q X K 2 7 V n Y O s E i 8 n F c j R 6 J e / e o O I J Y p r Z J J a 2 / X c G P 0 p N S i Y 5 L N S L 7 E 8 p m x M h 7 y b U k 0 V t / 5 0 f u u M n K X K g I S R S U s j m a u / J 6 Z U W T t R Q d q p K I 7 s s p e J / 3 n d B M M r f y p 0 n C D X b L E o T C T B i G S P k 4 E w n K G c p I Q y I 9 J b C R t R Q x m m 8 W Q h e M s v r 5 L W R d W r V W s P t U r 9 O o + j C C d w C u f g w S X U 4 R Y a 0 A Q G I 3 i G V 3 h z l P P i v D s f i 9 a C k 8 8 c w x 8 4 n z 8 T i o 2 c &lt; / l a t e x i t &gt; u 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d Y O a v Y 0 F h 1 I 5 0 H 4 8 x m I Y u l Y G o R A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 6 8 y r N b c u r s A W S d e Q W p Q o D W s f g 1 G E U s k K s s E N a b v u b H 1 U 6 o t Z w L n l U F i M K Z s S s f Y z 6 i i E o 2 f L m 6 d k 4 t M G Z E w 0 l k p S x b q 7 4 m U S m N m M s g 6 J b U T s + r l 4 n 9 e P 7 H h j Z 9 y F S c W F V s u C h N B b E T y x 8 m I a 2 R W z D J C m e b Z r Y R N q K b M Z v H k I X i r L 6 + T z l X d a 9 Q b D 4 1 a 8 7 a I o w x n c A 6 X 4 M E 1 N O E e W t A G B h N 4 h l d 4 c 6 T z 4 r w 7 H 8 v W k l P M n M I f O J 8 / P n m N u A = = &lt; / l a t e x i t &gt; u i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 t l / t H V l y j F a x G D 2 b f J l B 4 E Q c 4 4 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S MV G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 1 4 Z V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j i C U S l W W C G t P 3 3 N j 6 K d W W M 4 H z y i A x G F M 2 p W P s Z 1 R R i c Z P F 7 f O y U W m j E g Y 6 a y U J Q v 1 9 0 R K p T E z G W S d k t q J W f Vy 8 T + v n 9 j w x k + 5 i h O L i i 0 X h Y k g N i L 5 4 2 T E N T I r Z h m h T P P s V s I m V F N m s 3 j y E L z V l 9 d J 5 6 r u N e q N h 0 a t e V v E U Y Y z O I d L 8 O A a m n A P L W g D g w k 8 w y u 8 O d J 5 c d 6 d j 2 V r y S l m T u E P n M 8 f k 5 G N 8 A = = &lt; / l a t e x i t &gt; E uc &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r N 3 e 2 E m 7 C 3 c x S 5 T V n H E Z B t v 5 O n 8 = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k U d F k U w W U F + 4 A 2 h M l 0 0 g 6 d T M L M R C w h v + L G h S J u / R F 3 / o 2 T N g t t P T B w O O d e 7 p k T J J w p 7 T j f 1 t r 6 x u b W d m W n u r u 3 f 3 B o H 9 W 6 K k 4 l o R 0 S 8 1 j 2 A 6 w o Z 4 J 2 N N O c 9 h N J c R R w 2 g u m N 4 X f e 6 R S s V g 8 6 F l C v Q i P B Q s Z w d p I v l 0 b R l h P C O b Z b e 5 n q U / y q m / X n Y Y z B 1 o l b k n q U K L t 2 1 / D U U z S i A p N O F Z q 4 D q J 9 j I s N S O c 5 t V h q m i C y R S P 6 c B Q g S O q v G y e P U d n R h m h M J b m C Y 3 m 6 u + N D E d K z a L A T B Z J 1 b J X i P 9 5 g 1 S H V 1 7 G R J J q K s j i U J h y p G N U F I F G T F K i + c w Q T C Q z W R G Z Y I m J N n U V J b j L X 1 4 l 3 Y u G 2 2 w 0 7 5 v 1 1 n V Z R w V O 4 B T O w Y V L a M E d t K E D B J 7 g G V 7 h z c q t F + v d + l i M r l n l z j H 8 g f X 5 A w p g l H E = &lt; / l a t e x i t &gt; E u1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h l h k s E q P R A H M q u f 5 s S R Y I Q + J z X Q = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k U d F k U w W U F + 4 A 2 h M l 0 0 g 6 d T M L M R C w h v + L G h S J u / R F 3 / o 2 T N g t t P T B w O O d e 7 p k T J J w p 7 T j f 1 t r 6 x u b W d m W n u r u 3 f 3 B o H 9 W 6 K k 4 l o R 0 S 8 1 j 2 A 6 w o Z 4 J 2 N N O c 9 h N J c R R w 2 g u m N 4 X f e 6 R S s V g 8 6 F l C v Q i P B Q s Z w d p I v l 0 b R l h P C O b Z b e 5 n q e / m V d + u O w 1 n D r R K 3 J L U o U T b t 7 + G o 5 i k E R W a c K z U w H U S 7 W V Y a k Y 4 z a v D V N E E k y k e 0 4 G h A k d U e d k 8 e 4 7 O j D J C Y S z N E x r N 1 d 8 b G Y 6 U m k W B m S y S q m W v E P / z B q k O r 7 y M i S T V V J D F o T D l S M e o K A K N m K R E 8 5 k h m E h m s i I y w R I T b e o q S n C X v 7 x K u h c N t 9 l o 3 j f r r e u y j g q c w C m c g w u X 0 I I 7 a E M H C D z B M 7 z C m 5 V b L 9 a 7 9 b E Y X b P K n W P 4 A + v z B 7 4 l l D 8 = &lt; / l a t e x i t &gt; N u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D i t c P r l / H K t R i T i v S P 6 y / H u 8 O j k = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 g G 8 p m u 2 m X b j Z h d y K U 0 H / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D P z 2 0 9 c G x G r B 5 w k 3 I / o U I l Q M I p W e r z r p 6 S H I u K m X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P S 7 3 U 8 I S y M R 3 y r q W K 2 i V + N r 9 4 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h l Z 8 J l a T I F V s s C l N J M C a z 9 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 L J h u A t v 7 x K W h d V r 1 a t 3 d c q 9 e s 8 j i K c w C m c g w e X U I d b a E A T G C h 4 h l d 4 c 4 z z 4 r w 7 H 4 v W g p P P H M M f O J 8 / P h O Q p Q = = &lt; / l a t e x i t &gt; N d ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d z L K j x l h C N b l c V J 6 Z I V q O / 7 B e 7 4 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 g G 8 p m s 2 m X b j Z h d y K U 0 n / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B a k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l k k y z X i T J T L R n Y A a L o X i T R Q o e S f V n M a B 5 O 1 g d D P z 2 0 9 c G 5 G o B x y n 3 I / p Q I l I M I p W e r z r h 6 S H I u a m X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u q F C c t i r p B J a k z X c 1 P 0 J 1 S j Y J J P S 7 3 M 8 J S y E R 3 w r q W K 2 i X + Z H 7 x l J x Z J S R R o m 0 p J H P 1 9 8 S E x s a M 4 8 B 2 x h S H Z t m b i f 9 5 3 Q y j K 3 8i V J o h V 2 y x K M o k w Y T M 3 i e h 0 J y h H F t C m R b 2 V s K G V F O G N q S S D c F b f n m V t C 6 q X q 1 a u 6 9 V 6 t d 5 H E U 4 g V M 4 B w 8 u o Q 6 3 0 I A m M F D w D K / w 5 h j n x X l 3 P h a t B S e f O Y Y / c D 5 / A C P Y k J Q = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l Dm 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l Dm 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f d ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T u X T 7 U 7 F t J O E i v D Z c 4 S a F x J Z y W A = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 0 l s 1 m 0 y 7 d 7 I b d i V B C f 4 Y X D 4 p 4 9 d d 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X p o I b c N 1 v Z 2 1 9 Y 3 N r u 7 J T 3 d 3 b P z i s H R 1 3 j c o 0 Z R 2 q h N L 9 k B g m u G Q d 4 C B Y P 9 W M J K F g v X B y W / i 9 J 6 Y N V / I B p i k L E j K S P O a U g J X 8 e D i A M Q P y G F W H t b r b c O f A q 8 Q r S R 2 V a A 9 r X 4 N I 0 S x h E q g g x v i e m 0 K Q E w 2 c C j a r D j L D U k I n Z M R 8 S y V J m A n y + c k z f G 6 V C M d K 2 5 K A 5 + r v i Z w k x k y T 0 H Y m B M Z m 2 S v E / z w / g / g 6 y L l M M 2 C S L h b F m c C g c P E / j r h m F M T U E k I 1 t 7 d i O i a a U L A p F S F 4 y y + v k u 5 l w 2 s 2 m v f N e u u m j K O C T t E Z u k A e u k I t d I f a q I M o U u g Z v a I 3 B 5 w X 5 9 3 5 W L S u O e X M C f o D 5 / M H z r G Q 8 g = = &lt; / l a t e x i t &gt; E Ck &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t k r + X i o v j 6 / p h N T W b 0 D F k b X C g D Y = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k U d F k s g s s K 9 g F t C J P p p B 0 6 m Y S Z i V J i P 8 W N C 0 X c + i X u / B s n b R b a e m D g c M 6 9 3 D M n S D h T 2 n G + r b X 1 j c 2 t 7 d J O e X d v / + D Q r h x 1 V J x K Q t s k 5 r H s B V h R z g R t a 6 Y 5 7 S W S 4 i j g t B t M m r n f f a B S s V j c 6 2 l C v Q i P B A s Z w d p I v l 0 Z R F i P C e b Z z c z P m v 5 k 5 t t V p + b M g V a J W 5 A q F G j 5 9 t d g G J M 0 o k I T j p X q u 0 6 i v Q x L z Q i n s / I g V T T B Z I J H t G + o w B F V X j a P P k N n R h m i M J b m C Y 3 m 6 u + N D E d K T a P A T O Z B 1 b K X i / 9 5 / V S H V 1 7 G R J J q K s j i U J h y p G O U 9 4 C G T F K i + d Q Q T C Q z W R E Z Y 4 m J N m 2 V T Q n u 8 p d X S e e i 5 t Z r 9 b t 6 t X F d 1 F G C E z i F c 3 D h E h p w C y 1 o A 4 F H e I Z X e L O e r B f r 3 f p Y j K 5 Z x c 4 x / I H 1 + Q O O z 5 Q z &lt; / l a t e x i t &gt; C k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J R W g L C n Y I 1 z i T S r U A e u r k I w w I B 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M d i L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M G n O / 8 8 S 1 E b F 6 x G n C / Y i O l A g F o 2 i l h 8 Z g M i h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w x s / E y p J k S u 2 X B S m k m B M 5 n + T o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b S v q l 6 t W r u v V e q 3 e R x F O I N z u A Q P r q E O d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t h a c f O Y U / s D 5 / A E V I I 2 s &lt; / l a t e x i t &gt; u |Ck| &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k A S G Q v e A j w G 0 l F B p A Z T u h j t b a 3 0 = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V g h 6 L v X i s Y D + k X Z Z s m m 1 D k + y S Z I W y 7 a / w 4 k E R r / 4 c b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T R W i L x D x W 3 R B r y p m k L c M M p 9 1 E U S x C T j v h u D H 3 O 0 9 U a R b L B z N J q C / w U L K I E W y s 9 J g G 2 b Q R j K e z o F x x q + 4 C a J 1 4 O a l A j m Z Q / u o P Y p I K K g 3 h W O u e 5 y b G z 7 A y j H A 6 K / V T T R N M x n h I e 5 Z K L K j 2 s 8 X B M 3 R h l Q G K Y m V L G r R Q f 0 9 k W G g 9 E a H t F N i M 9 K o 3 F / / z e q m J b v y M y S Q 1 V J L l o i j l y M R o / j 0 a M E W J 4 R N L M F H M 3 o r I C C t M j M 2 o Z E P w V l 9 e J + 2 r q l e r 1 u 5 r l f p t H k c R z u A c L s G D a 6 j D H T S h B Q Q E P M M r v D n K e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H z I u k K w = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " b 4 T v 1 + r / 0 N H H 4 6 q A n t U g e Y Q 6 9 2 o = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P Q b 9 c 8 a v + H H i V B D m p o B y N f v m r N 1 A 0 F S A t 5 c S Y b u A n N s y I t o x y m J Z 6 q Y G E 0 D E Z Q t d R S Q S Y M J t f P M V n T h n g W G l X 0 u K 5 + n s i I 8 K Y i Y h c p y B 2 Z J a 9 m f i f 1 0 1 t f B V m T C a p B U k X i + K U Y 6 v w 7 H 0 8 Y B q o 5 R N H C N X M 3 Y r p i G h C r Q u p 5 E I I l l 9 e J a 2 L a l C r 1 u 5 q l f p 1 H k c R n a B T d I 4 C d I n q 6 B Y 1 U B N R J N E z e k V v n v F e v H f v Y 9 F a 8 P K Z Y / Q H 3 u c P 5 M a Q a Q = = &lt; / l a t e x i t &gt; ! &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q g h r 5 a H 4 t Q L M G 7 R V W t 0 F Y f E V O v I = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 4 G v V L Z r / g z o G U S 5 K Q M O e q 9 0 l e 3 r 0 g q q L S E Y 2 M 6 g Z / Y M M P a M s L p p N h N D U 0 w G e E B 7 T g q s a A m z G Y X T 9 C p U / o o V t q V t G i m / p 7 I s D B m L C L X K b A d m k V v K v 7 n d V I b X 4 Y Z k 0 l q q S T z R X H K k V V o + j 7 q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r A u p 6 E I I F l 9 e J s 3 z S l C t V O + q 5 d p V H k c B j u E E z i C A C 6 j B D d S h A Q Q k P M M r v H n G e / H e v Y 9 5 6 4 q X z x z B H 3 i f P w 3 3 k I Q = &lt; / l a t e x i t &gt; ! i L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H f e s P o d r L 2 1 g L 0 R J D e p a N x 9 A Q I o = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 6 y X q n s V / w Z 0 D I J c l K G H P V e 6 a v b V y Q V V F r C s T G d w E 9 s m G F t G e F 0 U u y m h i a Y j P C A d h y V W F A T Z r O L J + j U K X 0 U K + 1 K W j R T f 0 9 k W B g z F p H r F N g O z a I 3 F f / z O q m N L 8 O M y S S 1 V J L 5 o j j l y C o 0 f R / 1 m a b E 8 r E j m G j m b k V k i D U m 1 o V U d C E E i y 8 v k + Z 5 J a h W q n f V c u 0 q j 6 M A x 3 A C Z x D A B d T g B u r Q A A I S n u E V 3 j z j v X j v 3 s e 8 d c X L Z 4 7 g D 7 z P H 2 L X k L w = &lt; / l a t e x i t &gt; ! i 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 8 e I q u 0 Y 6 M Q M u 6 O d 0 N f 4 i B + B / n A = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P r F + u + F V / D r x K g p x U U I 5 G v / z V G y i a C p C W c m J M N / A T G 2 Z E W 0 Y 5 T E u 9 1 E B C 6 J g M o e u o J A J M m M 0 v n u I z p w x w r L Q r a f F c / T 2 R E W H M R E S u U x A 7 M s v e T P z P 6 6 Y 2 v g o z J p P U g q S L R X H K s V V 4 9 j 4 e M A 3 U 8 o k j h G r m b s V 0 R D S h 1 o V U c i E E y y + v k t Z F N a h V a 3 e 1 S v 0 6 j 6 O I T t A p O k c B u k R 1 d I s a q I k o k u g Z v a I 3 z 3 g v 3 r v 3 s W g t e P n M M f o D 7 / M H O b W Q o Q = = &lt; / l a t e x i t &gt; ! |Ck| L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t D 2 J S u P E W a 7 f x / Y u L D P + c 4 B v 3 S c = " &gt; A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I p 6 L L Y j Q s X F e w D 2 h g m 0 0 k 7 d B 5 h Z l I o a f / E j Q t F 3 P o n 7 v w b p 2 0 W 2 n r g w u G c e 7 n 3 n i h h V B v P + 3 Y K G 5 t b 2 z v F 3 d L e / s H h k X t 8 0 t I y V Z g 0 s W R S d S K k C a O C N A 0 1 j H Q S R R C P G G l H o / r c b 4 + J 0 l S K R z N J S M D R Q N C Y Y m S s F L p u T 3 I y Q O H 9 U z a t h 6 P p L H T L X s V b A K 4 T P y d l k K M R u l + 9 v s Q p J 8 J g h r T u + l 5 i g g w p Q z E j s 1 I v 1 S R B e I Q G p G u p Q J z o I F t c P o M X V u n D W C p b w s C F + n s i Q 1 z r C Y 9 s J 0 d m q F e 9 u f i f 1 0 1 N f B N k V C S p I Q I v F 8 U p g 0 b C e Q y w T x X B h k 0 s Q V h R e y v E Q 6 Q Q N j a s k g 3 B X 3 1 5 n b S u K n 6 1 U n 2 o l m u 3 e R x F c A b O w S X w w T W o g T v Q A E 2 A w R g 8 g 1 f w 5 m T O i / P u f C x b C 0 4 + c w r + w P n 8 A b 6 F k 7 0 = &lt; / l a t e x i t &gt; ! |Ck| 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G e h 0 P D 0 + 2 O c 4 u H V 2 V V 6 f p N G B o R Y = " &gt; A A A B + X i c b V B N T w I x E J 3 F L 8 S v V Y 9 e G o m J J 7 J r S P R I 5 O I R E w E T W D f d U q C h 2 2 7 a L g l Z + C d e P G i M V / + J N / + N B f a g 4 E s m e X l v J j P z o o Q z b T z v 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 7 v F J S 8 t U E d o k k k v 1 G G F N O R O 0 a Z j h 9 D F R F M c R p + 1 o V J / 7 7 T F V m k n x Y C Y J D W I 8 E K z P C D Z W C l 2 3 K 2 M 6 w K H / l E 3 r 4 W g 6 C 9 2 y V / E W Q O v E z 0 k Z c j R C 9 6 v b k y S N q T C E Y 6 0 7 v p e Y I M P K M M L p r N R N N U 0 w G e E B 7 V g q c E x 1 k C 0 u n 6 E L q / R Q X y p b w q C F + n s i w 7 H W k z i y n T E 2 Q 7 3 q z c X / v E 5 q + j d B x k S S G i r I c l E / 5 c h I N I 8 B 9 Z i i x P C J J Z g o Z m 9 F Z I g V J s a G V b I h + K s v r 5 P W V c W v V q r 3 1 X L t N o + j C G d w D p f g w z X U 4 A 4 a 0 A Q C Y 3 i G V 3 h z M u f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A l M G T o g = = &lt; / l a t e x i t &gt; y Ck &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y a K Z N Z i x F 4 p 2 p l 6 o w 4 / k Y I s l 9 9 M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j s x W M F + w F t C J v t p l 2 y 2 Q 2 7 G y G E / g g v H h T x 6 u / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T D n T x n W / n Y 3 N r e 2 d 3 c p e d f / g 8 O i 4 d n L a 0 z J T h H a J 5 F I N Q q w p Z 4 J 2 D T O c D l J F c R J y 2 g / j 9 t z v P 1 G l m R S P J k + p n + C J Y B E j 2 F i p n w d F O 4 h n Q a 3 u N t w F 0 D r x S l K H E p 2 g 9 j U a S 5 I l V B j C s d Z D z 0 2 N X 2 B l G O F 0 V h 1 l m q a Y x H h C h 5 Y K n F D t F 4 t z Z + j S K m M U S W V L G L R Q f 0 8 U O N E 6 T 0 L b m W A z 1 a v e X P z P G 2 Y m u v U L J t L M U E G W i 6 K M I y P R / H c 0 Z o o S w 3 N L M F H M 3 o r I F C t M j E 2 o a k P w V l 9 e J 7 3 r h t d s N B + a 9 d Z d G U c F z u E C r s C D G 2 j B P X S g C w R i e I Z X e H N S 5 8 V 5 d z 6 W r R t O O X M G f + B 8 / g B v 4 4 + k &lt; / l a t e x i t &gt; (a) Hierarchical encoder with MLP decoder performing single label prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 L 1 L</head><label>11</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " d Y O a v Y 0 F h 1 I 5 0 H 4 8 x m I Y u l Y G o R A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 6 8 y r N b c u r s A W S d e Q W p Q o D W s f g 1 G E U s k K s s E N a b v u b H 1 U 6 o t Z w L n l U F i M K Z s S s f Y z 6 i i E o 2 f L m 6 d k 4 t M G Z E w 0 l k p S x b q 7 4 m U S m N m M s g 6 J b U T s + r l 4 n 9 e P 7 H h j Z 9 y F S c W F V s u C h N B b E T y x 8 m I a 2 R W z D J C m e b Z r Y R N q K b M Z v H k I X i r L 6 + T z l X d a 9 Q b D 4 1 a 8 7 a I o w x n c A 6 X 4 M E 1 N O E e W t A G B h N 4 h l d 4 c 6 T z 4 r w 7 H 8 v W k l P M n M I f O J 8 / P n m N u A = = &lt; / l a t e x i t &gt; u i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 t l / t H V l y j F a x G D 2 b f J l B 4 E Q c 4 4 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 1 4 Z V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j i C U S l W W C G t P 3 3 N j 6 K d W W M 4 H z y i A x G F M 2 p W P s Z 1 R R i c Z P F 7 f O y U W m j E g Y 6 a y U J Q v 1 9 0 R K p T E z G W S d k t q J W f V y 8 T + v n 9 j w x k + 5 i h O L i i 0 X h Y k g N i L 5 4 2 T E N T I r Z h m h T P P s V s I m V F N m s 3 j y E L z V l 9 d J 5 6 r u N e q N h 0 a t e V v E U Y Y z O I d L 8 O A a m n A P L W g D g w k 8 w y u 8 O d J 5 c d 6 d j 2 V r y S l m T u E P n M 8 f k 5 G N 8 A = = &lt; / l a t e x i t &gt; E uc &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " r N 3 e 2 E m 7 C 3 c x S 5 T V n H E Z B t v 5 O n 8 = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k U d F k U w W U F + 4 A 2 h M l 0 0 g 6 d T M L M R C w h v + L G h S J u / R F 3 / o 2 T N g t t P T B w O O d e 7 p k T J J w p 7 T j f 1 t r 6 x u b W d m W n u r u 3 f 3 B o H 9 W 6 K k 4 l o R 0 S 8 1 j 2 A 6 w o Z 4 J 2 N N O c 9 h N J c R R w 2 g u m N 4 X f e 6 R S s V g 8 6 F l C v Q i P B Q s Z w d p I v l 0 b R l h P C O b Z b e 5 n q U / y q m / X n Y Y z B 1 o l b k n q U K L t 2 1 / D U U z S i A p N O F Z q 4 D q J 9 j I s N S O c 5 t V h q m i C y R S P 6 c B Q g S O q v G y e P U d n R h m h M J b m C Y 3 m 6 u + N D E d K z a L A T B Z J 1 b J X i P 9 5 g 1 S H V 1 7 G R J J q K s j i U J h y p G N U F I F G T F K i + c w Q T C Q z W R G Z Y I m J N n U V J b j L X 1 4 l 3 Y u G 2 2 w 0 7 5 v 1 1 n V Z R w V O 4 B T O w Y V L a M E d t K E D B J 7 g G V 7 h z c q t F + v d + l i M r l n l z j H 8 g f X 5 A w p g l H E = &lt; / l a t e x i t &gt; E u1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h l h k s E q P R A H M q u f 5 s S R Y I Q + J z X Q = " &gt; A A A B + 3 i c b V D L S s N A F L 3 x W e s r 1 q W b w S K 4 K o k U d F k U w W U F + 4 A 2 h M l 0 0 g 6 d T M L M R C w h v + L G h S J u / R F 3 / o 2 T N g t t P T B w O O d e 7 p k T J J w p 7 T j f 1 t r 6 x u b W d m W n u r u 3 f 3 B o H 9 W 6 K k 4 l o R 0 S 8 1 j 2 A 6 w o Z 4 J 2 N N O c 9 h N J c R R w 2 g u m N 4 X f e 6 R S s V g 8 6 F l C v Q i P B Q s Z w d p I v l 0 b R l h P C O b Z b e 5 n q e / m V d + u O w 1 n D r R K 3 J L U o U T b t 7 + G o 5 i k E R W a c K z U w H U S 7 W V Y a k Y 4 z a v D V N E E k y k e 0 4 G h A k d U e d k 8 e 4 7 O j D J C Y S z N E x r N 1 d 8 b G Y 6 U m k W B m S y S q m W v E P / z B q k O r 7 y M i S T V V J D F o T D l S M e o K A K N m K R E 8 5 k h m E h m s i I y w R I T b e o q S n C X v 7 x K u h c N t 9 l o 3 j f r r e u y j g q c w C m c g w u X 0 I I 7 a E M H C D z B M 7 z C m 5 V b L 9 a 7 9 b E Y X b P K n W P 4 A + v z B 7 4 l l D 8 = &lt; / l a t e x i t &gt; N u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D i t c P r l / H K t R i T i v S P 6 y / H u 8 O j k = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 g G 8 p m u 2 m X b j Z h d y K U 0 H / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L o X i T R Q o e S f R n E a B 5 O 1 g f D P z 2 0 9 c G x G r B 5 w k 3 I / o U I l Q M I p W e r z r p 6 S H I u K m X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u o N Y p Z G X C G T 1 J i u 5 y b o Z 1 S j Y J J P S 7 3 U 8 I S y M R 3 y r q W K 2 i V + N r 9 4 S s 6 s M i B h r G 0 p J H P 1 9 0 R G I 2 M m U W A 7 I 4 o j s + z N x P + 8 b o r h l Z 8 J l a T I F V s s C l N J M C a z 9 8 l A a M 5 Q T i y h T A t 7 K 2 E j q i l D G 1 L J h u A t v 7 x K W h d V r 1 a t 3 d c q 9 e s 8 j i K c w C m c g w e X U I d b a E A T G C h 4 h l d 4 c 4 z z 4 r w 7 H 4 v W g p P P H M M f O J 8 / P h O Q p Q = = &lt; / l a t e x i t &gt; N d ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d z L K j x l h C N b l c V J 6 Z I V q O / 7 B e 7 4 = " &gt; A A A B 8 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 g G 8 p m s 2 m X b j Z h d y K U 0 n / h x Y M i X v 0 3 3 v w 3 b t s c t P X B w O O 9 G W b m B a k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l k k y z X i T J T L R n Y A a L o X i T R Q o e S f V n M a B 5 O 1 g d D P z 2 0 9 c G 5 G o B x y n 3 I / p Q I l I M I p W e r z r h 6 S H I u a m X 6 6 4 V X c O s k q 8 n F Q g R 6 N f / u q F C c t i r p B J a k z X c 1 P 0 J 1 S j Y J J P S 7 3 M 8 J S y E R 3 w r q W K 2 i X + Z H 7 x l J x Z J S R R o m 0 p J H P 1 9 8 S E x s a M 4 8 B 2 x h S H Z t m b i f 9 5 3 Q y j K 3 8 i V J o h V 2 y x K M o k w Y T M 3 i e h 0 J y h H F t C m R b 2 V s K G V F O G N q S S D c F b f n m V t C 6 q X q 1 a u 6 9 V 6 t d 5 H E U 4 g V M 4 B w 8 u o Q 6 3 0 I A m M F D w D K / w 5 h j n x X l 3 P h a t B S e f O Y Y / c D 5 / A C P Y k J Q = &lt; / l a t e x i t &gt; y 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " U z N c 1 M 9 e y A k f y Q e / 0 J W r I o Z Y u r w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R A i h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J 7 c z v P H F t R K w e M U u 4 H 9 G R E q F g F K 3 0 k A 2 8 Q a X q 1 t w 5 y C r x C l K F A s 1 B 5 a s / j F k a c Y V M U m N 6 n p u g n 1 O N g k k + L f d T w x P K J n T E e 5 Y q G n H j 5 / N T p + T c K k M S x t q W Q j J X f 0 / k N D I m i w L b G V E c m 2 V v J v 7 n 9 V I M r / 1 c q C R F r t h i U Z h K g j G Z / U 2 G Q n O G M r O E M i 3 s r Y S N q a Y M b T p l G 4 K 3 / P I q a V / W v H q t f l + v N m 6 K O E p w C m d w A R 5 c Q Q P u o A k t Y D C C Z 3 i F N 0 c 6 L 8 6 7 8 7 F o X X O K m R P 4 A + f z B w 9 8 j a g = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v k i q Z J f 7 d B 9 o 3 E / I P N R r H L y z r 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K Q Y 9 F L x 4 r 2 g 9 o Q 9 l s J + 3 S z S b s b o R Q + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J a P J k v Q j + h I 8 p A z a q z 0 k A 1 q g 3 L F r b o L k H X i 5 a Q C O Z q D 8 l d / G L M 0 Q m m Y o F r 3 P D c x / p Q q w 5 n A W a m f a k w o m 9 A R 9 i y V N E L t T x e n z s i F V Y Y k j J U t a c h C / T 0 x p Z H W W R T Y z o i a s V 7 1 5 u J / X i 8 1 4 b U / 5 T J J D U q 2 X B S m g p i Y z P 8 m Q 6 6 Q G Z F Z Q p n i 9 l b C x l R R Z m w 6 J R u C t / r y O m n X q l 6 9 W r + v V x o 3 e R x F O I N z u A Q P r q A B d 9 C E F j A Y w T O 8 w p s j n B f n 3 f l Y t h a c f O Y U / s D 5 / A E R A I 2 p &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l Dm 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; f d ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T u X T 7 U 7 F t J O E i v D Z c 4 S a F x J Z y W A = " &gt; A A A B 8 n i c b V B N S 8 N A E N 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 0 l s 1 m 0 y 7 d 7 I b d i V B C f 4 Y X D 4 p 4 9 d d 4 8 9 + 4 a X P Q 1 g c D j / d m m J k X p o I b c N 1 v Z 2 1 9 Y 3 N r u 7 J T 3 d 3 b P z i s H R 1 3 j c o 0 Z R 2 q h N L 9 k B g m u G Q d 4 C B Y P 9 W M J K F g v X B y W / i 9 J 6 Y N V / I B p i k L E j K S P O a U g J X 8 e D i A M Q P y G F W H t b r b c O f A q 8 Q r S R 2 V a A 9 r X 4 N I 0 S x h E q g g x v i e m 0 K Q E w 2 c C j a r D j L D U k I n Z M R 8 S y V J m A n y + c k z f G 6 V C M d K 2 5 K A 5 + r v i Z w k x k y T 0 H Y m B M Z m 2 S v E / z w / g / g 6 y L l M M 2 C S L h b F m c C g c P E / j r h m F M T U E k I 1 t 7 d i O i a a U L A p F S F 4 y y + v k u 5 l w 2 s 2 m v f N e u u m j K O C T t E Z u k A e u k I t d I f a q I M o U u g Z v a I 3 B 5 w X 5 9 3 5 W L S u O e X M C f o D 5 / M H z r G Q 8 g = = &lt; / l a t e x i t &gt; E Ck &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t k r + X i o v j 6 / p h N T W b 0 D F k b X C g D Y = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k U d F k s g s s K 9 g F t C J P p p B 0 6 m Y S Z i V J i P 8 W N C 0 X c + i X u / B s n b R b a e m D g c M 6 9 3 D M n S D h T 2 n G + r b X 1 j c 2 t 7 d J O e X d v / + D Q r h x 1 V J x K Q t s k 5 r H s B V h R z g R t a 6 Y 5 7 S W S 4 i j g t B t M m r n f f a B S s V j c 6 2 l C v Q i P B A s Z w d p I v l 0 Z R F i P C e b Z z c z P m v 5 k 5 t t V p + b M g V a J W 5 A q F G j 5 9 t d g G J M 0 o k I T j p X q u 0 6 i v Q x L z Q i n s / I g V T T B Z I J H t G + o w B F V X j a P P k N n R h m i M J b m C Y 3 m 6 u + N D E d K T a P A T O Z B 1 b K X i / 9 5 / V S H V 1 7 G R J J q K s j i U J h y p G O U 9 4 C G T F K i + d Q Q T C Q z W R E Z Y4 m J N m 2 V T Q n u 8 p d X S e e i 5 t Z r 9 b t 6 t X F d 1 F G C E z i F c 3 D h E h p w C y 1 o A 4 F H e I Z X e L O e r B f r 3 f p Y j K 5 Z x c 4 x / I H 1 + Q O O z 5 Q z &lt; / l a t e x i t &gt; C k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J R W g L C n Y I 1 z i T S r U A e u r k I w w I B 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M d i L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 yi Q v B N M G n O / 8 8 S 1 E b F 6 x G n C / Y i O l A g F o 2 i l h 8 Z g M i h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w x s / E y p J k S u 2 X B S m k m B M 5 n + T o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b S v q l 6 t W r u v V e q 3 e R x F O I N z u A Q P r q E O d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t h a c f O Y U / s D 5 / A E V I I 2 s &lt; / l a t e x i t &gt; u |Ck| &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k A S G Q v e A j w G 0 l F B p A Z T u h j t b a 3 0 = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V g h 6 L v X i s Y D + k X Z Z s m m 1 D k + y S Z I W y 7 a / w 4 k E R r / 4 c b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T R W i L x D x W 3 R B r y p m k L c M M p 9 1 E U S x C T j v h u D H 3 O 0 9 U a R b L B z N J q C / w U L K I E W y s 9 J g G 2 b Q R j K e z o F x x q + 4 C a J 1 4 O a l A j m Z Q / u o P Y p I K K g 3 h W O u e 5 y b G z 7 A y j H A 6 K / V T T R N M x n h I e 5 Z K L K j 2 s 8 X B M 3 R h l Q G K Y m V L G r R Q f 0 9 k W G g 9 E a H t F N i M 9 K o 3 F / / z e q m J b v y M y S Q 1 V J L l o i j l y M R o / j 0 a M E W J 4 R N L M F H M 3 o r I C C t M j M 2 o Z E P w Vl 9 e J + 2 r q l e r 1 u 5 r l f p t H k c R z u A c L s G D a 6 j D H T S h B Q Q E P M M r v D n K e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H z I u k K w = &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " b 4 T v 1 + r / 0 N H H 4 6 q A n t U g e Y Q 6 9 2 o = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P Q b 9 c 8 a v + H H i V B D m p o B y N f v m r N 1 A 0 F S A t 5 c S Y b u A n N s y I t o x y m J Z 6 q Y G E 0 D E Z Q t d R S Q S Y M J t f P M V n T h n g W G l X 0 u K 5 + n s i I 8 K Y i Y h c p y B 2 Z J a 9 m f i f 1 0 1 t f B V m T C a p B U k X i + K U Y 6 v w 7 H 0 8 Y B q o 5 R N H C N X M 3 Y r p i G h C r Q u p 5 E I I l l 9 e J a 2 L a l C r 1 u 5 q l f p 1 H k c R n a B T d I 4 C d I n q 6 B Y 1 U B N R J N E z e k V v n v F e v H f v Y 9 F a 8 P K Z Y / Q H 3 u c P 5 M a Q a Q = = &lt; / l a t e x i t &gt; ! &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q g h r 5 a H 4 t Q L M G 7 R V W t 0 F Y f E V O v I = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 4 G v V L Z r / g z o G U S 5 K Q M O e q 9 0 l e 3 r 0 g q q L S E Y 2 M 6 g Z / Y M M P a M s L p p N h N D U 0 w G e E B 7 T g q s a A m z G Y X T 9 C p U / o o V t q V t G i m / p 7 I s D B m L C L X K b A d m k V v K v 7 n d V I b X 4 Y Z k 0 l q q S T z R X H K k V V o + j 7 q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r A u p 6 E I I F l 9 e J s 3 z S l C t V O + q 5 d p V H k c B j u E E z i C A C 6 j B D d S h A Q Q k P M M r v H n G e / H e v Y 9 5 6 4 q X z x z B H 3 i f P w 3 3 k I Q = &lt; / l a t e x i t &gt; ! i L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H f e s P o d r L 2 1 g L 0 R J D e p a N x 9 A Q I o = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 6 y X q n s V / w Z 0 D I J c l K G H P V e 6 a v b V y Q V V F r C s T G d w E 9 s m G F t G e F 0 U u y m h i a Y j P C A d h y V W F A T Z r O L J + j U K X 0 U K + 1 K W j R T f 0 9 k W B g z F p H r F N g O z a I 3 F f / z O q m N L 8 O M y S S 1 V J L 5 o j j l y C o 0 f R / 1 m a b E 8 r E j m G j m b k V k i D U m 1 o V U d C E E i y 8 v k + Z 5 J a h W q n f V c u 0 q j 6 M A x 3 A C Z x D A B d T g B u r Q A A I S n u E V 3 j z j v X j v 3 s e 8 d c X L Z 4 7 g D 7 z P H 2 L X k L w = &lt; / l a t e x i t &gt; ! i 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 8 e I q u 0 Y 6 M Q M u 6 O d 0 N f 4 i B + B / n A = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P r F + u + F V / D r x K g p x U U I 5 G v / z V G y i a C p C W c m J M N / A T G 2 Z E W 0 Y 5 T E u 9 1 E B C 6 J g M o e u o J A J M m M 0 v n u I z p w x w r L Q r a f F c / T 2 R E W H M R E S u U x A 7 M s v e T P z P 6 6 Y 2 v g o z J p P U g q S L R X H K s V V 4 9 j 4 e M A 3 U 8 o k j h G r m b s V 0 R D S h 1 o V U c i E E y y + v k t Z F N a h V a 3 e 1 S v 0 6 j 6 O I T t A p O k c B u k R 1 d I s a q I k o k u g Z v a I 3 z 3 g v 3 r v 3 s W g t e P n M M f o D 7 / M H O b W Q o Q = = &lt; / l a t e x i t &gt; ! |Ck| L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t D 2 J S u P E W a 7 f x / Y u L D P + c 4 B v 3 S c = " &gt; A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I p 6 L L Y j Q s X F e w D 2 h g m 0 0 k 7 d B 5 h Z l I o a f / E j Q t F 3 P o n 7 v w b p 2 0 W 2 n r g w u G c e 7 n 3 n i h h V B v P + 3 Y K G 5 t b 2 z v F 3 d L e / s H h k X t 8 0 t I y V Z g 0 s W R S d S K k C a O C N A 0 1 j H Q S R R C P G G l H o / r c b 4 + J 0 l S K R z N J S M D R Q N C Y Y m S s F L p u T 3 I y Q O H 9 U z a t h 6 P p L H T L X s V b A K 4 T P y d l k K M R u l + 9 v s Q p J 8 J g h r T u + l 5 i g g w p Q z E j s 1 I v 1 S R B e I Q G p G u p Q J z o I F t c P o M X V u n D W C p b w s C F + n s i Q 1 z r C Y 9 s J 0 d m q F e 9 u f i f 1 0 1 N f B N k V C S p I Q I v F 8 U p g 0 b C e Q y w T x X B h k 0 s Q V h R e y v E Q 6 Q Q N j a s k g 3 B X 3 1 5 n b S u K n 6 1 U n 2 o l m u 3 e R x F c A b O w S X w w T W o g T v Q A E 2 A w R g 8 g 1 f w 5 m T O i / P u f C x b C 0 4 + c w r + w P n 8 A b 6 F k 7 0 = &lt; / l a t e x i t &gt; ! |Ck| 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G e h 0 P D 0 + 2 O c 4 u H V 2 V V 6 f p N G B o R Y = " &gt; A A A B + X i c b V B N T w I x E J 3 F L 8 S v V Y 9 e G o m J J 7 J r S P R I 5 O I R E w E T W D f d U q C h 2 2 7 a L g l Z + C d e P G i M V / + J N / + N B f a g 4 E s m e X l v J j P z o o Q z b T z v 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 7 v F J S 8 t U E d o k k k v 1 G G F N O R O 0 a Z j h 9 D F R F M c R p + 1 o V J / 7 7 T F V m k n x Y C Y J D W I 8 E K z P C D Z W C l 2 3 K 2 M 6 w K H / l E 3 r 4 W g 6 C 9 2 y V / E W Q O v E z 0 k Z c j R C 9 6 v b k y S N q T C E Y 6 0 7 v p e Y I M P K M M L p r N R N N U 0 w G e E B 7 V g q c E x 1 k C 0 u n 6 E L q / R Q X y p b w q C F + n s i w 7 H W k z i y n T E 2 Q 7 3 q z c X / v E 5 q + j d B x k S S G i r I c l E / 5 c h I N I 8 B 9 Z i i x P C J J Z g o Z m 9 F Z I g V J s a G V b I h + K s v r 5 P W V c W v V q r 3 1 X L t N o + j C G d w D p f g w z X U 4 A 4 a 0 A Q C Y 3 i G V 3 h z M u f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A l M G T o g = = &lt; / l a t e x i t &gt; yC k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y a K Z N Z i x F 4 p 2 p l 6 o w 4 / k Y I s l 9 9 M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j s x W M F + w F t C J v t p l 2 y 2 Q 2 7 G y G E / g g v H h T x 6 u / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T D n T x n W / n Y 3 N r e 2 d 3 c p e d f / g 8 O i 4 d n L a 0 z J T h H a J 5 F I N Q q w p Z 4 J 2 D T O c D l J F c R J y 2 g / j 9 t z v P 1 G l m R S P J k + p n + C J Y B E j 2 F i p n w d F O 4 h n Q a 3 u N t w F 0 D r x S l K H E p 2 g 9 j U a S 5 I l V B j C s d Z D z 0 2 N X 2 B l G O F 0 V h 1 l m q a Y x H h C h 5 Y K n F D t F 4 t z Z + j S K m M U S W V L G L R Q f 0 8 U O N E 6 T 0 L b m W A z 1 a v e X P z P G 2 Y m u v U L J t L M U E G W i 6 K M I y P R / H c 0 Z o o S w 3 N L M F H M 3 o r I F C t M j E 2 o a k P w V l 9 e J 7 3 r h t d s N B + a 9 d Z d G U c F z u E C r s C D G 2 j B P X S g C w R i e I Z X e H N S 5 8 V 5 d z 6 W r R t O O X M G f + B 8 / g B v 4 4 + k &lt; / l a t e x i t &gt; (b) Hierarchical encoder with sequential decoder (either GRU or CRF).M LP &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t P h W 6 A 1 X D Q l + O z S E Y z Q G k 8 G c / v k = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F w 8 K F e w H t K F s t p t 2 6 e 4 m 7 E 6 E U v o X v H h Q x K t / y J v / x q T N Q V s f D D z e m 2 F m X h B L Y d F 1 v 5 3 C 2 v r G 5 l Z x u 7 S z u 7 d / U D 4 8 a t k o M Y w 3 W S Q j 0 w m o 5 V J o 3 k S B k n d i w 6 k K J G 8 H 4 5 v M b z 9 x Y 0 W k H 3 E S c 1 / R o R a h Y B Q z 6 f 6 u U e q X K 2 7 V n Y O s E i 8 n F c j R 6 J e / e o O I J Y p r Z J J a 2 / X c G P 0 p N S i Y 5 L N S L 7 E 8 p m x M h 7 y b U k 0 V t / 5 0 f u u M n K X K g I S R S U s j m a u / J 6 Z U W T t R Q d q p K I 7 s s p e J / 3 n d B M M r f y p 0 n C D X b L E o T C T B i G S P k 4 E w n K G c p I Q y I 9 J b C R t R Q x m m 8 W Q h e M s v r 5 L W R d W r V W s P t U r 9 O o + j C C d w C u f g w S X U 4 R Y a 0 A Q G I 3 i G V 3 h z l P P i v D s f i 9 a C k 8 8 c w x 8 4 n z 8 T i o 2 c &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " b 4 T v 1 + r / 0 N H H 4 6 q A n t U g e Y Q 6 9 2 o = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P Q b 9 c 8 a v + H H i V B D m p o B y N f v m r N 1 A 0 F S A t 5 c S Y b u A n N s y I t o x y m J Z 6 q Y G E 0 D E Z Q t d R S Q S Y M J t f P M V n T h n g W G l X 0 u K 5 + n s i I 8 K Y i Y h c p y B 2 Z J a 9 m f i f 1 0 1 t f B V m T C a p B U k X i + K U Y 6 v w 7 H 0 8 Y B q o 5 R N H C N X M 3 Y r p i G h C r Q u p 5 E I I l l 9 e J a 2 L a l C r 1 u 5 q l f p 1 H k c R n a B T d I 4 C d I n q 6 B Y 1 U B N R J N E z e k V v n v F e v H f v Y 9 F a 8 P K Z Y / Q H 3 u c P 5 M a Q a Q = = &lt; / l a t e x i t &gt; ! &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q g h r 5 a H 4 t Q L M G 7 R V W t 0 F Y f E V O v I = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 4 G v V L Z r / g z o G U S 5 K Q M O e q 9 0 l e 3 r 0 g q q L S E Y 2 M 6 g Z / Y M M P a M s L p p N h N D U 0 w G e E B 7 T g q s a A m z G Y X T 9 C p U / o o V t q V t G i m / p 7 I s D B m L C L X K b A d m k V v K v 7 n d V I b X 4 Y Z k 0 l q q S T z R X H K k V V o + j 7 q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r A u p 6 E I I F l 9 e J s 3 z S l C t V O + q 5 d p V H k c B j u E E z i C A C 6 j B D d S h A Q Q k P M M r v H n G e / H e v Y 9 5 6 4 q X z x z B H 3 i f P w 3 3 k I Q = &lt; / l a t e x i t &gt; ! i L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H f e s P o d r L 2 1 g L 0 R J D e p a N x 9 A Q I o = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 6 y X q n s V / w Z 0 D I J c l K G H P V e 6 a v b V y Q V V F r C s T G d w E 9 s m G F t G e F 0 U u y m h i a Y j P C A d h y V W F A T Z r O L J + j U K X 0 U K + 1 K W j R T f 0 9 k W B g z F p H r F N g O z a I 3 F f / z O q m N L 8 O M y S S 1 V J L 5 o j j l y C o 0 f R / 1 m a b E 8 r E j m G j m b k V k i D U m 1 o V U d C E E i y 8 v k + Z 5 J a h W q n f V c u 0 q j 6 M A x 3 A C Z x D A B d T g B u r Q A A I S n u E V 3 j z j v X j v 3 s e 8 d c X L Z 4 7 g D 7 z P H 2 L X k L w = &lt; / l a t e x i t &gt; ! i 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 8 e I q u 0 Y 6 M Q M u 6 O d 0 N f 4 i B + B / n A = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P r F + u + F V / D r x K g p x U U I 5 G v / z V G y i a C p C W c m J M N / A T G 2 Z E W 0 Y 5 T E u 9 1 E B C 6 J g M o e u o J A J M m M 0 v n u I z p w x w r L Q r a f F c / T 2 R E W H M R E S u U x A 7 M s v e T P z P 6 6 Y 2 v g o z J p P U g q S L R X H K s V V 4 9 j 4 e M A 3 U 8 o k j h G r m b s V 0 R D S h 1 o V U c i E E y y + v k t Z F N a h V a 3 e 1 S v 0 6 j 6 O I T t A p O k c B u k R 1 d I s a q I k o k u g Z v a I 3 z 3 g v 3 r v 3 s W g t e P n M M f o D 7 / M H O b W Q o Q = = &lt; / l a t e x i t &gt; u 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d Y O a v Y 0 F h 1 I 5 0 H 4 8 x m I Y u l Y G o R A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 6 8 y r N b c u r s A W S d e Q W p Q o D W s f g 1 G E U s k K s s E N a b v u b H 1 U 6 o t Z w L n l U F i M K Z s S s f Y z 6 i i E o 2 f L m 6 d k 4 t M G Z E w 0 l k p S x b q 7 4 m U S m N m M s g 6 J b U T s + r l 4 n 9 e P 7 H h j Z 9 y F S c W F V s u C h N B b E T y x 8 m I a 2 R W z D J C m e b Z r Y R N q K b M Z v H k I X i r L 6 + T z l X d a 9 Q b D 4 1 a 8 7 a I o w x n c A 6 X 4 M E 1 N O E e W t A G B h N 4 h l d 4 c 6 T z 4 r w 7 H 8 v W k l P M n M I f O J 8 / P n m N u A = = &lt; / l a t e x i t &gt; u i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 t l / t H V l y j F a x G D 2 b f J l B 4 E Q c 4 4 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 1 4 Z V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j i C U S l W W C G t P 3 3 N j 6 K d W W M 4 H z y i A x G F M 2 p W P s Z 1 R R i c Z P F 7 f O y U W m j E g Y 6 a y U J Q v 1 9 0 R K p T E z G W S d k t q J W f V y 8 T + v n 9 j w x k + 5 i h O L i i 0 X h Y k g N i L 5 4 2 T E N T I r Z h m h T P P s V s I m V F N m s 3 j y E L z V l 9 d J 5 6 r u N e q N h 0 a t e V v E U Y Y z O I d L 8 O A a m n A P L W g D g w k 8 w y u 8 O d J 5 c d 6 d j 2 V r y S l m T u E P n M 8 f k 5 G N 8 A = = &lt; / l a t e x i t &gt; N ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a X t T w P F D S q 5 b U I K C 1 h q T j w L 6 h x I = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 I G 8 p m u 2 m X 7 i Z h d y K U 0 l / h x Y M i X v 0 5 3 v w 3 b t o c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L k X E m y h Q 8 k 6 i O V W B 5 O 1 g f J P 5 7 S e u j Y i j B 5 w k 3 F d 0 G I l Q M I p W e r w j P R S K m 1 K / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L F U 8 Q i a p M V 3 P T d C f U o 2 C S T 4 r 9 V L D E 8 r G d M i 7 l k b U b v G n 8 4 N n 5 M w q A x L G 2 l a E Z K 7 + n p h S Z c x E B b Z T U R y Z Z S 8 T / / O 6 K Y Z X / l R E S Y o 8 Y o t F Y S o J x i T 7 n g y E 5 g z l x B L K t L C 3 E j a i m j K 0 G W U h e M s v r 5 L W R d W r V W v 3 t U r 9 O o + j C C d w C u f g w S X U 4 R Y a 0 A Q G C p 7 h F d 4 c 7 b w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w / i / o / R &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; E Ck &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t k r + X i o v j 6 / p h N T W b 0 D F k b X C g D Y = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k U d F k s g s s K 9 g F t C J P p p B 0 6 m Y S Z i V J i P 8 W N C 0 X c + i X u / B s n b R b a e m D g c M 6 9 3 D M n S D h T 2 n G + r b X 1 j c 2 t 7 d J O e X d v / + D Q r h x 1 V J x K Q t s k 5 r H s B V h R z g R t a 6 Y 5 7 S W S 4 i j g t B t M m r n f f a B S s V j c 6 2 l C v Q i P B A s Z w d p I v l 0 Z R F i P C e b Z z c z P m v 5 k 5 t t V p + b M g V a J W 5 A q F G j 5 9 t d g G J M 0 o k I T j p X q u 0 6 i v Q x L z Q i n s / I g V T T B Z I J H t G + o w B F V X j a P P k N n R h m i M J b m C Y 3 m 6 u + N D E d K T a P A T O Z B 1 b K X i / 9 5 / V S H V 1 7 G R J J q K s j i U J h y p G O U 9 4 C G T F K i + d Q Q T C Q z W R E Z Y 4 m J N m 2 V T Q n u 8 p d X S e e i 5 t Z r 9 b t 6 t X F d 1 F G C E z i F c 3 D h E h p w C y 1 o A 4 F H e I Z X e L O e r B f r 3 f p Y j K 5 Z x c 4 x / I H 1 + Q O O z 5 Q z &lt; / l a t e x i t &gt; C k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J R W g L C n Y I 1 z i T S r U A e u r k I w w I B 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M d i L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M G n O / 8 8 S 1 E b F 6 x G n C / Y i O l A g F o 2 i l h 8 Z g M i h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w x s / E y p J k S u 2 X B S m k m B M 5 n + T o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b S v q l 6 t W r u v V e q 3 e R x F O I N z u A Q P r q E O d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t h a c f O Y U / s D 5 / A E V I I 2 s &lt; / l a t e x i t &gt; u |Ck| &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k A S G Q v e A j w G 0 l F B p A Z T u h j t b a 3 0 = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V g h 6 L v X i s Y D + k X Z Z s m m 1 D k + y S Z I W y 7 a / w 4 k E R r / 4 c b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T R W i L x D x W 3 R B r y p m k L c M M p 9 1 E U S x C T j v h u D H 3 O 0 9 U a R b L B z N J q C / w U L K I E W y s 9 J g G 2 b Q R j K e z o F x x q + 4 C a J 1 4 O a l A j m Z Q / u o P Y p I K K g 3 h W O u e 5 y b G z 7 A y j H A 6 K / V T T R N M x n h I e 5 Z K L K j 2 s 8 X B M 3 R h l Q G K Y m V L G r R Q f 0 9 k W G g 9 E a H t F N i M 9 K o 3 F / / z e q m J b v y M y S Q 1 V J L l o i j l y M R o / j 0 a M E W J 4 R N L M F H M 3 o r I C C t M j M 2 o Z E P w V l 9 e J + 2 r q l e r 1 u 5 r l f p t H k c R z u A c L s G D a 6 j D H T S h B Q Q E P M M r v D n K e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H z I u k K w = &lt; / l a t e x i t &gt; ! |Ck| L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t D 2 J S u P E W a 7 f x / Y u L D P + c 4 B v 3 S c = " &gt; A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I p 6 L L Y j Q s X F e w D 2 h g m 0 0 k 7 d B 5 h Z l I o a f / E j Q t F 3 P o n 7 v w b p 2 0 W 2 n r g w u G c e 7 n 3 n i h h V B v P + 3 Y K G 5 t b 2 z v F 3 d L e / s H h k X t 8 0 t I y V Z g 0 s W R S d S K k C a O C N A 0 1 j H Q S R R C P G G l H o / r c b 4 + J 0 l S K R z N J S M D R Q N C Y Y m S s F L p u T 3 I y Q O H 9 U z a t h 6 P p L H T L X s V b A K 4 T P y d l k K M R u l + 9 v s Q p J 8 J g h r T u + l 5 i g g w p Q z E j s 1 I v 1 S R B e I Q G p G u p Q J z o I F t c P o M X V u n D W C p b w s C F + n s i Q 1 z r C Y 9 s J 0 d m q F e 9 u f i f 1 0 1 N f B N k V C S p I Q I v F 8 U p g 0 b C e Q y w T x X B h k 0 s Q V h R e y v E Q 6 Q Q N j a s k g 3 B X 3 1 5 n b S u K n 6 1 U n 2 o l m u 3 e R x F c A b O w S X w w T W o g T v Q A E 2 A w R g 8 g 1 f w 5 m T O i / P u f C x b C 0 4 + c w r + w P n 8 A b 6 F k 7 0 = &lt; / l a t e x i t &gt; ! |Ck| 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G e h 0 P D 0 + 2 O c 4 u H V 2 V V 6 f p N G B o R Y = " &gt; A A A B + X i c b V B N T w I x E J 3 F L 8 S v V Y 9 e G o m J J 7 J r S P R I 5 O I R E w E T W D f d U q C h 2 2 7 a L g l Z + C d e P G i M V / + J N / + N B f a g 4 E s m e X l v J j P z o o Q z b T z v 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 7 v F J S 8 t U E d o k k k v 1 G G F N O R O 0 a Z j h 9 D F R F M c R p + 1 o V J / 7 7 T F V m k n x Y C Y J D W I 8 E K z P C D Z W C l 2 3 K 2 M 6 w K H / l E 3 r 4 W g 6 C 9 2 y V / E W Q O v E z 0 k Z c j R C 9 6 v b k y S N q T C E Y 6 0 7 v p e Y I M P K M M L p r N R N N U 0 w G e E B 7 V g q c E x 1 k C 0 u n 6 E L q / R Q X y p b w q C F + n s i w 7 H W k z i y n T E 2 Q 7 3 q z c X / v E 5 q + j d B x k S S G i r I c l E / 5 c h I N I 8 B 9 Z i i x P C J J Z g o Z m 9 F Z I g V J s a G V b I h + K s v r 5 P W V c W v V q r 3 1 X L t N o + j C G d w D p f g w z X U 4 A 4 a 0 A Q C Y 3 i G V 3 h z M u f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A l M G T o g = = &lt; / l a t e x i t &gt; yC k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y a K Z N Z i x F 4 p 2 p l 6 o w 4 / k Y I s l 9 9 M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j s x W M F + w F t C J v t p l 2 y 2 Q 2 7 G y G E / g g v H h T x 6 u / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T D n T x n W / n Y 3 N r e 2 d 3 c p e d f / g 8 O i 4 d n L a 0 z J T h H a J 5 F I N Q q w p Z 4 J 2 D T O c D l J F c R J y 2 g / j 9 t z v P 1 G l m R S P J k + p n + C J Y B E j 2 F i p n w d F O 4 h n Q a 3 u N t w F 0 D r x S l K H E p 2 g 9 j U a S 5 I l V B j C s d Z D z 0 2 N X 2 B l G O F 0 V h 1 l m q a Y x H h C h 5 Y K n F D t F 4 t z Z + j S K m M U S W V L G L R Q f 0 8 U O N E 6 T 0 L b m W A z 1 a v e X P z P G 2 Y m u v U L J t L M U E G W i 6 K M I y P R / H c 0 Z o o S w 3 N L M F H M 3 o r I F C t M j E 2 o a k P w V l 9 e J 7 3 r h t d s N B + a 9 d Z d G U c F z u E C r s C D G 2 j B P X S g C w R i e I Z X e H N S 5 8 V 5 d z 6 W r R t O O X M G f + B 8 / g B v 4 4 + k &lt; / l a t e x i t &gt; (c) BERT encoder with MLP decoder performing single label prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>1 LFigure 5 :</head><label>15</label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " U z N c 1 M 9 e y A k f y Q e / 0 J W r I o Z Y u r w = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G i / Y A 2 l M 1 2 0 y 7 d b M L u R A i h P 8 G L B 0 W 8 + o u 8 + W / c t j l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J 7 c z v P H F t R K w e M U u 4 H 9 G R E q F g F K 3 0 k A 2 8 Q a X q 1 t w 5 y C r x C l K F A s 1 B 5 a s / j F k a c Y V M U m N 6 n p u g n 1 O N g k k + L f d T w x P K J n T E e 5 Y q G n H j 5 / N T p + T c K k M S x t q W Q j J X f 0 / k N D I m i w L b G V E c m 2 V v J v 7 n 9 V I M r / 1 c q C R F r t h i U Z h K g j G Z / U 2 G Q n O G M r O E M i 3 s r Y S N q a Y M b T p l G 4 K 3 / P I q a V / W v H q t f l + v N m 6 K O E p w C m d w A R 5 c Q Q P u o A k t Y D C C Z 3 i F N 0 c 6 L 8 6 7 8 7 F o X X O K m R P 4 A + f z B w 9 8 j a g = &lt; / l a t e x i t &gt; y 2 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 7 v k i q Z J f 7 d B 9 o 3 E / I P N R r H L y z r 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l K Q Y 9 F L x 4 r 2 g 9 o Q 9 l s J + 3 S z S b s b o R Q + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 7 d z v P K H S P J a P J k v Q j + h I 8 p A z a q z 0 k A 1 q g 3 L F r b o L k H X i 5 a Q C O Z q D 8 l d / G L M 0 Q m m Y o F r 3 P D c x / p Q q w 5 n A W a m f a k w o m 9 A R 9 i y V N E L t T x e n z s i F V Y Y k j J U t a c h C / T 0 x p Z H W W R T Y z o i a s V 7 1 5 u J / X i 8 1 4 b U / 5 T J J D U q 2 X B S m g p i Y z P 8 m Q 6 6 Q G Z F Z Q p n i 9 l b C x l R R Z m w 6 J R u C t / r y O m n X q l 6 9 W r + v V x o 3 e R x F O I N z u A Q P r q A B d 9 C E F j A Y w T O 8 w p s j n B f n 3 f l Y t h a c f O Y U / s D 5 / A E R A I 2 p &lt; / l a t e x i t &gt; t e x i t s h a 1 _ b a s e 6 4 = " b 4 T v 1 + r / 0 N H H 4 6 q A n t U g e Y Q 6 9 2 o = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a CI V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P Q b 9 c 8 a v + H H i V B D m p o B y N f v m r N 1 A 0 F S A t 5 c S Y b u A n N s y I t o x y m J Z 6 q Y G E 0 D E Z Q t d R S Q S Y M J t f P M V n T h n g W G l X 0 u K 5 + n s i I 8 K Y i Y h c p y B 2 Z J a 9 m f i f 1 0 1 t f B V m T C a p B U k X i + K U Y 6 v w 7 H 0 8 Y B q o 5 R N H C N X M 3 Y r p i G h Cr Q u p 5 E I I l l 9 e J a 2 L a l C r 1 u 5 q l f p 1 H k c R n a B T d I 4 C d I n q 6 B Y 1 U B N R J N E z e k V v n v F e v H f v Y 9 F a 8 P K Z Y / Q H 3 u c P 5 M a Q a Q = = &lt; / l a t e x i t &gt; ! &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Q g h r 5 a H 4 t Q L M G 7 R V W t 0 F Y f E V O v I = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 4 G v V L Z r / g z o G U S 5 K Q M O e q 9 0 l e 3 r 0 g q q L S E Y 2 M 6 g Z / Y M M P a M s L p p N h N D U 0 w G e E B 7 T g q s a A m z G Y X T 9 C p U / o o V t q V t G i m / p 7 I s D B m L C L X K b A d m k V v K v 7 n d V I b X 4 Y Z k 0 l q q S T z R X H K k V V o + j 7 q M 0 2 J 5 W N H M N H M 3 Y r I E G t M r A u p 6 E I I F l 9 e J s 3 z S l C t V O + q 5 d p V H k c B j u E E z i C A C 6 j B D d S h A Q Q k P M M r v H n G e / H e v Y 9 5 6 4 q X z x z B H 3 i f P w 3 3 k I Q = &lt; / l a t e x i t &gt; ! i L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " H f e s P o d r L 2 1 g L 0 R J D e p a N x 9 AQ I o = " &gt; A A A B 8 X i c b V D L S g N B E O z 1 G e M r 6 t H L Y B A 8 h V 0 J 6 D H o x Y O H C O a B y R p m J 7 P J k H k s M 7 N C W P I X X j w o 4 t W / 8 e b f O E n 2 o I k F D U V V N 9 1 d U c K Z s b 7 / 7 a 2 s r q 1 v b B a 2 i t s 7 u 3 v 7 p Y P D p l G p J r R B F F e 6 H W F D O Z O 0 Y Z n l t J 1 o i k X E a S s a X U / 9 1 h P V h i l 5 b 8 c J D Q U e S B Y z g q 2 T H r p K 0 A H u 3 T 6 y X q n s V / w Z 0 D I J c l K G H P V e 6 a v b V y Q V V F r C s T G d w E 9 s m G F t G e F 0 U u y m h i a Y j P C A d h y V W F A T Z r O L J + j U K X 0 U K + 1 K W j R T f 0 9 k W B g z F p H r F N g O z a I 3 F f / z O q m N L 8 O M y S S 1 V J L 5 o j j l y C o 0 f R / 1 m a b E 8 r E j m G j m b k V k i D U m 1 o V U d C E E i y 8 v k + Z 5 J a h W q n f V c u 0 q j 6 M A x 3 A C Z x D A B d T g B u r Q A A I S n u E V 3 j z j v X j v 3 s e 8 d c X L Z 4 7 g D 7 z P H 2 L X k L w = &lt; / l a t e x i t &gt; ! i 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " g 8 e I q u 0 Y 6 M Q M u 6 O d 0 N f 4 i B + B / n A = " &gt; A A A B 8 X i c b V D L S g N B E J y N r x h f U Y 9 e B o P g K e x K Q I 9 B L x 4 j m A c m a 5 i d 9 C Z D 5 r H M z A p h y V 9 4 8 a C I V / / G m 3 / j J N m D J h Y 0 F F X d d H d F C W f G + v 6 3 V 1 h b 3 9 j c K m 6 X d n b 3 9 g / K h 0 c t o 1 J N o U k V V 7 o T E Q O c S W h a Z j l 0 E g 1 E R B z a 0 f h m 5 r e f Q B u m 5 L 2 d J B A K M p Q s Z p R Y J z 3 0 l I A h 6 Q e P r F + u + F V / D r x K g p x U U I 5 G v / z V G y i a C p C W c m J M N / A T G 2 Z E W 0 Y 5 T E u 9 1 E B C 6 J g M o e u o J A J M m M 0 v n u I z p w x w r L Q r a f F c / T 2 R E W H M R E S u U x A 7 M s v e T P z P 6 6 Y 2 v g o z J p P U g q S L R X H K s V V 4 9 j 4 e M A 3 U 8 o k j h G r m b s V 0 R D S h 1 o V U c i E E y y + v k t Z F N a h V a 3 e 1 S v 0 6 j 6 O I T t A p O k c B u k R 1 d I s a q I k o k u g Z v a I 3 z 3 g v 3 r v 3 s W g t e P n M M f o D 7 / M H O b W Q o Q = = &lt; / l a t e x i t &gt; u 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " d Y O a v Y 0 F h 1 I 5 0 H 4 8 x m I Y u l Y G o R A = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 6 8 y r N b c u r s A W S d e Q W p Q o D W s f g 1 G E U s k K s s E N a b v u b H 1 U 6 o t Z w L n l U F i M K Z s S s f Y z 6 i i E o 2 f L m 6 d k 4 t M G Z E w 0 l k p Sx b q 7 4 m U S m N m M s g 6 J b U T s + r l 4 n 9 e P 7 H h j Z 9 y F S c W F V s u C h N B b E T y x 8 m I a 2 R W z D J C m e b Z r Y R N q K b M Z v H k I X i r L 6 + T z l X d a 9 Q b D 4 1 a 8 7 a I o w x n c A 6 X 4 M E 1 N O E e W t A G B h N 4 h l d 4 c 6 T z 4 r w 7 H 8 v W k l P M n M I f O J 8 / P n m N u A = = &lt; / l a t e x i t &gt; u i &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 t l / t H V l y j F a x G D 2 b f J l B 4 E Q c 4 4 = " &gt; A A A B 6 3 i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 4 8 V 7 A e 0 o W y 2 k 3 b p 7 i b s b o Q S + h e 8 e F D E q 3 / I m / / G p M 1 B W x 8 M P N 6 b Y W Z e E A t u r O t + O 6 W N z a 3 t n f J u Z W / / 4 P C o e n z S M V G i G b Z Z J C L d C 6 h B w R W 2 L b c C e 7 F G K g O B 3 W B 6 l / v d J 9 S G R + r R z m L 0 J R 0 r H n J G b S 4 l Q 1 4 Z V m t u 3 V 2 A r B O v I D U o 0 B p W v w a j i C U S l W W C G t P 3 3 N j 6 K d W W M 4 H z y i A x G F M 2 p W P s Z 1 R R i c Z P F 7 f O y U W m j E g Y 6 a y U J Q v 1 9 0 R K p T E z G W S d k t q J W f V y 8 T + v n 9 j w x k + 5 i h O L i i 0 X h Y k g N i L 5 4 2 T E N T I r Z h m h T P P s V s I m V F N m s 3 j y E L z V l 9 d J 5 6 r u N e q N h 0 a t e V v E U Y Y z O I d L 8 O A a m n A P L W g D g w k 8 w y u 8 O d J 5 c d 6 d j 2 V r y S l m T u E P n M 8 f k 5 G N 8 A = = &lt; / l a t e x i t &gt; N ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " a X t T w P F D S q 5 b U I K C 1 h q T j w L 6 h x I = " &gt; A A A B 8 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M e i F 0 9 S w X 5 I G 8 p m u 2 m X 7 i Z h d y K U 0 l / h x Y M i X v 0 5 3 v w 3 b t o c t P X B w O O 9 G W b m B Y k U B l 3 3 2 y m s r W 9 s b h W 3 S z u 7 e / s H 5 c O j l o l T z X i T x T L W n Y A a L k X E m y h Q 8 k 6 i O V W B 5 O 1 g f J P 5 7 S e u j Y i j B 5 w k 3 F d 0 G I l Q M I p W e r w j P R S K m 1 K / X H G r 7 h x k l X g 5 q U C O R r / 8 1 R v E L F U 8 Q i a p M V 3 P T d C f U o 2 C S T 4 r 9 V L D E 8 r G d M i 7 l k b U b v G n 8 4 N n 5 M w q A x L G 2 l a E Z K 7 + n p h S Z c x E B b Z T U R y Z Z S 8 T / / O 6 K Y Z X / l R E S Y o 8 Y o t F Y S o J x i T 7 n g y E 5 g z l x B L K t L C 3 E j a i m j K 0 G W U h e M s v r 5 L W R d W r V W v 3 t U r 9 O o + j C C d w C u f g w S X U 4 R Y a 0 A Q G C p 7 h F d 4 c 7 b w 4 7 8 7 H o r X g 5 D P H 8 A f O 5 w / i / o / R &lt; / l a t e x i t &gt; f u ? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M n I d s Z f A n o N W q / z v G E 8 k b T Q s G h c = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j 0 4 r G C / Y A 2 l s 1 2 0 y 7 d b M L u R C i h P 8 O L B 0 W 8 + m u 8 + W / c t D l o 6 4 O B x 3 s z z M w L E i k M u u 6 3 s 7 a + s b m 1 X d o p 7 + 7 t H x x W j o 7 b J k 4 1 4 y 0 W y 1 h 3 A 2 q 4 F I q 3 U K D k 3 U R z G g W S d 4 L J b e 5 3 n r g 2 I l Y P O E 2 4 H 9 G R E q F g F K 3 U C w d 9 H H O k j 2 l 5 U K m 6 N X c O s k q 8 g l S h Q H N Q + e o P Y 5 Z G X C G T 1 J i e 5 y b o Z 1 S j Y J L P y v 3 U 8 I S y C R 3 x n q W K R t z 4 2 f z k G T m 3 y p C E s b a l k M z V 3 x M Z j Y y Z R o H t j C i O z b K X i / 9 5 v R T D a z 8 T K k m R K 7 Z Y F K a S Y E z y / 8 l Q a M 5 Q T i 2 h T A t 7 K 2 F j q i l D m 1 I e g r f 8 8 i p p X 9 a 8 e q 1 + X 6 8 2 b o o 4 S n A K Z 3 A B H l x B A + 6 g C S 1 g E M M z v M K b g 8 6 L 8 + 5 8 L F r X n G L m B P 7 A + f w B 6 I a R A w = = &lt; / l a t e x i t &gt; E Ck &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t k r + X i o v j 6 / p h N T W b 0 D F k b X C g D Y = " &gt; A A A B + n i c b V D L S s N A F L 3 x W e s r 1 a W b w S K 4 K o k U d F k s g s s K 9 g F t C J P p p B 0 6 m Y S Z i V J i P 8 W N C 0 X c + i X u / B s n b R b a e m D g c M 6 9 3 D M n S D h T 2 n G + r b X 1 j c 2 t 7 d J O e X d v / + D Q r h x 1 V J x K Q t s k 5 r H s B V h R z g R t a 6 Y 5 7 S W S 4 i j g t B t M m r n f f a B S s V j c 6 2 l C v Q i P B A s Z w d p I v l 0 Z R F i P C e b Z z c z P m v 5 k 5 t t V p + b M g V a J W 5 A q F G j 5 9 t d g G J M 0 o k I T j p X q u 0 6 i v Q x L z Q i n s / I g V T T B Z I J H t G + o w B F V X j a P P k N n R h m i M J b m C Y 3 m 6 u + N D E d K T a P A T O Z B 1 b K X i / 9 5 / V S H V 1 7 G R J J q K s j i U J h y p G O U 9 4 C G T F K i + d Q Q T C Q z W R E Z Y 4 m J N m 2 V T Q n u 8 p d X S e e i 5 t Z r 9 b t 6 t X F d 1 F G C E z i F c 3 D h E h p w C y 1 o A 4 F H e I Z X e L O e r B f r 3 f p Y j K 5 Z x c 4 x / I H 1 + Q O O z 5 Q z &lt; / l a t e x i t &gt; C k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " J R W g L C n Y I 1 z i T S r U A e u r k I w w I B 0 = " &gt; A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m k o M d i L x 4 r 2 g 9 o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D E q 7 / I m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I J H C o O t + O 4 W N z a 3 t n e J u a W / / 4 P C o f H z S N n G q G W + x W M a 6 G 1 D D p V C 8 h Q I l 7 y a a 0 y i Q v B N M G n O / 8 8 S 1 E b F 6 x G n C / Y i O l A g F o 2 i l h 8 Z g M i h X 3 K q 7 A F k n X k 4 q k K M 5 K H / 1 h z F L I 6 6 Q S W p M z 3 M T 9 D O q U T D J Z 6 V + a n h C 2 Y S O e M 9 S R S N u / G x x 6 o x c W G V I w l j b U k g W 6 u + J j E b G T K P A d k Y U x 2 b V m 4 v / e b 0 U w x s / E y p J k S u 2 X B S m k m B M 5 n + T o d C c o Z x a Q p k W 9 l b C x l R T h j a d k g 3 B W 3 1 5 n b S v q l 6 t W r u v V e q 3 e R x F O I N z u A Q P r q E O d 9 C E F j A Y w T O 8 w p s j n R f n 3 f l Y t h a c f O Y U / s D 5 / A E V I I 2 s &lt; / l a t e x i t &gt; u |Ck| &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " k A S G Q v e A j w G 0 l F B p A Z T u h j t b a 3 0 = " &gt; A A A B 8 H i c b V B N S w M x E J 2 t X 7 V + V T 1 6 C R b B U 9 m V g h 6 L v X i s Y D + k X Z Z s m m 1 D k + y S Z I W y 7 a / w 4 k E R r / 4 c b / 4 b 0 3 Y P 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 5 e O T t o 5 T R W i L x D x W 3 R B r y p m k L c M M p 9 1 E U S x C T j v h u D H 3 O 0 9 U a R b L B z N J q C / w U L K I E W y s 9 J g G 2 b Q R j K e z o F x x q + 4 C a J 1 4 O a l A j m Z Q / u o P Y p I K K g 3 h W O u e 5 y b G z 7 A y j H A 6 K / V T T R N M x n h I e 5 Z K L K j 2 s 8 X B M 3 R h l Q G K Y m V L G r R Q f 0 9 k W G g 9 E a H t F N i M 9 K o 3 F / / z e q m J b v y M y S Q 1 V J L l o i j l y M R o / j 0 a M E W J 4 R N L M F H M 3 o r I C C t M j M 2 o Z E P w V l 9 e J + 2 r q l e r 1 u 5 r l f p t H k c R z u A c L s G D a 6 j D H T S h B Q Q E P M M r v D n K e X H e n Y 9 l a 8 H J Z 0 7 h D 5 z P H z I u k K w = &lt; / l a t e x i t &gt; ! |Ck| L &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " t D 2 J S u P E W a 7 f x / Y u L D P + c 4 B v 3 S c = " &gt; A A A B + X i c b V D L S s N A F J 3 U V 6 2 v q E s 3 g 0 V w V R I p 6 L L Y j Q s X F e w D 2 h g m 0 0 k 7 d B 5 h Z l I o a f / E j Q t F 3 P o n 7 v w b p 2 0 W 2 n r g w u G c e 7 n 3 n i h h V B v P + 3 Y K G 5 t b 2 z v F 3 d L e / s H h k X t 8 0 t I y V Z g 0 s W R S d S K k C a O C N A 0 1 j H Q S R R C P G G l H o / r c b 4 + J 0 l S K R z N J S M D R Q N C Y Y m S s F L p u T 3 I y Q O H 9 U z a t h 6 P p L H T L X s V b A K 4 T P y d l k K M R u l + 9 v s Q p J 8 J g h r T u + l 5 i g g w p Q z E j s 1 I v 1 S R B e I Q G p G u p Q J z o I F t c P o M X V u n D W C p b w s C F + n s i Q 1 z r C Y 9 s J 0 d m q F e 9 u f i f 1 0 1 N f B N k V C S p I Q I v F 8 U p g 0 b C e Q y w T x X B h k 0 s Q V h R e y v E Q 6 Q Q N j a s k g 3 B X 3 1 5 n b S u K n 6 1 U n 2 o l m u 3 e R x F c A b O w S X w w T W o g T v Q A E 2 A w R g 8 g 1 f w 5 m T O i / P u f C x b C 0 4 + c w r + w P n 8 A b 6 F k 7 0 = &lt; / l a t e x i t &gt; ! |Ck| 1 &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " G e h 0 P D 0 + 2 O c 4 u H V 2 V V 6 f p N G B o R Y = " &gt; A A A B + X i c b V B N T w I x E J 3 F L 8 S v V Y 9 e G o m J J 7 J r S P R I 5 O I R E w E T W D f d U q C h 2 2 7 a L g l Z + C d e P G i M V / + J N / + N B f a g 4 E s m e X l v J j P z o o Q z b T z v 2 y l s b G 5 t 7 x R 3 S 3 v 7 B 4 d H 7 v F J S 8 t U E d o k k k v 1 G G F N O R O 0 a Z j h 9 D F R F M c R p + 1 o V J / 7 7 T F V m k n x Y C Y J D W I 8 E K z P C D Z W C l 2 3 K 2 M 6 w K H / l E 3 r 4 W g 6 C 9 2 y V / E W Q O v E z 0 k Z c j R C 9 6 v b k y S N q T C E Y 6 0 7 v p e Y I M P K M M L p r N R N N U 0 w G e E B 7 V g q c E x 1 k C 0 u n 6 E L q / R Q X y p b w q C F + n s i w 7 H W k z i y n T E 2 Q 7 3 q z c X / v E 5 q + j d B x k S S G i r I c l E / 5 c h I N I 8 B 9 Z i i x P C J J Z g o Z m 9 F Z I g V J s a G V b I h + K s v r 5 P W V c W v V q r 3 1 X L t N o + j C G d w D p f g w z X U 4 A 4 a 0 A Q C Y 3 i G V 3 h z M u f F e X c + l q 0 F J 5 8 5 h T 9 w P n 8 A l M G T o g = = &lt; / l a t e x i t &gt; yC k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " Y a K Z N Z i x F 4 p 2 p l 6 o w 4 / k Y I s l 9 9 M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k U 9 F j s x W M F + w F t C J v t p l 2 y 2 Q 2 7 G y G E / g g v H h T x 6 u / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T D n T x n W / n Y 3 N r e 2 d 3 c p e d f / g 8 O i 4 d n L a 0 z J T h H a J 5 F I N Q q w p Z 4 J 2 D T O c D l J F c R J y 2 g / j 9 t z v P 1 G l m R S P J k + p n + C J Y B E j 2 F i p n w d F O 4 h n Q a 3 u N t w F 0 D r x S l K H E p 2 g 9 j U a S 5 I l V B j C s d Z D z 0 2 N X 2 B l G O F 0 V h 1 l m q a Y x H h C h 5 Y K n F D t F 4 t z Z + j S K m M U S W V L G L R Q f 0 8 U O N E 6 T 0 L b m W A z 1 a v e X P z P G 2 Y m u v U L J t L M U E G W i 6 K M I y P R / H c 0 Z o o S w 3 N L M F H M 3 o r I F C t M j E 2 o a k P w V l 9 e J 7 3 r h t d s N B + a 9 d Z d G U c F z u E C r s C D G 2 j B P X S g C w R i e I Z X e H N S 5 8 V 5 d z 6 W r R t O O X M G f + B 8 / g B v 4 4 + k &lt; / l a t e x i t &gt; (d) BERT encoder with sequential decoder (either GRU or CRF) Schema of the different models evaluated on SILICONE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Yeah , b It can be a pain . sd It's, it's nice riding to school because it's all along a canal path, uh, sd Because it's just, it's along the Erie Canal up here. sd So, what school is it? qw Uh, University of Rochester. sd Oh, okay.bk Examples of dialogs labelled with DA taken from SwDA. The labels qw, sd, b, bk respectively correspond to wh-question, statement-non-opinion, backchannel and response acknowledgement.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Corpus |T rain| |V al| |T est| Utt. |Labels| Task Utt./|Labels|</figDesc><table><row><cell>SwDA</cell><cell>1k</cell><cell>100</cell><cell>11</cell><cell>200k</cell><cell>42</cell><cell>DA</cell><cell>4.8k</cell></row><row><cell>MRDA</cell><cell>56</cell><cell>6</cell><cell>12</cell><cell>110k</cell><cell>5</cell><cell>DA</cell><cell>2.6k</cell></row><row><cell>DyDA a</cell><cell>11k</cell><cell>1k</cell><cell>1k</cell><cell>102k</cell><cell>4</cell><cell>DA</cell><cell>25.5k</cell></row><row><cell>MT</cell><cell>121</cell><cell>22</cell><cell>25</cell><cell>36k</cell><cell>12</cell><cell>DA</cell><cell>3k</cell></row><row><cell>Oasis</cell><cell>508</cell><cell>64</cell><cell>64</cell><cell>15k</cell><cell>42</cell><cell>DA</cell><cell>357</cell></row><row><cell>DyDA e</cell><cell>11k</cell><cell>1k</cell><cell>1k</cell><cell>102k</cell><cell>7</cell><cell>E</cell><cell>2.2k</cell></row><row><cell>MELD s</cell><cell>934</cell><cell>104</cell><cell>280</cell><cell>13k</cell><cell>3</cell><cell>S</cell><cell>4.3k</cell></row><row><cell>MELD e</cell><cell>934</cell><cell>104</cell><cell>280</cell><cell>13k</cell><cell>7</cell><cell>S</cell><cell>1.8k</cell></row><row><cell>IEMO</cell><cell>108</cell><cell>12</cell><cell>31</cell><cell>10k</cell><cell>6</cell><cell>E</cell><cell>1.7k</cell></row><row><cell>SEM</cell><cell>62</cell><cell>7</cell><cell>10</cell><cell>5,6k</cell><cell>3</cell><cell>S</cell><cell>1.9k</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of datasets composing SILICONE. E stands for emotion label and S for sentiment label; stands for datasets with available official split. Sizes of Train, Val and Test are given in number of conversations.</figDesc><table><row><cell></cell><cell cols="3">Avg Avg DA Avg E/S</cell></row><row><cell cols="2">BERT (+MLP) 72,8</cell><cell>81.5</cell><cell>64.0</cell></row><row><cell cols="2">BERT (+GRU) 69.9</cell><cell>80.4</cell><cell>59.3</cell></row><row><cell cols="2">BERT (+CRF) 72.8</cell><cell>81.5</cell><cell>64.1</cell></row><row><cell>HR (+MLP) HR (+GRU) HR (+CRF)</cell><cell>69.8 67.6 70.5</cell><cell>79.1 79.4 80.3</cell><cell>60.4 55.7 60.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Experiments comparing decoder perfor- mances. Results are given on SILICONE for two types of baseline encoders (pre-trained BERT mod- els and hierarchical recurrent encoders HR).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Avg SwDA MRDA DyDA DA MT Oasis DyDA e MELD s MELD e IEMO SEM BERT-4layers 70.4 77.8 90.7 79.0 88.4 66.8 90.3 55.3 53.4 43.0 58.8 BERT 72.8 79.2 90.7 82.6 88.2 66.9 91.9 59.3 61.4 45.0 62.7 HR 69.8 77,5 90,9 80,1 82,8 64,3 91.5 59,3 59.9 40.3 51.1 HT (? u,d M LM ) (TINY) 73.3 79.3 92.0 80.1 90.0 68,3 92.5 62.6 59.9 42.0 66.6 HT (? d GAP ) (TINY) 71.6 78.6 91.8 78.1 89.3 64.1 91.6 60.5 55.7 42.2 63.9 HT (? u,d M LM ) (SMALL) 74.3 79.2 92.4 81.5 90.6 69.4 92.7 64.1 60.1 45.0 68.2</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performances of different encoders when decoding using a MLP on SILICONE. The datasets are grouped by label type (DA vs E/S) and ordered by decreasing size. MT stands for Map Task, IEM for IEMOCAP and Sem for Semaine.</figDesc><table><row><cell></cell><cell cols="2">Avg DA Avg E/S</cell></row><row><cell>BERT (4 layers)</cell><cell>80.5</cell><cell>60.2</cell></row><row><cell>HT (? BERT ?2layers ) HT (? u M LM )</cell><cell>80.5 80.8</cell><cell>61.1 64.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results of ablation studies on SILICONE hierarchical encoder where utterance embeddings are obtained with the hidden vector representing the first token [CLS] (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison of GAP and MLM with a comparable number of parameters. For all models a MLP decoder is used on top of a TINY pre-trained encoder.</figDesc><table><row><cell></cell><cell cols="4">Emb. Word Seq Total</cell></row><row><cell>BERT</cell><cell></cell><cell>87</cell><cell></cell><cell>110</cell></row><row><cell>BERT (4-layer)</cell><cell></cell><cell>43</cell><cell></cell><cell>66</cell></row><row><cell>HMLP</cell><cell>23</cell><cell>8.6</cell><cell>7.8</cell><cell>40</cell></row><row><cell>(TINY)</cell><cell></cell><cell>2.9</cell><cell>2.8</cell><cell>28.7</cell></row><row><cell>(SMALL)</cell><cell></cell><cell cols="2">10.6 10.6</cell><cell>45</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Number of parameters for the encoders. Sizes are given in million of parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>T instead of size 512. First, it allows parameters sharing, reducing the number of model parameters. The different model sizes are reported inTable 7.</figDesc><table><row><cell>TINY hierarchical models are pre-trained dur-</cell></row><row><cell>ing 180K iterations (1.5 days) on 4 NVIDIA</cell></row><row><cell>V100.</cell></row><row><cell>Our TINY model contains half the parameters</cell></row><row><cell>of BERT (4-layers). Furthermore, modelling</cell></row><row><cell>long-range dependencies hierarchically makes</cell></row><row><cell>learning faster and allows to get rid of learn-</cell></row><row><cell>ing tricks (e.g., partial order prediction (Yang</cell></row><row><cell>et al., 2019), two-stage pre-training based on</cell></row><row><cell>sequence length (Devlin et al., 2018)) required</cell></row><row><cell>for non-hierarchical encoders. Lastly, original</cell></row><row><cell>BERT and XLNET are pre-trained using re-</cell></row><row><cell>spectively 16 and 512 TPUs. Pre-training lasts</cell></row><row><cell>several days with over 500K iterations. Our</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil. 2007. Multi-task feature learning. In Advances in neural information processing systems, pages 41-48. Cristian Danescu-Niculescu-Mizil and Lillian Lee. 2011. Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. In Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, ACL 2011. Tanvi Dinkar, Pierre Colombo, Matthieu Labeau, and Chlo? Clavel. 2020. The importance of fillers for text representations of speech transcripts. arXiv preprint arXiv:2009.11340. Dan Jurafsky, and Joelle Pineau. 2020. Towards the systematic reporting of the energy and carbon footprints of machine learning. arXiv preprint arXiv:2002.05651. Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415. Hamid Jalalzai, Pierre Colombo, Chlo? Clavel, Eric Gaussier, Giovanna Varni, Emmanuel Vignon, and Anne Sabourin. 2020. Heavytailed representations, text polarity classification &amp; data augmentation. arXiv preprint arXiv:2003.11593. Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou, and Yoshua Bengio. 2017. A structured selfattentive sentence embedding. arXiv preprint arXiv:1703.03130. Deep contextualized word representations. arXiv preprint arXiv:1802.05365. St?phan Cl?men, et al. 2020b. The area of the convex hull of sampled curves: a robust functional statistical depth measure. In International Conference on Artificial Intelligence and Statistics, pages 570-579. Stephan Cl?men?on, and Florence d'Alch? Buc. 2019. Functional isolation forest. arXiv preprint arXiv:1904.04573. Omer Levy, and Samuel R Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27.</figDesc><table><row><cell>Alex Wang, Amanpreet Singh, Julian Michael, Fe-TINY SMALL</cell><cell>models beyond a fixed-length context. arXiv</cell></row><row><cell cols="2">Valentin Barriere, Chlo? Clavel, and Slim Essid. 2018. Attitude classification in adjacency pairs of a human-agent interaction with hidden con-ditional random fields. In 2018 IEEE Inter-national Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4949-4953. IEEE. Pawe l Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I?igo Casanueva, Ultes Stefan, Ramadan Osman, and Milica Ga?i?. 2018a. Mul-tiwoz -a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empir-ical Methods in Natural Language Processing (EMNLP). Pawe l Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga?i?. 2018b. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling. arXiv preprint arXiv:1810.00278. Sheng-Yeh Chen, Chao-Chun Hsu, Chuan-Chun Kuo, Lun-Wei Ku, et al. 2018a. Emotionlines: An emotion corpus of multi-party conversations. arXiv preprint arXiv:1802.08379. Zheqian Chen, Rongqin Yang, Zhou Zhao, Deng Cai, and Xiaofei He. 2018b. Dialogue act recog-nition via crf-attentive structured network. In The 41st International ACM SIGIR Conference on Research &amp; Development in Information Re-trieval, pages 225-234. Chloe Clavel and Zoraida Callejas. 2015. Senti-ment analysis: from opinion mining to human-agent interaction. IEEE Transactions on affec-tive computing, 7(1):74-93. Pierre Colombo, Emile Chapuis, Matteo Manica, Emmanuel Vignon, Giovanna Varni, and Chloe Clavel. 2020. Guiding attention in sequence-to-sequence models for dialogue act prediction. arXiv preprint arXiv:2002.08801. Pierre Colombo, Wojciech Witon, Ashutosh Modi, act sequence labeling using hierarchical encoder with crf. In Thirty-Second AAAI Conference on Artificial Intelligence. Zhenzhong Lan, Mingda Chen, Sebastian Good-man, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942. Hang Le, Lo?c Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen, Beno?t Crabb?, Laurent Besacier, and Didier Schwab. 2019. Flaubert: Unsupervised language model pre-training for french. arXiv preprint arXiv:1912.05372. Geoffrey Leech and Martin Weisser. 2003. Generic speech act annotation for task-oriented dia-logues. Ruizhe Li, Chenghua Lin, Matthew Collinson, Xiao Li, and Guanyi Chen. 2018a. A dual-attention hierarchical recurrent neural net-work for dialogue act classification. CoRR, abs/1810.09154. Ruizhe Li, Chenghua Lin, Matthew Collinson, Xiao Li, and Guanyi Chen. 2018b. A dual-attention hierarchical recurrent neural network for dialogue act classification. CoRR. Ruizhe Li, Chenghua Lin, Matthew Collinson, Xiao Li, and Guanyi Chen. 2018c. A dual-. 2015. Hierarchical neural network genera-tive models for movie dialogues. CoRR, abs/1507.04808. Elizabeth Shriberg, Raj Dhillon, Sonali Bhagat, Jeremy Ang, and Hannah Carvey. 2004. The ICSI meeting recorder dialog act (MRDA) cor-pus. In Proceedings of the 5th SIGdial Work-shop on Discourse and Dialogue at HLT-NAACL 2004, pages 97-100, Cambridge, Massachusetts, USA. Association for Computational Linguis-tics. Elizabeth E Shriberg. 1999. Phonetic conse-quences of speech disfluency. Technical report, SRI INTERNATIONAL MENLO PARK CA. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research, 15(1):1929-1958. Guillaume Staerman, Pierre Laforgue, Pavlo Mozharovskyi, and Florence d'Alch? Buc. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-bonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet: Generalized autoregressive pre-training for language understanding. In Ad-vances in neural information processing systems, pages 5754-5764. Sanghyun Yi, Rahul Goel, Chandra Khatri, Alessandra Cervone, Tagyoung Chung, Behnam Hedayatnia, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2019. Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators. arXiv preprint arXiv:1904.13015. Xingxing Zhang, Furu Wei, and Ming Zhou. 2019a. Hibert: Document level pre-training of hierarchi-Brunskill, Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, preprint arXiv:1901.02860. Luna De Bruyne, Pepa Atanasova, and Isabelle Augenstein. 2019. Joint emotion label space modelling for affect lexica. arXiv preprint arXiv:1911.08782. Ludovic Denoyer and Patrick Gallinari. 2006. The wikipedia xml corpus. In International Work-shop of the Initiative for the Evaluation of XML Retrieval, pages 12-19. Springer. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language un-derstanding. arXiv preprint arXiv:1810.04805. Alexandre Garcia, Pierre Colombo, Slim Essid, Florence d'Alch? Buc, and Chlo? Clavel. 2019. From the token to the review: A hierarchical multimodal approach to opinion mining. arXiv preprint arXiv:1908.11216. Deepanway Ghosal, Navonil Majumder, Soujanya Poria, Niyati Chhaya, and Alexander Gelbukh. 2019. Dialoguegcn: A graph convolutional neu-ral network for emotion recognition in conversa-tion. arXiv preprint arXiv:1908.11540. John J. Godfrey, Edward C. Holliman, and Jane McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proceedings of the 1992 IEEE International Con-ference on Acoustics, Speech and Signal Process-ing -Volume 1, ICASSP'92, page 517-520, USA. IEEE Computer Society. Devamanyu Hazarika, Soujanya Poria, Roger Zim-mermann, and Rada Mihalcea. 2019. Emo-tion recognition in conversations with transfer learning from generative conversation modeling. arXiv preprint arXiv:1910.04980. Peter Henderson, Jieru Hu, Joshua Romoff, Emma Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed rep-resentations of words and phrases and their com-positionality. In Advances in neural information processing systems, pages 3111-3119. R. Passonneau and E. Sachar. 2014. Loqui human-human dialogue corpus (transcriptions and an-notations). Jeffrey Pennington, Richard Socher, and Christo-pher D Manning. 2014. Glove: Global vectors Pedro Javier Ortiz Su?rez, Beno?t Sagot, and Laurent Romary. 2019. Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. Challenges in the Management of Large Corpora (CMLC-7) 2019, page 9. Dinoj Surendran and Gina-Anne Levow. 2006. Di-alog act tagging with support vector machines and hidden markov models. In Ninth Interna-tional Conference on Spoken Language Process-ing. Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. 2019. Tinybert: Distilling bert for nat-ural language understanding. arXiv preprint arXiv:1909.10351. Simon Keizer, Rieks op den Akker, and Anton Nijholt. 2002. Dialogue act recognition with bayesian networks for dutch dialogues. In Pro-ceedings of the Third SIGdial Workshop on Dis-course and Dialogue. Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Harshit Kumar, Arvind Agarwal, Riddhiman Das-gupta, and Sachindra Joshi. 2018. Dialogue Pierre Lison and J?rg Tiedemann. 2016. Open-subtitles2016: Extracting large parallel corpora from movie and tv subtitles. Pierre Lison, J?rg Tiedemann, Milen Kouylekov, et al. 2019. Open subtitles 2018: Statistical rescoring of sentence alignments in large, noisy parallel corpora. In LREC 2018, Eleventh In-ternational Conference on Language Resources and Evaluation. European Language Resources Association (ELRA). Yinhan Liu, Myle Ott, Naman Goyal, Jingfei arXiv:1907.11692. IEEE. mized bert pretraining approach. arXiv preprint ference Proceedings, volume 1, pages 405-408. Stoyanov. 2019. Roberta: A robustly opti-Acoustics, Speech, and Signal Processing Con-Mike Lewis, Luke Zettlemoyer, and Veselin cies. In 1996 IEEE International Conference on Du, Mandar Joshi, Danqi Chen, Omer Levy, Soujanya Poria, Devamanyu Hazarika, Navonil Ma-jumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2018a. Meld: A multimodal multi-party dataset for emotion recognition in conver-sations. Soujanya Poria, Devamanyu Hazarika, Navonil Ma-jumder, Gautam Naik, Erik Cambria, and Rada Guillaume Staerman, Pavlo Mozharovskyi, Nbs of heads 1 6 N d 2 4 N u 2 4 T 50 50 lix Hill, Wojciech Witon, Pierre Colombo, Ashutosh Modi, C 5 5 and Mubbasir Kapadia. 2018. Disney at iest 2018: Predicting emotions using an ensemble. In Proceedings of the 9th Workshop on Computa-6 6 T d nbs of heads Inner dimension 768 768 Andreas Stolcke, Klaus Ries, Noah Coccaro, Eliz-tional Approaches to Subjectivity, Sentiment and Model Dimension 768 768 abeth Shriberg, Rebecca Bates, Daniel Juraf-sky, Paul Taylor, Rachel Martin, Carol Van Ess-Social Media Analysis, pages 248-253. Vocab length 32000 32000 Mihalcea. 2018b. Meld: A multimodal multi-party dataset for emotion recognition in conver-sations. arXiv preprint arXiv:1810.02508. Sebastian Ruder. 2017. An overview of multi-task arXiv:1706.05098. tistical language modeling for speech disfluen-language processing. ArXiv, abs/1910.03771. Andreas Stolcke and Elizabeth Shriberg. 1996. Sta-gingface's transformers: State-of-the-art natural learning in deep neural networks. arXiv preprint Dykema, and Marie Meteer. 2000. Dialogue act modeling for automatic tagging and recognition Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony 768 768 T d : Emb. size d k : 64 64 of conversational speech. Computational linguis-Morgan Funtowicz, and Jamie Brew. 2019. Hug-tics, 26(3):339-373. Moi, Pierric Cistac, Tim Rault, R'emi Louf, d v : 64 64</cell></row><row><cell>James Kennedy, and Mubbasir Kapadia. 2019. attention hierarchical recurrent neural network 2020a. When ot meets mom: Robust estima-cal bidirectional transformers for document sum-</cell><cell>for word representation. In Proceedings of the</cell></row><row><cell>Affect-driven dialog generation. arXiv preprint for dialogue act classification. arXiv preprint tion of wasserstein distance. arXiv preprint</cell><cell>2014 conference on empirical methods in natu-</cell></row><row><cell>arXiv:1904.02793. arXiv:1810.09154. arXiv:2006.10325.</cell><cell>ral language processing (EMNLP), pages 1532-</cell></row><row><cell></cell><cell>1543.</cell></row><row><cell>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Guillaume Staerman, Pavlo Mozharovskyi,</cell><cell></cell></row><row><cell>Carbonell, Quoc V Le, and Ruslan Salakhutdi-</cell><cell></cell></row><row><cell>nov. 2019. Transformer-xl: Attentive language</cell><cell></cell></row></table><note>Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. Dailydialog: A manually labelled multi-turn dialogue dataset. Zhouhan Lin, Minwei Feng,Ilya Loshchilov and Frank Hutter. 2017. Decou- pled weight decay regularization. arXiv preprint arXiv:1711.05101. Ryan Lowe, Nissan Pow, Iulian Serban, and Joelle Pineau. 2015. The ubuntu dialogue corpus: A large dataset for research in unstructured multi- turn dialogue systems. CoRR, abs/1506.08909. Gary Mckeown, Michel Valstar, Roddy Cowie, Maja Pantic, and M. Schroder. 2013. The se- maine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent. Affective Comput- ing, IEEE Transactions on, 3:5-17. Shikib Mehri, Evgeniia Razumovsakaia, Tiancheng Zhao, and Maxine Eskenazi. 2019. Pretraining methods for dialog context representation learn- ing. arXiv preprint arXiv:1906.00414.Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018.Victor Sanh, Thomas Wolf, and Sebastian Ruder. 2019. A hierarchical multi-task approach for learning embeddings from semantic tasks. In Proceedings of the AAAI Conference on Artifi- cial Intelligence, volume 33, pages 6949-6956. Ozan Sener and Vladlen Koltun. 2018. Multi-task learning as multi-objective optimization. In Ad- vances in Neural Information Processing Sys- tems, pages 527-538. Iulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C. Courville, and Joelle PineauThomas Wolf, Quentin Lhoest, Patrick von Platen, Yacine Jernite, Mariama Drame, Julien Plu, Julien Chaumond, Clement Delangue, Clara Ma, Abhishek Thakur, Suraj Patil, Joe Davi- son, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angie McMillan-Major, Simon Brandeis, Sylvain Gugger, Fran?ois Lagunas, Lysandre Debut, Morgan Funtowicz, Anthony Moi, Sasha Rush, Philipp Schmidd, Pierric Cis- tac, Victor Mu?tar, Jeff Boudier, and Anna Tordjmann. 2020. Datasets. GitHub. Note: https://github.com/huggingface/datasets, 1. Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.marization. arXiv preprint arXiv:1905.06566. Yazhou Zhang, Qiuchi Li, Dawei Song, Peng Zhang, and Panpan Wang. 2019b. Quantum- inspired interactive networks for conversational sentiment analysis. Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Architecture hyperparameters used for the hierarchical pre-training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Avg SwDA MRDA DyDA DA MT Oasis DyDA e MELD s MELD e IEMO SEM</figDesc><table><row><cell>BERT-4layers (+MLP)</cell><cell>69.45</cell><cell>77.8 90.7</cell><cell>79.0</cell><cell>88.4</cell><cell>66.8</cell><cell>90.3</cell><cell>49.3</cell><cell>50.4</cell><cell>43.0 58.8</cell></row><row><cell>BERT (+MLP)</cell><cell>72.79</cell><cell>79.2 90.7</cell><cell>82.6</cell><cell>88.2</cell><cell>66.9</cell><cell>91.9</cell><cell>59.3</cell><cell>61.4</cell><cell>45.0 62.7</cell></row><row><cell>BERT (+GRU)</cell><cell>69.84</cell><cell>78.2 90.4</cell><cell>80.8</cell><cell>88.7</cell><cell>63.7</cell><cell>90</cell><cell>50.4</cell><cell>48.9</cell><cell>45.0 62.3</cell></row><row><cell>BERT (+CRF)</cell><cell>72.8</cell><cell>79.0 90.8</cell><cell>88.3</cell><cell>67.2</cell><cell>81.9</cell><cell>91.5</cell><cell>59.4</cell><cell>61.0</cell><cell>44.2 61.5</cell></row><row><cell>HR (+MLP) HR (+GRU) HR (+CRF) HT (? u,d M LM ) (TINY) HT (? d M LM ) (TINY) M LM ) (TINY) HT (? u HBERT (w) ? BERTmilmil (TINY)</cell><cell>69.77 67.54 70.5 73.3 72.4 72.4 70.8</cell><cell>77,5 90,9 78.2 90.9 77.8 91,3 79.3 92.0 78.5 91.8 78.6 91.8 77.6 91.4</cell><cell>80,1 79,9 79,7 80.1 78.0 79.0 79.3</cell><cell>82,8 84,4 87,5 90.0 89.8 89.8 88.3</cell><cell>64,3 63,5 65,3 68,3 66.0 65.0 65.8</cell><cell>91.5 91.5 91,1 92.5 92.5 91.8 91.9</cell><cell>59,3 50,7 62,1 62.6 62.6 61.8 58.0</cell><cell>59.9 50.4 57,4 59.9 59.3 58.1 56.3</cell><cell>40.3 51.1 35.2 50.7 42.1 50.7 42.0 66.6 42.0 63.5 39.2 68.9 40.0 59.1</cell></row><row><cell>HT (? u,d M LM ) (SMALL) HT (? d GAP ) (TINY) HT (? u GAP ) (TINY)</cell><cell>74.32 71.58 71.52</cell><cell>79.2 92.4 78.6 91.8 78.5 90.9</cell><cell>81.5 78.1 79.0</cell><cell>90.6 89.3 88.9</cell><cell>69.4 64.1 66.3</cell><cell>92.7 91.6 92.0</cell><cell>64.1 60.5 59.2</cell><cell>60.1 55.7 57.5</cell><cell>45.0 68.2 42.2 63.9 39.9 63.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>Performances of all mentioned model with different decoders such as MLP, GRU, CRF SILICONE. The datasets are grouped by label type (DA vs E/S) and order by decreasing size. Illustration of improvement of accuracy during pre-training stage on SEM for both a TINY and SMALL model.</figDesc><table><row><cell></cell><cell>0.65</cell></row><row><cell>Accuracy</cell><cell>0.55 0.60</cell></row><row><cell></cell><cell>0.50</cell></row><row><cell></cell><cell>0.45</cell><cell>( du MLM )) (SMALL) ( du MLM )) (TINY)</cell></row><row><cell></cell><cell>0</cell><cell>25 50 75 100 125 150 175 Number of iterrations</cell></row><row><cell cols="2">Figure 6:</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Upon publication, we will release the code, models and especially the preprocessing scripts to replicate our results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We choose ?T = 6s 7 Using pre-training method based on the next utterance proposed byMehri et al. (2019)  requires dropping conversation shorter than T + 1 leading to a nonnegligible loss in the preprocessing stage.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">We investigate a similar setting for GAP which lead to poor results, the loss hit a plateau suggesting that objectives are competing against each other. More advanced optimisations techniques(Sener and Koltun,  2018)  are left for future work.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by a grant overseen from the French National Research Agency (ANR-17-MAOI).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agarap</forename><surname>Abien Fred</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08375</idno>
		<title level="m">Deep learning using rectified linear units (relu)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
