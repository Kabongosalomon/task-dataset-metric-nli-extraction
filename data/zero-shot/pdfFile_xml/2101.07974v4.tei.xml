<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TCLR: Temporal Contrastive Learning for Video Representation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Dave</surname></persName>
							<email>ishandave@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mamshad</roleName><forename type="first">Rohit</forename><surname>Gupta</surname></persName>
							<email>rohitg@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayeem</forename><surname>Rizve</surname></persName>
							<email>nayeemrizve@knights.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
							<email>shah@crcv.ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Center for Research in Computer Vision</orgName>
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TCLR: Temporal Contrastive Learning for Video Representation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Contrastive learning has nearly closed the gap between supervised and self-supervised learning of image representations, and has also been explored for videos. However, prior work on contrastive learning for video data has not explored the effect of explicitly encouraging the features to be distinct across the temporal dimension. We develop a new temporal contrastive learning framework consisting of two novel losses to improve upon existing contrastive self-supervised video representation learning methods. The local-local temporal contrastive loss adds the task of discriminating between non-overlapping clips from the same video, whereas the global-local temporal contrastive aims to discriminate between timesteps of the feature map of an input clip in order to increase the temporal diversity of the learned features. Our proposed temporal contrastive learning framework achieves significant improvement over the state-of-the-art results in various downstream video understanding tasks such as action recognition, limited-label action classification, and nearest-neighbor video retrieval on multiple video datasets and backbones. We also demonstrate significant improvement in fine-grained action classification for visually similar classes. With the commonly used 3D ResNet-18 architecture with UCF101 pretraining, we achieve 82.4% (+5.1% increase over the previous best) top-1 accuracy on UCF101 and 52.9% (+5.4% increase) on HMDB51 action classification, and 56.2% (+11.7% increase) Top-1 Recall on UCF101 nearest neighbor video retrieval. Code released at https://github.com/ DAVEISHAN/TCLR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Large-scale labeled datasets such as Kinetics <ref type="bibr" target="#b9">[10]</ref>, LSHVU <ref type="bibr" target="#b16">[17]</ref> etc have been crucial for recent advances in video understanding tasks. Since training a video encoder using existing supervised learning approaches is labelinefficient <ref type="bibr">[32]</ref>, annotated video data is required at a large scale. This costs enormous human effort and time, much more so than annotating images. At the same time, a  <ref type="figure">Figure 1</ref>. Videos from standard action recognition datasets often have distinct temporal stages. For example, in figure (a) we can see the two distinct stages (Running and Jumping) of the LongJump action. Typically predictions across multiple short clips are aggregated, as a single short clip may not capture both stages of the action. We show the comparison of vanilla instance discrimination based contrastive (IC) self-supervision and our proposed TCLR method on (b) Nearest neighbour retrieval and (c) Linear classification tasks. We find that IC trained models do not benefit much from using multiple clips during evaluation. This is a result of IC imposing within instance temporal invariance. This motivates our proposed TCLR pre-training, which explicitly encourages learning distinct features across time.</p><p>tremendous amount of unlabeled video data is easily available on the internet. Research in self-supervised video representation learning can unlock the corpus of readily available unlabeled video data and unshackle progress in video understanding.</p><p>Recently, Contrastive Self-supervised Learning (CSL) based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr">27]</ref> have demonstrated the ability to learn powerful image representations in a self-supervised manner, and have narrowed down the performance gap between unsupervised and supervised representation learning on various image understanding downstream tasks.</p><p>A simple yet effective extension of CSL to the video domain can be obtained by using the InfoNCE instance discrimination objective, where the model learns to distinguish clips of a given video from the clips of other videos in the dataset (see <ref type="figure" target="#fig_1">Figure 2a</ref>). Unlike images, videos have both time-invariant and the temporally varying properties. For example, in a LongJump video from UCF101 (See <ref type="figure">Figure 1</ref>), running and jumping represent two very different stages of the action. Usually, video understanding models utilize temporally varying features by aggregating along the temporal dimension to obtain a video level prediction. While the significant success can be achieved on many video understanding tasks by only modelling the temporally invariant properties, it maybe possible that the temporally varying properties can also play an important role in further improvements on these tasks. Whether video representations should be invariant or distinct along the temporal dimension is an open question in the literature. Instance contrastive pre-training, however, encourages the model to learn similar features to represent temporally distant clips from the video, i.e. it enforces temporal invariance on the features. While instance level contrastive learning lies on one end of the spectrum, some recent works have tried to relax the invariance constraint through various means, such as, using a weighted temporal sampling strategy to avoid enforcing invariance between temporally distant clips [49], cross-modal mining of positive samples from across video instances <ref type="bibr">[25]</ref> or adding additional pretext tasks that require learning temporal features <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">50,</ref><ref type="bibr">63,</ref><ref type="bibr">69]</ref>.</p><p>We take a different approach by explicitly encouraging the learning of temporally distinct video representations. The challenge with video classification is modelling variable length videos with a fixed number of parameters. 3D CNNs tackle this challenge by temporal aggregation of features across two levels: averaging across distinct fixed length temporal segments of a video (clips) and also temporal pooling across the feature map of each clip. Based on this observation, we propose two different temporal contrastive losses in order to learn temporally distinct features across the video: one which acts across clips of the same video, and another which acts across the timesteps of the feature map of the same clip. Combined with the vanilla instance contrastive loss, these novel losses result in an increase in the temporal diversity of the learned features, and better accuracy on downstream tasks.</p><p>Our first proposed loss is the local-local temporal contrastive loss <ref type="figure" target="#fig_1">(Figure 2b</ref>), which ensures that temporally nonoverlapping clips from the same video are mapped to distinct representations. This loss treats randomly augmented versions of the same clip as positive pairs to be brought together, and other non-overlapping clips from the same video as negative matches to be pushed away. While the locallocal loss ensures that distinct clips have distinct representations, in order to encourage temporal variation within each clip, we introduce a second temporal contrastive loss, the global-local temporal contrastive loss <ref type="figure" target="#fig_1">(Figure 2c</ref>). This loss constrains the timesteps of the feature map of a long "global" video clip to match the representations of the temporally aligned shorter "local" video clips.</p><p>Our complete framework is called Temporal Contrastive Learning of video Representations (henceforth referred to as TCLR). TCLR retains the ability of representations to successfully discriminate between video instances due to its instance contrastive loss. In addition, TCLR attempts to capture the within-instance temporal variation. Through extensive experiments on various downstream video understanding tasks, we demonstrate that both of our proposed Temporal Contrastive losses contribute to the learning of powerful video representations, and provide significant improvements. The original contributions of this work can be summarized as below:</p><p>? TCLR is the first contrastive learning framework to explicitly enforce within instance temporal feature variation for video understanding tasks. ? Novel local-local and global-local temporal contrastive losses, which when combined with the standard instance contrastive loss significantly outperform the state-of-the-art on various downstream video understanding tasks like action recognition, nearest neighbor video retrieval and action classification with limited labeled data, while using 3 different 3D CNN architectures and 2 datasets (UCF101 &amp; HMDB51). ? We propose the use of the challenging Diving48 finegrained action classification task for evaluating the quality of learned video representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent approaches for self-supervised video representation learning can be categorized into two major groups based on the self-supervised learning objective: (1) Pretext task based methods, and (2) Contrastive Learning based methods. Pretext task based approaches: Various pretext tasks have been devised for self-supervised video representation learning based on learning the correct temporal order of the data: verifying correct frame order [44], identifying the correctly ordered tuple from a set of shuffled orderings <ref type="bibr" target="#b19">[20,</ref><ref type="bibr">53]</ref>, sorting frame order <ref type="bibr">[37]</ref>, and predicting clip order <ref type="bibr">[66]</ref>. Some methods extend existing pretext tasks from the image domain to video domain, for example, solving spatiotemporal jigsaw puzzles <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">28,</ref><ref type="bibr">33]</ref> and identifying the rotation of transformed video clips <ref type="bibr">[31]</ref>. Many recent works rely on predicting video properties like playback rate of the video <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">50,</ref><ref type="bibr">63,</ref><ref type="bibr">70]</ref>, temporal transformation that has been applied from a given set <ref type="bibr">[29,</ref><ref type="bibr">30]</ref>, speediness of moving objects <ref type="bibr" target="#b6">[7]</ref>, and motion and appearance statistics of the video [61, 62]. Contrastive Self-supervised Learning (CSL) based approaches: Following the success of contrastive learning approaches of self-supervised image representation learning such as SimCLR <ref type="bibr" target="#b11">[12]</ref> and MoCo [27], there have been many extensions of contrastive learning to the video domain. For instance, various video CSL methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">46,</ref><ref type="bibr">49,</ref><ref type="bibr">50,</ref><ref type="bibr">54,</ref><ref type="bibr">56,</ref><ref type="bibr">63,</ref><ref type="bibr">68,</ref><ref type="bibr">69]</ref> leverage Instance level Discrimination objectives, and build their method upon them, where clips from the same video are treated as positives and clips from the different videos as negatives. CVRL [49] studies the importance of temporal augmentation and develops a temporal sampler to avoid enforcing excessive temporal invariance in learning video representation. VideoMoCo [46] improves image-based MoCo framework for video representation by encouraging temporal robustness of the encoder and modeling temporal decay of the keys. VTHCL [68] employs SlowFast architecture <ref type="bibr" target="#b17">[18]</ref> and uses contrastive loss with the slow and fast pathway representations as the positive pair. VIE [72] is proposed as a deep neural embeddingbased method to learn video representation in an unsupervised manner, by combining both static image representation from 2D CNN and dynamic motion representation from 3D CNN. Generative contrastive learning-based approaches such as predicting the the dense representation of the next video block [23,24], or Contrastive Predictive Coding (CPC) [45] for videos <ref type="bibr">[40]</ref> have also been studied in the literature.</p><p>AMDIM <ref type="bibr" target="#b3">[4]</ref> is another CSL approach for image representation learning, where a local view (spatial slice of the feature map taken from an intermediate layer) and a global view (full feature map) of differently augmented versions of the same image are considered as a positive pair, and global views of other images form negative pair of the contrastive loss. The method is adapted for the video domain <ref type="bibr" target="#b15">[16,</ref><ref type="bibr">67]</ref> by generating local views from the spatio-temporal features. Unlike this class of methods, which try to maximize agreement across features from different levels of the encoder, our Global-Local loss tries to learn distinct features across temporal slices of the feature map instead.</p><p>Some recent works combine pretext tasks along with contrastive learning in a multi-task setting to learn temporally varying features in the video representation. For example, using video clips with different playback rates as positive pairs for contrastive loss along with predicting the playback rate [63], or temporal transforms <ref type="bibr" target="#b4">[5]</ref>. Other works propose frame-based contrastive learning, along with existing pretext tasks of frame rotation prediction [35] and frame-tuple order verification <ref type="bibr">[69]</ref>. Unlike these works, TCLR takes a different approach by adding explicit temporal contrastive losses that encourage temporal diversity in the learned features, instead of utilizing a pretext task for this purpose.</p><p>Some works which try to capture intra-video variance using optical flow, but are nevertheless interesting to compare with. IIC [54] uses intra-instance negatives, but it relies on frame repeating and shuffling to generate these "hard" negatives, and does not focus on learning distinct features across the temporal axis. DSM [60] tries to decouple scene and motion features by an intra-instance triplet loss, which uses negatives generated by optical flow scaling and spatial warping. Some recent works use extra supervisory signals in addition to the RGB video data to learn video representation in a self-supervised manner. However, these methods either require additional cross-modal data (e.g. text narration [42], audio <ref type="bibr" target="#b0">[1]</ref>) or expensive and timeconsuming computation of hand-crafted visual priors (e.g. optical flow <ref type="bibr">[25,</ref><ref type="bibr">52,</ref><ref type="bibr">54,</ref><ref type="bibr">55,</ref><ref type="bibr">64]</ref> or dense trajectories [56]). In this work we focus only on learning from RGB data without using any auxiliary data from any extra modality or additionally computed visual priors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The key idea in our proposed framework is to learn two levels of contrastive discrimination: instance discrimination using the instance contrastive loss and within-instance temporal level discrimination using our novel temporal contrastive losses. The two different temporal contrastive losses which are applied within the same video instance: Local-Local Loss and Global-Local loss. Each of these losses is explained in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Instance Contrastive Loss</head><p>We leverage the idea of instance discrimination using In-foNCE [22] based contrastive loss for learning video representations. In the video domain, in addition to leveraging image-based spatial augmentations, temporal augmentations can also be applied to generate different transformed versions of a particular instance. For a video instance, we extract various clips (starting from different timestamps and/or having different frame sampling rates). We consider a randomly sampled mini-batch of size N B from different video instances, and from each instance we extract a pair of clips from random timesteps resulting in a total of 2N clips. The extracted clips are augmented using standard stochastic appearance and geometric transformations. <ref type="bibr" target="#b0">1</ref> Each of the transformed clips is then passed through a 3D-CNN based video encoder which is followed by a non-linear projection head (multi-layer perceptron) to project the encoded features on the representation space. Hence, for each videoinstance i we get two clip representations (G i , G ? i ). The instance contrastive loss is defined as follows:</p><formula xml:id="formula_0">L i IC = ? log h (Gi, G ? i ) N B j=1 [1 [j? =i] h(Gi, Gj) + h(Gi, G ? j )] ,<label>(1)</label></formula><p>where, h(u, v) = exp u T v/(?u??v??) is used to compute the similarity between u and v vectors with an adjustable parameter temperature, ?. 1 [j? =i] ? {0, 1} is an indicator function which equals 1 iff j ? = i. <ref type="bibr" target="#b0">1</ref> More details about the augmentations are available in Section 4 and Section C of the supplementary material.  <ref type="figure">Figure 3</ref>. Local-Local Temporal Contrastive Loss is applied to representations of non-overlapping clips extracted from same video instance i. For the clip starting at timestep p, two randomly transformed versions are generated and their representations Gi,p and G ? i,p serve as the positive pair for the loss, whereas the other non-overlapping clips along with the anchor, Gi,p, form the negative pairs. p = 1 serves as anchor, further details in Section 3.2.1. ?A, ? A ? , ..., ? D ? are random set of augmentation sampled from universal set T r.</p><formula xml:id="formula_1">Representation (G i,t ) Representation (G' i,t ) G i, 1 3D CNN MLP 3D CNN MLP G i, 2 G i, 3 G i, 4 G' i, 1 G' i, 2 G' i, 3 G' i, 4 Time F x 3 x H x W Video (V i ) Clip-A' Clip-B' Clip-C' Clip-D' Clip-A Clip-B Clip-C Clip-D T x 3 x H x W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Temporal Contrastive Losses</head><p>For self-supervised training using the instance contrastive loss of Equation 1, the model is presented with multiple clips cropped from random spatio-temporal locations within a single video as positive matches. This encourages the model to become invariant to the inherent variation present within an instance. In order to enable contrastive learning to represent within instance temporal variation, we introduce two novel temporal contrastive losses: local-local temporal contrastive loss and global-local temporal contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Local-Local Temporal Contrastive Loss</head><p>For this loss, we treat non-overlapping clips sampled from different temporal segments of the same video instance as negative pairs, and randomly transformed versions of the same clip as a positive pair.</p><p>The local-local loss is defined by Equation 2 and illustrated in <ref type="figure">Figure 3</ref>. A given video instance i is divided into N clips non-overlapping clips. For the anchor clip starting at timestep p, its representation G i,p , and the representation of its transformed version form the positive pair (G i,p ,G ? i,p ) for this loss; whereas the other N clips ? 1 clips from the same video instance (and their transformed versions) form the negative pairs. Hence, for every positive pair, the locallocal contrastive loss has 2 ? N clips ? 2 negative pairs as defined in the following loss:</p><formula xml:id="formula_2">L i LL = ? N clips p=1 log h Gi,p, G ? i,p N clips q=1 [1 [q? =p] h(Gi,p, Gi,q) + h(Gi,p, G ? i,q )] .<label>(2)</label></formula><p>The key difference between the Instance contrastive loss (Equation 1) and the proposed Local-Local Temporal contrastive loss (Equation 2) is that for the local-local loss the negatives come from the same video instance but from a different temporal segment (clips), whereas in Equation 1, the negative pairs come from different video instances. x 512 x H' x W'</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Global-Local Temporal Contrastive Loss</head><formula xml:id="formula_3">Clip-A Clip-E L i, 2 L i, 3 L i, 4</formula><p>Feature Map  The feature map in the higher layers of 3D CNNs are capable of representing temporal variation in the input clip, which is temporally pooled before being used for classification, or projected in the representation space in the case of contrastive learning. The objective of our proposed globallocal temporal contrastive loss is to explicitly encourage the model to learn feature maps that represent the temporal locality of the input clip across temporal dimension of the feature map.</p><formula xml:id="formula_4">G i, 1 G i, 2 G i, 3 G i.</formula><p>This loss is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. The notion of local and global is used at two different levels: at the input clip level and the feature level. Clip-E is a global clip and Clip A-D are local clips contained within Clip-A. Features are referred to as global after the final pooling operation and a temporal slice of the feature map before the pooling operation is referred to as a local feature. In <ref type="figure" target="#fig_4">Figure 4</ref>, L i,1 is the local feature of the global Clip-A and G i,1 is the global feature of the local Clip-B.</p><p>For a video instance i, divided into N clips clips, the local clip k can either be represented by a global (pooled) representation G i,k or a local representation L i,k of the corresponding timestep in the feature map of the global clip. This loss has two sets of reciprocal terms, with G i,k and L i,k serving as the anchor for each term. The negative pairs are supplied by matching the anchors with representations corresponding to other non-overlapping local clips. Note that similar to our local-local temporal contrastive loss we do not use negatives from other video instances for calculating this loss. The loss is defined by the following equations:</p><formula xml:id="formula_5">L i GL k = log h (L i,k , G i,k ) N clips q=1 h(L i,k , Gi,q) +log h (G i,k , L i,k ) N clips q=1 h(G i,k , Li,q) ,<label>(3)</label></formula><formula xml:id="formula_6">L i GL = ? N clips k=1 L i GL k .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets and Implementation: We use three action recog- For non-linear projection head, we use a multi-layer perceptron with 1-hidden layer following experimental setting of <ref type="bibr" target="#b11">[12]</ref>. We utilize 4 local clips per global clip for the global-local temporal contrastive loss. For all reported results, we utilize commonly used random augmentations including appearance-based transforms such as grayscale, channel dropping, and color jittering and geometry-based transforms like random scaling, random cropping, random cut-out and random horizontal-flip.</p><p>Our results can be further improved by using more complex augmentations like Gaussian blurring, shearing and ro-tation, however these are not used in the results reported in this paper. We provide results with more complex augmentation in Section D of the supplementary material. For selfsupervised pretraining we use UCF101 training set (split-1) or Kinetics400 training set, without using any class labels. For all self-supervised pretraining, supervised finetuning and other downstream tasks, we use clips of 16 frames with a resolution of 112?112. More implementation details can be found in Section C of the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluating Self-supervised Representations</head><p>We evaluate the learned video representation using different downstream video understanding tasks: i) action recognition and ii) nearest neighbor video retrieval on UCF101 and HMDB51 datasets, and iii) limited label training on UCF101, following protocols from prior works <ref type="bibr">[24]</ref>. We also evaluate our learned representations on the challenging Diving-48 fine-grained action recognition task [39]; to the best of our knowledge TCLR is the first work that reports result on this challenging task. Our method is also employed in Knights <ref type="bibr" target="#b14">[15]</ref> to get first place in ICCV-21 Action recognition challenge <ref type="bibr">[38]</ref>. Action Recognition on UCF101 and HMDB51:</p><p>For the action recognition task on UCF101 and HMDB51, we first pretrain different video encoders in selfsupervised manner on UCF101 or Kinetics400, and then perform supervised fine-tuning. In order to ensure fair comparison, we evaluate the method on the three most commonly used 3D CNN backbones in the prior works, while also listing details about the input clip resolution and number of frames used, as it is known to affect the results significantly <ref type="bibr">[47,</ref><ref type="bibr">58]</ref>. Comparison results are shown in <ref type="table" target="#tab_2">Table 1</ref>. Previous results based on multi-modal approaches that utilize text, audio etc are excluded <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">43,</ref><ref type="bibr">47]</ref>. Results from prior works which do not utilize the three common architectures or use optical flow as input are presented in grey. We reproduce the results for CVRL [49] using the R3D-18 model and 112 resolution, and carefully implement their temporal sampling and augmentation strategy. TCLR consistently outperforms the state-of-art by wide margins for all comparable combinations of backbone, pre-training dataset and fine-tuning dataset. The best prior results are reported by TaCo <ref type="bibr" target="#b4">[5]</ref>, which relies on learning temporal features using pretext tasks on top of instance discrimination. Our consistent improvement over TaCo suggests that using temporal contrastive losses results in better features than using existing temporal pre-text tasks in a multi-task setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nearest Neighbor Video Retrieval:</head><p>We evaluate the learned representation by performing nearest neighbor retrieval after self-supervised pretraining on UCF101 videos and without any supervised finetuning. Videos from the test set are used as the query and the training set as the search gallery, following the protocol used in prior work <ref type="bibr">[24]</ref>. Results for retrieval are presented for both UCF101 and HMDB51 in <ref type="table" target="#tab_3">Table 2</ref>. TCLR outperforms previous state-of-the-art in UCF101 Top-1 Retrieval by 12% to 30% depending on the architecture Label Efficiency/ Finetuning with limited data: We evaluate our pretrained model for action recognition task on UCF101 (split-1) with limited labeled training data following the protocols from prior work [21, 24, 31]. Our method outperforms MotionFit [21], MemDPC [24] and RotNet3D <ref type="bibr">[31]</ref> in all settings of limited percentage of training data as shown in <ref type="figure" target="#fig_5">Figure 5</ref>. This result in addition to NN results demonstrate that the learned representations from TCLR are significantly better than other recent works, TCLR can achieve competitive performance to MemDPC with only 10% of the labeled data. Experiments on Diving-48 Dataset: This task presents some additional challenges over and above the standard action recognition task: action categories in Diving48 are defined by a combination of takeoff (dive groups), movements in flight (somersaults and/or twists), and entry (dive positions) stages. Two otherwise identical categories may only have fine grained differences limited to only one of the three stages. This makes Diving48 useful for evaluating the fine-grained representation capabilities of the model, which are not well tested by action recognition tasks on common benchmark datasets like UCF101 and HMDB51. Our proposed evaluation protocol consists of self-supervised pretraining followed by supervised finetuning on the Diving48-Train set. We adopt the 3D ResNet-18 architecture, with input resolution and clip length fixed at 112 ? 112 and 16 frames, respectively. Results are summarized in <ref type="table" target="#tab_4">Table 3</ref>. TCLR pretraining on Diving48 without extra data outperforms random initialization and MiniKinetics [65] supervised pretraining. The within-instance temporal discrimination losses in TCLR help it outperform the instance contrastive loss. This is due to TCLR learning features to represent fine-grained differences between parts of diving video instances.    In order to study the impact of each contrastive loss used in TCLR, we test R3D-18 models pre-trained on UCF101 videos with a subset of the losses on each downstream task. The results for linear evaluation, full fine-tuning, transfer learning to HMDB51 and nearest neighbour retrieval are shown in <ref type="table" target="#tab_5">Table 4</ref>. Addition of each temporal contrastive loss (L LL &amp; L GL ) leads to significant gains over instance contrastive and random initialization baselines, with the best results coming from combined use of all losses. We verify the correctness of our baselines by comparing them with similar results reported in prior work. Details can be found in Section E of the supplementary material. One interesting observation is that purely temporal contrastive learning, without instance discrimination, does not learn strong features directly (as can be seen from results on linear evaluation and NN-Retrieval), but it provides an useful initialization prior for supervised finetuning experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Temporal diversity helps video understanding</head><p>To study the impact of temporal feature diversity directly, we utilize self-supervised pre-trained models only to avoid influence of supervised fine-tuning. As shown in <ref type="figure">Fig  7,</ref> increasing from 1 clip to 10 clips per video, we observe that the pretraining strategies using temporal contrastive losses get significant performance gains (about 7-8% for each individual loss and 14.67% for TCLR) with increasing number of clips. Instance Contrastive pretraining which  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Distinguishing confusing class pairs</head><p>To examine the ability of TCLR to distinguish confusing classes, we looked at the most confused action class pairs for UCF101 action recognition models trained from scratch. We observe that the these pairs mostly consist of fine-grained variants of action classes, for example the swimming actions BreastStroke and FrontCrawl.</p><p>Some such pairs of classes are visualized in <ref type="figure">Figure 6d</ref>. We can see that these classes are confusing because the corresponding frames are visually similar. In this study we considered a model without pretraining as a baseline, and tried to see the impact of instance contrastive and TCLR pretraining on it. We observe that despite a significant overall improvement in accuracy, instance contrastive pre-training does not provide any significant gain in distinguishing these confused class pairs over the scratch baseline. On the other hand, TCLR pre-training helps remarkably with the confused classes. Average recall for these 8 classes is 42.5% for the scratch model, 44.9% for the IC model and 74.8% for the TCLR model. Since the classes are visually similar, distinguishing them requires learning the temporal variation in videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose two novel Temporal Contrastive losses to improve the quality of learned self-supervised video representations over standard instance discrimination contrastive learning. We provide extensive experimental evidence on three diverse datasets and obtain state-of-theart results across various downstream video understanding tasks. The success of our approach underscores the benefits of contrastive learning beyond instance discrimination. and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Overview</head><p>The supplementary material is organized into the following sections:</p><p>? Section B: Dataset details ? Section C: Implementation details such as network architectures, data augmentations, self supervised training details (losses, optimizers, etc) and and downstream task evaluation protocols.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Network Architecture</head><p>We use the implementation of 3D-ResNet-18, R(2+1)D-18 and C3D provided by authors of prior works [63,66,70]. For 3D-ResNet and R(2+1)D, to get a feature map with temporal dimension 4 at the end of the penultimate layer, we change the temporal stride of the conv5 layer to 1, and the dilation factor to 2 in order to keep the temporal receptive field the same, while increasing the temporal resolution of the features. We spatially pool the feature map only after the penultimate layer which results in an encoded feature of dimension 4 ? 512 where 4 is temporal dimension and 512 is number of channels.</p><p>The MLP projection head consists of 2 layers in the default setting: Linear(512, 512) with ReLU activation and Linear(512, 128) followed by L2-Normalization. We use the 128-D output as representation vector to compute losses. We use shared weights for the 3D-CNN backbone and MLP across computation of different losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Augmentation</head><p>The augmentations we use for reporting benchmark scores are standard and prior methods also utilize similar augmentation strategies. We apply augmentation transformations from two broad categories: (1) Appearance based transforms and (2) Geometry based transforms. Geometry based transforms Basic geometry-based augmentations include random scaling, random cropping, random cut-out and random horizontal-flip (details in Algorithm 1). Apart from basic augmentations, we also try the use of shear and rotation transformation with a small range (details in Section D), however for our benchmark results we only use basic augmentations. Appearance based transforms Basic appearance-based transforms include random grayscale, random color drop, and color jittering with HSV color space (details in Algorithm 1). We also experiment with using Gaussian blurring filter (details in Section D), but we don't use it for our reported results. Each augmentation is applied with a certain probability and random strength selected from a specified interval. The exact same augmentation is applied on each frames of a video clip to ensure temporal consistency. Our augmentation pipeline is explained in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Self-supervised training</head><p>The model for self-supervised pretraining consists of a backbone architecture combined with an MLP projection head. The loss is computed using the embedding from the final layer of the MLP. We use an input clip resolution of 112 ? 112 and 16 frames. From a set of 64 consecutive frames, the global clip is composed of 16 frames sampled with a skip rate of 4, whereas the 4 local clips also have 16 frames but are sampled with a skip rate of 1. We train our model by combining all loss terms with batch size of 40 as shown in <ref type="figure" target="#fig_5">Equation 5</ref>, where, N B is the size of minibatch, L IC is the instance contrastive loss, L LL is the locallocal temporal contrastive loss and L GL is the global-local temporal contrastive loss. <ref type="figure" target="#fig_10">Figure 8</ref> shows the values of different losses as the training progresses. We run the self-supervised pretraining for 400 epochs for UCF101 and 100 epochs for pre-Algorithm 1: TCLR self-supervised training data augmentation. training of Kinetics400. We use Adam [34] optimizer with ? 1 = 0.9, ? 2 = 0.999, ? = 10 ?8 , the default parameters for PyTorch v1.6.0. We use an initial learning of 0.001 and decay the learning rate by a factor of 10 when the loss plateaus. We also use linear warm-up for the first 10 epochs.</p><formula xml:id="formula_7">L = N B i=1 L i IC + L i LL + L i GL ,<label>(5)</label></formula><formula xml:id="formula_8">1 Input: Frames F , Output: Transformed Frames F ? / * Random Resized Crop * / 2 w, h = dimensions(F ) 3 Generate scale ? U (0.6, 1.0) 4 Generate sx ? U (0, 1), sy ? U(0, 1) 5 x0, y0 = sx ? (w ? w ? scale), sy ? (h ? h ? scale) 6 x1, y1 = x0 + scale ? w, y0 + scale ? h 7 F ? = F .spatial crop(x0,</formula><p>As mentioned in <ref type="table" target="#tab_2">Table-1</ref> of the main paper, we reproduce results of CVRL for the commonly used configuration (model depth = 18, input frames = 16, resolution = 112?112). In our replication, we use the exact same temporally consistent augmentations and temporal interval sampler P (t) ? ?t + c. Pretraining is done on UCF101 split-1 training set with batch size of 512 for 800 epochs.</p><p>Step Global-Local </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Downstream Task Protocols</head><p>We evaluate self-supervised pretrained model for three downstream tasks: Action Recognition, Nearest Neighbor Video Retrieval, and Label Efficiency. Action Recognition: For the action recognition task we follow the protocol used in prior works [23, 24, 30, 56, 63]. We attach a fully-connected layer (randomly initialized) to the pretrained video encoder and train all layers of the model using the training data with a cross-entropy loss. Following the prior works, we also utilize basic augmentations such as random crop, random scale and horizontal flip while training. These augmentations are standard for action recognition training <ref type="bibr">[26,</ref><ref type="bibr">58]</ref>. For the testing, we sample 10 uniformly spaced clips from the video average their predictions to get a video-level prediction. We utilize input clips of 112 ? 112 resolution and 16 frames with a skip rate of 2. A base learning rate of 0.001 is used, which decreases by a factor of 10 on loss plateau. Linear warm-up starting from minimum learning rate of 0.00001 and rising to base Learning Rate of 0.001 is used for the first 10 epochs of the training. Label Efficiency/ Finetuning with limited data: We use the same experimental setting as used for action recognition task and train with limited training data. For each limited fraction of training data, we repeat the experiment 3 times with different randomly sampled training-subsets and report average result. Nearest Neighbor Video Retrieval: In this task, we directly use the self-supervised pretrained video encoder without further supervised finetuning. In the paper we report results using features from the final layer of the encoder with spatial pooling and averaged over 10 uniformly spaced clips from each video as followed by prior work [24].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Ablations</head><p>We perform additional ablations using 3D-ResNet-18 with self-supervised pretraining on UCF101 training set (split-1) and linear evaluation on UCF101 test set (split-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Impact of losses</head><p>In the main paper, we carry out ablation studies to measure the impact of each of our novel loss functions on the downstream tasks. A more complete set of results for these experiments are presented in <ref type="table">Tables 5, 6</ref>   <ref type="table">Table 6</ref>. Nearest Neighbor Video Retrieval on HMDB51 using self-supervised R3D-18 models pretrained on UCF101 videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Augmentation Types</head><p>We perform an ablation study to measure the impact of computationally expensive transformations (specifically random Gaussian Blurring, Shearing and Rotation). We observe (Results in <ref type="table">Table 8</ref>) that adding these transforms only has a small effect on the performance of the model on downstream tasks, and removing them can reduce training time by as much as 30%. In these augmentation ablations, we use Gaussian blur with 30% probability, kernel size <ref type="bibr" target="#b14">(15,</ref><ref type="bibr" target="#b14">15)</ref>   <ref type="table">Table 7</ref>. Top-1 Accuracy after limited label finetuning of selfsupervised R3D-18 models pretrained on UCF101 (split-1) videos. and standard deviation selected randomly from (0, 0.5) interval. We apply shear and rotation with 30% probability, with rotation angles selected randomly from (?20 ? , 20 ? ). Even though it's possible to obtain slightly better results by using blur and other complex augmentations we do not use them for our main reported results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transforms</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Embedding Size</head><p>It has previously been reported that the size of the embedding doesn't have a significant effect <ref type="bibr" target="#b11">[12]</ref> on image self supervised learning. In our experiments we observe that using a large embedding size typically results in slightly better performance, as reported in <ref type="table">Table 9</ref>. The main paper utilizes embedding of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Embedding size</head><p>Linear Evaluation (Top-1 Accuracy) 128 69.91 256 70.12 (+0.21) 512 70.63 (+0.72) <ref type="table">Table 9</ref>. Effect of embedding size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Number of Timesteps</head><p>A key hyperparameter for the Temporal Constrastive losses is the number of Timesteps (N T ) that a given video instance is sliced into. Apart from the default setting of N T = 4 we also study N T = 2 and find that it degrades performance. We do not use higher values of N T since that causes a significant increase in amount of GPU memory and computation required. Results are available in <ref type="table" target="#tab_2">Table 10</ref>.</p><formula xml:id="formula_9">Timesteps (N T )</formula><p>Linear Evaluation (Top-1 Accuracy) 4 69.91 2 66.60 (-3.31) <ref type="table" target="#tab_2">Table 10</ref>. Effect of timesteps used in temporal contrastive losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5. Skip Rate</head><p>For the instance contrastive loss we generally use the same skip rate for both the anchor and pairing clip. We try using different skip rates (selected from {4, 8, 12, 16}), but this results in a performance degradation of about 2.3% over the baseline.</p><p>For the Global-Local Temporal contrastive loss, we use a fixed skip rate of 4 for the global clips and a skip rate of 1 for the local clips. We perform an additional experiment, where we randomly select a skip rate for the Global clip from this set: {4, 8, 12, 16}. We set the skip rate for the local clip to 1/4 th of the selected skip rate for global clip. This leads to a 2.6% degradation from the fixed skip rate baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6. Contrastive Loss Temperature</head><p>Prior works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr">49,</ref><ref type="bibr">63]</ref> demonstrate that a temperature of about 0.1 is optimal for instance contrastive losses. We try different settings (Results in <ref type="table" target="#tab_2">Table 11</ref>) to discover the optimal temperature for the temporal contrastive losses. We discover that 0.1 is a good choice, and also that having it set to be higher than the temperature for the instance contrastive loss is very important. Setting the temperature for temporal contrastive loss lower than the instance contrastive loss leads to significantly poor performance (right of the origin in the chart), as can be seen in <ref type="figure" target="#fig_11">Figure 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Baseline Verification</head><p>Instance Contrastive Baseline: Our instance contrastive baseline using similar model architectures and input clip resolution (112 ? 112, 16 frames) achieves 71.3% . We use the same input size, architecture and data augmentations for our instance contrastive baseline and TCLR model.</p><p>To ensure our instance contrastive baseline is correctly trained we also provide results from prior works in <ref type="table" target="#tab_2">Table 13</ref> Random Initialization Baseline: Our baseline results with random initialization UCF101 finetuning (Top-1 classification accuracy 62.3%) which matches the prior results reported in [24] (61.8%). We report this random initializa-   <ref type="table" target="#tab_5">Table 4</ref> of the main paper along with other downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Benefit of Temporal Diversity</head><p>We discussed the effects of temporal diversity on NN-Retrieval task for UCF101 in the main paper and found that increased temporal diversity leads to an increased gap with respect to the IC baseline in the multi-clip setting. Here we provide similar results for other downstream tasks and datasets. Detailed results across 10 different clip counts for UCF101 linear evaluation can be seen in <ref type="figure" target="#fig_12">Figure 11</ref> and HMDB51 nearest neighbour retrieval can be seen in <ref type="figure">Figure</ref> 12. Summary results for 1 and 10 clip evaluation for 5 different tasks can be seen in <ref type="table" target="#tab_2">Table 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Comparison</head><p>Results from prior work which were excluded from the main paper are presented in <ref type="table" target="#tab_2">Table 15</ref>. <ref type="table">The Table is</ref> divided into 3 sections. The first section includes results from papers which utilize visual modality but only provide results on specialized architectures or larger input sizes which are not widely used and hence cannot be compared fairly with other methods. The second section includes works which utilize multi-modal data, beyond the visual domain, such as text and audio. The third section shows our results for temporal contrastive learning from the main paper to allow for comparison with these works, such a comparison however is not fair for reasons explained earlier. We also perform lin- ear classification of Kinetics-400 using our self-supervised pre-trained R2+1D model, which gives 21.8% top-1 accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Feature Slice Representation Similarity</head><p>The goal of this visualization (see <ref type="figure">Figure 13</ref> is to verify if the model learns temporal diversity in the feature map as intended by the TCLR framework. With a long global clip (16 frames with a skip rate of 4, hence covering 64 frames), representations for 4 different timesteps of the video feature map were obtained using the video encoder and projection head. Cosine similarity was computed between timesteps for each video and then averaged across the dataset. As can be observed, L GL has the maximum impact on increasing within clip feature diversity, while L LL also induces more diversity than the IC baseline.  <ref type="table" target="#tab_2">Table 15</ref>. Additional Finetuning Results (average of 3 splits) for action classification on UCF101 and HMDB51 from prior work that were excluded from the main paper, along with our results for comparison. Please note that these results are not strictly comparable since the prior work uses specialized architectures and larger input sizes, which has significant effect on performance and require excessive computational resources. V denotes Video(RGB) modality, A denotes audio, and T denotes text modality. * ? modified architecture</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Qualitative: Nearest Neighbour Retrieval</head><p>Qualitative results for nearest neighbour retrieval task on UCF101 are presented in <ref type="figure" target="#fig_4">Figure 14</ref>. These results are obtained using self-supervised pretrained models, without any supervised training with labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Visualization of Learned Representations</head><p>We use t-SNE [59] to visualize and study the representation learned by the model during self-supervised pretraining (without any supervised finetuning). We compare our method to the standard instance contrastive pretraining and randomly initialized features for a selected set of classes in <ref type="figure" target="#fig_5">Figure 15</ref> for better visibility. In <ref type="figure">Figure 16</ref> we compare TCLR to standard instance contrastive training for different sets of classes are selected at random to get a broad look at the overall representation space. The training set (split-1) of UCF101 without labels is used for pre-training and the test set is used for visualization. In order to ensure reproducible t-SNE results, we use PCA initialization and set perplexity to 50. As can be seen by comparing the representations, we can see that TCLR results in well separated clusters compared to the standard instance contrastive loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Model Attention</head><p>In order to further examine the effect of TCLR pretraining on model performance, we use the method of Zagoruyko and Komodakis [71] to generate model attention for our method (from the fourth convolutional block of the R3D model) and compare it with the baseline supervised model. The qualitative samples are presented in <ref type="figure" target="#fig_10">Figures 17, 18</ref>, 19, and 20. We observe that the TCLR pretrained model has significantly better focus on relevant portions of the video.    <ref type="figure">Figure 13</ref>. Cosine similarity between timesteps of the feature map learned using different losses. It can be observed that temporal distinctiveness learned features increases with the addition of temporal contrastive losses.  ; and TCLR (right), for 10 randomly chosen action classes from UCF101 Test set: FieldHockeyPenalty, GolfSwing, SoccerJuggling, SoccerPenalty, PlayingGuitar, PlayingPiano, PlayingSitar, ApplyEyeMakeup, ApplyLipstick and BlowDryHair. TCLR results in more coherent clusters compared to the instance contrastive lossand is able to discriminate between similar classes with fine-grained differences such as ApplyEyeMakeup, ApplyLipstick and BlowDryHair.  . We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. Detailed comparison with recent prior work</head><p>We contrast our method with 6 recent works in the area: CVRL, TaCo, CoCLR, IIC, Video DeepInfoMax and SeCo. CVRL introduces the Temporal Interval Sampler, the key idea behind it being to not take positives from uniformly random timestamps from the same instance, rather take positives which close to each other temporally. CVRL improves upon standard instance contrastive learning by trying to avoid learning excessive temporal invariance while TCLR explicitly learns within-instance temporal distinctiveness by taking negatives from the same instance, and hence is an orthogonal approach to the problem of excessive temporal invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.1. CVRL [49]</head><p>L.2. TaCo <ref type="bibr" target="#b4">[5]</ref> ... TaCo combines instance contrastive learning with multiple temporal pretext tasks (rotation, shuffled, reverse, or speed) in a multi-task setting in order to learn temporally varying features. The different tasks have their own projection and task heads, but share a common backbone. The TaCo approach to learning temporally distinct features is a differnt approach from TCLR, where we utilize temporal contrastive losses instead of any pretext tasks. IIC framework builds on the contrastive loss by utilizing positive pairs from different modalities (such as optical flow) and creating a new class of negatives by shuffling and repeating the anchor. Unlike TCLR's L LL loss, IIC does not use temporally distinct clips as negative pairs. L.5. VideoDeepInfoMax <ref type="bibr" target="#b15">[16]</ref> VideoDIM learns through instance discrimination using both local and global views of the features. Positive pairs are formed from local and global features of the same instance, whereas negative pairs are formed from local and global features of different instances.The local features are obtained from lower level convolutional layers, whereas the global feature comes from the final layer. Like other forms of instance discrimination based learning VideoDIM does not achieve temporal distinctiveness, and rather enforces invariance. Whereas, L GL loss in TCLR promotes distinctiveness in the local features along the temporal dimension. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L.6. Comparison with SeCo [69]</head><p>SeCo combines multiple pretext tasks and its results cannot be compared with VideoSSL methods like TCLR which start from scratch; SeCo uses a 2D CNN initialized with ImageNet MoCov2 pre-trained weights. Moreover, most of the gain of SeCo over ImageNet trained initialization comes from instance contrastive loss and adding intra-frame loss in SeCo only results in gain of ?1-2% across downstream tasks ( <ref type="table" target="#tab_2">Table 1</ref> of [69]). Whereas with TCLR, gains across downstream tasks upon adding the Local-Local loss are significant (?6-11%, see <ref type="table" target="#tab_5">Table 4</ref> of the main paper). SeCo doesn't include any temporal component (like 3D-Conv or RNN) in the backbone during the SSL phase; it uses 2D CNN and as a result their intra-frame contrastive loss only learns to discriminate visual appearance instead of spatiotemporal features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Running and Jumping: distinct stages of LongJump action</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(a) Instance Contrastive Loss (? )Instance discrimination by maximizing agreement b/w clips which belong to the same video(b) Local-Local Temporal Contrastive Loss (? ) Learning distinct features for non-overlapping local clips from the same video by maximizing agreement b/w differently augmented clips which belong to the same timestamp, while repelling negatives from distant timestamps of the same video instance (c) Global-Local Temporal Contrastive Loss ( ) Learning distinct temporal features for the global clip by maximizing agreement b/w the temporal feature slices of the global clip and the representations of the local clips (Clip-A or B) from the same timestamp The proposed temporal contrastive learning framework (TCLR) for learning temporally distinct video representations consists of three different losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Global-Local Temporal Contrastive Loss A global clip (Clip E) is extracted from a video instance and divided into 4 equal length local clips (Clips A through D). The global clip is temporally downsampled to have the same number of frames as each local clip. The local representations Li,1 through Li,4 from the global clip are obtained from the penultimate layer of the 3D-CNN (prior to temporal pooling). Global representations of the local clips, Gi,1 through Gi,4 are obtained from the CNN (after temporal pooling layer). This loss aims to maximize the similarity between the local representation of the global clip and the global representations of the corresponding local clip. Further details in Section 3.2.2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Percentage of labelled training data (axis not scaled)Top-1 Classification Accuracy (%) Evaluating Label Efficiency using Limited Label Learning on UCF101 (split-1) action classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Confusion matrices for 4 highly confused class-pairs from UCF101 classification models with (a) no pretraining, (b) IC pretraining, and (c) TCLR pretraining. (d) Classes illustrated with a sample frame. TCLR significantly improves over IC in distinguishing visually similar classes.enforces temporal invariance in learned features of video, does not see a similar improvement. It is also worth noting that each of the LL and GL losses help in learning different types of temporal diversity which results in TCLR having bigger improvements relative to either of the temporal contrastive losses. Performance gains of a similar nature can also be observed in other downstream tasks as well, which are reported in Section F of the supplementary material.Number of clips per videoNearest Neighbor Retrieval R@1 (%) Temporally distinct features learned by TCLR result in a significant improvement in NN-Retrieval on UCF101 (split-1) with increasing number of clips per video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>? Section D: Results for additional ablation experiments, to assess the impact of augmentation types, projection head design and loss hyperparameters on performance. ? Section E: Comparison of our instance contrastive and random initialization baseline with results reported in prior work ? Section F: Effect of temporal diversity on downstream tasks ? Section G: Additional results from prior work which were excluded from the main paper ? Section H: Feature slice similarity matrix of the learned representation ? Section I: Qualitative results of NN-Retrieval ? Section J: tSNE visualization of learned features and comparison with baselines ? Section K: Visualization of model attention ? Section L: Detailed comparison with the most relevant recent prior works</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>11 F 15 F 33 F</head><label>111533</label><figDesc>y0, x1, y1).resize(112, 112) / * Random contrast * / 8 Generate p0 ? U (0, 1) 9 if p0 &lt; 0.25 then<ref type="bibr" target="#b9">10</ref> Generate m0 ? U (0.75, 1.25) ? = F ? .adjust contrast(m0)/ * Random Hue * / 12 Generate p1 ? U (0, 1) 13 if p1 &lt; 0.3 then 14 Generate m1 ? U (?0.1, 0.1) ? = F ? .adjust hue(m1) / * Random Saturation * / 16 Generate p2 ? U (0, 1) 17 if p2 &lt; 0.3 then 18 Generate m2 ? U (0.75, 1.25) 19 F ? = F ? .adjust saturation(m2) / * Random Brightness * / 20 Generate p3 ? U (0, 1) 21 if p3 &lt; 0.3 then 22 Generate m3 ? U (0.75, 1.25) 23 F ? = F ? .adjust brightness(m3) / * Color Dropping * / 24 Generate p4 ? U (0, 1) 25 if p4 &gt; 0.7 then 26 if p4 &lt; 0.875 then 27 F ? = F ? .to grayscale() 28 Generate p5 ? U (0, 1) 29 if p5 &lt; 0.75 then 30 Generate m4 ? U (0.75, 1.25) 31 F ? = F ? .adjust gamma(m4) 32 else ? = F ? .drop random channel() / * Random Horizontal Flip * / 34 Generate p6 ? U (0, 1) 35 if p6 &lt; 0.5 then 36 F ? = F ? .horizontal flip() / * Random Erase * / 37 Generate p7 ? U (0, 1) 38 if p7 &lt; 0.5 then 39 Generate ex ? 112 ? U (0, 1), ey ? 112 ? U (0, 1) 40 Generate escalex, escaley ? 20 ? U (0.5, 1) 41 F ? = F ? .erase(ex, ey, escalex, escaley)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Values of contrastive losses during progression of selfsupervised pre-training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 .</head><label>9</label><figDesc>Effect of Contrastive Loss Temperature on downstream linear evaluation task. Using a lower temperature for temporal contrastive loss relative to the instance contrastive loss leads to poor results, as can seen from the right side of the origin in the chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Temporally distinct features learned by TCLR result in a significant improvement in Linear Evaluation on UCF101 (split-1) with increasing number of clips per video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>Temporally distinct features learned by TCLR result in a significant improvement in NN Retrieval on HMDB51 with increasing number of clips per video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 .</head><label>14</label><figDesc>Qualitative Results: Nearest Neighbour Video Retrieval results on UCF101. For each query, the upper row show videos retrieved by our method and the lower row shows results from standard instance contrastive loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15</head><label>15</label><figDesc>. t-SNE visualization of Randomly Initialized (left) features; features learned using Instance Contrastive loss (center)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>PlayingSitar, PlayingPiano, GolfSwing, HighJump, ShavingBeard, Ap-plyLipstick, PoleVault, BasketballDunk, Typing, MixingBatter FieldHockeyPenalty, HighJump, Knitting, Typing, Basketball, Uneven-Bars, ApplyLipstick, BlowDryHair, FrontCrawl, BreastStroke Knitting, Typing, Diving, BreastStroke, HammerThrow, Nunchucks, Haircut, ShavingBeard, Biking, TrampolineJumping Skijet, BreastStroke, Basketball, PommelHorse, PlayingSitar, Play-ingDaf, FrisbeeCatch, GolfSwing, WallPushups, HandstandWalking</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 .Figure 19 .Figure 20 .</head><label>181920</label><figDesc>Swing, BodyWeightSquats, CuttingInKitchen, MixingBatter, Shotput, Archery, BabyCrawling, WritingOnBoard, PlayingDhol, PlayingSitar Figure 16. t-SNE visualization of representations learned using Instance Contrastive loss (left) and TCLR (right) of randomly chosen action classes from UCF101 Test set. In case of TCLR, we observe that the class boundaries are more compact and discriminative. Model attention (continued): Ground-Truth Label on Video frames (Top Row), Attention for baseline fully supervised model (Middle Row) and Attention for TCLR pre-trained model(Bottom Row). We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions. Model attention (continued): Ground-Truth Label on Video frames (Top Row), Attention for baseline fully supervised model (Middle Row) and Attention for TCLR pre-trained model(Bottom Row).We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions. Model attention (continued): Ground-Truth Label on Video frames (Top Row), Attention for baseline fully supervised model (Middle Row) and Attention for TCLR pre-trained model(Bottom Row)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 21 .</head><label>21</label><figDesc>CVRL Framework from Qian et al. [49]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 22 .</head><label>22</label><figDesc>TaCo Framework from Bai et al.<ref type="bibr" target="#b4">[5]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure 23 .Figure 24 .</head><label>2324</label><figDesc>CoCLR Framework from Han et al. [25] CoCLR introduces a multi-stage pretraining process that improves upon instance contrastive learning by mining positive pairs across instances in a cross-modal fashion, i.e. positive pairs mined from RGB modality are used for optical flow, and vice versa. IIC Framework from Tao et al. [54]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 25 .</head><label>25</label><figDesc>Video DIM Framework from Devon Hjelm and Bachman [25]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Venue</cell><cell>Input Size</cell><cell cols="4">UCF101 Pre-Training Kinetics400 Pre-Training UCF101 HMDB51 UCF101 HMDB51</cell></row><row><cell></cell><cell></cell><cell cols="2">Backbone: R3D-18</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ST-Puzzle [33]</cell><cell>AAAI-19</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>65.8</cell><cell>33.7</cell></row><row><cell>STS [61]</cell><cell>TPAMI-21</cell><cell>16 ? 112</cell><cell>67.2</cell><cell>32.7</cell><cell>68.1</cell><cell>34.4</cell></row><row><cell>DPC [23]</cell><cell>ICCVw-19</cell><cell>40 ? 128</cell><cell>60.6</cell><cell>?</cell><cell>68.2</cell><cell>34.5</cell></row><row><cell>VCOP [66]</cell><cell>CVPR-20</cell><cell>16 ? 112</cell><cell>64.9</cell><cell>29.5</cell><cell>?</cell><cell>?</cell></row><row><cell>Pace Pred [63]</cell><cell>ECCV-20</cell><cell>16 ? 112</cell><cell>65.0</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>VCP [41]</cell><cell>AAAI-20</cell><cell>16 ? 112</cell><cell>66.0</cell><cell>31.5</cell><cell>?</cell><cell>?</cell></row><row><cell>PRP [70]</cell><cell>CVPR-20</cell><cell>16 ? 112</cell><cell>66.5</cell><cell>29.7</cell><cell>?</cell><cell>?</cell></row><row><cell>Var. PSP [13]</cell><cell>Access-21</cell><cell>16 ? 112</cell><cell>69.0</cell><cell>33.7</cell><cell>?</cell><cell>?</cell></row><row><cell>MemDPC [24]</cell><cell>ECCV-20</cell><cell>40 ? 224</cell><cell>69.2</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>TCP [40]</cell><cell>WACV-21</cell><cell>? ? 224</cell><cell>64.8</cell><cell>34.7</cell><cell>70.5</cell><cell>41.1</cell></row><row><cell>VIE [72]</cell><cell>CVPR-20</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>72.3</cell><cell>44.8</cell></row><row><cell>UnsupIDT [56]</cell><cell>ECCVw-20</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>73.0</cell><cell>41.6</cell></row><row><cell>CSJ [28]</cell><cell>-</cell><cell>16 ? 224</cell><cell>70.4</cell><cell>36.0</cell><cell>76.2</cell><cell>46.7</cell></row><row><cell>BFP [6]</cell><cell>WACV-21</cell><cell>40 ? 128</cell><cell>63.6</cell><cell>?</cell><cell>66.4</cell><cell>45.3</cell></row><row><cell>IIC (RGB) [54]</cell><cell>ACMMM-20</cell><cell>16 ? 112</cell><cell>61.6</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>CVRL (Reproduced) [49]</cell><cell>CVPR-21</cell><cell>16 ? 112</cell><cell>75.77</cell><cell>44.6</cell><cell>?</cell><cell>?</cell></row><row><cell>SSTL [50]</cell><cell>-</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>79.1</cell><cell>49.7</cell></row><row><cell>VTHCL [68]</cell><cell>-</cell><cell>8 ? 224</cell><cell>?</cell><cell>?</cell><cell>80.6</cell><cell>48.6</cell></row><row><cell>VideoMoCo [46]</cell><cell>CVPR-21</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>74.1</cell><cell>43.6</cell></row><row><cell>RSPNet [11]</cell><cell>AAAI-21</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>74.3</cell><cell>41.8</cell></row><row><cell>Temp Trans [30]</cell><cell>ECCV-20</cell><cell>16 ? 112</cell><cell>77.3</cell><cell>47.5</cell><cell>79.3 *</cell><cell>49.8 *</cell></row><row><cell>TaCo [5]</cell><cell>-</cell><cell>16 ? 224</cell><cell>?</cell><cell>?</cell><cell>81.4</cell><cell>45.4</cell></row><row><cell>MFO [48]</cell><cell>ICCV-21</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>79.1</cell><cell>47.6</cell></row><row><cell>TCLR</cell><cell></cell><cell>16 ? 112</cell><cell>82.4</cell><cell>52.9</cell><cell>84.1</cell><cell>53.6</cell></row><row><cell>TCLR (Best Ablation)</cell><cell></cell><cell>16 ? 112</cell><cell>83.9</cell><cell>53.5</cell><cell>85.4</cell><cell>55.4</cell></row><row><cell></cell><cell></cell><cell cols="2">Backbone: R(2+1)D-18</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VCP [41]</cell><cell>AAAI-20</cell><cell>16 ? 112</cell><cell>66.3</cell><cell>32.2</cell><cell>?</cell><cell>?</cell></row><row><cell>PRP [70]</cell><cell>CVPR-20</cell><cell>16 ? 112</cell><cell>72.1</cell><cell>35.0</cell><cell>?</cell><cell>?</cell></row><row><cell>VCOP [66]</cell><cell>CVPR-20</cell><cell>16 ? 112</cell><cell>72.4</cell><cell>30.9</cell><cell>?</cell><cell>?</cell></row><row><cell>Pace Pred [63]</cell><cell>ECCV-20</cell><cell>16 ? 112</cell><cell>75.9</cell><cell>35.9</cell><cell>77.1</cell><cell>36.6</cell></row><row><cell>STS [61]</cell><cell>TPAMI-21</cell><cell>16 ? 112</cell><cell>73.6</cell><cell>34.1</cell><cell>77.8</cell><cell>40.5</cell></row><row><cell>VideoMoCo [46]</cell><cell>CVPR-21</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>78.7</cell><cell>49.2</cell></row><row><cell>VideoDIM [16]</cell><cell>-</cell><cell>32 ? 128</cell><cell>?</cell><cell>?</cell><cell>79.7 *</cell><cell>49.2 *</cell></row><row><cell>RSPNet [11]</cell><cell>AAAI-21</cell><cell>16 ? 112</cell><cell>?</cell><cell>?</cell><cell>81.1</cell><cell>44.6</cell></row><row><cell>Temp Trans [30]</cell><cell>ECCV-20</cell><cell>16 ? 112</cell><cell>81.6</cell><cell>46.4</cell><cell>?</cell><cell>?</cell></row><row><cell>TaCo [5]</cell><cell>-</cell><cell>16 ? 224</cell><cell>?</cell><cell>?</cell><cell>81.8</cell><cell>46.0</cell></row><row><cell>TCLR</cell><cell></cell><cell>16 ? 112</cell><cell>82.8</cell><cell>53.6</cell><cell>88.2</cell><cell>60.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Backbone: C3D</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MA Stats-1 [62]</cell><cell>CVPR-19</cell><cell>16 ? 112</cell><cell>58.8</cell><cell>32.6</cell><cell>61.2</cell><cell>33.4</cell></row><row><cell>Temp Trans [30]</cell><cell>ECCV-20</cell><cell>16 ? 112</cell><cell>68.3</cell><cell>38.4</cell><cell>69.9 *</cell><cell>39.6 *</cell></row><row><cell>PRP [70]</cell><cell>CVPR-20</cell><cell>16 ? 112</cell><cell>69.1</cell><cell>34.5</cell><cell>?</cell><cell>?</cell></row><row><cell>VCP [41]</cell><cell>AAAI-20</cell><cell>16 ? 112</cell><cell>68.5</cell><cell>32.5</cell><cell>?</cell><cell>?</cell></row><row><cell>VCOP [66]</cell><cell>CVPR-20</cell><cell>16 ? 112</cell><cell>65.6</cell><cell>28.4</cell><cell>?</cell><cell>?</cell></row><row><cell>Pace Pred [63]</cell><cell>ECCV-20</cell><cell>16 ? 112</cell><cell>68.0</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>STS [61]</cell><cell>TPAMI-21</cell><cell>16 ? 112</cell><cell>69.3</cell><cell>34.2</cell><cell>71.8</cell><cell>37.8</cell></row><row><cell>Var. PSP [13]</cell><cell>Access-21</cell><cell>16 ? 112</cell><cell>70.4</cell><cell>34.3</cell><cell>?</cell><cell>?</cell></row><row><cell>DSM [60]</cell><cell>AAAI-21</cell><cell>16 ? 112</cell><cell>70.3</cell><cell>40.5</cell><cell>?</cell><cell>?</cell></row><row><cell>TCLR</cell><cell></cell><cell>16 ? 112</cell><cell>76.1</cell><cell>48.6</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell cols="2">Other Configurations</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CVRL (R3D-50) [49]</cell><cell>CVPR-21</cell><cell>32 ? 224</cell><cell>?</cell><cell>?</cell><cell>92.2</cell><cell>66.7</cell></row><row><cell>RSPNet (S3D-G) [11]</cell><cell>AAAI-21</cell><cell>64 ? 224</cell><cell>?</cell><cell>?</cell><cell>93.7</cell><cell>64.7</cell></row><row><cell>CoCLR  ? (S3D-23) [25]</cell><cell>NeurIPS-20</cell><cell>16 ? 112</cell><cell>87.3</cell><cell>58.7</cell><cell>90.6</cell><cell>62.9</cell></row><row><cell>SpeedNet (S3D-G) [7]</cell><cell>CVPR-20</cell><cell>16 ? 224</cell><cell>?</cell><cell>?</cell><cell>81.1</cell><cell>48.8</cell></row><row><cell>?SimCLR (R50) [19]</cell><cell>CVPR-21</cell><cell>8 ? 224</cell><cell>?</cell><cell>?</cell><cell>85.6</cell><cell>?</cell></row><row><cell>SeCO (R50+TSN) [69]</cell><cell>AAAI-21</cell><cell>50 ? 224</cell><cell>?</cell><cell>?</cell><cell>88.3 ?</cell><cell>55.6 ?</cell></row></table><note>. Finetuning Results (average of 3 splits) for action classification on UCF101 and HMDB51. Self supervised pretraining was done on UCF101 (left) and Kinetics (right).? indicates models that utilize optical flow. * indicates Kinetics-600 self-supervised pretraining. ? indicates ImageNet+Kinetics pre-training. Best and second best results are highlighted.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Nearest neighbor video retrieval results on UCF101 and HMDB51, after self-supervised pretraining on UCF101. * marks models pretrained on Kinetics-400. Best and second best results highlighted. Methods based on optical flow and audio modalities are excluded.</figDesc><table><row><cell>Pre-Training</cell><cell>Accuracy</cell></row><row><cell>None (Random Initialization)</cell><cell>13.4</cell></row><row><cell>MiniKinetics Supervised [14]</cell><cell>18.0</cell></row><row><cell>Instance Contrastive</cell><cell>15.8</cell></row><row><cell>VCOP [66]</cell><cell>14.7</cell></row><row><cell>CVRL [49]</cell><cell>17.6</cell></row><row><cell>TCLR</cell><cell>22.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Diving48 fine-grained action classification results. +10% 76.30 +5% 47.87+10% 47.32 +7% ? ? ? 69.91 +15% 82.40+11% 52.80+14% 56.17+15%</figDesc><table><row><cell cols="4">4.2. Ablation Study</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Contrastive Losses</cell><cell cols="3">Classification Top1 Acc. Linear Eval Finetune Transfer</cell><cell>Retrieval R@1</cell></row><row><cell cols="3">LIC LLL LGL</cell><cell>UCF101</cell><cell>UCF101</cell><cell>HMDB51</cell><cell>UCF101</cell></row><row><cell></cell><cell cols="2">Random Init.</cell><cell>17.15</cell><cell>62.39</cell><cell>26.95</cell><cell>8.21</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>21.58</cell><cell>68.42</cell><cell>?</cell><cell>13.66</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>20.61</cell><cell>70.19</cell><cell>?</cell><cell>12.83</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>23.39</cell><cell>74.29</cell><cell>47.35</cell><cell>14.17</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>54.58</cell><cell>71.31</cell><cell>38.32</cell><cell>40.76</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="4">62.70 +8% 77.70 +6% 49.77+11% 51.10+10%</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>64.55</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of the impact of temporal contrastive losses on downstream tasks. Green indicates improvements over instance contrastive baseline.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3636-3645, 2017. 3, 19 [21] Kirill Gavrilyuk, Mihir Jain, Ilia Karmanov, and Cees GM Snoek. Motion-augmented self-training for video recognition at smaller scale. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10429-10438, 2021. 6 rotation prediction. arXiv preprint arXiv:1811.11387, 2018. 3, 6 [32] Hirokatsu Kataoka, Tenga Wakamiya, Kensho Hara, and Yutaka Satoh. Would mega-scale datasets further enhance spatiotemporal 3d cnns? arXiv preprint arXiv:2004.04968, 2020. 1 [33] Dahun Kim, Donghyeon Cho, and In So Kweon. Selfsupervised video representation learning with spacetime cubic puzzles. In Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, Jiangliu Wang, Jianbo Jiao, and Yun-Hui Liu. Selfsupervised video representation learning by pace prediction. In The European Conference on Computer Vision, 2020. 2, 3, 7, 8, 14, 15, 17, 18 [64] Donglai Wei, Joseph J. Lim, Andrew Zisserman, and William T. Freeman. Learning and using the arrow of time. In Proceedings of the IEEE Conference on Com-</figDesc><table><row><cell>[22] Michael Gutmann and Aapo Hyv?rinen. Noise-contrastive estimation: A new estimation principle for ficial Intelligence, vol. 33, pages 11701-11708, 2020. 7, 8 [42] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-to-end learning of visual representations from un-curated instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 9879-9889, 2020a. 4 [43] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, [63] puter Vision and Pattern Recognition, pages 8052-8060, 2018. 4</cell><cell>using contrastive bidirectional transformer. arXiv preprint arXiv:1906.05743, 2019. 4 [53] Tomoyuki Suzuki, Takahiro Itazuri, Kensho Hara, and Hirokatsu Kataoka. Learning spatiotemporal 3d con-volution with video order self-supervision. In Pro-ceedings of the European Conference on Computer Vi-sion, 2018. 3 [54] Li Tao, Xueting Wang, and Toshihiko Yamasaki. Self-supervised video representation learning using inter-</cell></row><row><cell>unnormalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial In-telligence and Statistics, pages 297-304, 2010. 4 [23] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive cod-ing. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 2019. 3, 7, 15 Ivan Laptev, Josef Sivic, and Andrew Zisserman. End-[65] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen to-end learning of visual representations from un-Tu, and Kevin Murphy. Rethinking spatiotemporal curated instructional videos. In Proceedings of the feature learning: Speed-accuracy trade-offs in video IEEE/CVF Conference on Computer Vision and Pat-tern Recognition, pages 9879-9889, 2020b. 6, 19 classification. In Proceedings of the European Con-ference on Computer Vision, pages 305-321, 2018. 6 [66] Dejing Xu, Jun Xiao, Zhou Zhao, Jian Shao, Di Xie, [44] Ishan Misra, C. Lawrence Zitnick, and Martial Hebert. and Yueting Zhuang. Self-supervised spatiotemporal Shuffle and learn: unsupervised learning using tempo-ral order verification, pages 527-544. 2016. 3, 19 learning via video clip order prediction. In Proceed-ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10334-10343, 2019. 3, 7, 8, 14</cell><cell>intra contrastive framework. In Proceedings of the pages 8545-28th ACM International Conference on Multimedia, 8552, 2019. 3, 7 pages 2193-2201, 2020. 3, 4, 7, 27 [34] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and [55] Yuan Tian, Zhaohui Che, Wenbo Bao, Guangtao Zhai, and Zhiyong Gao. Self-supervised Motion Represen-Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceed-ings, 2015. 15 tation via Scattering Local Motion Cues, pages 71-89. 2020. 4 [56] Pavel Tokmakov, Martial Hebert, and Cordelia Schmid. Unsupervised learning of video represen-tations via dense trajectory clustering. In Euro-pean Conference on Computer Vision, pages 404-421.</cell></row><row><cell>[67] Fei Xue, Hongbing Ji, Wenbo Zhang, and Yi Cao.</cell><cell>Springer, 2020. 3, 4, 7, 15, 18</cell></row><row><cell>Self-supervised video representation learning by max-</cell><cell></cell></row><row><cell>imizing mutual information. Signal Processing: Im-</cell><cell></cell></row><row><cell>age Communication, 88:115967, 2020. 3, 19</cell><cell></cell></row><row><cell>3, [29] Simon Jenni and Hailin Jin. Time-equivariant con-7, 8 trastive video representation learning. In Proceedings of the IEEE/CVF International Conference on Com-puter Vision, pages 9970-9980, 2021. 3 [68] Ceyuan Yang, Yinghao Xu, Bo Dai, and Bolei Zhou. 11205-Video representation learning with visual tempo con-11214, 2021. 3, 7 [47] Mandela Patrick, Yuki Asano, Polina Kuznetsova, sistency. arXiv preprint arXiv:2006.15489, 2020. 3, 7 Ruth Fong, Joao F. Henriques, Geoffrey Zweig, and [69] Ting Yao, Yiheng Zhang, Zhaofan Qiu, Yingwei Pan, Andrea Vedaldi. Multi-modal self-supervision from and Tao Mei. Seco: Exploring sequence supervision generalized data transformations, 2021. 6, 19 for unsupervised representation learning. In AAAI, [48] Rui Qian, Yuxi Li, Huabin Liu, John See, Shuangrui volume 2, page 7, 2021. 2, 3, 7, 28 Ding, Xian Liu, Dian Li, and Weiyao Lin. Enhanc-ing self-supervised video representation learning via multi-level feature optimization. In Proceedings of the International Conference on Computer Vision, 2021. 7, 8 [49] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learn-ing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6964-6974, 2021. 2, 3, 6, 7, 8, 17, 27 [50] Hao Shao, Yu Liu, and Hongsheng Li. Self-supervised temporal learning, 2021. 2, 3, 7, 8, 18 [51] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 5, 14 [52] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representations [70] Yuan Yao, Chang Liu, Dezhao Luo, Yu Zhou, and Qixiang Ye. Video playback rate perception for self-supervised spatio-temporal representation learning. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 6548-6557, 2020a. 3, 7, 14 [71] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the perfor-mance of convolutional neural networks via attention transfer. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenRe-view.net, 2017. 20 [72] Chengxu Zhuang, Tianwei She, Alex Andonian, Max Sobol Mark, and Daniel Yamins. Unsupervised learning from video with deep neural embeddings. In Proceedings of the IEEE/CVF Conference on Com-puter Vision and Pattern Recognition, pages 9563-9572, 2020. 3, 7</cell><cell>5, 14 [37] Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Unsupervised representation learning by sorting sequences. In Proceedings of the 2015. 5 [58] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6450-6459, IEEE International Conference on Computer Vision, 2018. 5, 6, 15 pages 667-676, 2017. 3 [38] Attila Lengyel, Robert-Jan Bruintjes, Marcos Baptista Rios, Osman Semih Kayhan, Davide Zambrano, Ner-gis Tomen, and Jan van Gemert. Vipriors 2: Visual inductive priors for data-efficient deep learning chal-lenges. arXiv preprint arXiv:2201.08625, 2022. 6 [39] Yingwei Li, Yi Li, and Nuno Vasconcelos. Re-sound: Towards action recognition without represen-tation bias. In Proceedings of the European Confer-ence on Computer Vision, pages 513-528, 2018. 6 [40] Guillaume Lorre, Jaonary Rabarisoa, Astrid Orcesi, Samia Ainouz, and Stephane Canu. Temporal con-trastive pretraining for video action recognition. In The IEEE Winter Conference on Applications of Com-puter Vision, pages 662-670, 2020. 3, 7 [59] Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. The Journal of Machine Learn-ing Research, 15(1):3221-3245, 2014. 19 [60] Jinpeng Wang, Yuting Gao, Ke Li, Xinyang Jiang, Xi-aowei Guo, Rongrong Ji, and Xing Sun. Enhancing unsupervised video representation learning by decou-pling the scene and the motion. In The AAAI Confer-ence on Artificial Intelligence, 2021. 3, 7 [61] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Wei Liu, and Yun-Hui Liu. Self-supervised video representation learning by uncovering spatio-temporal statistics. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 3, 7, 8 [62] Jiangliu Wang, Jianbo Jiao, Linchao Bao, Shengfeng He, Yunhui Liu, and Wei Liu. Self-supervised spatio-temporal representation learning for videos by predict-ing motion and appearance statistics. In Proceedings of the IEEE Conference on Computer Vision and Pat-tern Recognition, pages 4006-4015, 2019. 3, 7</cell></row></table><note>[24] Tengda Han, Weidi Xie, and Andrew Zisserman. Memory-augmented dense predictive coding for video representation learning, pages 312-329. 2020a. 3, 6, 7, 8, 15, 16, 17 [25] Tengda Han, Weidi Xie, and Andrew Zisserman. Self-supervised Co-Training for Video Representation Learning, pages 5679-5690. 2020b. 2, 4, 7, 27, 28 [26] K. Hara, H. Kataoka, and Y. Satoh. Towards good practice for action recognition with spatiotemporal 3d convolutions. In 2018 24th International Conference on Pattern Recognition, pages 2516-2521, 2018. 5, 15 [27] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 9729-9738, 2020. 1, 3 [28] Yuqi Huo, Mingyu Ding, Haoyu Lu, Zhiwu Lu, Tao Xiang, Ji-Rong Wen, Ziyuan Huang, Jianwen Jiang, Shiwei Zhang, Mingqian Tang, Songfang Huang, and Ping Luo. Self-supervised video representation learn- ing with constrained spatiotemporal jigsaw, 2021.[30] Simon Jenni, Givi Meishvili, and Paolo Favaro. Video representation learning by recognizing temporal trans- formations. In The European Conference on Com- puter Vision, 2020. 3, 7, 8, 15 [31] Longlong Jing, Xiaodong Yang, Jingen Liu, and Yingli Tian. Self-supervised spatiotemporal feature learning via video[35] Joshua Knights, Ben Harwood, Daniel Ward, An- thony Vanderkop, Olivia Mackenzie-Ross, and Pey- man Moghadam. Temporally coherent embeddings for self-supervised video representation learning. In 2020 25th International Conference on Pattern Recog- nition (ICPR), pages 8914-8921. IEEE, 2021. 3, 19 [36] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: a large video database for human mo- tion recognition. In Proceedings of the International Conference on Computer Vision, 2011.[41] Dezhao Luo, Chang Liu, Yu Zhou, Dongbao Yang, Can Ma, Qixiang Ye, and Weiping Wang. Video cloze procedure for self-supervised spatio-temporal learn- ing. In Proceedings of the AAAI Conference on Arti-[45] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 3 [46] Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu. Videomoco: Contrastive video represen- tation learning with temporally adversarial examples. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages[57] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor- resani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceed- ings of the IEEE International Conference on Com- puter Vision, pages 4489-4497,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>and 7.</figDesc><table><row><cell cols="5">Method Recall@1 Recall@5 Recall@10 Recall@20</cell></row><row><cell>IC</cell><cell>40.76</cell><cell>56.41</cell><cell>65.33</cell><cell>74.44</cell></row><row><cell>IC+LL</cell><cell>51.1</cell><cell>67.83</cell><cell>74.57</cell><cell>80.89</cell></row><row><cell>IC+GL</cell><cell>47.32</cell><cell>63.10</cell><cell>71.42</cell><cell>78.72</cell></row><row><cell>TCLR</cell><cell>56.17</cell><cell>72.16</cell><cell>79.01</cell><cell>85.30</cell></row><row><cell cols="5">Table 5. Nearest Neighbor Video Retrieval on UCF101 using self-</cell></row><row><cell cols="5">supervised R3D-18 models pretrained on UCF101 videos.</cell></row><row><cell cols="5">Method Recall@1 Recall@5 Recall@10 Recall@20</cell></row><row><cell>IC</cell><cell>14.38</cell><cell>35.62</cell><cell>48.37</cell><cell>61.57</cell></row><row><cell>IC+LL</cell><cell>19.07</cell><cell>42.42</cell><cell>54.97</cell><cell>69.35</cell></row><row><cell>IC+GL</cell><cell>18.43</cell><cell>41.70</cell><cell>53.59</cell><cell>67.19</cell></row><row><cell>TCLR</cell><cell>22.75</cell><cell>45.36</cell><cell>57.84</cell><cell>73.07</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>32.10 44.39 57.35 IC 22.86 53.91 60.26 65.74 LL + GL 9.78 48.66 59.45 68.64 IC + LL 26.43 63.09 69.68 73.53 IC + GL 23.93 60.16 67.40 72.34 TCLR 26.90 66.08 73.41 76.68</figDesc><table><row><cell>Method</cell><cell>1%</cell><cell>10% 20% 50%</cell></row><row><cell>Scratch</cell><cell>6.13</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .Table 12 .</head><label>1112</label><figDesc>Effect of temperatures used in contrastive losses. Comparison of TCLR vs Vanilla IC with longer pretraining on Kinetics400 as reported in prior work.</figDesc><table><row><cell>L IC</cell><cell>L LL , L GL</cell><cell cols="2">Linear Evaluation</cell></row><row><cell>Temperature</cell><cell cols="2">Temperature</cell><cell>(Top-1 Accuracy)</cell></row><row><cell>0.01</cell><cell>0.01</cell><cell></cell><cell>62.75</cell></row><row><cell>0.01</cell><cell>0.1</cell><cell></cell><cell>64.03</cell></row><row><cell>0.01</cell><cell>1</cell><cell></cell><cell>60.32</cell></row><row><cell>0.05</cell><cell>0.05</cell><cell></cell><cell>64.28</cell></row><row><cell>0.1</cell><cell>0.01</cell><cell></cell><cell>44.22</cell></row><row><cell>0.1</cell><cell>0.1</cell><cell></cell><cell>69.93</cell></row><row><cell>0.1</cell><cell>0.5</cell><cell></cell><cell>67.78</cell></row><row><cell>0.1</cell><cell>1</cell><cell></cell><cell>66.33</cell></row><row><cell>1</cell><cell>0.01</cell><cell></cell><cell>40.93</cell></row><row><cell>1</cell><cell>0.1</cell><cell></cell><cell>51.10</cell></row><row><cell cols="4">Method Epochs UCF HMDB</cell></row><row><cell>IC [50]</cell><cell>200</cell><cell>70.0</cell><cell>39.9</cell></row><row><cell>IC [44]</cell><cell>300</cell><cell>69.5</cell><cell>-</cell></row><row><cell>IC [7]</cell><cell>200 *</cell><cell>73.0</cell><cell>40.6</cell></row><row><cell>TCLR</cell><cell>50</cell><cell>82.2</cell><cell>51.9</cell></row><row><cell>TCLR</cell><cell>100</cell><cell>84.1</cell><cell>53.6</cell></row><row><cell>*  email from authors</cell><cell></cell><cell></cell><cell></cell></row><row><cell>tion baseline in</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 14 .</head><label>14</label><figDesc>Effect of using multiple clips on different evaluation results. 1-clip/ 10-clip performance on different downstream tasks.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>UCF101</cell><cell></cell><cell></cell><cell></cell><cell>HMDB51</cell></row><row><cell></cell><cell>Linear Eval</cell><cell></cell><cell cols="2">Finetuning</cell><cell cols="2">NN-Retrieval</cell><cell>Finetuning</cell><cell>NN-Retrieval</cell></row><row><cell>Scratch</cell><cell cols="2">14.80/ 17.15 (+2.35)</cell><cell cols="4">56.85/ 62.30 (+5.45) 8.21/ 9.77 (+1.56)</cell><cell cols="2">21.91/ 26.95 (+5.04)</cell><cell>4.77/ 4.05 (-0.72)</cell></row><row><cell>IC</cell><cell cols="2">49.43/ 54.58 (+5.15)</cell><cell cols="4">63.41/ 68.99 (+5.58) 38.52/ 40.76 (+2.24)</cell><cell cols="2">33.39/ 38.32 (+4.93)</cell><cell>15.1/ 15.38 (+0.28)</cell></row><row><cell cols="3">IC + LL 55.83/ 62.70 (+6.87)</cell><cell cols="4">66.85/ 74.38 (+7.53) 43.64/ 51.10 (+7.46)</cell><cell cols="2">42.80/ 49.77 (+6.96)</cell><cell>14.53/ 19.07 (+4.54)</cell></row><row><cell cols="3">IC + GL 55.14/ 64.55 (+9.41)</cell><cell cols="4">68.96/ 75.44 (+6.48) 38.54/ 47.32 (+8.78)</cell><cell cols="2">39.67/ 47.87 (+8.20)</cell><cell>15.03/ 18.43 (+3.4)</cell></row><row><cell>TCLR</cell><cell cols="8">59.50/ 69.91 (+10.41) 71.73/ 81.10 (+9.37) 41.50/ 56.17 (+14.67) 41.37/ 52.80 (+11.48) 16.67/ 22.75 (+6.08)</cell></row><row><cell cols="2">Method</cell><cell cols="4">H/W ? T Backbone Modality</cell><cell>Params (?10 6 )</cell><cell>Pretraining</cell><cell>UCF101 finetune</cell><cell>HMDB51 finetune</cell></row><row><cell cols="3">Specialized Architectures</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DVIM [67]</cell><cell cols="2">? ? ?</cell><cell>R2D-18  *</cell><cell>V</cell><cell>-</cell><cell>UCF101</cell><cell>62.1</cell><cell>28.2</cell></row><row><cell cols="2">DVIM [67]</cell><cell cols="2">? ? ?</cell><cell>R2D-18  *</cell><cell>V</cell><cell>-</cell><cell>K400</cell><cell>64.0</cell><cell>29.7</cell></row><row><cell cols="2">TCE [35]</cell><cell cols="2">224 ? ?</cell><cell>R2D50</cell><cell>V</cell><cell>23.0</cell><cell>K400</cell><cell>71.2</cell><cell>36.6</cell></row><row><cell cols="2">O3N [20]</cell><cell cols="2">227 ? ?</cell><cell>AlexNet</cell><cell>V</cell><cell>61.0</cell><cell>UCF101</cell><cell>60.3</cell><cell>32.5</cell></row><row><cell cols="2">Shuffle-Learn [44]</cell><cell cols="2">227 ? ?</cell><cell>AlexNet</cell><cell>V</cell><cell>61.0</cell><cell>UCF101</cell><cell>50.2</cell><cell>18.1</cell></row><row><cell cols="2">B?chler et al. [8]</cell><cell cols="2">227 ? ?</cell><cell>AlexNet  *</cell><cell>V</cell><cell>-</cell><cell>UCF101</cell><cell>58.6</cell><cell>25.0</cell></row><row><cell cols="2">Video Jigsaw [2]</cell><cell cols="2">224 ? 25</cell><cell>CaffeNet</cell><cell>V</cell><cell>61.0</cell><cell>UCF101</cell><cell>46.4</cell><cell>-</cell></row><row><cell cols="2">Video Jigsaw [2]</cell><cell cols="2">224 ? 25</cell><cell>CaffeNet</cell><cell>V</cell><cell>61.0</cell><cell>K400</cell><cell>55.4</cell><cell>27.0</cell></row><row><cell cols="2">CBT [3]</cell><cell cols="2">112 ? 16</cell><cell>S3D</cell><cell>V</cell><cell>20.0</cell><cell>K600</cell><cell>79.5</cell><cell>44.6</cell></row><row><cell cols="3">Multi-Modal Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">AVTS [47]</cell><cell cols="2">224 ? 25</cell><cell>I3D</cell><cell>V+A</cell><cell>-</cell><cell>K400</cell><cell>83.7</cell><cell>53.0</cell></row><row><cell cols="2">AVTS [47]</cell><cell cols="2">224 ? 25</cell><cell>MC3</cell><cell>V+A</cell><cell>-</cell><cell>AudioSet</cell><cell>89.0</cell><cell>61.6</cell></row><row><cell cols="2">MIL-NCE [43]</cell><cell cols="2">224 ? ?</cell><cell>S3D</cell><cell>V+T</cell><cell>-</cell><cell>HowTo100M</cell><cell>91.3</cell><cell>61.0</cell></row><row><cell cols="2">GDT [47]</cell><cell cols="2">112 ? 32</cell><cell>R(2+1)D</cell><cell>V+A</cell><cell>-</cell><cell>K400</cell><cell>89.3</cell><cell>60.0</cell></row><row><cell cols="2">GDT [47]</cell><cell cols="2">112 ? 32</cell><cell>R(2+1)D</cell><cell>V+A</cell><cell>-</cell><cell>IG65M</cell><cell>95.2</cell><cell>72.8</cell></row><row><cell cols="2">XDC [47]</cell><cell cols="2">224 ? 32</cell><cell>R(2+1)D</cell><cell>V+A</cell><cell>-</cell><cell>K400</cell><cell>84.2</cell><cell>47.1</cell></row><row><cell>TCLR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TCLR (Ours)</cell><cell cols="2">112 ? 16</cell><cell>R3D-18</cell><cell>V</cell><cell>13.5</cell><cell>UCF101</cell><cell>82.4</cell><cell>52.9</cell></row><row><cell cols="2">TCLR (Ours)</cell><cell cols="2">112 ? 16</cell><cell>R3D-18</cell><cell>V</cell><cell>13.5</cell><cell>K400</cell><cell>84.1</cell><cell>53.6</cell></row><row><cell cols="2">TCLR (Ours)</cell><cell cols="2">112 ? 16</cell><cell>R(2+1)D</cell><cell>V</cell><cell>14.4</cell><cell>UCF101</cell><cell>82.8</cell><cell>53.6</cell></row><row><cell cols="2">TCLR (Ours)</cell><cell cols="2">112 ? 16</cell><cell>R(2+1)D</cell><cell>V</cell><cell>14.4</cell><cell>K400</cell><cell>84.3</cell><cell>54.2</cell></row><row><cell cols="2">TCLR (Ours)</cell><cell cols="2">112 ? 16</cell><cell>C3D</cell><cell>V</cell><cell>27.7</cell><cell>UCF101</cell><cell>76.1</cell><cell>48.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Ishan Dave would like to acknowledge support from the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&amp;D Contract No. D17PC00345. The views and conclusions contained herein are those of the authors</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-supervised learning of audio-visual objects from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Video Jigsaw: Unsupervised learning of spatiotemporal context for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Unaiza Ahsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Madhok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-Supervised Learning by Cross-Modal Audio-Video Clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Can temporal information help with contrastive self-supervised learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13046</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised video representation learning by bidirectional feature prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadine</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Speednet: Learning the speediness in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving spatiotemporal self-supervision by deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uta</forename><surname>Buchler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9912" to="9924" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rspnet: Relative speed perception for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-supervised visual learning by variable playback speeds prediction of a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taehoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="79562" to="79571" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Why can&apos;t i dance in the mall? learning to mitigate scene bias in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="853" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Biyani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07758</idno>
		<title level="m">First place submission for vipriors21 action recognition challenge at iccv 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Representation learning with video deep infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13278</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Large scale holistic video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno>593-610. 2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention for baseline fully supervised model (Middle Row) and Attention for TCLR pre-trained model(Bottom Row)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings attention: Ground-Truth Label on Video frames (Top Row)</title>
		<meeting>attention: Ground-Truth Label on Video frames (Top Row)</meeting>
		<imprint/>
	</monogr>
	<note>Self-supervised video representation learning with odd-one-out networks. We notice that for the TCLR pre-trained model the attention is more focused on action-centric regions</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
