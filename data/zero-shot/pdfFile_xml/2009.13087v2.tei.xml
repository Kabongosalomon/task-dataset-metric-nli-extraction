<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PERF-Net: Pose Empowered RGB-Flow Net</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
							<email>yinxiao@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
							<email>jonathanhuang@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PERF-Net: Pose Empowered RGB-Flow Net</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, many works in the video action recognition literature have shown that two stream models (combining spatial and temporal input streams) are necessary for achieving state-of-the-art performance. In this paper we show the benefits of including yet another stream based on human pose estimated from each frame -specifically by rendering pose on input RGB frames. At first blush, this additional stream may seem redundant given that human pose is fully determined by RGB pixel values -however we show (perhaps surprisingly) that this simple and flexible addition can provide complementary gains. Using this insight, we propose a new model, which we dub PERF-Net (short for Pose Empowered RGB-Flow Net), which combines this new pose stream with the standard RGB and flow based input streams via distillation techniques and show that our model outperforms the state-of-the-art by a large margin in a number of human action recognition datasets while not requiring flow or pose to be explicitly computed at inference time. The proposed pose stream is also part of the winner solution of the ActivityNet Kinetics Challenge 2020 <ref type="bibr" target="#b0">[1]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose is intuitively intimately linked to human centric activity recognition. For example, by localizing the two legs from a human in a collection of frames, one is often able to easily recognize actions such as jumping, walking or sitting. As such, the idea of using pose explicitly as a cue for activity recognition tasks is one that has been explored in a number of works in the computer vision literature, including <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b48">49]</ref>. In this paper we revisit this conceptually simple idea of using pose as a cue for activity recognition using modern large scale datasets and models. Specifically, we exploit pose in activity recognition using 3D CNNs, which in recent years have been a dominant architecture in the subfield due to the rise of massive scale video datasets such as Kinetics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>To achieve state-of-the-art results on Kinetics, many recent works that rely on 3D CNNs <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> have found it * Equal contribution</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Pose Flow</head><p>Bench Press Bench Press Sit-up <ref type="figure">Figure 1</ref>. Visualizations of models trained on RGB, Pose, and Flow modalities. The top row shows input multi-modality data. The middle row shows the response maps from the networks using Grad-CAM <ref type="bibr" target="#b32">[33]</ref>. Note that the response maps are overlaid on RGB and pose images for better visualization. The bottom row shows the model predictions on each of the modalities. Our proposed pose modality focuses the attention on the entire human body, providing a useful complementary cue to the standard RGB and Flow modalities, here allowing for our model to correctly predict the "sit-up" action. necessary to rely on a "two-stream" approach <ref type="bibr" target="#b33">[34]</ref> that combines spatial and temporal input streams using late fusion. Concretely, this has typically referred to models trained independently to do activity recognition on (1) a sequence of RGB images and (2) a sequence of optical flow fields (or other motion representation) and fusing the results of both models via ensembling.</p><p>In addition to this two-stream framework, we propose to add a third input stream based on human pose. Unlike the two-stream approach which is (very) loosely based on the two-stream hypothesis of the human visual system <ref type="bibr" target="#b12">[13]</ref>, our approach takes no specific inspiration from biologyinstead we rely on the natural intuition that since action datasets tend to be human centric, if we had explicit pose cues, it would often be much more straightforward to infer action from pose compared to directly from raw pixels or flow. As an example, consider our model's results on a person performing a sit-up using the three possible input "modalities", RGB, pose and flow. However, this sit-up is more specifically a "barbell sit-up on a decline bench" which is easily confused for "bench press" due to strong cues from the appearance and motion of the barbell. With such, the pose modality offers a complementary signal allowing our model to infer the correct activity.</p><p>How to provide the pose cues properly requires care however -using pose alone as an input stream is intuitively not enough, as recognition often requires contextual cues (e.g. from props, objects that the human is interacting with, etc). Instead, as the "pose stream", we render pose via exaggerated colored lines on top of each corresponding RGB frame, which allows us to benefit from both a clear pose based signal as well as contextual cues from surrounding appearance. We demonstrate via ablations that this choice to superimpose pose with the corresponding RGB frame is critical for good results.</p><p>A reasonable question to ask is: why is pose not simply a redundant input stream? After all, it is fully determined by RGB values -and even more redundant given that we render poses on top of the RGB frames. So even though pose is intuitively connected to activity recognition, what additional specific benefit is pose bringing in our setting?</p><p>We have a few answers. First, by using an off-the-shelf pose estimation algorithm that was trained on the COCO dataset <ref type="bibr" target="#b22">[23]</ref>, we are injecting additional semantic knowledge that the model can leverage. Second, we note that optical flow is also fully determined by the sequence of RGB inputs. And as with flow, we show that models using the pose stream are quantitatively different (better) than simply ensembling with a second RGB-only model. In very recent work, Stroud et al. <ref type="bibr" target="#b35">[36]</ref> showed that the benefits of the temporal stream could be captured by an "RGB-only" model via distillation training, obviating the need for redundant input streams at inference time.</p><p>Taking inspiration from Stroud et al. 's flow based results <ref type="bibr" target="#b35">[36]</ref>, we similarly apply distillation techniques to our problem with both pose and flow. Combining this with a novel self-gating based architecture, we are able to obtain a state-of-the-art RGB-only model that requires us to compute neither flow nor pose. We dub this model the Pose Empowered RGB-Flow Net (or PERF-Net).</p><p>To summarize, our contributions are as follows.</p><p>? We demonstrate strong evidence that pose is an important modality for video action recognition and can provide a complementary input stream to the standard RGB and Flow streams.</p><p>? We propose PERF-Net, an approach that leverages RGB, Flow and Pose input streams in a multi-teacher distillation setting to train an RGB-only model with state of the art performance on the challenging Kinetics dataset.</p><p>? We study the impact of using different representations of the human pose input stream. We propose a contextaware human pose rendering which can bridge the gap between pose information and RGB within a collection of frames.</p><p>? We perform detailed analysis on the response of networks from different input streams (RGB, Flow, and Pose). Our qualitative results show that when trained on our Pose stream, our model sometimes attends to different regions of a frame compared to RGB or Flow, allowing this third stream to offer complementary cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fusion of multiple modalities</head><p>In contrast to image data, videos are multi-modal. How to best utilize this special characteristic of video data has been a long-standing topic in the video understanding research community. One of the standard approaches, introduced by <ref type="bibr" target="#b33">[34]</ref>, captures complementary information from appearance and motion by averaging predictions from two separately trained 2D CNNs, one from RGB frames and the other from stacked optical flow frames. Following <ref type="bibr" target="#b33">[34]</ref>, Feichtenhofer et al. <ref type="bibr" target="#b11">[12]</ref> investigated the optimal locations within CNNs to combine the two streams.</p><p>A more recent trend has been to train a 3D ConvNet to directly model temporal patterns without relying explicitly on optical flow. This is easier said than done, as <ref type="bibr" target="#b3">[4]</ref> showed that performance (of their 3D convolutional architecture, I3D) could be greatly improved by including an optical flow stream. However there have been some promising approaches; Feichtenhofer et al. <ref type="bibr" target="#b10">[11]</ref> recently proposed a two-stream architecture where both streams take RGB frames as inputs, but extracted at different frame rates. Unlike the late fusion approach taken by two-stream I3D models, the fusion in <ref type="bibr" target="#b10">[11]</ref> is implemented as lateral connections at different layers of the network. Ryoo et al. <ref type="bibr" target="#b31">[32]</ref> adapted the Evolution algorithm to search such lateral connections in a multi-stream architecture. In addition to different frame rates of RGB streams, they also include optical flow as an additional stream of input.</p><p>In addition to optical flow, human pose is another input modality that has been widely studied for understanding videos involving human activities <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17]</ref>. Ch?ron et al. <ref type="bibr" target="#b4">[5]</ref> showed that training RGB and flow streams on the patches centered at human joint locations can improve over the global approach. In addition to RGB and flow frames, Zolfaghari et al. <ref type="bibr" target="#b50">[51]</ref> proposed a new modality using human body part segmentation results from an existing network. Another novelty from their work is that multistream fusion is done sequentially through a Markov chain. Choutas1 et al. <ref type="bibr" target="#b5">[6]</ref> also proposed an representation to en-code pose information and use that as an additional stream, but they used black background in the presentation, so on Kinetics, the top-1 and top-5 accuracies decreased by 2% and 1% respectively when using their representation with I3D compared to I3D alone. Our study focuses on how to best represent human pose as an input stream for a 3D CNN. Our experiments highlight the importance of this issue, and we show that a naive representation of human pose indeed degrades the final ensemble performance. More generally we run our experiments on the large scale Kinetics dataset which are properly able to leverage the expressiveness of 3D CNNs leading to stronger results and "clearer" ablation signals throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Distillation between modalities</head><p>While achieving state-of-the-art performance, multistream models are computationally more expensive. For example, the computation of optical flow could be more expensive than ConvNet inference. Distillation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> is a technique to transfer the knowledge of a complex teacher model to a smaller student model by optimizing the student model to mimic the behavior of the teacher. Recently, researchers have adapted this idea to multi-modal model training. Zhang et al. <ref type="bibr" target="#b49">[50]</ref> used a teacher model trained on optical flow to guide a student CNN whose input is motion vectors, which can be directly obtained from compressed videos. Luo et al. <ref type="bibr" target="#b24">[25]</ref> proposed a graph distillation approach to address the modality discrepancy between the source and target domain. Our study is most similar to recent works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b6">7]</ref> which distill the flow stream into the RGB stream (e.g. flow stream is the teacher while RGB stream is the student). Besides the flow stream, our experiments show the benefits of using multiple teachers, e.g. flow and human pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pose Empowered RGB-Flow Nets</head><p>In this section we describe our main contribution, the Pose Empowered RGB-Flow Nets (or PERF-Net) approach. We begin by constructing a model that predicts actions based on pose information. Specifically we describe how we represent pose and how our pose representations can be fed to a 3D CNN. The final goal is to fuse the predictions that we can obtain via this pose stream with predictions from RGB and flow streams. The standard approach of applying "late fusion" to combine disparate input streams is accurate but very slow since it requires multiple runs through the 3d convnet architecture. Instead, in the PERF-Net setting, we propose to use multi-teacher distillation to train a final model that takes RGB inputs at test time, but can benefit all three modalities (RGB, Flow, Pose) at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pose representation</head><p>By pose information we refer to human body joint positions (as is typical in the literature) which we first estimate from each RGB frame using an off-the-shelf pose estimation model and then feed to a 3D CNN as a sequence of frames. For pose estimation we use the PoseNet approach <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b26">27]</ref> with ResNet backbones which is pre-trained on the COCO dataset <ref type="bibr" target="#b22">[23]</ref> and produces 17 estimated pose keypoints for each detected human in a frame. We note that the success of our model does not depend on our specific choice of pose estimation approach. Additionally, we have not specifically tuned the pose model with respect to the final performance of PERF-Net. We also note that in our datasets, such as Kinetics-600, human poses are not available in many samples.</p><p>How specifically to render pose as a frame (which can then be sent as input to a convolutional network) is a more important design decision. Our approach is to render pose via colored lines (using a different color for each limb to allow the model to more easily distinguish between the limbs). The simplest approach (similar to that taken by <ref type="bibr" target="#b50">[51]</ref>) is to simply render the estimated pose on a black background. However using pose information alone in this way is intuitively not enough, as activity recognition often requires contextual cues -for example, having a golf club in the frame is highly indicative of the action. So instead we render the pose of each human on top of each corresponding RGB frame, which as we show in experiments, can have a sizeable impact on performance. We experiment with three additional variations of the rendering scheme:</p><p>? Dots vs bars: we render joint locations with filled circles instead of limbs with line segments.</p><p>? Fine vs coarse-grained coloring: in our coarse-grained setting we use 6 colors for the joints, assigning a unique color to the left arm, right arm, body, head, left leg, and right leg. In our fine-grained setting, each limb gets its own color (e.g., left forearm vs left upper arm).</p><p>? Uniform vs ratio-aware line thickness: in the former setting, we render lines with a uniform width; whereas in the latter setting, we set line thickness proportional to the size of the corresponding person detection's bounding box. <ref type="figure">Figure 2</ref> shows example of these pose rendering variants. As we show in the next section, using the fine-grained coloring scheme and using ratio-aware line thicknesses can lead to improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Backbone architecture</head><p>We now describe our backbone architecture which is based on a 3D version of ResNet50 where some of the convolution kernels have been "inflated" (specifically described <ref type="figure">Figure 2</ref>. A few different human pose rendering effects that have been explored. Column A uses 6 different colors to represent poses, where the top row is rendered using the same thickness of the segments and bottom row uses ratio-aware thickness of the segments. Column B and C explore two different rendering markers, points and segments with 13 different colors. The top row in column B and C uses a black background. Both column B and C also add ratio-aware radius or thickness while rendering the poses.</p><formula xml:id="formula_0">(C) (B) (A)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block</head><p>Output</p><formula xml:id="formula_1">sizes T ? S 2 ? C input 64 ? 224 2 ? 3 conv 1 5 ? 7 2 64 ? 112 2 ? 64 stride 1 ? 2 2 pool 1 1 ? 3 2 64 ? 56 2 ? 64 stride 1 ? 2 2 res 2 ? ? 3 ? 1 2 1 ? 3 2 1 ? 1 2 ? ? ? 3 64 ? 56 2 ? 256 feature gating res 3 ? ? t i ? 1 2 1 ? 3 2 1 ? 1 2 ? ? ? 4 64 ? 28 2 ? 512 feature gating res 4 ? ? t i ? 1 2 1 ? 3 2 1 ? 1 2 ? ? ? 6 64 ? 14 2 ? 1024 feature gating res 5 ? ? t i ? 1 2 1 ? 3 2 1 ? 1 2 ? ? ? 3 64 ? 7 2 ? 2048</formula><p>feature gating <ref type="table">Table 1</ref>. R3D50-G architecture used in our experiments. The kernel dimensions are T ? S 2 where T is the temporal kernel size and S is the spatial size. The strides are denoted as temporal stride ? spatial stride 2 . For res3, res4, and res5 blocks the temporal convolution only applies at every other cell. E.g., ti = 3 when i is an odd number and ti = 1 when i is even.</p><p>by <ref type="bibr" target="#b44">[45]</ref> with a few key modifications). First, we remove all max pooling operations in the temporal dimension. We find that applying temporal downsampling in any layer degrades the performance. Second, we add a feature gating module <ref type="bibr" target="#b46">[47]</ref> after each residual block. Feature gating is a selfattention mechanism that re-weights the channels based on context (i.e., the feature map averaged over time and space). We also explored adding feature gating modules after every residual cell which achieved similar results, so we decided to keep the former configuration given that it is more computationally efficient. These two modifications (no temporal downsampling, feature gating) can significantly improve the final performance and ablation studies can be found in the supplementary materials. In our experiments, we denote this modified ResNet50 as R3D50-G (see <ref type="table">Table 1</ref>). Note that our methodology for using pose as an input stream does not depend specifically on the choice of backbone, and indeed we also demonstrate results using the recent S3D-G backbone <ref type="bibr" target="#b46">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-stream fusion via distillation</head><p>Much as flow is used as a complementary signal to RGB input streams in typical action recognition papers, the intention of our pose model is to be used as a complementary signal to both RGB and flow. We now turn to how to combine these multiple streams (RGB, flow, pose) into a single model that takes RGB as its only input. Specifically we assume now that we have trained 3 models based on RGB, flow and pose respectively. The goal of our distillation approach will be to train an RGB-only model that requires much less computation compared to running all three models separately while capturing their complementary strengths.</p><p>Our approach is inspired by the D3D model <ref type="bibr" target="#b35">[36]</ref>, an RGB-only model which captures the benefits of having a temporal stream by using distillation techniques. Specifically, Stroud et al. <ref type="bibr" target="#b35">[36]</ref> trained a student model which takes a spatial (RGB-only) stream as input to do action recognition, adding an additional distillation loss which compares against the output of a teacher model that was trained on temporal stream inputs.</p><p>We apply a natural extension of the D3D approach to allow it to handle multiple distillation losses (corresponding to multiple non-spatial input streams). The total loss that we jointly minimize encourages our PERF-Net RGB-only student model to mimic logits from each teacher network while simultaneously minimizing the loss from groundtruth labels via backpropagation, and can be written as follows:</p><formula xml:id="formula_2">L = L c (S ) + N i M SE(T i , S )<label>(1)</label></formula><p>where S denotes logits from student network and T i denotes the logits from the ith teacher network. We use mean squared loss (applied to logits of student and teacher models) as the distillation loss. <ref type="figure" target="#fig_2">Figure 3</ref> shows the structure of our multi-teacher distillation framework. Note that our loss function is distinct from the natural alternative of training the student to directly mimic the standard late fusion model (by regression towards the sum of all teacher-produced logits, referred as unified loss). In experiments we show that our approach achieves significantly better performance (See <ref type="table">Table 4</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Training details Our R3D50-G models are trained on Google TPUs (v3) <ref type="bibr" target="#b20">[21]</ref> using Momentum SGD with weight decay 0.0001 and momentum 0.9. We construct each batch using 2048 clips on 256 TPU cores, yielding a per-core batch size of 8. In order to fit 8 clips in TPU memory, we use mixed precision training with bfloat16 type in all our TPU training runs <ref type="bibr" target="#b43">[44]</ref>. We train our R3D50-G models on Kinetics-600 with random initialization ("from scratch"). We also experimented with initializing from an inflated <ref type="bibr" target="#b3">[4]</ref> ImageNet <ref type="bibr" target="#b7">[8]</ref> pre-trained model but this turns out to be unnecessary in our setup. We train using a linear learning rate warm-up for the first 2k steps increasing from 0 to a base learning rate of 1.6, then use a cosine annealed learning rate <ref type="bibr" target="#b23">[24]</ref> for 20K steps. Our S3D-G models are trained on 51 GPUs with a percore batch size of 6 clips (so the total mini-batch size is 306). All S3D-G models are initialized using inflation <ref type="bibr" target="#b3">[4]</ref> with a pre-trained Inception [37] model on ImageNet <ref type="bibr" target="#b7">[8]</ref>.</p><p>All models are trained on 64 consecutive frames (at 25 FPS) from the original videos and those clips are randomly cropped from the original sequence. For each frame in the clip, we first resize the video to have a shorter side equaling to 256, and randomly crop a 224?224 region as the input to the networks. For UCF-101 and HMDB-51, we use random crops of 224 ? 298 as inputs. Random mirroring, contrast, and brightness are also applied as data augmentation. Finally, to extract flow, we use the TV-L1 approach <ref type="bibr" target="#b37">[38]</ref>.</p><p>Inference. Unlike previous work <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b10">11]</ref>, we use a single central crop of the video to evaluate our models' performance. The crop size is set to 250 ? 256 ? 256 ? 3 for Kinetics-600, 128 ? 224 ? 298 ? 3 for UCF-101, and 64 ? 224 ? 298 ? 3 for HMDB-51, (input shapes follow the frames? height? width?channels convention). For sequences that do not have sufficiently many frames, we pad by duplicating the first or the last frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">What is the best representation for pose?</head><p>Our first question is which pose rendering methods achieve the best performance ( <ref type="figure">Figure 2)</ref>? We first take the approach of rendering pose on a black background, which as shown in <ref type="table">Table 2</ref> yields an accuracy much lower than the other approaches. We argue that the reason is because there are quite a few action training examples that are missing more than 50% of the human body; thus pose cannot be determined in such frames. Instead, pose rendered on top of the RGB frames not only provides rich context beyond the pose itself, but also learns useful signals on the frames without pose.</p><p>We also experiment with dot and bar rendering markers and notice that bars yield slightly better results. We believe that this is because bars provides more geometric information about joint connections.</p><p>We also see that the fine-grained coloring scheme with ratio-aware rendering achieves the highest accuracy. This outcome is intuitive for the following reasons. First, finegrained pose rendering can provide detailed body joint relations such as fore-arm vs. upper-arm. Actions like pull-ups, hug, and throw can benefit from such joint relations. Second, with the ratio-aware line thickness, the pose itself provides information about relative distances which can serve as useful hints about group actions, e.g. playing games.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Is pose complementary to RGB?</head><p>We demonstrate that pose offers a complementary signal to the RGB (and Flow) streams. In order to demonstrate the value-add of Pose, we use the standard late-fusion approach to combining multiple streams (so as to not have potential confounding effects from distillation, which requires a more complex training setup).  <ref type="table">Table 2</ref>. Pose stream results using R3D50-G on Kinetics-600 dataset with markers: dot or bar, and ratio-aware marker size. The pose model is trained to validate performance. We also evaluate the approach of rendering on a black background, but since many training frames have no detected pose the performance of this na?ve approach tends to be very low.  <ref type="table" target="#tab_2">Table 3</ref>. Late Multi-Stream Fusion Results on Kinetics-600. To test our multi-fusion framework, we employ S3D-G and R3D50-G backbones. Here, the "G" refers to the usage of self-gating. The first block shows results using S3D-G (pretrained with Imagenet) as the backbone. The second block shows results on R3D50-G as the backbone. Pose(BB) refers to the model trained with pose rendered on black background in <ref type="table">Table 2</ref>. Among all settings, combination of all three modalities outperform other combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Kinetics datasets</head><p>In this section we focus on the the Kinetics-600 dataset <ref type="bibr" target="#b3">[4]</ref>, a large-scale, high-quality dataset containing YouTube video URLs with a diverse range of human focused actions. The dataset consists of approximately 500k video clips, and covers 600 human action classes with at least 600 video clips for each type of action. Each clip is at least 10 seconds and is labeled with one single class. The actions cover a broad range of classes including human-object interactions such as playing instruments, working out, as well as humanhuman interactions such as sword fighting and hugging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Late multi-stream fusion</head><p>In the standard "late-fusion" approach, we run models independently on multiple streams, combining their predicted logits at the end through simple addition (see <ref type="bibr" target="#b11">[12]</ref> for details). <ref type="table" target="#tab_2">Table 3</ref> shows a comparison of standard late-fusion (across different combinations of the three streams, RGB, Flow and Pose) among our two backbone models (R3D50-G and S3D-G).</p><p>For both S3D-G and R3D50-G backbones, we can see that by incorporating additional modalities, we can always achieve performance gains. Adding flow or pose to the existing RGB stream yields similar improvements. Since flow and pose are somewhat independent modalities, by adding both of them to the RGB stream, we also observe "stacking" of the performance gains. Most importantly, we see that adding the pose stream always yields benefits (independent of backbone network and independent of whether we are already using a flow stream).</p><p>One might wonder if the benefits of adding a pose stream come simply from the ensembling effect of two modelsto show that this is not the case, we show that ensembling two RGB-only models (RGB+RGB in   interaction. The middle example shows a person performing a situp at a gym. It is difficult to classify this action correctly by focusing on the barbell regions of the image, as the RGB and flow model do. Instead, pose drives the model to "look at" the entire body configuration which allows the model to decide that it is a situp and not bench press, etc. The rightmost example shows a baby climbing a ladder. The pose stream focuses the attention on the legs where the climbing action happens, providing a useful complementary cue to the standard RGB and Flow modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Visualization and explanation</head><p>The second row comprises three examples where all modalities make the prediction correctly. From the response map, we can tell the three modalities mostly focus on similar locations among the video frames. For the leftmost example (playing polo), pose helps to focus more on the entire group of players, where the other two modalities put more weight on the right-most player. By looking at the original video clip, the motion of the right-most player is the largest, which is likely why RGB and flow give more weight to this player. The middle example shows a pillow fight where the pose modality response is greater on the pillow region. The pose model may learn additional information from the interaction of the two persons by looking at the pose and arm orientation, etc. The rightmost example shows swording where the pose stream focus more on the left-side acting player.</p><p>The third row shows three examples without any pose detected. There are quite a few frames in Kinetics-600 and other datasets where no pose is available. In such cases, since the RGB is still available via the pose stream, our pose based model can still learn reasonably good responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Distilling down to PERF-Net</head><p>As discussed in Section 3.3, distillation can effectively incorporate multiple modalities with no additional cost to the complexity of the final model. In <ref type="table">Table 4</ref>, we show the results of multi-teacher distillation using Kinetics-600 dataset, which can jointly optimize over multiple input modalities. The advantage of the distillation is that our model size can remain the same while leveraging knowledge distilled from other modalities. Taking RGB as an example, after distilling on flow and pose using separate  <ref type="table">Table 5</ref> shows a comparison between PERF-Nets and other state-of-the-art single-stream works. Note that PERF-Net can easily achieve state-of-the-art performance by using a shallower ResNet50-G network. One can apply PERF-Net on stronger backbones to further boost the performance. <ref type="table">Table 6</ref> shows the three single stream results, along with the distillation on RGB stream with flow and pose streams as the teacher models on the Kinects-700 dataset <ref type="bibr" target="#b2">[3]</ref>. With 700 classes, the training tasks become considerably more challenging. In this setting, PERF-Net results show even more gain from distillation compared to the model trained on Kinectics-600, as shown in <ref type="table">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Will distilled checkpoint transfer well?</head><p>We select two human action datasets for transfer learning experiments initialized using checkpoints on Kinetics-600 or Kinetics-700 with distillation. The Kinetics-700 dataset has 100 more classes with more video clips, which is harder to learn. During fine-tuning, we use only the classification loss, but not distillation. For both of the datasets, we show that PERF-Net achieves the state-of-the-art performance among single stream models. The results also indicate that PERF-Net generalizes well given a harder dataset for pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">HMDB-51</head><p>HMDB-51 <ref type="bibr" target="#b19">[20]</ref> contains 6849 clips divided into 51 action categories, each containing a minimum of 101 clips for each category. We apply the same pose detection and rendering method to the HMDB-51 dataset. We finetune S3D-G model pre-trained on Kinetics-600 or Kinetics-700 for 30 epochs and report the accuracy by averaging the results <ref type="bibr">Model</ref> UCF-101 HMDB-51</p><p>P3D <ref type="bibr" target="#b29">[30]</ref> 88.6 -C3D <ref type="bibr" target="#b39">[40]</ref> 82.3 51.6 Res3D <ref type="bibr" target="#b40">[41]</ref> 85.8 54.9 TSM <ref type="bibr" target="#b21">[22]</ref> 95.9 73.5 I3D <ref type="bibr" target="#b3">[4]</ref> 95.6 74.8 R(2+1)D <ref type="bibr" target="#b41">[42]</ref> 96.8 74.5 S3D-G <ref type="bibr" target="#b46">[47]</ref> 96.8 75.9 HATNet <ref type="bibr" target="#b8">[9]</ref> 97.7 76.2 MARS+RGB+Flow <ref type="bibr" target="#b6">[7]</ref> 97.8 80.9 Two-stream I3D <ref type="bibr" target="#b3">[4]</ref> 98.0 80.9 RepFlow-50 <ref type="bibr">[</ref>  <ref type="table">Table 7</ref>.</p><p>Comparison with state-of-the-art on UCF-101 and HMDB-51. The backbone of the PERF-Net here is S3D-G. from 3 splits. <ref type="table">Table 7</ref> shows the averaged performance of our PERF-Net models. Our PERF-Net with backbone S3D-G, outperforms the current best on the leaderboard using single stream model <ref type="bibr" target="#b15">[16]</ref>. Note that it also outperforms two ensemble models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">UCF-101</head><p>UCF-101 <ref type="bibr" target="#b34">[35]</ref> is an action recognition data set of 13,320 realistic action videos, collected from YouTube, with 101 action categories. Similar to HMDB51, in <ref type="table">Table 7</ref>, we also report the accuracy by averaging over the 3 dataset splits. Similarly, for both Kinetics-600 and Kinetics-700 pretrainings, our PERF-Net model achieves the state-of-the-art at time of submission on the leaderboard <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented an empirical study of the effects of different pose rendering methods and how to effectively incorporate it into a video recognition model to benefit human action recognition. We have shown strong evidence that, with the human pose modality and the proposed rendering method, by using distillation, the model can outperform the state-of-the-art performance. We hope such pose modality can be further studied to extend to other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1which visualizes arXiv:2009.13087v2 [cs.CV] 20 Oct 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The distillation framework is composed of two pieces: student network and teacher network(s). The input modality can be any representations, such as RGB, flow, or pose. The losses are computed on each of the logits from the corresponding teacher networks (separate loss). Additionally, we experimented with the loss computed on the summation of logits (1, 2, ... N) from all teacher networks and added to the regression loss (unified loss). We show separate loss outperforms unified loss in the experimental section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4 shows a few such rendered examples used in the pose stream for the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Samples of fine-grained, ratio-aware rendering of PoseNet detection results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5</head><label>5</label><figDesc>shows 9 examples of RGB, pose, and flow, as well as the corresponding response map from a layer from block5 in R3D50-G. The main purpose of this figure is to show the performance of the individual models trained on each modality. The first row shows three sets of examples where the pose model is correct, and the RGB and Flow models are incorrect. For example, the leftmost example depicts an arm wrestling action. The pose response map responds most on the hands region of the frame where the wrestling happens.The response heatmap can be treated as an attention area in a tube of action sequences. For such actions, flow is not informative as there is little motion. Moveover, the RGB response could be distracted by elements in the background. However, pose can provide clear signal to the hand-to-hand</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Nine Grad-CAM visualizations [33] of our R3D50-G model. Each row contains three examples. For each example, the top row contains the original RGB, pose overlay, and Flow frames and the bottom row are the normalized response maps from RGB, pose, and flow streams, respectively. ROW1: arm wrestling, situp action, ladder climbing. ROW2: playing polo, pillow fight, swording. ROW3: unboxing, weaving basket, napkin folding. The top two rows show examples with pose detected. The bottom row shows three actions without any pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>) does not</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Comparison with the state-of-the-art on Kinetics-600. Results on Kinetics-700 distillation. The first three rows are single stream results. The last row is the PERF-Net results.</figDesc><table><row><cell></cell><cell></cell><cell>Model</cell><cell></cell><cell></cell><cell>Backbone</cell><cell>Top-1</cell><cell>Top-5</cell><cell>GFLOPs</cell></row><row><cell></cell><cell></cell><cell>I3D [4]</cell><cell></cell><cell></cell><cell>Inception</cell><cell>71.9</cell><cell>90.1</cell><cell>544</cell></row><row><cell></cell><cell></cell><cell cols="2">StNet-IRv2 RGB [14]</cell><cell></cell><cell>InceptionResNet-V2</cell><cell>79.0</cell><cell>-</cell><cell>440</cell></row><row><cell></cell><cell></cell><cell cols="2">P3D two-stream [30]</cell><cell></cell><cell>ResNet152</cell><cell>80.9</cell><cell>94.9</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">SlowFast R101+NL [11]</cell><cell></cell><cell>ResNet101</cell><cell>81.8</cell><cell>95.1</cell><cell>7020</cell></row><row><cell></cell><cell></cell><cell cols="2">LGD-3D RGB [31]</cell><cell></cell><cell>ResNet101</cell><cell>81.5</cell><cell>95.6</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>X3D-XL [10]</cell><cell></cell><cell></cell><cell>Custom</cell><cell>81.9</cell><cell>95.5</cell><cell>1452</cell></row><row><cell></cell><cell></cell><cell>PERF-Net (ours)</cell><cell></cell><cell></cell><cell>ResNet50-G</cell><cell>82.0</cell><cell>95.7</cell><cell>3666</cell></row><row><cell cols="3">Backbone Student Teacher(s)</cell><cell cols="3">Top-1 Top-5 pretrain</cell></row><row><cell></cell><cell>RGB</cell><cell>-</cell><cell>63.5</cell><cell>85.1</cell><cell>-</cell></row><row><cell>S3D-G</cell><cell>Flow Pose</cell><cell>--</cell><cell>51.0 61.3</cell><cell>75.8 83.5</cell><cell>--</cell></row><row><cell></cell><cell>RGB</cell><cell>Flow+Pose</cell><cell>67.9</cell><cell>87.9</cell><cell>-</cell></row><row><cell cols="6">losses, the performance can be improved beyond single</cell></row><row><cell cols="6">modality training -thus our final RGB-only model (a.k.a.</cell></row><row><cell cols="6">PERF-Net) achieves 82.0 top-1 accuracy on Kinetics-600,</cell></row><row><cell cols="5">which outperforms the state-of-the-art work.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<ptr target="http://activity-net.org/challenges/2020/tasks/guestkinetics.html.1" />
	</analytic>
	<monogr>
		<title level="j">ActivityNet Kinetics Challenge</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilhem</forename><surname>Ch?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3218" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="7024" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mars: Motion-augmented rgb stream for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nieves</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7882" to="7891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Large scale holistic video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Melvyn A Goodale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Milner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Exploiting spatial-temporal modelling and multi-modal fusion for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongliang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Action recognition in videos on hmdb-51</title>
		<idno>HMDB-51-Leaderboard</idno>
		<ptr target="https://paperswithcode.com/sota/action-recognition-in-videos-on-hmdb-51" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose for action-action for pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2017)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="2938" to="2946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hmdb51: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Estibaliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Bitorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blake</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09756</idno>
		<title level="m">models on google tpu-v3 pods</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph distillation for action detection with privileged modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="166" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Diogo C Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="3711" to="3719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Evolving space-time neural architectures for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representation flow for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal representation with local and global diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong-Wah</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piergiovanni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13209</idno>
		<title level="m">Mingxing Tan, and Anelia Angelova. Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">TV-L1 Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>S?nchez P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enric</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Facciolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Convnet architecture search for spatiotemporal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Leaderboard: Action recognition in videos on ucf101</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ucf101-Leaderboard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Bfloat16: the secret to high performance on cloud tpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Kanwar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Google Cloud Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1293" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pa3d: Poseaction 3d machine for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Does human action recognition benefit from pose estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.25.67.1</idno>
		<ptr target="http://dx.doi.org/10.5244/C.25.67.1" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="67" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
