<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Masked Feature Prediction for Self-Supervised Visual Pre-Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Masked Feature Prediction for Self-Supervised Visual Pre-Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* equal technical contribution</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pretrained on unlabeled videos achieves unprecedented results of 86.7% with MViT-L on Kinetics-400, 88.3% on Kinetics-600, 80.4% on Kinetics-700, 38.8 mAP on AVA, and 75.0% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Self-supervised pre-training has been phenomenally successful in natural language processing powering large-scale Transformers <ref type="bibr" target="#b87">[88]</ref> with billion-scale data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25]</ref>. The underlying idea is an astonishingly simple mask-and-predict task, that is, first masking out some tokens within a text and then predicting the invisible content given the visible text.</p><p>Humans have a remarkable ability to predict how the world appears and moves when observing it as a continuous stream of spatiotemporal information. Consider the examples in the 1 st column of <ref type="figure" target="#fig_0">Fig. 1</ref>. Even without seeing the masked content, we are able to understand the object structure and draw a rough outline or silhouette of imagined information (up to some details), by using visual knowledge about the visible structures. In this work, we show that predicting certain masked features (e.g. gradient histograms in the 2 nd column) can be a powerful objective for self-supervised visual pre-training, especially in the video domain which contains rich visual information. One essential difference between vision and language is that vision has no pre-existing vocabulary to shape the prediction task into a well-defined classification problem. In contrast, the raw spatiotemporal visual signal is continuous and dense posing a major challenge to masked visual prediction. One immediate solution is to imitate the language vocabulary by building a visual vocabulary that discretizes frame patches into tokens, as explored in BEiT <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b72">73]</ref>. However, this requires an external tokenizer which can be limited in compute-intensive video understanding scenario.</p><p>We present Masked Feature Prediction (MaskFeat), a pre-training objective that directly regresses features of the masked content. Specifically, our approach ingests the masked space-time input with a vision Transformer backbone <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b55">56]</ref> and predicts a certain feature representation of the masked content. In this way, the pre-trained model acquires an adequate understanding of the complex spacetime structures within dense visual signals.</p><p>We study a broad spectrum of feature types, from pixel colors and hand-crafted feature descriptors, to discrete visual tokens, activations of deep networks, and pseudo-labels from network predictions. Our study reveals:</p><p>(i) Simple histogram of oriented gradients (center column in <ref type="figure" target="#fig_0">Fig. 1)</ref>, as in the popular HOG <ref type="bibr" target="#b21">[22]</ref> and SIFT <ref type="bibr" target="#b61">[62]</ref> descriptors which dominated visual recognition for over a decade, is a particularly effective target for MaskFeat in terms of both performance and efficiency.</p><p>(ii) The discretization (tokenization) of visual signals is not necessary for masked visual prediction, and continuous feature regression (i.e. MaskFeat) can work well.</p><p>(iii) Semantic knowledge from human annotations is not always helpful for MaskFeat, but characterizing local patterns seems important. For example, predicting supervised features from CNNs or ViTs trained on labeled data leads to degraded performance.</p><p>Our approach is conceptually and practically simple. Compared to contrastive methods that require a siamese structure and two or more views of each training sample (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref>), MaskFeat uses a single network with a single view of each sample; and unlike contrastive methods that strongly rely on carefully designed data augmentation, MaskFeat works fairly well with minimal augmentation.</p><p>Compared to previous masked visual prediction methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b81">82]</ref>, MaskFeat with HOG does not involve any external model, such as a dVAE tokenizer <ref type="bibr" target="#b72">[73]</ref> that introduces not only an extra pre-training stage on 250M images, but also non-negligible training overhead in masked modeling.</p><p>We show that MaskFeat can pre-train large-scale video models that generalize well. Transformer-based video models, though powerful, are previously known to be prone to over-fitting and heavily rely on supervised pre-training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b55">56]</ref> on large-scale image datasets, e.g., ImageNet-21K (IN-21K) <ref type="bibr" target="#b23">[24]</ref>. While MaskFeat opens the door for directly pretraining on unlabeled videos which shows enormous benefits for video understanding.</p><p>Our results on standard video benchmarks are groundbreaking: MaskFeat pre-trained MViT-L <ref type="bibr" target="#b55">[56]</ref> gets 86.7% top-1 accuracy on Kinetics-400 <ref type="bibr" target="#b50">[51]</ref> without using any external data, greatly surpassing the best prior number of this kind by +5.2%, and also methods using large-scale image datasets, e.g., IN-21K and JFT-300M <ref type="bibr" target="#b79">[80]</ref>. When transferring to downstream tasks, MaskFeat gets unprecedented results of 38.8 mAP on action detection (AVA <ref type="bibr" target="#b41">[42]</ref>) and 75.0% top-1 accuracy on human-object interaction classification (SSv2 <ref type="bibr" target="#b39">[40]</ref>). When generalized to the image domain, MaskFeat also obtains competitive 84.0% top-1 with ViT-B and 85.7% with ViT-L using only ImageNet-1K <ref type="bibr" target="#b23">[24]</ref>.</p><p>Our code will be available in PyTorchVideo 1,2 <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We start by describing MaskFeat and its instantiations for video and image understanding in ?2.1. We then introduce and discuss five candidates for target features in ?2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Masked Feature Prediction</head><p>Our method performs a masked visual prediction task, motivated by humans' ability to inpaint masked visual content up to some details. The task first randomly masks out a few space-time cubes of a video, and then predicts the masked ones given the remaining ones. By modeling masked samples, the model attains video understanding in the sense of recognizing parts and motion of objects. For instance, to solve the examples in Figs. 1 and 5, a model has to first recognize the objects based on the visible area, and also know what the objects typically appear and how they usually move to inpaint the missing area.</p><p>One key component of the task is the prediction target. Masked language modeling tokenizes the corpus with a vocabulary to serve as the target <ref type="bibr" target="#b24">[25]</ref>. In contrast, the raw visual signal is continuous and high-dimensional and there is no natural vocabulary available. In MaskFeat, we propose to predict features of the masked area. And the supervision is provided by features extracted from the original, intact sample. We use a wide interpretation of features <ref type="bibr" target="#b12">[13]</ref>, from hand-crafted feature descriptors, to activations of deep networks. The choice of the target feature largely defines the task and impacts the property of the pre-trained model, which we discuss in ?2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instantiations. We first describe MaskFeat for video input.</head><p>A video is first divided into space-time cubes as in typical video Vision Transformers <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref>. The cubes are then projected (i.e. convolved) to a sequence of tokens. To perform masking, some of the tokens in the sequence are randomly masked out by being replaced with a [MASK] token. This is a learnable embedding indicating masked patches. A block of tokens is masked together which we detail in ?4.3. To make a prediction, the token sequence after [MASK] token replacement, with positional embedding added, is processed by the Transformer. Output tokens corresponding to the masked cubes are projected to the prediction by a linear layer. The prediction is simply the feature of the 2-D spatial patch temporally centered in each masked cube (see discussions in ? 4.3). The number of output channels is adjusted to the specific target feature (e.g., 3?16?16 if predicting RGB colors of pixels in a 16?16 patch). The loss is only operated on the masked cubes. Our instantiation is inspired by BERT <ref type="bibr" target="#b24">[25]</ref> and BEiT <ref type="bibr" target="#b1">[2]</ref>, illustrated in <ref type="figure">Fig. 2</ref>.</p><p>MaskFeat can be easily instantiated in the image domain, which can be interpreted as a video with one single frame. Most operations are shared, except that there is no temporal dimension and each token now represents only a spatial patch instead of a space-time cube.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Target Features</head><p>We consider five different types of target features. The targets are categorized into two groups: 1) one-stage targets that can be directly obtained including pixel colors and HOG, and 2) other two-stage targets extracted by a trained deep network or teacher. As predicting two-stage targets is effectively learning from a trained deep network teacher, it resembles a form of model distillation <ref type="bibr" target="#b47">[48]</ref>; thereby, an extra computational cost of pre-training and inference of the teacher model is inevitable. The five feature types are:</p><p>Pixel colors. The most straightforward target is arguably the colors of video pixels. Specifically, we use RGB values that are normalized by the mean and the standard deviation of the dataset. We minimize the 2 distance between the model's prediction and the ground-truth RGB values. A similar idea has been explored in <ref type="bibr" target="#b68">[69]</ref> as a image inpainting task and in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> for masked image prediction. Though simple, pixels as target have a potential downside of overfitting to local statistics (e.g. illumination and contrast variations) and high-frequency details, which are presumably insignificant <ref type="bibr" target="#b75">[76]</ref> for interpretation of visual content.</p><p>HOG. Histograms of Oriented Gradients (HOG) <ref type="bibr" target="#b21">[22]</ref> is a feature descriptor that describes the distribution of gradient orientations or edge directions within a local subregion. A HOG descriptor is implemented by a simple gradient filtering (i.e. subtracting neighboring pixels) to compute magnitudes and orientations of gradients at each pixel. The gradients within each small local subregion or cell are then accumulated into orientation histogram vectors of several bins, voted by gradient magnitudes. The histogram is normalized to unit length. These features are also used in well-known SIFT <ref type="bibr" target="#b61">[62]</ref> descriptors for detected keypoints or in a dense fashion for classification <ref type="bibr" target="#b12">[13]</ref>. Similarly, we extract HOG on a dense grid for the whole image, which suits the prediction target for randomly masked patches.</p><p>HOG is characteristic of capturing local shapes and appearances while being partially invariant to geometric changes as long as translations are within the spatial cell and rotations are smaller than orientation bin size. Further, it provides invariance to photometric changes as image gradients and local contrast normalization absorb brightness (e.g. illumination) and foreground-background contrast variation. These invariances are vital for good results when using HOG for pedestrian detection in both image <ref type="bibr" target="#b21">[22]</ref> and video <ref type="bibr" target="#b22">[23]</ref> domains. In accordance to this, our studies ( ?5.2) reveal local-contrast normalization in HOG is also essential for MaskFeat pre-training.</p><p>Finally, HOG computation is cheap and introduces negligible overhead. It can be implemented as a two-channel convolution to generate gradients in x and y axis (or by subtracting neighboring horizontal and vertical pixels) , followed by histogramming and normalization.</p><p>Our method then simply predicts the histograms summarizing masked patches. Instead of computing HOG only on masked patches, we first obtain a HOG feature map on the whole image and then split the map into patches. In this way, we reduce padding on boundaries of each masked patch. The histograms of masked patches are then flattened and concatenated into a 1-D vector as the target feature. Our loss minimizes the 2 distance between the predicted and original HOG feature. We collect HOG in each RGB channel to include color information which can slightly improve its performance ( ?5.2).</p><p>Discrete variational autoencoder (dVAE). To address the continuous high-dimensional nature of visual signals, DALL-E <ref type="bibr" target="#b72">[73]</ref> proposes to compress an image with a dVAE codebook. In particular, each patch is encoded into a token which can assume 8192 possible values using a pre-trained dVAE model. Now the task is to predict the categorical distribution of the masked token by optimizing a cross-entropy loss, as explored in BEiT <ref type="bibr" target="#b1">[2]</ref>. However, there is an extra computational cost induced by pre-training the dVAE and tokenizing images alongside masked feature prediction.</p><p>Deep features. In comparison to discretized tokens, we consider directly using continuous deep network features as the prediction target. We use a pre-trained model to produce features as a teacher, either a CNN or ViT, and our loss minimizes the cosine distance (i.e. mean squared error of 2 -normalized features).</p><p>For CNN teachers, we use the last layers' features corresponding to the masked patches and for ViT we use the respective output patch tokens. We mainly compare features from self-supervised models, which are considered to contain more diverse scene layout <ref type="bibr" target="#b8">[9]</ref> and preserve more visual details <ref type="bibr" target="#b98">[99]</ref> than features from supervised models.</p><p>(Though, the usage of human annotations makes the pretraining technically not self-supervised.) Supervised features are expected to be more semantic as they are trained through human annotations. Similar to dVAE, a non-trivial amount of extra computation is involved when using extra model weights for masked feature generation.</p><p>Pseudo-label. To explore an even more high-level semantic prediction target, we consider predicting class labels of masked patches. We utilize labels provided by Token Labeling <ref type="bibr" target="#b49">[50]</ref>, where each patch is assigned an individual location-specific IN-1K pseudo-label. This class label map is generated by a pre-trained high-performance supervised deep network <ref type="bibr" target="#b4">[5]</ref> teacher. The masked feature prediction stage is optimized by a cross-entropy loss.</p><p>We next study the features discussed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Study: Target Features for MaskFeat</head><p>Settings. We use a pre-training and fine-tuning protocol, following BEiT <ref type="bibr" target="#b1">[2]</ref>. We pre-train MViT-S, 16?4 <ref type="bibr" target="#b55">[56]</ref> with MaskFeat on Kinetics-400 (K400) <ref type="bibr" target="#b50">[51]</ref> training set for 300 epochs. We also apply MaskFeat on images, where we pretrain ViT-B <ref type="bibr" target="#b26">[27]</ref> on the ImageNet-1K (IN-1K) <ref type="bibr" target="#b23">[24]</ref> training set for 300 epochs. We report top-1 fine-tuning accuracy (%) on both datasets. We pre-train and fine-tune all targets with the same recipe which we find generally good in practice ( ?B.1). For targets that involve a teacher model, we use official models released by the authors. Most features are compared on both video and image domains except pseudo-label for which the pseudo-label map is only available on IN-1K <ref type="bibr" target="#b49">[50]</ref>. Results are summarized in Tables 1 (video) and 2 (image), analyzed next:</p><p>One-stage methods. The fine-tuning accuracy for pixel color prediction in <ref type="table" target="#tab_1">Tables 1 &amp; 2</ref> shows, that compared to the from-scratch baselines, regressing RGB colors produces a slight drop of -0.4% for video classification and a relatively small gain of +0.7% for image. Even though our predicting pixel colors result on IN-1K (82.5%) is better than that reported in BEiT <ref type="bibr" target="#b1">[2]</ref> (81.0%), we similarly observe that pixel values are not ideal direct targets, presumably because they are considered to be too explicit <ref type="bibr" target="#b72">[73]</ref>. In comparison, HOG, by summarizing the local gradient distribution, contributes to large improvements of +1.1% on K400 and +1.8% on IN-1K over the from-scratch baselines without any extra model which is typical in two-stage methods.</p><p>Two-stage methods. First, dVAE improves by +0.6% for K400 and +1.0% for IN-1K over their from-scratch baselines. This is better than pixel colors, but outperformed by HOG which does not use an external model.</p><p>Next, compared to dVAE, we study MaskFeat to predict continuous, unsupervised features: We compare DINO <ref type="bibr" target="#b8">[9]</ref> (with ViT-B) and MoCo <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> (with ResNet50 <ref type="bibr" target="#b45">[46]</ref> and ViT-B), all pre-trained on IN-1K, even for the video pretraining. Unsupervised features contribute a notable gain for both video and image classification: The DINO variant achieves a gain of +1.4% on K400 and +2.2% on IN-1K compared to their baselines. However, this approach has two main drawbacks, (i) the unsupervised feature extractor needs to be pre-trained e.g. worth over thousand epochs in the case of DINO, (ii) the unsupervised features need to be computed on the target data. Still, MaskFeat w/ DINO and MoCo v3 features boosts their original accuracy <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>Finally, supervised features (from ResNet50 or ViT-B) as well as token labels, though utilizing human annotations, lag behind unsupervised features and HOG. In fact, we notice significant over-fitting during fine-tuning for supervised  features and token labels, suggesting that predicting features learned from class labels is not suitable in MaskFeat. We hypothesize that class label being invariant to local shapes and textures of the same object disables the ability of MaskFeat to model object's internal structure.</p><p>Discussion. Our results suggest that a broad spectrum of image features can serve as targets in masked visual prediction, and provide gains over the train-from-scratch baseline. We find that although masked language modeling <ref type="bibr" target="#b24">[25]</ref> originally predicts the categorical distribution over a pre-defined vocabulary, discretization as in BEiT <ref type="bibr" target="#b1">[2]</ref> is not required for vision. We find that continuous unsupervised features and image descriptors can be strong prediction targets, while the latter come without cost compared to the former which also entail a form of model distillation <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b83">84]</ref>. An interesting observation is that supervisedly trained target features produce poor results, which might relate to class-level specific information being present in features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b99">100]</ref> that is too global for local mask modeling. Overall, considering the trade-off between performance and computational cost, predicting HOG holds a good balance and therefore we use it as default feature for MaskFeat in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments: Video Recognition</head><p>Settings. We evaluate with base and large models of improved MViT <ref type="bibr" target="#b55">[56]</ref>. The original MViT in <ref type="bibr" target="#b30">[31]</ref> is termed as MViTv1. The models are pre-trained only on video clips in the training set of K400 <ref type="bibr" target="#b23">[24]</ref> without labels. Our augmentation includes random resized cropping and horizontal flipping. Our models are pre-trained and fine-tuned at 224 2 resolution if not specified. We randomly mask out 40% of total space-time cubes with cube masking detailed in ?4.3. More implementation details are in Appendix B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Main Results on Kinetics</head><p>Kinetics-400.  <ref type="table" target="#tab_3">Table 3</ref>. Comparison with previous work on Kinetics-400. We report the inference cost with a single "view" (temporal clip with spatial crop) ? the number of views (FLOPs?viewspace?viewtime). Each "view" consists of T frames with ? temporal stride, T ? ? . Magnitudes are Giga (10 9 ) for FLOPs and Mega (10 6 ) for Param. Accuracy of models trained with external data is de-emphasized. reaches 84.3% top-1, outperforming its scratch baseline by a large margin of +3.8% and its IN-21K supervised counterpart by +0.8%. Similar to the image domain, MaskFeat is more significant with larger models, showing that our approach is salable to model capacity. The result also suggests that MaskFeat adapts to different model types, as MViT is a Transformer model with convolutions. We provide ablation on the pre-training schedule in ? 4.3.</p><p>We further explore the data scalability of MaskFeat. In particular, we pre-train MViT-L, 16?4 with Kinetics-600 (K600) <ref type="bibr" target="#b9">[10]</ref> containing 387K training videos, 1.6? more than K400. We pre-train for 300 epochs on K600 to use a slightly smaller training budget as the 800 epochs on K400. We again fine-tune on K400 and observe that pre-training on K600, without any labels, contributes to another +0.8% gain over K400 pre-training to reach 85.1% top-1.</p><p>Next, we fine-tune the 84. Our best 87.0% top-1 accuracy is achieved by fine-tuning the 85.1% MViT-L, 16?4 pre-trained with MaskFeat on 387K training videos in K600 using no labels.</p><p>Our results with just K400 (86.7%) is already similar to recent 86.5% Florence <ref type="bibr" target="#b94">[95]</ref> and 86.8% SwinV2-G <ref type="bibr" target="#b57">[58]</ref>. Florence uses 900M curated text-image pairs. SwinV2-G utilizes a giant model with three billion parameters, and is first self-supervisedly then supervisedly pre-trained on a large dataset of IN-21K plus 70M in-house images. The efficiency of our approach in terms of parameter count, compute cost, data, and annotation suggests again the advantage of MaskFeat directly pre-training on unlabeled videos.</p><p>Kinetics-600 and Kinetics-700. <ref type="table" target="#tab_4">Table 4</ref> compares with prior work on K600 <ref type="bibr" target="#b9">[10]</ref> and K700 <ref type="bibr" target="#b10">[11]</ref>. Both are larger versions of Kinetics. An MViT-L, 16?4 is pre-trained with MaskFeat for 300 epochs and fine-tune for 75 epochs on both datasets. The models achieve the top accuracy of 86.4% on K600 and 77.5% on K700, using no external image data amd over 10?fewer FLOPs compared to previous Transformer-based methods.</p><p>Finally, we fine-tune these MViT-L, 16?4 models at a larger input resolution of 312 and a longer duration of 40?3 to achieve 88.3% top-1 on K600 and 80.4% top-1 on K700, setting a new state-of-the-art with a large margin over the previous best approaches on each dataset, without any external supervised pre-training (e.g. on IN-21K or JFT-300M).  <ref type="table">Table 5</ref>. Transferring to AVA v2.2 <ref type="bibr" target="#b41">[42]</ref>. We use single center crop inference (center) following MViTv1 <ref type="bibr" target="#b30">[31]</ref> and full resolution inference (full) to compare to the 2020 AVA Challenge winner ACAR <ref type="bibr" target="#b65">[66]</ref>. Inference cost is with the center strategy.  <ref type="table">Table 6</ref>. Transferring to Something-Something v2 <ref type="bibr" target="#b39">[40]</ref>. We report FLOPs with a single "view". All entries use one temporal clip and three spatial crops (inference cost is FLOPs?3?1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transfer Learning</head><p>We evaluate transfer learning in downstream tasks using the MViT-L?312, 40?3 Kinetics models in <ref type="table" target="#tab_3">Tables 3 and 4a</ref>.</p><p>Action detection. AVA v2.2 <ref type="bibr" target="#b41">[42]</ref> is a benchmark for spatiotemporal localization of human actions. We fine-tune the MViT-L?312, 40?3 Kinetics models on AVA v2.2. Details are in ?B.2. <ref type="table">Table 5</ref> reports mean Average Precision (mAP) of our MaskFeat models compared with prior state-of-theart. MaskFeat only using K400 contributes to a significant gain of +4.7 mAP over its IN-21K pre-trained counterpart using identical architectures. By utilizing a larger video dataset, K600, the model reaches an unprecedented accuracy of 38.8 mAP with full resolution testing, greatly surpassing all previous methods, including ActivityNet challenge winners. The strong performance of MaskFeat on AVA suggests a clear advantage of masked modeling on video over supervised classification on image pre-training for this localization-sensitive recognition task.</p><p>Human-object interaction classification. We fine-tune the MViT-L?312, 40?3 Kinetics models in <ref type="table" target="#tab_3">Tables 3 and 4a</ref> to Something-Something v2 (SSv2) <ref type="bibr" target="#b39">[40]</ref> which focuses on human-object interaction classification. <ref type="table">Table 6</ref> presents the results and details are in Appendix B.3. In contrast to Kinetics, SSv2 requires fine-grained motion distinctions and temporal modeling to distinguish interactions like picking something up and putting something down.</p><p>Despite the differences between the supervised tasks of Kinetics and SSv2, pre-training on Kinetics without supervised labels using MaskFeat still contributes to a large gain on fine-tuning accuracy of SSv2. Specifically, MaskFeat with only K400 data contributes to +1.1% top-1 over its IN-21K+K400 pre-trained counterpart. By utilizing the larger K600, the model reaches an unprecedented 75.0% top-1 accuracy, surpassing all previous methods. This suggests that MaskFeat can learn spatiotemporal representations from unlabeled Kinetics data which is known as appearancebiased, through self-supervised masked feature prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablations for Video Recognition</head><p>The ablations are with MViT-S, 16?4 pre-trained for 300 epochs on K400 if not specified. We report 200-epoch fine-tuning accuracy (%) on K400.</p><p>Masking strategy. We study the masking strategy for spatiotemporal video data. In video, tokens sharing the same spatial position usually also share visual patterns. Therefore, we explore how to handle this redundancy brought by the addition of the temporal dimension. We consider three different ways of masking and present the results in <ref type="table" target="#tab_7">Table 7</ref>. All entries share the same 40% masking ratio. First, we consider "frame" masking, which independently masks out consecutive frames. This strategy mostly masks different spatial blocks in consecutive frames, but the model could temporally "interpolate" between frames to solve the task. This strategy only obtains 81.0% top-1.</p><p>Second, we consider "tube" masking. Namely, we first sample a 2-D mask map by block-wise masking as for images, and then extend the 2-D map by repeating it in the temporal dimension. Thus, the masked area is a straight tube in a video clip, in which the spatially masked area is the same for every frame. Tube masking refrains from relying on the temporal repetition to predict the masked content in static video. It leads to 81.9% accuracy.</p><p>Third, we consider "cube" masking, which includes both spatial and temporal blocks that are masked out. This is achieved by sampling random "cubes" of tokens until a certain masking ratio is reached. Cubes are sampled by first creating a 2-D block at a random time step, then extending in the temporal dimension with a random number of consecutive frames. Therefore, cube masking can be considered as an generalization of tube and frame masking. It produces 82.2% accuracy when used for pre-training.</p><p>Overall, the results in <ref type="table" target="#tab_7">Table 7</ref> show that cube masking performs best, suggesting both spatial and temporal cues are helpful in masked spatiotemporal prediction. Target design. On video, each output token corresponds to a space-time cube. Our default setting is to simply predict the feature of the 2-D spatial patch temporally centered in each masked space-time cube. In <ref type="table" target="#tab_8">Table 8</ref> we consider another straightforward way of predicting the entire cube, i.e., HOG features of each 2-D patch in the 3-D cube. Results are similar and we use center patch prediction for simplicity.  <ref type="table">Table 9</ref>. Masking ratio. Varying the percentage of masked patches. MaskFeat is robust to masking ratio in video domain.</p><p>Masking ratio. We study the effect of the masking ratio in <ref type="table">Table 9</ref>. Interestingly, a wide range of masking ratios from 40% to the extreme 80% can produce similar fine-tuning accuracy, and only a small ratio of 20% leads to a slight drop of -0.3%. This is different from the observation on images, where ratios larger than 40% lead to degraded accuracy (see discussions in Appendix A). This indicates that in the video domain visual patterns are indeed more redundant than in images, and thus MaskFeat enjoys a larger masking ratio to create a properly difficult task. Pre-training schedule. We show different pre-training schedule lengths on K400 in <ref type="table" target="#tab_1">Table 10</ref>. Each result is finetuned from a fully trained model instead of an intermediate checkpoint. For MViT-S with 36M parameters, extending pre-training from 300 epochs to 800 epochs results in a small performance degradation of 0.2% accuracy. In contrast, for MViT-L longer pre-training provides a significant gain of +1.2% accuracy. This suggests that MaskFeat is a scalable pre-training task that can be better utilized by models with larger capacity and longer schedule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments: Image Recognition</head><p>Settings. The evaluation protocol is pre-training followed by end-to-end fine-tuning. We use vanilla base and large models in ViT <ref type="bibr" target="#b26">[27]</ref> without modification. Our models are pre-trained at 224 2 resolution on IN-1K <ref type="bibr" target="#b23">[24]</ref> training set without labels. We use minimal data augmentation: random resized cropping and horizontal flipping. We randomly mask out 40% of total image patches with block-wise masking following BEiT <ref type="bibr" target="#b1">[2]</ref>. More details are in Appendix B. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Main Results on ImageNet-1K</head><p>In <ref type="table" target="#tab_1">Table 11</ref> we compare MaskFeat to previous work including from-scratch, IN-21K supervised pre-training, and previous self-supervised methods. We pre-train MaskFeat for 1600 epochs here while for 300 epochs in <ref type="table" target="#tab_2">Table 2</ref>. The fine-tuning schedule is the same everywhere and rather short, 100 epochs for ViT-B and 50 epochs for ViT-L.</p><p>We observe that MaskFeat pre-training significantly boosts the scratch baselines for both ViT-B and ViT-L. Our approach at image size 224 2 is on par with (ViT-B), or even outperforms (ViT-L) supervised pre-training on IN-21K that has 10?more images and labels at image size 384 2 . It has been shown <ref type="bibr" target="#b26">[27]</ref> that ViT models are data-hungry and require large-scale supervised pre-training, possibly due to the lack of typical CNN inductive biases. Our results suggest that MaskFeat pre-training can overcome this without external labeled data by solving our feature inpainting task. Interestingly, more gains are observed on ViT-L compared with ViT-B, suggesting that it is scalable to larger models.</p><p>Compared to self-supervised pre-training approaches, MaskFeat is more accurate and simpler. MoCo v3 <ref type="bibr" target="#b17">[18]</ref> and DINO <ref type="bibr" target="#b8">[9]</ref> are contrastive methods that require multiview training and carefully designed augmentation, while MaskFeat only uses single-views and minimal augmentation. See Appendix A for ablation on data augmentation of MaskFeat. Compared with BEiT <ref type="bibr" target="#b1">[2]</ref>, MaskFeat gets rid of the dVAE tokenizer, which introduces both an extra pre-training stage on the 250M DALL-E dataset, and a non-negligible inference overhead during masked prediction. While MaskFeat simply calculates HOG features.</p><p>MaskFeat in <ref type="table" target="#tab_1">Table 11</ref> is pre-trained for 1600 epochs with a single 224 2 view. DINO uses multiple global-local views and an extra momentum encoder, leading to 1535 effective epochs ? <ref type="table" target="#tab_2">(Table 2)</ref>. MoCo v3 saturates after 600 effective epochs <ref type="bibr" target="#b17">[18]</ref>. BEiT is pre-trained for 800 epochs on IN-1K but requires another 1199 effective epochs for dVAE.</p><p>We also train the best model in   <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>. HOG as target is (c) robust to the number of orientation bins, and (d) benefits from 8 ? 8 spatial cell. Opp. represents opponent color space <ref type="bibr" target="#b85">[86]</ref>. Default entries are marked as gray .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablations for Image Recognition</head><p>We ablate the design choices of MaskFeat in the image domain first. We use ViT-B pre-trained for 300 epochs by default and report fine-tuning top-1 accuracy (%) on IN-1K. More ablations (e.g. on training epochs) are in Appendix A.</p><p>HOG implementation. We ablate HOG implementation details in <ref type="table" target="#tab_1">Table 12</ref>. We first investigate the local contrast normalization in HOG, which is key to its performance in image recognition <ref type="bibr" target="#b21">[22]</ref>. It is applied by normalizing each histogrammed vector of local 8?8 pixel cells, which leads e.g. to local invariance in illumination change. We show in <ref type="table" target="#tab_1">Table 12a</ref> that normalization is essential for MaskFeat. Compared with default 2 normalization, using 1 normalization results in a 0.8% drop and not using any normalization causes a large -1.4% drop. Similar results are reported in <ref type="bibr" target="#b21">[22]</ref> for directly using HOG for image recognition.</p><p>We next investigate the effectiveness of color information in <ref type="table" target="#tab_1">Table 12b</ref>. Gray corresponds to HOG on gray-scale images, which only contains intensity information. To include color information in HOG, rbg calculates separate gradients for each color channel and concatenates the three histograms. And opp. is an affine transformation of RGB to an opponent color space <ref type="bibr" target="#b85">[86]</ref>. Results show that using color information provides a small gain of around +0.4% compared with only using gray-scaled intensity information.</p><p>We vary the number of orientation and spatial bins in <ref type="table" target="#tab_1">Table 12c and Table 12d</ref>, which provides geometric invariance in HOG descriptors. Following HOG <ref type="bibr" target="#b21">[22]</ref>, we use 9 orientation bins that are evenly spaced from 0?to 180?(unsigned), and use 8?8 pixel cells (SIFT [62] uses 8 bins in 8?8 cells). We observe that these default settings in <ref type="bibr" target="#b21">[22]</ref> are good for MaskFeat and that it is robust to different numbers of orientation bins, but a specific size of 8?8 pixels in a cell is the best.</p><p>Pixel vs. HOG. We qualitatively compare HOG to pixel colors as the feature target of MaskFeat in <ref type="figure">Fig. 3</ref>. Both pixel and HOG predictions look reasonable in close proximity to the unmasked input. However, compared to HOG, pixel color targets come with more ambiguity. In the bal-masked input pixel prediction HOG prediction original image Both two predictions make good sense given a small visible region at the bird's head.</p><p>Pixel with color ambiguity: Though pixel prediction makes a sensible guess on the balloon, the loss penalty is large because of unmatched color (red vs. black).</p><p>Pixel with texture ambiguity: Pixel prediction is blurry in texture-rich area because of ambiguity, while HOG successfully characterizes major edge directions. <ref type="figure">Figure 3</ref>. Pixel vs. HOG predictions on IN-1K validation images ? . Pixel targets can have large errors for ambiguous problems, and HOG is more robust to ambiguity by histogramming and normalizing local gradients. Best viewed in color and zoomed in. ? The unmasked regions are not used for loss and thus qualitatively poor. loon (second) example, the model makes a sensible guess predicting a red balloon, which is black in the original image, resulting in a high loss penalty. In the sea urchin (third) example, the model is just able to make a blurry colorwise guess on the object, which is a natural consequence of minimizing a pixel-wise MSE loss in texture-rich, highfrequency regions <ref type="bibr" target="#b54">[55]</ref>. In both cases, HOG reduces the risk of ambiguity: normalizing gradients handles the color ambiguity and spatial binning of gradients texture ambiguity.  <ref type="table" target="#tab_1">Table 13</ref>. Multi-tasking. Simply combining two targets with two separate linear prediction heads results in a drop, suggesting conflict in the objectives. The default entry is marked as gray .</p><p>Multi-tasking. Finally, we investigate if combining different targets in a multi-task loss helps. Specifically, we combine pixel and HOG, two single-stage target features, by predicting each target with a separate linear layer. The two prediction losses are simply averaged with equal weighting. The results are summarized in <ref type="table" target="#tab_1">Table 13</ref>. We see that multi-tasking of pixel and HOG provides a small gain over the scratch baseline (82.3% vs. 81.8%), but the accuracy is lower than pixel or HOG only. Though further tuning the loss weighting might improve this result, it signals that the two objectives can not benefit each other. This is reasonable, as HOG targets are locally normalized while pixel colors are strongly influenced by local brightness changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Masked visual prediction was pioneered with stacked autoencoders <ref type="bibr" target="#b88">[89]</ref> and inpainting tasks <ref type="bibr" target="#b68">[69]</ref> using ConvNets. Since the introduction of ViT <ref type="bibr" target="#b26">[27]</ref>, masked prediction has re-attracted attention of the vision community, partially inspired by the success of BERT <ref type="bibr" target="#b24">[25]</ref> in NLP. BERT performs masked language modeling where some input tokens are masked at random and the task it to predict those. BERT pre-trained models scale well and generalize to a wide range of different downstream tasks.</p><p>For vision, different masked prediction objectives have been proposed. iGPT <ref type="bibr" target="#b13">[14]</ref> predicts the next pixels of a sequence. ViT <ref type="bibr" target="#b26">[27]</ref> predicts mean colors of masked patches. BEiT <ref type="bibr" target="#b1">[2]</ref> and VIMPAC <ref type="bibr" target="#b81">[82]</ref> encode masked patches with discrete variational autoencoder (dVAE) <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b86">87]</ref>. Masked visual prediction has also been explored in the field of vision-language learning (e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b78">79]</ref>). Compared to BEiT and VIMPAC, our method does not rely on dVAEs but directly regresses specific features of the input.</p><p>Self-supervised learning aims to learn from unlabeled visual data by a pre-text task that is constructed by image/patch operations (e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b97">98]</ref>) and spatiotemporal operations (e.g., <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b89">90]</ref>). Recently, contrastive learning <ref type="bibr" target="#b27">[28]</ref> capitalizes on augmentation invariance. The invariance is achieved by enforcing similarity over distorted views of one image while avoiding model collapse <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b92">93]</ref>. The line of contrastive methods learns linearly separable representations, commonly evaluated by linear probing. While in this work we focus on maximizing model performance for the end task with an end-to-end fine-tuning protocol <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b81">82]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present Masked Feature Prediction (MaskFeat), a simple visual pre-training approach that regresses features of masked regions. In particular, HOG, a hand-designed feature that was driving visual recognition before the deep learning era, works surprisingly well as the prediction target. MaskFeat is efficient, generalizes well, and scales to large models for both video and image domains.</p><p>Our results are especially groundbreaking for video understanding: There has been a large gap of over 5% accuracy between supervised pre-training on large-scale image datasets and training-from-scratch methods. MaskFeat has closed this gap by directly pre-training on unlabeled videos. Transfer learning performance is even more impressive where an MaskFeat model surpasses its IN-21K counterpart, which uses 60? more labels, by +4.7 mAP on action detection (AVA) and +1.1% top-1 on human-object interaction recognition (SSv2). These results suggest a clear benefit of masked prediction in the visually richer space-time domain to explore in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this Appendix, we provide further ablations for image ( ?A) classification. ?B contains the implementation details, and ?C provides more qualitative results. Pre-training schedule. We show different lengths of pretraining in <ref type="table" target="#tab_1">Table 14</ref>. Each result is fine-tuned from a fully trained model instead of an intermediate checkpoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ablations on Image Classification</head><p>For both base and large size models, improvements are observed with longer pre-training schedules. Interestingly, the large size model benefits more from longer pre-training with +1% gain from 300 epochs to 800 epochs, while the base-size model is only improved by +0.3%. This suggests that MaskFeat is a sufficiently difficult task such that (i) excessive long pre-training does not cause over-fitting of large models, and (ii) MaskFeat is sufficiently difficult for high capacity models. Training for 1600 epochs only gives another +0.1% improvement for ViT-B. Masking ratio. We vary the percentage of masked patches in <ref type="table" target="#tab_1">Table 15</ref> with block-wise masking following BEiT <ref type="bibr" target="#b1">[2]</ref>. We observe that masking out 20% 40% patches works well and that stronger masking degrades accuracy. MaskFeat requires enough visible patches to set up a meaningful objective. Note that 20% 40% masking is more than 15% masking used in masked language modeling (BERT <ref type="bibr" target="#b24">[25]</ref>), reflecting redundancy in raw visual signals.  Data augmentation. We study the effect of data augmentation during MaskFeat pre-training in <ref type="table" target="#tab_1">Table 16</ref>. All three entries in <ref type="table" target="#tab_1">Table 16a</ref> use random horizontal flipping. Our approach works best with only random resized crop (RRC), while color jittering has no influence on the result and stronger augmentation (RandAugment <ref type="bibr" target="#b20">[21]</ref>) degrades the performance slightly by 0.4%. This suggests that strong augmentations might lead to artificial patterns that in turn lead to a gap in pre-training and finetuning and MaskFeat works nearly augmentation-free. Conversely, contrastivebased methods are arguably dependent on "augmentation engineering" to provide prior knowledge (e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">41]</ref>), which could lead to conflicting clues <ref type="bibr" target="#b70">[71]</ref> and over-fitting to a specific combination of augmentations <ref type="bibr" target="#b93">[94]</ref>. We further study the effect of the RRC [min, max] scales in <ref type="table" target="#tab_1">Table 16b</ref>. Our approach is robust to this hyperparameter. MaskFeat works best with low strength of RRC, [0.5, 1.0], which covers a large fraction of each sample. Linear probing. Besides the fine-tuning protocol, we consider linear probing in <ref type="table" target="#tab_1">Table 17</ref> which is commonly used to evaluate contrastive methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b43">44]</ref>. We train randomly initialized linear classifiers right at transformer block outputs. Specifically, we consider the average pooled outputs of the 8 th , 16 th and 24 th (last) transformer blocks of a ViT-L pre-trained with 1600 epochs of MaskFeat on IN-1K. We observe that lower layers (e.g., the 8 th ) tend to have higher linear accuracy. This is different from contrastive based methods whose higher layers tend to obtain better linear accuracy <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b92">93]</ref>. All layers lag behind contrastive methods by a large margin. For instance, MoCo v3 <ref type="bibr" target="#b17">[18]</ref> has 77.6% at the last block of ViT-L. This suggests that contrastive-based and masked visual prediction methods have very different features. MaskFeat learns good visual knowledge revealed by fine-tuning protocol but not linearly separable features.</p><p>Our hypothesis here is that instance discrimination losses in contrastive learning create different embeddings (classes) for different images which can be largely reduced to classlevel information (a subset of classes) with a linear layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. ImageNet and Kinetics Experiments</head><p>Architecture. For ImageNet experiments, we use the standard ViT architecture <ref type="bibr" target="#b26">[27]</ref> in base and large sizes. We use a single linear layer to transform the output of the last block to form the target predictions. We do not use relative positional bias or layer scaling.</p><p>For Kinetics experiments, we use the improved MViT architecture <ref type="bibr" target="#b55">[56]</ref> and we term the original architecture in <ref type="bibr" target="#b30">[31]</ref> as MViTv1. There are two main modifications.  First, instead of using absolute positional embeddings as in MViTv1, relative positional embeddings <ref type="bibr" target="#b76">[77]</ref> are incorporated, which are decomposed in height, width, and temporal axes. Second, a new residual pooling connection is introduced inside the attention blocks. Specifically, the pooled query tensor is added to the output sequence of self-attention. These two modifications improve the training-from-scratch and supervised-pre-trained baselines. We do not use channel dimension expansion within attention blocks <ref type="bibr" target="#b55">[56]</ref> but at MLP outputs <ref type="bibr" target="#b30">[31]</ref> which has similar accuracy. Our approach which focuses on pre-training techniques is orthogonal to these architectural modifications and provides further gains over the improved baselines. Unlike ViT models sharing the spatial size of 14 2 for all blocks, the MViT architecture is multi-scale and has four scale stages. Stage 1 output is of spatial size 56 2 and stage 4 output is of spatial size 7 2 . To share hyper-parameters with ViT models which are of spatial size 14 2 , we remove MViTs' query pooling before the last MViT stage for MaskFeat pre-training only, resulting in a 14 2 final output size, the same as ViT models. This modification introduces little extra computation as stage 4 is small and has only two Transformer blocks. For the fine-tuning stage, the MViT models are unchanged, with 7 2 output to fairly compare with the MViT baselines. Relative positional embeddings are linearly interpolated when the shape is not matched.</p><p>When sampling masked tokens for MViT models on the pre-training stage, we first sample a map of the final output size, 14 2 . This masking map is then nearest-neighbor resized to the stage 1 size or input size, 56 2 . In this way the set of input tokens corresponding to the same output token are masked out together, avoiding trivial predictions. Pre-training. <ref type="table" target="#tab_1">Table 18a</ref> summarizes the pre-training configurations. Most of the configurations are shared by Im-ageNet and Kinetics, without specific tuning. This shows that MaskFeat is general across tasks. The gradient clipping value is set after monitoring training loss over short runs. It is 0.02 for HOG targets and 0.3 for pixel color prediction and deep feature targets. Fine-tuning. <ref type="table" target="#tab_1">Table 18b</ref> summarizes the fine-tuning configurations. Most of the configurations are shared across models, except that deeper models use larger layer-wise learning rate decay and larger drop path rates.</p><p>For extra-large, long-term video models with 312 and 352 spatial resolutions as well as 32?3 and 40?3 temporal durations, we initialize from their 224 resolution, 16?4 duration counterparts, disable mixup, and fine-tune for 30 epochs with a learning rate of 1.6e-5 at batch size 128, a weight decay of 1e-8, a drop path <ref type="bibr" target="#b53">[54]</ref> rate of 0.75 and a drop out rate of 0.5 for the final linear projection. Other parameters are shared with <ref type="table" target="#tab_1">Table 18b</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. AVA Experiments</head><p>The AVA action detection dataset <ref type="bibr" target="#b41">[42]</ref> assesses the spatiotemporal localization of human actions in videos. It has 211K training and 57K validation video segments. We evaluate methods on AVA v2.2 and use mean Average Precision (mAP) on 60 classes as is standard in prior work <ref type="bibr" target="#b32">[33]</ref>.</p><p>We use MViT-L?312, 40?3 as the backbone and follow the same detection architecture in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b55">56]</ref> that adapts Faster R-CNN <ref type="bibr" target="#b73">[74]</ref> for video action detection. Specifically, we extract region-of-interest (RoI) features <ref type="bibr" target="#b36">[37]</ref> by framewise RoIAlign <ref type="bibr" target="#b44">[45]</ref> on the spatiotemporal feature maps from the last MViT layer. The RoI features are then maxpooled and fed to a per-class sigmoid classifier for action prediction. The training recipe is identical to <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref> and summarized next. The region proposals are identical to the ones used in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b55">56]</ref>. We use proposals that have overlaps with ground-truth boxes by IoU &gt; 0.9 for training. The models are trained with synchronized SGD training with a batch size of 64. The base learning rate is 0.6 per 64 batch size with cosine decay <ref type="bibr" target="#b59">[60]</ref>. We train for 30 epochs with linear warm-up <ref type="bibr" target="#b38">[39]</ref> for the first five epochs and use a weight decay of 1e-8, a drop path of 0.4 and a head dropout of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. SSv2 Experiments</head><p>The SSv2 dataset <ref type="bibr" target="#b39">[40]</ref> contains 169K training, and 25K validation videos with 174 human-object interaction classes. We fine-tune the pre-trained MViT-L?312, 40?3 Kinetics models and take the same recipe as in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b55">56]</ref>. Specifically, we train for 40 epochs with a batch size of 128. The base learning rate is 0.02 per 128 batch size with cosine decay <ref type="bibr" target="#b59">[60]</ref>. We adopt synchronized SGD and use weight decay of 1e-4 and drop path rate of 0.75. The training augmentation is the same as Kinetics in <ref type="table" target="#tab_1">Table 18b</ref>, except we disable random flipping in training. We use the segmentbased input frame sampling <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b56">57]</ref> (split each video into segments, and sample one frame from each segment to form a clip). During inference, we take a single temporal clip and three spatial crops over a single video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Experiments</head><p>We provide more qualitative results of image HOG predictions in <ref type="figure" target="#fig_2">Fig. 4</ref> using ImageNet-1K validation images and for video HOG predictions in <ref type="figure" target="#fig_3">Fig. 5</ref> using Kinetics-400 validation videos.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Example HOG predictions on unseen validation input. Our model is learned by predicting features (middle) given masked inputs (left). Original images (right) are not used for prediction. More qualitative examples for video and image are in Figs. 4 and 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>For all targets, ViT-B is pre-trained with MaskFeat for 300 epochs on IN-1K. We report 100-epoch fine-tuning accuracy on IN-1K. For two-stage targets, we report the teacher architecture, number of parameters (M), and effective epoch ? on IN-1K. The default entry is marked in gray . The plot on the left visualizes the acc/epoch trade-off of the table. ? Different teachers use different training strategies. dVAE is pre-trained on an external 250M dataset, while self-supervised methods require multi-view training. To measure the cost in a unified way, we normalize the number of epochs by the cost of one epoch on IN-1K training set with one 224 2 view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>More visualizations of HOG predictions. The images are from IN-1K validation set. For each column, we show masked input (left), HOG predictions (middle) and original images (right). Original images are not used for prediction. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>More visualizations of HOG predictions (video). The video clips are from K400 validation set. For each column, we show masked input (left), HOG predictions (middle) and original video frames (right), and we show eight frames from top to bottom. Original video clips are not used for prediction. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparing target features for MaskFeat (video). All variants are pre-trained with MaskFeat for 300 epochs on MViT-S, 16?4. We report fine-tuning accuracy on K400. Default is gray .</figDesc><table><row><cell>feature type</cell><cell cols="2">one-stage variant</cell><cell>top-1</cell></row><row><cell>scratch</cell><cell>-</cell><cell>MViT-S [56]</cell><cell>81.1</cell></row><row><cell>pixel</cell><cell></cell><cell>RGB</cell><cell>80.7</cell></row><row><cell>image descriptor</cell><cell></cell><cell>HOG [22]</cell><cell>82.2</cell></row><row><cell>dVAE</cell><cell></cell><cell>DALL-E [73]</cell><cell>81.7</cell></row><row><cell>unsupervised feature</cell><cell></cell><cell cols="2">DINO [9], ViT-B 82.5</cell></row><row><cell>supervised feature</cell><cell></cell><cell>MViT-B [31]</cell><cell>81.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparing target features for MaskFeat (image).</figDesc><table><row><cell>IN1K top-1 accuracy (%)</cell><cell>79 80 81 82 83 84</cell><cell cols="3">ViTB RN50 HOG MoCo v3 MoCo v2 dVAE pixel scratch DINO pseudo label pixel HOG dVAE token unsup. feat. sup. feat. pseudo label</cell><cell>feature type scratch pixel colors image descriptor dVAE token unsupervised feature unsupervised feature unsupervised feature supervised feature</cell><cell>one-stage variant -DeiT [84] RGB HOG [22] DALL-E [73] MoCo v2 [16] MoCo v3 [18] DINO [9] pytorch [67]</cell><cell>arch. ---dVAE ResNet50 ViT-B ViT-B ResNet50</cell><cell>param. epoch  ? top-1 --81.8 --82.5 --83.6 54 1199 82.8 23 800 83.6 85 600 83.9 85 1535 84.0 23 90 82.6</cell></row><row><cell></cell><cell></cell><cell>0</cell><cell>500 # teacher epoch 1000</cell><cell>1500</cell><cell>supervised feature pseudo-label</cell><cell cols="2">DeiT [84] Token Labeling [50] NFNet-F6 ViT-B</cell><cell>85 438</cell><cell>300 360</cell><cell>81.9 78.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 compares</head><label>3</label><figDesc>MaskFeat with prior work on K400 dataset. From top to bottom, it has three sections.The first section presents prior work using CNNs, which commonly do not use any pre-training. The second section presents representative Transformer-based methods, most of which are heavily dependent on supervised pre-training on large-scale image datasets.The third section provides direct comparisons on MViT models. Note that these models are strong baselines and are state-of-the-art for training-from-scratch on their own. Still, 300 epochs of MaskFeat pre-training improve the scratch MViT-S, 16?4<ref type="bibr" target="#b55">[56]</ref> with 81.1% top-1 accuracy by +1.1%. Here, the suffix 16?4 represents that the model takes 16 frames with a temporal stride of 4 as input during training. Next, we explore larger models for which supervised IN-21K pre-training is popular. Pre-trained with MaskFeat for 800 epochs on K400, the large model MViT-L, 16?4 MaskFeat, K400 86.3 97.1 2063?3?5 218 MViT-L?312, 40?3 [56] MaskFeat, K400 86.4 97.1 2828?3?4 218 MViT-L?352, 40?3 [56] MaskFeat, K400 86.7 97.3 3790?3?4 218 MViT-L?352, 40?3 [56] MaskFeat, K600 87.0 97.4 3790?3?4 218</figDesc><table><row><cell>model</cell><cell>pre-train</cell><cell cols="3">top-1 top-5 FLOPs?views Param</cell></row><row><cell>Two-Stream I3D [12]</cell><cell>-</cell><cell>71.6 90.0</cell><cell>216 ? NA</cell><cell>25</cell></row><row><cell>ip-CSN-152 [85]</cell><cell>-</cell><cell cols="2">77.8 92.8 109?3?10</cell><cell>33</cell></row><row><cell>SlowFast 16?8 +NL [33]</cell><cell>-</cell><cell cols="2">79.8 93.9 234?3?10</cell><cell>60</cell></row><row><cell>X3D-XL [32]</cell><cell>-</cell><cell>79.1 93.9</cell><cell>48?3?10</cell><cell>11</cell></row><row><cell>MoViNet-A6 [53]</cell><cell>-</cell><cell>81.5 95.3</cell><cell>386?1?1</cell><cell>31</cell></row><row><cell>MViTv1-B, 64?3 [31]</cell><cell>-</cell><cell>81.2 95.1</cell><cell>455?3?3</cell><cell>37</cell></row><row><cell>Swin-B, 32?2 [59]</cell><cell>Sup., IN-21K</cell><cell>82.7 95.5</cell><cell>282?3?4</cell><cell>88</cell></row><row><cell cols="2">ViT-B-TimeSformer [4] Sup., IN-21K</cell><cell cols="3">80.7 94.7 2380?3?1 121</cell></row><row><cell>Swin-L, 32?2 [59]</cell><cell>Sup., IN-21K</cell><cell>83.1 95.9</cell><cell cols="2">604?3?4 197</cell></row><row><cell>ViViT-L [1]</cell><cell cols="4">Sup., JFT-300M 83.5 94.3 3980?3?1 308</cell></row><row><cell cols="2">Swin-L?384, 32?2 [59] Sup., IN-21K</cell><cell cols="3">84.9 96.7 2107?5?10 200</cell></row><row><cell>ViViT-H [1]</cell><cell cols="4">Sup., JFT-300M 84.9 95.8 3981?3?4 654</cell></row><row><cell>TokenLearner [75]</cell><cell cols="4">Sup., JFT-300M 85.4 N/A 4076?3?4 450</cell></row><row><cell>Florence?384 [95]</cell><cell cols="2">Text, FLD-900M 86.5 97.3</cell><cell cols="2">N/A?3?4 647</cell></row><row><cell>SwinV2-G?384 [58]</cell><cell>MIM + Sup. IN-21K+Ext-70M</cell><cell>86.8 N/A</cell><cell cols="2">N/A?5?4 3000</cell></row><row><cell>MViT-S, 16?4 [56]</cell><cell>-</cell><cell>81.1 94.9</cell><cell>71?1?10</cell><cell>36</cell></row><row><cell>MViT-S, 16?4 [56]</cell><cell>Sup., IN-21K</cell><cell>82.6 95.3</cell><cell>71?1?10</cell><cell>36</cell></row><row><cell>MViT-S, 16?4 [56]</cell><cell cols="2">MaskFeat, K400 82.2 95.1</cell><cell>71?1?10</cell><cell>36</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell>-</cell><cell cols="3">80.5 94.1 377?1?10 218</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell>Sup., IN-21K</cell><cell cols="3">83.5 95.9 377?1?10 218</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell cols="4">MaskFeat, K400 84.3 96.3 377?1?10 218</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell cols="4">MaskFeat, K600 85.1 96.6 377?1?10 218</cell></row><row><cell>MViT-L?312, 32?3 [56]</cell><cell>-</cell><cell cols="3">82.2 94.7 2063?3?5 218</cell></row><row><cell cols="2">MViT-L?312, 32?3 [56] Sup., IN-21K</cell><cell cols="3">85.3 96.6 2063?3?5 218</cell></row><row><cell>MViT-L?312, 32?3 [56]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>3% top-1 MViT-L, 16?4 MaskFeat model for 30 epochs to larger spatial sizes of 312 2 and 352 2 , as well as longer temporal durations of 32 and 40 frames with a temporal stride of three. The resulting extra large model MViT-L?352, 40?3, without using any external data, achieves a top accuracy of 86.7%. Previously, Transformer-based video models heavily rely on supervised pre-training on large image datasets to reach high accuracy. Comparison with previous work on K600 &amp; K700. For example, 84.9% top-1 Swin-L?384 [59] with IN-21K and 84.9% ViViT-H [1] with JFT-300M [80]. MaskFeat opens the door for directly pre-training on unlabeled videos which shows enormous benefits for video understanding, as we can boost the previous best accuracy without external data on K400 (81.5% MoViNet-A6 [53]) by +5.2%.</figDesc><table><row><cell>model</cell><cell>pre-train</cell><cell cols="3">top-1 top-5 FLOPs?views Param</cell></row><row><cell>SlowFast 16?8 +NL [33]</cell><cell>-</cell><cell cols="2">81.8 95.1 234?3?10</cell><cell>60</cell></row><row><cell>X3D-XL [32]</cell><cell>-</cell><cell>81.9 95.5</cell><cell>48?3?10</cell><cell>11</cell></row><row><cell>MoViNet-A6 [53]</cell><cell>-</cell><cell>84.8 96.5</cell><cell>386?1?1</cell><cell>31</cell></row><row><cell>MViTv1-B-24, 32?3 [31]</cell><cell>-</cell><cell>84.1 96.5</cell><cell>236?1?5</cell><cell>53</cell></row><row><cell>Swin-B, 16?2 [59]</cell><cell>Sup., IN-21K</cell><cell>84.0 96.5</cell><cell>282?3?4</cell><cell>88</cell></row><row><cell cols="2">Swin-L?384, 32?2 [59] Sup., IN-21K</cell><cell cols="3">86.1 97.3 2107?5?10 200</cell></row><row><cell>ViViT-H [1]</cell><cell cols="4">Sup., JFT-300M 85.8 96.5 3981?3?4 654</cell></row><row><cell>Florence?384 [95]</cell><cell cols="2">Text, FLD-900M 87.8 97.8</cell><cell cols="2">N/A?3?4 647</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell>Sup., IN-21K</cell><cell cols="3">85.8 97.1 377?1?10 218</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell cols="4">MaskFeat, K600 86.4 97.4 377?1?10 218</cell></row><row><cell cols="2">MViT-L?312, 40?3 [56] Sup., IN-21K</cell><cell cols="3">87.5 97.8 2828?3?4 218</cell></row><row><cell cols="5">MViT-L?312, 40?3 [56] MaskFeat, K600 88.3 98.0 2828?3?4 218</cell></row><row><cell></cell><cell cols="2">(a) Kinetics-600</cell><cell></cell><cell></cell></row><row><cell>model</cell><cell>pre-train</cell><cell cols="3">top-1 top-5 FLOPs?views Param</cell></row><row><cell>SlowFast 16?8 +NL [33]</cell><cell>-</cell><cell cols="2">71.0 89.6 234?3?10</cell><cell>60</cell></row><row><cell>MoViNet-A6 [53]</cell><cell>-</cell><cell>72.3 N/A</cell><cell>386?1?1</cell><cell>31</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell>Sup., IN-21K</cell><cell cols="3">76.7 93.4 377?1?10 218</cell></row><row><cell>MViT-L, 16?4 [56]</cell><cell cols="4">MaskFeat, K700 77.5 93.8 377?1?10 218</cell></row><row><cell cols="2">MViT-L?312, 40?3 [56] Sup., IN-21K</cell><cell cols="3">79.4 94.9 2828?3?4 218</cell></row><row><cell cols="5">MViT-L?312, 40?3 [56] MaskFeat, K700 80.4 95.7 2828?3?4 218</cell></row><row><cell></cell><cell cols="2">(b) Kinetics-700</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Masking strategy. Varying the strategy of masking in spatiotemporal data. The default entry is highlighted in gray .</figDesc><table><row><cell>masking</cell><cell>frame</cell><cell>tube</cell><cell>cube</cell></row><row><cell cols="2">top-1 81.0 (-1.2)</cell><cell>81.9 (-0.3)</cell><cell>82.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Target design. Predicting center patch HOG or all HOG in a cube gives similar results. Default in gray .</figDesc><table><row><cell>type</cell><cell>center patch</cell><cell>cube</cell></row><row><cell>top-1</cell><cell>82.2</cell><cell>82.0 (-0.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 .</head><label>10</label><figDesc>Pre-training schedule. Large model benefits more from longer pre-training schedule.</figDesc><table><row><cell>epoch</cell><cell>param. (M)</cell><cell>300</cell><cell>800</cell></row><row><cell>MViT-S, 16?4</cell><cell>36</cell><cell>82.2</cell><cell>82.0 (-0.2)</cell></row><row><cell>MViT-L, 16?4</cell><cell>218</cell><cell>83.1</cell><cell>84.3 (+1.2)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 .</head><label>11</label><figDesc>Comparison with previous work on IN-1K. All entries are pre-trained on IN-1K train split, except supervised384 using IN-21K. MoCo v3 and DINO use momentum encoder. BEiT uses 250M DALL-E data to pre-train dVAE. All entries are trained and evaluated at image size 224 2 except supervised384 at 384 2 .</figDesc><table><row><cell>1.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 ,</head><label>2</label><figDesc>MaskFeat w/ DINO, for 1600 epochs and it reaches 84.2%; however, this uses a separate ViT-B model that is trained with another 1535 effective epochs using DINO. MaskFeat w/ HOG can reach 84.0% without extra model.</figDesc><table><row><cell cols="2">norm. none</cell><cell>1</cell><cell>2</cell><cell cols="2">channel gray rgb opp.</cell></row><row><cell cols="4">top-1 82.2 82.8 83.6</cell><cell cols="2">top-1 83.2 83.6 83.5</cell></row><row><cell cols="4">(a) Contrast normalization.</cell><cell>(b) Color channel.</cell></row><row><cell>#bins</cell><cell>6</cell><cell>9</cell><cell>12</cell><cell cols="2">cell size 4?4 8?8 16?16</cell></row><row><cell cols="4">top-1 83.4 83.6 83.5</cell><cell>top-1 83.2 83.6</cell><cell>83.2</cell></row><row><cell cols="3">(c) Orientation bins.</cell><cell></cell><cell>(d) Spatial cell size.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 .</head><label>12</label><figDesc>HOG implementation.</figDesc><table><row><cell>(a) Local contrast normaliza-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 14 .</head><label>14</label><figDesc>Pre-training schedule. Gains with longer schedules are observed. The large model benefits more from longer schedules.</figDesc><table><row><cell>epoch</cell><cell>300</cell><cell>800</cell><cell>1600</cell></row><row><cell>ViT-B</cell><cell>83.6</cell><cell>83.9 (+0.3)</cell><cell>84.0 (+0.4)</cell></row><row><cell>ViT-L</cell><cell>84.4</cell><cell>85.4 (+1.0)</cell><cell>85.7 (+1.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 15 .</head><label>15</label><figDesc>Masking ratio (image). Varying the percentage of masked patches. A smaller percentage of masking is preferred.</figDesc><table><row><cell>ratio</cell><cell>20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell></row><row><cell>top-1</cell><cell>83.5 (-0.1)</cell><cell>83.6</cell><cell>83.1 (-0.5)</cell><cell>82.5 (-1.1)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 16 .</head><label>16</label><figDesc>Data augmentation in MaskFeat. Defaults are gray .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 17 .</head><label>17</label><figDesc>Linear probing. We perform linear probing after the 8 th , 16 th , 24 th (last) block of MaskFeat pre-trained ViT-L. Lower layers obtain better linear accuracy.</figDesc><table><row><cell>block</cell><cell>8 th</cell><cell>16 th</cell><cell>24 th</cell></row><row><cell>top-1</cell><cell>67.7</cell><cell>66.0</cell><cell>55.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 18 .</head><label>18</label><figDesc>Configurations for ImageNet and Kinetics. ? We use the linear lr scaling rule<ref type="bibr" target="#b38">[39]</ref>: lr = base lr?batch size / 256.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This project was partially influenced by initial signals of the MAE project <ref type="bibr" target="#b42">[43]</ref>. We thank Kaiming He and Huiyu Wang for feedback on the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ViViT: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">BEiT: BERT pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno>ICML, 2021. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 3, 4</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A short note about kinetics-600. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2020</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 4</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">ELECTRA: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno>ICLR, 2020. 11</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RandAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navneet</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast,2020.2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">PyTorchVideo: A deep learning library for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tullie</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Murrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Vasudev Alwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhila</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<ptr target="https://pytorchvideo.org/.2" />
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">X3D: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spatiotemporally coherent metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeruIPS, 2020</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatio-temporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Object-region video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben-Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06915</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">All tokens matter: Token labeling for training better vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ViLT: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">MoviNets: Mobile video networks for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kondratyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">FractalNet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Improved multiscale vision transformers for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01526</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TSM: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09883</idno>
		<title level="m">Swin transformer v2: Scaling up capacity and resolution</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Video swin transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Object recognition from local scaleinvariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">ViL-BERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Actor-context-actor relation network for spatio-temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<editor>Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Keeping your eye on the ball: Trajectory attention in video transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra Florian Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Spatiotemporal contrastive video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Tokenlearner: What can 8 learned tokens do for images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11297</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155</idno>
		<title level="m">Selfattention with relative position representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">VL-BERT: Pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno>ICLR, 2020. 9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">VIM-PAC: Video pre-training via masked token prediction and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11250</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Video classification with channel-separated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Evaluating color descriptors for object and scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koen</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Iterative reorganization with weak spatial constraints: Solving arbitrary jigsaw puzzles for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Towards longform video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Chao-Yuan Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Alexei A Efros, and Trevor Darrell. What should not be contrastive in contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<idno>ICLR, 2020. 10</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuedong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11432</idno>
		<title level="m">A new foundation model for computer vision</title>
		<meeting><address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
		<title level="m" type="main">What makes instance discrimination good for transfer learning? In ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Rynson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<idno>2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
