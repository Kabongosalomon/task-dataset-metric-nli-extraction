<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MANNER: MULTI-VIEW ATTENTION NETWORK FOR NOISE ERASURE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Byung</roleName><forename type="first">Hyun</forename><forename type="middle">Joon</forename><surname>Park</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ha</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wooseok</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Sob</forename><surname>Kim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Won</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Industrial and Management Engineering</orgName>
								<orgName type="institution">Korea University</orgName>
								<address>
									<settlement>Seoul</settlement>
									<country key="KR">Republic of Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MANNER: MULTI-VIEW ATTENTION NETWORK FOR NOISE ERASURE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-multi-view attention</term>
					<term>speech enhance- ment</term>
					<term>time domain</term>
					<term>u-net</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the field of speech enhancement, time domain methods have difficulties in achieving both high performance and efficiency. Recently, dual-path models have been adopted to represent long sequential features, but they still have limited representations and poor memory efficiency. In this study, we propose Multi-view Attention Network for Noise ERasure (MANNER) consisting of a convolutional encoder-decoder with a multi-view attention block, applied to the time-domain signals. MANNER efficiently extracts three different representations from noisy speech and estimates high-quality clean speech. We evaluated MANNER on the VoiceBank-DEMAND dataset in terms of five objective speech quality metrics. Experimental results show that MAN-NER achieves state-of-the-art performance while efficiently processing noisy speech.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech enhancement (SE), which is the task of improving the quality and intelligibility of a noisy speech signal, has been widely used in many applications, such as automatic speech recognition and hearing aids. Recently, researchers have studied deep neural network (DNN) models for SE, as DNN models have shown powerful noise reduction ability in complex noise environments compared to statistical methods.</p><p>DNN models in SE are divided into time and timefrequency (T-F) domain methods. T-F domain methods <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref> estimate clean speech from the spectrogram created by applying the short-time Fourier transform (STFT) to a raw signal. Although the spectrogram contains the time and frequency of the signal, some limitations have been pointed out to use it <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. T-F domain methods need to address both magnitude and phase information, thus increasing the model complexity. In addition, it is challenging to handle complex values for estimating complex-valued masks.</p><p>Recently, researchers have studied time domain methods <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, which directly estimate clean speech from the raw This research was supported by Brain Korea 21 FOUR. This research was also supported by Korea University Grant (K2107521) and a Korea Tech-noComplex Foundation Grant (R2112651). signal because the raw signal implicitly contains all of the signal's information. Among them, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> adopted a U-net <ref type="bibr" target="#b14">[15]</ref> based architecture, which is utilized for efficient feature compression. However, it is not effective for representing the long sequence of the signal owing to its limited receptive field.</p><p>In contrast, dual-path models were adopted by <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref> to represent the long sequence of the signal in speech separation. They considered the long sequential features by dividing the signal into small chunks and repeatedly processing local and global information. In SE, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref> also applied dual-path models, but they are not efficient in terms of memory usage because they maintain the long signal length during training. In addition, the repeated feature extraction by dual-path processing on a small channel size results in limited representation and lower performance.</p><p>In this study, we propose an efficient speech enhancement model, Multi-view Attention Network for Noise ERasure (MANNER), in the time domain. MANNER, based on U-net, compresses the enriched channel representations with convolution blocks. The multi-view attention block enables the estimation of clean speech by emphasizing the channel and long sequential features from each view. A comparison of results on the VoiceBank-Demand dataset suggests that MANNER achieves state-of-the-art performance with high inference speed and efficient memory usage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MANNER</head><p>In this section, we introduce MANNER in detail. MANNER is based on an encoder-decoder consisting of a convolution layer, a convolution block, and an attention block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Encoder and Decoder</head><p>Before the encoder layer, we use a 1-D convolution layer, followed by batch normalization and ReLU activation, on the noisy input x ? R 1?T , where T is the signal length. The 1-D convolution layer expands the channel size according to x ? R N ?T , where N denotes the channel size.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the encoder and decoder consist of L layers containing Down and Up Conv layers, a Residual Conformer (ResCon) block, and a Multi-view Attention (MA) block. We use the linear transformation of the encoder output To appear in Proc. ICASSP 2022 ? IEEE 2022 to pass the decoder layer. Each encoded output is connected with each decoding input by element-wise summation.</p><p>Up &amp; Down Conv. We use Down and Up Conv in the encoder and decoder, respectively. Down Conv, which reduces the signal length, consists of a convolution layer followed by batch normalization and ReLU activation. In contrast, Up Conv, which restores the signal to its original length, consists of a transposed convolution layer instead of a convolution layer. Up and Down Conv adjust the signal length with a kernel size of K and a stride of S. We denote the signal length of each layer as T l , where l = 1, 2, ..., L. Mask Gate. We obtain the mask m ? R N ?T by applying a mask gate to the decoder output. The mask is estimated by the multiplication between the sigmoid and hyperbolic activation on the output, followed by ReLU activation. A convolution layer is always used before each activation function. We obtain the denoised x ? R N ?T through element-wise multiplication between the mask and the output of the first convolution layer, x ? R N ?T . Finally, the enhanced speech is obtained by applying the convolution layer, which reduces the channel size from N to 1, to the denoised x .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Residual Conformer block</head><p>Inspired by the efficient convolution block of Conformer <ref type="bibr" target="#b18">[19]</ref>, we design a ResCon block to obtain enriched channel representation by expanding the channel size in deep layers. We modify the normalization and add a residual connection using a convolution layer. In addition, we redesign the method used to adjust the channel size. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, pointwise and depthwise convolution layers are followed by normalization and the activation function. G 1 adjusts the final channel size in the block, and we set G 1 = 2 and G 1 = 1/2 for the encoder and decoder layers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-view Attention block</head><p>We design a MA block consisting of channel, global, and local attention to fully represent the signal information. Channel attention emphasizes representations from compressed channels. Global and local attention based on dual paths efficiently reflect long sequential features. In the MA block, the input passes through three paths consisting of a convolution layer that adjusts the channel size from N to N/3. For global and local attention paths, we adopt chunking with an overlap ratio of 50% to split x ? R N/3?T l into x ? R N/3?P ?C , where P and C denote the number of chunks and chunk size, respectively. By separating the global and local information, we can efficiently represent long sequential features. Channel Attention. We adopt the channel attention used in <ref type="bibr" target="#b19">[20]</ref> to emphasize channel-wise representations. To aggregate the signal information, we apply average and max pooling to x C ? R N/3?T l , where x C is the input of the channel attention path after a convolution layer. Each pooling output passes through shared linear layers. The channel attention weight ? C ? R N/3?1 is estimated as follows:</p><formula xml:id="formula_0">? C = ?(W 1 (W 0 (x avg C )) + W 1 (W 0 (x max C )))<label>(1)</label></formula><p>where each weight is W 0 ? R N/3?N/6 and W 1 ? R N/6?N/3 . The channel attention output is defined as</p><formula xml:id="formula_1">x C = x C ? ? C .</formula><p>Global Attention. We propose global attention based on the self-attention of Transformer <ref type="bibr" target="#b20">[21]</ref>. To extract global sequential information, global attention considers chunk-wise representations in the chunked input x G ? R N/3?P ?C . The global attention weight, ? G ? R N/3?P ?P , and the output of global attention, x G ? R N/3?P ?C , are obtained based on self-attention, where d k is the chunk size for scaling.</p><formula xml:id="formula_2">? G = sof tmax( QK ? d k ) x G = W (? G V ) (2) Q, K, and V ? R N/3?P ?C q,k,v</formula><p>are represented by linear transformation with each weight, W q,k,v ? R 1?C?C , and x G ? R N/3?P ?C . Finally, we apply a linear layer, W ? R 1?C?C , to ? G V to obtain the global attention output. Local Attention. Local attention represents the local sequential features in each chunk. We design local attention using convolution layers to reduce the model complexity compared to self-attention. By adopting a small chunk size and large kernel size, the convolution layer can sufficiently represent local sequential features. We use a depthwise convolution layer with a kernel size of C/2 ? 1 on the chunked input x L ? R P ?N/3?C . After the depthwise convolution layer, we estimate the local attention weight ? L ? R P ?1?C by concatenating the channel-wise average and max pooling as follows:</p><formula xml:id="formula_3">? L = ?(F ([x avg L ; x max L ]))<label>(3)</label></formula><p>where F is the convolution layer reducing the channel size from 2 to 1. Finally, we represent the output as x L = x L ?? L . For global and local outputs, we merge the chunked data. After the three-path attention, we concatenate each output and pass it through a convolution layer. We apply the mask gate process as the residual gate to adjust the amount of information flow, followed by a residual connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Loss function</head><p>We combine L1 loss (time) and multi-resolution STFT loss (time-frequency) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> to optimize the model. We adopt the STFT loss of <ref type="bibr" target="#b8">[9]</ref>, which is the sum of the spectral convergence and magnitude loss. To obtain the spectral convergence and magnitude loss, the F robenius and L 1 norms are applied, respectively. The loss of the clean and estimated speech is the sum of the L 1 and multi-resolution STFT loss as follows:</p><formula xml:id="formula_4">loss ST F T (y,?) = |ST F T (y)| ? |ST F T (?)| F |ST F T (y)| F + 1 T log(|ST F T (y)|) ? log(|ST F T (?)|) 1 loss(y,?) = 1 T y ?? 1 + 1 R R r=1 loss r ST F T (y,?)<label>(4)</label></formula><p>where y and? are the clean and estimated speech. The loss r ST F T indicates the STFT loss of different resolutions with combinations of hyperparameter (i.e., window lengths, hop sizes, FFT bins), as in <ref type="bibr" target="#b8">[9]</ref>.</p><p>We also apply the weighted loss <ref type="bibr" target="#b0">[1]</ref> to consider both clean and noise loss. Given that n is the noise, the input signal is defined as x = y + n. The total loss of the proposed model is as follows, wheren = x ??.</p><p>loss total (x, y,?) = ?loss(y,?) + (1 ? ?)loss(n,n) (5)</p><p>The weight ? is defined as ? = y 2 2 /( y 2 2 + n 2 2 ), adjusting the ratio between the clean and noise speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We evaluate MANNER on the VoiceBank-DEMAND dataset <ref type="bibr" target="#b22">[23]</ref> by mixing the VoiceBank Corpus and DEMAND dataset. The train set consists of 11,572 utterances (14 male and 14 female) mixed with noise data with four signal-to-noise ratios (SNRs) (15, 10, 5, and 0 dB). The test set consists of 824 utterances (one male and one female) mixed with unseen noise data with four SNRs (17.5, 12.5, 7.5, and 2.5 dB). We use two speakers from the train set as the validation set. The data are downsampled from 48 kHz to 16 kHz for a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluation metrics</head><p>We adopt five objective measures to evaluate MANNER and the previous models. Perceptual Evaluation of Speech Quality (PESQ) <ref type="bibr" target="#b23">[24]</ref> with a score ranging from -0.5 to 4.5 is used to evaluate speech quality. Short-time objective intelligibility (STOI) <ref type="bibr" target="#b24">[25]</ref> with a score ranging from 0 to 100 is for speech intelligibility. We also consider three mean opinion score (MOS)-based measures whose scores ranging from 1 to 5 <ref type="bibr" target="#b25">[26]</ref>. CSIG is the MOS prediction of the signal distortion, CBAK is the MOS prediction of the noise intrusiveness, and COVL is the MOS prediction of the overall signal quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation details</head><p>For training, we segment the signal into 4 seconds with a 1second overlap and set a batch size of 4. We train MAN- NER for 300 epochs and maintain the best weights based on the validation score. Furthermore, we adopt the Adam optimizer and OneCycleLR scheduler to optimize the model. We set lr min = 10 ?5 and lr max = 10 ?2 for the OneCycleLR scheduler adjusting the learning rate during each epoch. During training, we vary the tempo of the signal within the range of 90% to 110% <ref type="bibr" target="#b26">[27]</ref>. For MANNER, we use K = 8, S = 4, N = 60, L = 4, and C = 64. To verify the performance and efficiency, we also include MANNER (small) in the comparison, containing MA block only in the L th layer and using the same parameters as MANNER. <ref type="figure">Fig. 4</ref>. Efficiency comparison performed on the same machine with RTX A6000 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Experimental results</head><p>We compared the proposed models with existing models, including time and time-frequency domain methods. As shown in <ref type="table" target="#tab_0">Table 1</ref>, MANNER achieves state-of-the-art performance in terms of five objective speech quality measures. Although MANNER (small) does not achieve the best performance, it still outperforms the previous methods.</p><p>To verify the efficiency of the proposed models, we compared them with the time domain methods, DEMUCS <ref type="bibr" target="#b8">[9]</ref> and TSTNN <ref type="bibr" target="#b5">[6]</ref>, in terms of inference speed and memory usage. We measured these quantities with the signal length set from 1 to 10 seconds. <ref type="figure">Fig. 4</ref> shows that MANNER has high inference speed and relatively low memory usage compared to the previous methods. In addition, MANNER (small) achieves not only higher performance than the previous methods, but also the highest efficiency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">The influence of attention block and loss</head><p>We conducted an ablation experiment to understand the influence of the proposed attention block and weighted loss on MANNER's performance. We examined the effects of each component of the proposed methods. <ref type="table" target="#tab_1">Table 2</ref> shows that each attention and weighted loss contributes to the improvement of performance. The result of Ver. 4 suggests the importance of considering long signal information, but considering all views of the signal is necessary to achieve higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this study, we proposed MANNER, which efficiently represents channel and long sequential features of the signal, designed for speech enhancement in the time domain. MAN-NER's results on the VoiceBank-DEMAND dataset highlight that MANNER achieves state-of-the-art performance compared to existing models. In addition, MANNER (small) is superior to previous time-domain methods in terms of performance and efficiency. Finally, the ablation experiment suggests that it is important to consider all representations of the signal and optimize both clean and noise loss.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The overall architecture of MANNER.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Residual Conformer block. G 0,1 indicates the channel growth rate of each pointwise convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Multi-view Attention block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison results on the VoiceBank-DEMAND dataset in terms of objective speech quality metrics.</figDesc><table><row><cell>Model</cell><cell>Domain</cell><cell>PESQ</cell><cell>STOI(%)</cell><cell>CSIG</cell><cell>CBAK</cell><cell>COVL</cell></row><row><cell>SEGAN [11]</cell><cell>T</cell><cell>2.16</cell><cell>-</cell><cell>3.48</cell><cell>2.94</cell><cell>2.80</cell></row><row><cell>Wave U-Net [10]</cell><cell>T</cell><cell>2.40</cell><cell>-</cell><cell>3.52</cell><cell>3.24</cell><cell>2.96</cell></row><row><cell>PHASEN [4]</cell><cell>T-F</cell><cell>2.99</cell><cell>-</cell><cell>4.21</cell><cell>3.55</cell><cell>3.62</cell></row><row><cell>SN-Net [5]</cell><cell>T-F</cell><cell>3.12</cell><cell>-</cell><cell>4.39</cell><cell>3.60</cell><cell>3.77</cell></row><row><cell>DEMUCS (large) [9]</cell><cell>T</cell><cell>3.07</cell><cell>95</cell><cell>4.31</cell><cell>3.4</cell><cell>3.63</cell></row><row><cell>MetricGAN+ [3]</cell><cell>T-F</cell><cell>3.15</cell><cell>-</cell><cell>4.14</cell><cell>3.16</cell><cell>3.64</cell></row><row><cell>TSTNN [6]</cell><cell>T</cell><cell>2.96</cell><cell>95</cell><cell>4.33</cell><cell>3.53</cell><cell>3.67</cell></row><row><cell>MANNER (small)</cell><cell>T</cell><cell>3.12</cell><cell>95</cell><cell>4.45</cell><cell>3.61</cell><cell>3.82</cell></row><row><cell>MANNER</cell><cell>T</cell><cell>3.21</cell><cell>95</cell><cell>4.53</cell><cell>3.65</cell><cell>3.91</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison results depending on attention types and weighted loss (wLoss).</figDesc><table><row><cell>Ver.</cell><cell>wLoss. Channel Att. Global Att. Local Att. PESQ</cell></row><row><cell>Base</cell><cell>3.00</cell></row><row><cell>1</cell><cell>3.04</cell></row><row><cell>2</cell><cell>3.12</cell></row><row><cell>3</cell><cell>3.16</cell></row><row><cell>4</cell><cell>3.18</cell></row><row><cell>MANNER</cell><cell>3.21</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Phase-aware speech enhancement with deep complex u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Choi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Metricgan: Generative adversarial networks based black-box metric scores optimization for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Metricgan+: An improved version of metricgan for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03538</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Phasen: A phaseand-harmonics-aware speech enhancement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI, 2020</title>
		<meeting>AAAI, 2020</meeting>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9458" to="9465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Interactive speech and noise modeling for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09408</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tstnn: Two-stage transformer based neural network for speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-P</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7098" to="7102" />
			<date type="published" when="2021" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A wavenet for speech denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rethage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5069" to="5073" />
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dual-path self-attention rnn for real-time speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Real time speech enhancement in the waveform domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Defossez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12847</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improved speech enhancement with the wave-u-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macartney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.11307</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Segan: Speech enhancement generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pascual</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bonafonte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Serra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09452</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected neural network with dilated convolutions for real-time speech enhancement in the time domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6629" to="6633" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense cnn with self-attention for time-domain speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1270" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wavecrn: An efficient convolutional recurrent neural network for end-to-end speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-A</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2149" to="2153" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICM</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain singlechannel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="46" to="50" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Dual-path transformer network: Direct context-aware modeling for end-toend monaural speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13975</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need in speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="21" to="25" />
			<date type="published" when="2021" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6199" to="6203" />
			<date type="published" when="2020" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Noisy speech database for training speech enhancement algorithms and tts models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Valentini-Botinhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Perceptual evaluation of speech quality (pesq): An objective method for end-to-end speech quality assessment of narrow-band telephone networks and speech codecs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-T</forename><surname>Recommendation</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Rec. ITU-T P. 862</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of timefrequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Evaluation of objective quality measures for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Loizou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="229" to="238" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Audio augmentation for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
