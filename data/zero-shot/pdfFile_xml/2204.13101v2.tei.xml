<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Supervised Learning of Object Parts for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Ziegler</surname></persName>
							<email>adrian.ziegler@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
							<email>y.m.asano@uva.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">QUVA Lab University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Supervised Learning of Object Parts for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Progress in self-supervised learning has brought strong image representation learning methods. Yet so far, it has mostly focused on image-level learning. In turn, tasks such as unsupervised image segmentation have not benefited from this trend as they require spatially-diverse representations. However, learning dense representations is challenging, as in the unsupervised context it is not clear how to guide the model to learn representations that correspond to various potential object categories. In this paper, we argue that self-supervised learning of object parts is a solution to this issue. Object parts are generalizable: they are a-priori independent of an object definition, but can be grouped to form objects a-posteriori. To this end, we leverage the recently proposed Vision Transformer's capability of attending to objects and combine it with a spatially dense clustering task for fine-tuning the spatial tokens. Our method surpasses the state-of-the-art on three semantic segmentation benchmarks by 3%-17%, showing that our representations are versatile under various object definitions. Finally, we extend this to fully unsupervised segmentation -which refrains completely from using label information even at test-time -and demonstrate that a simple method for automatically merging discovered object parts based on community detection yields substantial gains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Defining what makes an object an object is hard. In philosophy, Peirce defines an object as anything we can think and talk about <ref type="bibr" target="#b46">[48]</ref>. In computer vision, object definitions for semantic segmentation are more pragmatic and feature various notions of objectness as well as different levels of granularity. For instance, the COCO-Stuff benchmark distinguishes between stuff (objects without a clear shape) and things (objects with a "well-defined" shape) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">42]</ref> and features coarse and fine object categories. Others, like Code: https://github.com/MkuuWaUjinga/leopart <ref type="figure">Figure 1</ref>. ViTs and Resnets compared under foreground extraction and semantic segmentation. We use Jaccard distance as a measure for foreground extraction. Starting from a DINO initialization, our method, Leopart, closes the performance gap between self-supervised ViTs and their supervised counterparts as well as Resnets. Leopart (CBFE+CD) further improves a ViT's object extraction capabilities and sets new state-of-the-art for fully unsupervised semantic segmentation.</p><p>Cityscapes <ref type="bibr" target="#b11">[12]</ref>, choose a segmentation that is most informative for a specific application like autonomous driving and therefore also include sky and road as object classes.</p><p>This variedness of object definitions is challenging for self-supervised or unsupervised semantic segmentation as human annotations that carry the object definitions are, at most, used at test time. However, the ability to learn selfsupervised dense representations is desirable as this would allow scaling beyond object-centric images and allow effective learning on billions more generic images. Furthermore, unsupervised segmentation can be highly useful as a starting point for more efficient data labeling, as segmentation annotations are even more expensive than image labelling <ref type="bibr" target="#b40">[42]</ref>. To tackle the lack of a principled object definition during training, many methods resort to defining object priors such as saliency and contour detectors to induce a notion of objectness into their pretext tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b62">64]</ref>, effectively rendering such methods semi-supervised and potentially not generalizeable. In this paper, we instead stay in the fully unsupervised domain and explore a novel, yet simple alternative for training densely. We learn object parts (Leopart) through a dense image patch clustering pretext task. Object part learning promises a principled formulation for self-supervised dense representation learning as object parts can be composed to form objects as defined in each benchmark, after generic pretraining.</p><p>In this paper, we explore the use of a Vision Transformer (ViT) with our new loss and excavate its unique aptness for self-supervised segmentation. While vision transformers have shown great potential unifying architectures and scaling well with data into billions, they have mostly been shown to work for image-level tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref> or dense tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b42">44,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b58">60]</ref> but in a supervised manner. Our work aims to close this gap by self-supervisedly learning dense ViT models. We combine the recently discovered property of self-supervised ViTs to localise objects <ref type="bibr" target="#b5">[6]</ref> with our dense loss to train spatial tokens for unsupervised segmentation.</p><p>We validate our method from two different angles: First, we conduct a transferability study and show that our representations perform well on downstream semantic segmentation tasks. Second, we tackle the more challenging fully unsupervised setup proposed in <ref type="bibr" target="#b55">[57]</ref> based on directly clustering the pixel or patch embeddings. For that, two model characteristics are important: unsupervised foreground extraction and a semantically-structured embedding space, see <ref type="figure">Figure 1</ref>. To our surprise, even though self-supervised ViTs excel at extracting objects, they do not learn a spatial token embedding space that is discriminative for different object categories. On the other hand, ViTs trained under supervision achieve better semantic segmentation performance, but the attention heads perform poorly at localizing objects. In contrast, our method outperforms self-supervised ViTs and ResNets in fully unsupervised semantic segmentation as well as in learning transferable dense representations.</p><p>Thus, our contributions are as follows:</p><p>? We propose a dense clustering pretext task to learn semantically-rich spatial tokens, closing the gap between supervised ViTs and self-supervised ViTs.</p><p>? We show that our pretext task yields transferable representations that surpass the state-of-the-art on Pascal VOC, COCO-Thing and COCO-Stuff semantic segmentation at the same time by 17%-3%.</p><p>? We develop a novel cluster-based foreground extraction and overclustering technique based on community detection to tackle fully unsupervised semantic segmentation and surpass the state-of-the-art by &gt;6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work takes inspiration from standard image-level self-supervised pretraining while extending this to the domain of dense representation learning using Vision Transformers.</p><p>Image-level self-supervised learning. Self-supervised learning aims to learn powerful representations by replacing human annotation with proxy tasks derived from data alone. Current methods can be roughly categorized into instance-level and group-level objectives. Instance-level objectives include predicting augmentations applied to an image <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b34">36,</ref><ref type="bibr" target="#b43">45,</ref><ref type="bibr" target="#b45">47,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b61">63]</ref> or learning to discriminate between images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b59">61]</ref>, often done by the use of contrastive losses <ref type="bibr" target="#b26">[28]</ref>.</p><p>On the other hand, group-level objectives explicitly allow learning shared concepts between images by leveraging clustering losses <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b54">56]</ref>. <ref type="bibr" target="#b3">[4]</ref> proposes k-means clustering in feature space to produce pseudo labels for training a neural network. <ref type="bibr" target="#b0">[1]</ref> casts the problem of finding pseudo labels as an optimal transport problem unifying the clustering and representation learning objectives. This formulation was adapted to an online setting in SwAV <ref type="bibr" target="#b4">[5]</ref> together with a new multi-crop augmentation strategy, a random cropping method that distinguishes between global and local crops of an image. The IIC method <ref type="bibr" target="#b35">[37]</ref>, also conducts clustering, however using a mutual information objective. While it can also be used densely, it has been found to focus on lowerlevel features specific to each dataset <ref type="bibr" target="#b55">[57]</ref>. Another recent line of works completely refrains from group level clustering or instance-based discrimination by predicting targets from a slowly moving teacher network <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">27]</ref>.</p><p>Our work adapts this teacher-student setup and shows its benefits beyond image-level tasks. To this effect, we build on the clustering pretext task from <ref type="bibr" target="#b4">[5]</ref> and reformulate it such that it can be used on an image patch level with teacher-student setups. We also use the multi-crop augmentation strategy and provide an interpretation from the perspective of dense prediction tasks.</p><p>Dense self-supervised learning. Based on the observation that image-level learning does not imply expressive dense representations <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b47">49]</ref>, dedicated self-supervised dense representation learning has attracted a lot of attention <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b33">35,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b50">52,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b55">57,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b63">65]</ref>. DenseCL reformulates the contrastive objective used in MoCo <ref type="bibr" target="#b27">[29]</ref> to work on spatial features by establishing dense correspondences accross views and is currently the stateof-the-art in transfer learning for semantic segmentation on <ref type="figure">Figure 2</ref>. Leopart training pipeline. We start from a DINO initialization. We feed different crops to the student and teacher network to produce patch-level cluster predictions and optimal cluster assignments targets. This requires an alignment step of cluster targets and assignments. We further focus clustering on foreground patches by leveraging the ViT's attention map.</p><p>PVOC <ref type="bibr" target="#b39">[41]</ref>. Other methods resort to defining an unsupervised object prior such as region proposals <ref type="bibr" target="#b8">[9]</ref>, contour detectors <ref type="bibr" target="#b33">[35,</ref><ref type="bibr" target="#b63">65]</ref>, saliency <ref type="bibr" target="#b55">[57]</ref> or object masks <ref type="bibr" target="#b30">[32]</ref>. For instance, the current state-of-the-art for unsupervised semantic segmentation, MaskContrast <ref type="bibr" target="#b55">[57]</ref>, uses a pretrained saliency estimator to mine positive and negative pixels for contrastive learning.</p><p>Concurrent to our work, <ref type="bibr" target="#b39">[41]</ref> proposes an intra-image clustering step of pixel embeddings before applying a contrastive loss on the identified pixel groups to segment images. However, as they are reliant on combining the former with an image-level loss, it is not well-suited for more generic images with multiple objects where image-level and pixel-level semantics do not match. In contrast, our method uses a single clustering objective that is generalized for the dense setting, but that also works on object-centric images. Furthermore, by leveraging ViT's natural ability to direct its attention to objects, we do not require any external saliency generator like <ref type="bibr" target="#b55">[57]</ref>.</p><p>There are also works that have explored unsupervised object parts segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b32">34]</ref>, with the explicit goal to determine parts given object masks. However, our goal is different as we use part representations as an intermediary for semantic segmentation on classic, object-level settings.</p><p>Superficially similar to our work is also another concurrent work <ref type="bibr" target="#b52">[54]</ref>, which tackles object detection by using the similarity between DINO's frozen last layer self-attention patch keys as a metric for merging image patches to objects. In contrast, we use DINO's spatial tokens and propose to fine-tune them for semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our goal is to learn an embedding space that groups image patches containing the same part of an object. This is motivated by the hypothesis that object part representations are more general than object representations, as parts can be recombined in multiple ways to different objects. As an example, a wheel representation can be combined to a car representation but also a bus representation. Therefore, object part representations should transfer better across datasets. For that, we aim to design a pretext task that allows for intraimage category learning on an image-patch-level. Thus, a clustering pretext task is a natural choice. As shown in <ref type="figure">Figure</ref> 2, we retrieve patch-level optimal cluster assignments from a teacher network and predict them from the student network. The choice of a clustering pretext task is further supported by empirical evidence showing that clustering pretext outperforms contrastive pretext for dense prediction tasks <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b39">41]</ref>. Instead of pretraining models from scratch which requires substantial GPU budgets, we use our loss to fine-tune pretrained neural networks. Further, this circumvents known cluster stability issues and clusters capturing low-level image features when applied to a patch-level as reported in <ref type="bibr" target="#b54">[56]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fine-tuning loss for spatial tokens</head><p>Image Encoder. Given an image x ? R 3?H?W , we flatten the image into N = H P ? W P separate patches x i , i ? 1, . . . , N of size P ? P each. The vision encoder we use is a ViT <ref type="bibr" target="#b16">[17]</ref>, which maps the image patches x i to a vector of N spatial tokens f (</p><formula xml:id="formula_0">x) = [f (x 1 ), . . . f (x N )].</formula><p>Leopart fine-tuning loss. To train the ViT's spatial tokens, we first randomly crop the image V -times into v g global views and v l local views. When sampling the views we compute their pairwise intersection in bounding box format and store it in a matrix B. We denote the transformed version of the image as x tj , j ? {1, . . . , V }. Then, we forward the spatial tokens through a MLP projection head g with a L2-normalization bottleneck to get spatial features for each crop: g(f (x tj )) = Z tj ? R D?N . To create prediction targets, we next find an optimal soft cluster assignment Q tj of all spatial token's feature vector Z tj to K prototype vectors [c 1 , . . . , c K ] = C ? R D?K . For that, we follow the online optimization objective of SwAV <ref type="bibr" target="#b4">[5]</ref> that works on the whole image batch b. Q is optimized such that the similarity between all feature vectors in the batch and the prototypes is maximized, while at the same time being regularized towards assigning equal probability mass to each prototype vector. This can be cast to an optimal transport problem and is solved efficiently with the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14]</ref>. Instead of optimizing over |b| feature vectors, we instead optimize over N ?|b| spatial feature vectors as we have N spatial tokens for each image. As our batchsizes are small, we utilize a small queue that keeps the past 8192 features, as is done in SwAV.</p><p>With the optimal cluster assignment of all image crops' spatial tokens Q t k ? R N ?K , we formulate a swapped pre-diction task:</p><formula xml:id="formula_1">L(x t1 , ..., x t V ) = vg j=0 V i=0 1 k? =j l(x ti , x tj ) (1)</formula><p>Here, l is the 2D cross entropy between the softmaxed and aligned cluster assignment predictions and the aligned optimal cluster assignments:</p><formula xml:id="formula_2">l(x ti , x tj ) = H (s ? (? Bj,i (g(?(x ti )) T C), ? Bij (Q tj ) ,<label>(2)</label></formula><p>where H is cross-entropy and s ? a softmax scaled by temperature ? . We use L to jointly minimize the prototypes C as well as the neural networks f and g. C is further L2normalized after each gradient step such that Z T C directly computes the cosine similarity between spatial features and prototypes.</p><p>Since global crops capture the majority of an image, we solely use these to compute Q tj , as the spatial tokens can attend to global scene information such that the overall prediction target quality improves. Further, as local crops just cover parts of images and thus also parts of objects, using these produces cluster assignment predictions that effectively enable object-parts-to-object-category reasoning, an important ability for scene understanding. Alignment. In Equation 2 we introduce the alignment operator ? Bij (?). This is necessary because x tj and x ti cover different parts of the input image and thus Q tj and the cluster assignment prediction Z T ti C correspond to different image areas. To tackle this, ?(?) restores the spatial dimensions H P ? W P and aligns the tensor using the crop intersection bounding boxes B ji and B ij respectively. In our experiments we use RoI-Align <ref type="bibr" target="#b29">[31]</ref> that produces features with a fixed and compatible output size. Foreground-focused clustering. To focus the clustering on the foreground tokens, we further leverage the ViT's CLS token attention maps A i ? [0, 1] N of each of its attention heads. To create a foreground clustering mask that can be used during training, we first average the attention heads to one map and apply a Gaussian filter for smoothing. We then obtain a binary mask A b by thresholding the map to keep 60% of the mass following <ref type="bibr" target="#b5">[6]</ref>. We use ? Bji to align the global crop's attention to the intersection with crop j. The resulting mask is then applied as 0-1 weighting to the 2D cross entropy loss, l ? A b . Note that we extract the attention maps and spatial tokens with the same forward pass, thus not impacting training speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fully unsupervised semantic segmentation</head><p>In this section we describe our method that enables us to do fully unsupervised semantic segmentation. Its constituent parts work directly in the learned spatial token embedding space and leverage simple K-means clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Cluster-based Foreground Extraction (CBFE)</head><p>Under the hypothesis that clusters in our learned embedding space correspond to object parts, we should be able to extract foreground objects by assigning each cluster id to foreground object (fg) or background (bg): ? : {1, . . . , K} ? {fg, bg}. Thus, at evaluation time, we construct ? without supervision, by using ViT's merged attention maps A b as a noisy foreground hint. Similar to how we process the attention maps to focus our clustering pretext on foreground, we average the attention heads, apply Gaussian filtering with a 7x7 kernel size and keep 60% of the mass to obtain a binary mask. Using train data, we rank all clusters by pixel-wise precision with A b and find a good threshold c for classifying a cluster as foreground. This gives us ? that we apply to the patch-level clustering to get a foreground mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Overclustering with community detection (CD)</head><p>As we will see from <ref type="table">Table 1</ref>, the segmentation results improve substantially with higher clustering granularities. However, this is mainly because overclustering draws on label information to group clusters to ground-truth objects and in the limit of providing one cluster for each pixel, it would be equivalent to providing full supervision signals.</p><p>Here, we propose a novel overclustering method that requires no additional supervision at all.</p><p>The key idea we leverage is that clusters correspond to object parts, and that a set of object parts frequently cooccur together. Thus, local co-occurrence of clusters in an image should provide a hint about an object's constituent parts. Using co-occurrence statistics to categorize objects has been proposed before in <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b48">50]</ref>. However, we are the first to work with object parts and no labels and employ a novel network science method to discover objects. To group the clusters, we construct an undirected and weighted cooccurence network G = (V, E, w), with v i , i ? {1, . . . , K} corresponding to each cluster. We use a localized cooccurrence variant that regards the 8-neighborhood up to a pixel distance d. Then, we calculate the conditional cooccurrence probability P (v j |v i ) for clusters i and j over all images D. With the co-occurrence probabilities at hand, we define w(e i,j ) = min(P (v j |v i ), P (v j |v i )). This asymmetric edge weight definition is motivated by the fact that parts need not be mutually predictive: For instance, a car windshield might co-occur significantly with sky but presence of a sky is not predictive for a car windshield.</p><p>To find communities in G, we use the common Infomap algorithm <ref type="bibr" target="#b51">[53]</ref> as it works with weighted graphs and scales linearly with |E|. It works by leveraging an information-theoretic definition of network communities: Random walks sample information flow in networks and inform the construction a map ? K from nodes to M communities minimizing the expected description length of a ran-dom walk. With the discrete many-to-one mapping ? K : V ? {1, . . . , M } obtained from Infomap and computed on train data, we merge the clusters of the validation data to the desired number of ground-truth classes and do Hungarian matching <ref type="bibr" target="#b38">[40]</ref>. Note that Hungarian matching does not extract any meaningful label information; it merely makes the evaluation metric permutation-invariant <ref type="bibr" target="#b35">[37]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the image patch representations learned by Leopart. We first ablate design decisions of our method to find an optimal configuration. To evaluate whether some datasets are more information-rich for object parts learning than others, we also ablate training on different datasets. We further test the performance of our dense representations under a transfer learning setup for semantic segmentation. Furthermore, we show that Leopart can also be used for fully unsupervised segmentation requiring no label information at all for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Evaluation protocols. For all experiments, we discard the projection head used during training. Instead we directly evaluate the ViT's spatial tokens. We use two main techniques for evaluation: linear classifier and overclustering. For linear classifier (LC), we fine-tune a 1x1 convolutional layer on top of the frozen spatial token or the pre-GAP layer4 features, following <ref type="bibr" target="#b55">[57]</ref>. For overclustering, we run K-Means on all spatial tokens of a given dataset. We then group cluster to ground-truth classes by greedily matching by pixel-wise precision and run Hungarian matching <ref type="bibr" target="#b38">[40]</ref> on the merged cluster maps to make our evaluation metric permutation-invariant following <ref type="bibr" target="#b35">[37]</ref>. We always report overclustering results averaged over five different seeds. Overclustering is of special interest as it works directly in the learned embedding space and therefore requires less supervision than training a linear classifier. For completeness we also report results fine-tuning a deeper fullyconvolutional net (FCN) following <ref type="bibr" target="#b57">[59]</ref>. Generally, we follow the fine-tuning procedures of prior works <ref type="bibr" target="#b55">[57,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b63">65]</ref>. We report results in mean Intersection over Union (mIoU) unless specified otherwise.</p><p>Model training. We train a ViT-Small with patch size 16 and start training from DINO weights <ref type="bibr" target="#b5">[6]</ref>. All models were trained for 50 epochs using batches of size 32 on 2 GPUs. Further training details are provided in Appendix A.1.</p><p>Datasets. We train our model on ImageNet-100, comprising 100 randomly sampled ImageNet classes <ref type="bibr" target="#b53">[55]</ref>, COCO <ref type="bibr" target="#b40">[42]</ref> and Pascal VOC (PVOC) <ref type="bibr" target="#b20">[21]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fine-Tuning Loss Ablations</head><p>In this section, we ablate the most important design decisions and hyperparameters of our fine-tuning loss as well as the aptness of different datasets for learning object parts. We evaluate on PVOC val and report three different overclustering granularities next to LC results.</p><p>Model Ablation. In <ref type="table">Table 1</ref> we report the model ablation results. As described in Section 3, we propose to leverage attention maps to guide our clustering algorithm. Note that the attention maps are just a noisy approximation of foreground objects. Thus, it only focuses spatial token clustering on foreground but does not neglect objects in the background. We find that foreground clustering gives substantial performance gains over our two ablated versions: clustering of all spatial tokens (up to 3%) and clustering mostly background tokens (up to 10%), as shown in <ref type="table">Table 1a</ref>.</p><p>In <ref type="table">Table 1b</ref> we ablate the multi-crop augmentation strategy. More specifically, we compare using four local crops against using only two or no local crops. The usage of local crops (last vs. second to last row) gives a much larger performance gain than using just more local crops (second to last vs. first row). This shows that predicting cluster assignments from constrained local image information is an important aspect for learning expressive dense representations. Interestingly, the overclustering results are effected more by this ablation, showing that local crops are important for  learning a semantically-structured embedding space. We also ablate the number of prototypes used for Sinkhorn-Knopp clustering in <ref type="table">Table 1d</ref>. We find that the best performance is achieved with a moderate overclustering of 300 prototypes. Note however, that the number of prototypes we use for training is not equivalent to the number of clusters used for evaluation, which we denote for instance by K=500. Lastly, even though we fine-tune a pretrained model, we find that an EMA teacher still helps with learning more expressive representations as can be seen in <ref type="table">Table 1c</ref>.</p><p>Training Data. In <ref type="table">Table 2</ref> we report results under varying training data: ImageNet-100, COCO and PVOC. ImageNet-100 usually features object-centric images with few objects. In comparison, COCO and PVOC contain images with more complex scenes. For comparability, we adapt the number of epochs for PVOC to 500 such that all models are trained for the same number of iterations. We find that our method's performance improves by up to 6% when trained on COCO instead of ImageNet-100. This shows the potential of our dense clustering loss when applied to less object-centric images and is in stark contrast to other methods reporting that their results get worse when training on COCO instead of ImageNet <ref type="bibr" target="#b39">[41,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b63">65]</ref>. Finally, we see the worse results on PVOC as a confirmation of the fact that even for fine-tuning, larger datasets perform better for ViTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer learning</head><p>Next, we study how well our dense representations, once learned, generalize to other datasets. We train our model on ImageNet-100 or COCO and report LC and overclustering results on PVOC12, COCO-Things and COCO-Stuff. As shown in <ref type="table">Table 3</ref>  <ref type="table">Table 6</ref>. FCN transfer learning results. We follow the same notation as in <ref type="table">Table 3</ref>. Note that Hierarchical Grouping and Segsort fine-tune a larger ASPP decoder. ? indicates result taken from <ref type="bibr" target="#b57">[59,</ref><ref type="bibr" target="#b62">64]</ref> works by large margins on all three datasets even though some use further datasets and supervision. On PVOC12 we surpass the state-of-the-art by more than 17% for linear evaluation and by more than 5% for overclustering. On COCO-Things and COCO-Stuff we improve linear classifier by &gt; 5% and &gt; 3% and overclustering by &gt; 8% and &gt; 10% respectively. Note that these gains are not due to the DINO initialisation nor due to ViTs per-se as the starting DINO model performs on par with other instance-level self-supervised methods that use ResNets like SwAV. In fact, DINO's embedding space exhibits inferior semantic structure in comparison to MoCo-v2 and SwAV as can be seen from the overclustering results on PVOC12 (-18%) and COCO-Things (-12%). Our method is also on par with the performance of a supervised ViT even though it was trained  <ref type="figure">Figure 3</ref>. Qualitative Segmentations by DINO and our gradual improvements. We cluster the spatial tokens and visualize the resulting clusters obtained after each step of our method. on a &gt;10x times larger full ImageNet (IN-21k) dataset <ref type="bibr" target="#b49">[51]</ref>. When fine-tuning on COCO instead of IN-100, we see further improvements on all datasets. The results confirm that it is desirable to learn object parts representations, as they work well under different object definitions, as evidenced by strong performances across datasets.</p><p>In <ref type="table">Table 6</ref>, we evaluate Leopart by fine-tuning a full FCN on top of frozen features. Again, we outperform all prior works, including DenseCL, the current state-of-the-art. Interestingly, while DenseCL shows a performance gain of more than 20% when fine-tuning a FCN instead of a linear layer, our performance gain from fine-tuning is relatively low at around 2%. We hypothesize that this behaviour is because our learned embedding space is already close to maximally informative for semantic segmentation under linear transformations. In contrast, DenseCL's embedding space alone is less informative in itself and requires a more powerful non-linear transformation. We push state-of-the-art even further by fine-tuning a larger ViT-Base with patch size 8 (ViT-B/8) improving FCN performance by around 5%. We report further details and experiments in the Appendix Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fully unsupervised semantic segmentation</head><p>Encouraged by our strong K=500 overclustering results in <ref type="table">Table 3</ref>, we next evaluate fully unsupervised semantic segmentation. This relies only on the learned embedding space's structure and refrains from using any test-time label information, i.e. the number of final clusters needs to be equivalent to the ground-truth. To that extent, we start with a simple K-means clustering of the spatial tokens to get cluster assignments for each token. As prior works, we base our evaluation on PVOC12 val and train self-supervised on an arbitrary dataset <ref type="bibr" target="#b55">[57]</ref>, in this case COCO. In <ref type="table" target="#tab_1">Table 4</ref> we compare our method to prior state-of-the-art. We outperform our closest competitor, MaskContrast, by &gt; 6%. While like MaskConstrast, we cluster only foreground tokens, we use our embedding space clustering instead of a pretrained unsupervised saliency estimator to do clusterbased foreground extraction (CBFE). Also, instead of averaging the feature representations per image, we use our novel unsupervised overclustering method with community   detection (CD), allowing us to detect multiple object categories in one image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Performance gain study</head><p>In <ref type="figure">Figure 3</ref>, we show the gradual visual improvement of the segmentations. By using Leopart we substantially improve the DINO baseline by more than 14%, as shown quantitatively in <ref type="table" target="#tab_2">Table 5</ref>. This is also apparent when comparing <ref type="figure">Figure 3a</ref> to <ref type="figure">Figure 3b</ref>. The DINO segmentations show no correspondence to object categories, whereas the segmentations obtained by Leopart assign the same colors to the bus in the first and third image of the top row as an example. However, our segmentations do not correspond well with PVOC's object definitions, as we oversegment background. To further improve this, we extract foreground resulting in the segmentation maps shown in <ref type="figure">Figure 3c</ref>. The segmentation focuses on the foreground and object categories start to emerge more visibly. However, some objects are still oversegemented such as busses and cats. Thus, we run our proposed community detection algorithm to do fully unsupervised overclustering, resulting in the segmentations shown in <ref type="figure">Figure 3d</ref>. CBFE. For foreground extraction, we follow the method proposed in Section 3.2.1. As shown in <ref type="table">Table 7</ref>, our foreground masks obtained through CBFE outperform DINO's attention maps by more than 9%. This is remarkable as we can only improve the attention map if the foreground clusters also segment the foreground correctly where the noisy foreground hint from DINO's attention is wrong. In <ref type="figure" target="#fig_2">Figure 4</ref>, we show mask visualizations to provide a qualitative idea of this phenomenon. While the attention masks only mark the most discriminative regions they fail to capture the foreground object's shape ( <ref type="figure" target="#fig_2">Fig. 4(a)</ref>). Our cluster masks, however, alleviate this providing a crisp foreground object segmentation ( <ref type="figure" target="#fig_2">Fig. 4(b)</ref>). With the foreground masks extracted, we can specify K-Means to run only on foreground spatial tokens. This further improves our fully unsupervised segmentation performance by &gt; 17%, as shown in <ref type="table" target="#tab_2">Table 5</ref>. CD. We have seen that overclustering yields benefits in terms of performance but requires additional supervision for merging clusters during evaluation. To reap the benefits of this process whilst staying unsupervised, we construct a network based on cluster co-occurrences and run community detection (CD) following the method proposed in Section 3.2.2. We find that CD can further improve our performance by &gt; 5% and brings our fully unsupervised semantic segmentation results closer to the upper bound of supervised overclustering at test-time with K = 150, as shown in <ref type="table" target="#tab_2">Table 5</ref>. Finally, in <ref type="figure" target="#fig_3">Figure 5</ref> we show a visualization of the constructed network, the discovered communities as well as some exemplary parts clusters. For instance, Leopart discovers bicycle wheels and car wheels separately. This demonstrates that we can learn high-level semantic clusters that do not latch on low-level information such as shape. Furthermore, we can observe that clusters that are semantically similar, such as human hair and human faces, are also part of the same community and close in the resulting network. Also, a gradual semantic transition within connected components can be observed as shown for dog snout and cat ears being part of different communities that are interconnected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>Limitations. Since we do not learn on a pixel but on a patch level, our segmentation maps are limited in their resolution and detection capabilities. Thus, our method will fail when fine-grained pixel-level segmentation is required or very small objects covering less than an image patch are supposed to be segmented. Further, our unsupervised overclustering method does a hard assignment of clusters to communities. This has the limitation that object parts which occur in several objects are assigned to the wrong object category when they appear in a specific context. We show an example of this phenomenon in <ref type="figure">Fig. 8</ref> in the Appendix, but leave a solution to future work.</p><p>Potential negative societal impact. Self-supervised semantic segmentation can scale to large datasets with little to no human labelling effort and extract information from it. However, as human input is kept to a minimum, rigorous monitoring of the segmentation results is mandatory, as objects might not be segmented in a way that we are used to or problematic biases in the data might be manifested. Lack of monitoring could have potential negative impacts in areas such as autonomous driving and virtual reality.</p><p>Conclusion. In this paper, we propose a dense clustering pretext task for the spatial tokens of a ViT that learns a semantically richer embedding space in contrast to other self-supervised ViTs. We motivate our pretext task by observing that object definitions are brittle and demonstrate object parts learning as a principled alternative. Our experiments show that this formulation is favorable as we improve state-of-the-art on PVOC, COCO-Stuff and COCO-Thing semantic segmentation benchmarks featuring different object defintions and granularities. Finally, our embed-ding space can also be directly used for fully unsupervised segmentation, showing that objects can be defined as cooccurring object parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Implementation details</head><p>Model training Our model is implemented in Torch <ref type="bibr" target="#b44">[46]</ref> and PyTorch Lightning <ref type="bibr">[22]</ref>. We use Faiss <ref type="bibr" target="#b36">[38]</ref> for K-Means clustering and the MapEquation software package <ref type="bibr" target="#b14">[15]</ref> for community detection.</p><p>We chose to train a ViT-Small <ref type="bibr" target="#b16">[17]</ref> as the amount of parameters is roughly equivalent to a ResNet-50's (21M vs. 23M). Further, we use a student-teacher setup and the teacher weights are updated by the exponential moving average of the student weights following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">27]</ref>. The exponential moving average for updating the teacher weights is adapted with a cosine schedule starting at 0.9995 and going up to 1 i.e. a hard copy. We train the ViT-Small with a cosine learning rate schedule going down to 0 over 50 training epochs. The initial projection head learning rate is 1e?4 and the backbone's learning rate is 1e?5. The projection head consists out of three linear layers with hidden dimensionality of 2048 and Gaussian error linear units as activation function <ref type="bibr" target="#b31">[33]</ref>. The output dimensionality is 256 and the resulting tensors are then passed through a l2-bottleneck and the prototype matrix C to produce cluster assignment predictions. As discussed, we use a queue for Sinkhorn-Knopp clustering with a length of 8192. We set the tem- <ref type="figure">Figure 6</ref>. Bounding box generation example for cluster assignment alginment. The left column shows the global crops, the right column the local crops. Each global crop has N ? 1 bounding boxes as it produces prediction targets for all remaining N ? 1 crops. Each local crop has Ngc, the number of global crops, bounding boxes as it is used to predict the prediction targets of each global crop. perature to 0.1 and use Adam as an optimizer with a cosine weight decay schedule. The alignment happens to a fixed output size of 7x7 during training. This makes sure that the local and global crop feature maps have the same spatial resolution. The augmentations used are: random color-jitter, Gaussian blur, grayscale and multi-crop augmentations and the global crop's resolution is 224x224 and the local crop's resolution is 96x96, as in <ref type="bibr" target="#b4">[5]</ref>. We generate global and local crops with the constraint that they intersect at least by 1% of the original image size to make sure that there is a nonnegligible intersection where we can apply our clustering loss to. In <ref type="figure">Figure 6</ref> we show the generation process for two ImageNet pictures.</p><p>Fully unsupervised semantic segmentation For CBFE and CD we take PVOC12 train to find good hyperparameter configurations, i.e. clustering granularities K, the precision threshold for CBFE as well as Markov time and the co-occurrence probability threshold for CD. We use a segmentation of our embedding space to 200 clusters as we found this granularity to work best on PVOC for CBFE. Before doing foreground-focused clustering using the cluster mask, we bilinearly interpolate the embeddings to the desired mask size. For CBFE we report the precision thresholds used in <ref type="table">Table 8</ref>. To evaluate the unsupervised saliency estimator baseline method, we use the saliency head provided by the MaskContrast authors <ref type="bibr" target="#b55">[57]</ref>. For the computation of the Jaccard distance we include unlabelled objects as foreground. We can identify these objects as they have a separate class in the PVOC dataset.</p><p>For the CD experiments on PVOC, we cluster the embedding space to 150 clusters as we found this granularity to work well here. To construct the co-occurrence network, we calculate the conditional co-occurrence probability on each image and then average over all images the cluster appeared. The MapEquation software package can be instructed to constrain the number of found communities. We use this setting to find exactly as many communities as there are object categories in the given dataset (for PVOC it is 20). All clusters that are not in communities are assigned to background, which are just 4 out of 100 for our network, as we already focus clustering on foreground. We set the cooccurrence probability threshold to 9% and all edges below this threshold are ignored by Infomap. Further as stopping criterion we set the Markov time to 2 and all other parameters are left at default value. We report results averaged over 10 seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Precision Threshold Leopart IN 35% Leopart IN+CC 40% <ref type="table">Table 8</ref>. Precision values used for classifying clusters as foreground. <ref type="figure">Figure 7</ref>. More cluster masks for PVOC12 val obtained by our CBFE method. Overall, the masks capture the object shape well but at times they include too much background. Also small objects are not detected at times as can be seen in the first picture from the right in the first row, which is a limitation discussed.</p><p>Evaluation details Since we evaluate the pre-GAP layer4 features or the spatial tokens, their output resolution does not match the mask resolution. To fix that, we bilinearly interpolate before applying the linear or FCN head; or directly interpolate the clustering results by nearest neighbor upsampling. For a fair comparison between ResNets and ViTs, we use dilated convolution in the last bottleneck layer of the ResNet such that the spatial resolution of both network architectures match (28x28 for 448x448 input images). All overclustering results were computed using downsampled 100x100 masks to speed up the Hungarian matching as we found that the results do not differ from using full resolution masks.</p><p>We fine-tune the linear head for 25 epochs with a learning rate drop after 20 epochs and a batch size of 120. For most checkpoints we found a learning rate of 0.01 to work well except for the baselines of MaskContrast <ref type="bibr" target="#b55">[57]</ref> and MoCo-v2 <ref type="bibr" target="#b27">[29]</ref> where we use a learning rate of 0.1. All heads were trained on downsampled 100x100 masks to increase training speed. For evaluation, we stick to 448x448 masks as it does not require Hungarian matching and is thus fast.</p><p>The FCN head is fine-tuned for 30 epochs equaling the 20k iterations used in <ref type="bibr" target="#b57">[59]</ref>. Again we use a learning rate of 0.01 with a drop to 0.001 after 15 epochs and a batch size of 64. The design of our fully convolutional head follows <ref type="bibr" target="#b57">[59]</ref>: We use two convolutional layers with ReLU nonlinearites. Backbone features are fed into the second layer through a skip connection. The resulting feature maps ? are then transformed to the desired output classes by a 1x1 convolution. During training we apply 2D-dropout on ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Additional Experiments</head><p>Fine-Tuning a larger backbone To push the boundaries of state-of-the-art even further, we fine-tune a ViT-Base with patch size 8 (ViT-B/8) for 100 epochs with Leopart, starting from a DINO initialization. The results are reported in Tab. 6 in the paper and Tab. 9. Training a larger backbone boosts transfer learning performance by up to 6% on PVOC as shown in Tab. 9a. The gains on COCO-Thing and COCO-Stuff are around 1% and 2% respectively. For fully unsupervised semantic segmentation, training a larger backbone even shows more relative gain than training a ViT-Small. This is apparent from the 41.9% relative gain for a ViT-B/8 in comparison to the 37.1% relative gain for a ViT-S/16 over their respective DINO initialization, as can be deduced from Tab. 9b. Overall, we are able to improve state-of-the-art by additional 5.5% just by taking a larger model. For CD, we found K=110, an edge threshold of 7% and a Markov time of 0.4 to perform best for ViT-B/8. A larger model also improves foreground extraction using our CBFE method by more than 4% as shown in Tab. 9c.  that enough diversity for equi-partitioned clustering can be achieved with this simple mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Leopart with different initializations</head><p>Predicting ADE20k parts To quantitatively support our claim that we learn object parts, we run experiments on ADE20K <ref type="bibr" target="#b64">[66]</ref> street scenes that feature annotations for 111 different part classes on 1983 images. We pretrain on COCO and report overclustering results given ground-truth parts annotations. As shown in <ref type="table" target="#tab_0">Table 12</ref>, Leopart improves DINO's parts mIoU by 1.9% and 3% with a clustering granularity of K = 500 and K = 1000 respectively. This shows that our method increases object part correspondence. Also, while the supervised ViT outperformed DINO in transfer learning it is not superior when it comes to discovering object parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional visualizations</head><p>We provide further cluster masks in <ref type="figure">Figure 7</ref> and segmentation map visualizations on PVOC12. Next to community detection results shown in <ref type="figure">Figure 9</ref>, we also show unmerged foreground clustering results with K=100 in <ref type="figure">Figure 8</ref> to give the reader an impression of the segmentation granularities of each object. In <ref type="figure">Figure 10</ref>, we also show segmentation maps obtained from classic overclustering results by grouping clusters to objects using label information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Datasets Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 PASCAL</head><p>For fine-tuning linear heads as well as the FCN head, we use the trainaug split featuring 10582 images and their annotations. We evaluate on PVOC12 val that has 1449 images. During evaluation we ignore unlabelled objects as well and the boundary class following <ref type="bibr" target="#b55">[57]</ref>. For hyperparameter tuning of our fully unsupervised segmentation method, we use the PVOC12 train split with 1464 images. <ref type="figure">Figure 8</ref>. K=100 overclustering visualization without merging clusters to objects. Note that the cluster colors are not unique as we have 100 different clusters: Same cluster means same color but not the other way around. Interestingly, Leopart learns a different segmentation granularity depending on the object category. For instance, cars and humans are segmented into various parts, but birds are usually kept whole. <ref type="figure">Figure 9</ref>. More fully unsupervised segmentation results obtained through our community detection method. Our method, manages to merge the object parts clusters from <ref type="figure">Figure 8</ref> to objects in most of the cases. However, as our method does a hard cluster to community assignment, each cluster can only be used for one object. This limitation can be seen for the car wheel class in the 4th row and 5th and 6th pictures from the right. The bus' wheel is mistakenly assigned to the car category. Also, objects that share many parts such as bicycles and motorcycles are mistakenly merged to one category. <ref type="figure">Figure 10</ref>. Overclustering results by merging 500 clusters using ground-truth labels. The resulting segmentation maps are more crisp than their fully unsupervised counterparts in <ref type="figure">Figure 9</ref>. Further, similar object categories are not merged together. However at times, the object is not fully segmented but just parts of it. This is likely due to the fact that some clusters segmenting an object also have a significant background overlap and thus our precision-based cluster matching matches them to the background class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. COCO</head><p>We use the COCO benchmark in two ways to further isolate different object definitions. For instance, COCO-thing has one class for vehicles whereas PVOC distinguishes between boats, busses and cars. Also, things have a fundamentally different object definition as stuff. First, we extract stuff annotations i.e. object w/o a clear boundary, often in the background. For that, we use the COCO-Stuff annotations <ref type="bibr" target="#b2">[3]</ref>. We further merge the 91 fine labels to 15 coarse labels, as in <ref type="bibr" target="#b35">[37]</ref>. We also assign the coarse label "other" to non-stuff object as the label does not carry any semantic meaning. The resulting labels are:</p><p>['water', 'structural', 'ceiling', 'sky', 'building', 'furniture-stuff', 'solid', 'wall', 'raw-material', 'plant', 'textile', 'floor', 'food-stuff', 'ground', 'window'] Non-Stuff objects are ignored during training and evaluation. <ref type="figure">Figure Fig. 11</ref> shows some exemplary coco-stuff images and their corresponding masks.</p><p>Second, we extract foreground annotations by using the panoptic labels provided by <ref type="bibr" target="#b37">[39]</ref>. We merge the instancelevel annotations to an object category with a script the authors provided. Further, we merge the 80 fine categories to coarse categories obtaining 12 unique object classes:</p><p>['electronic', 'kitchen', 'appliance', 'sports', 'vehicle', 'animal', 'food', 'furniture', 'person', 'accessory', 'indoor', 'outdoor'] The background class is ignored during training and evaluation. <ref type="figure" target="#fig_4">Figure Fig. 12</ref> shows some exemplary coco-thing (a) COCO stuff images (b) COCO stuff masks <ref type="figure">Figure 11</ref>. COCO-stuff images and masks data visualized as used for head fine-tuning and evaluation. Some pictures are completely dark green and thus ignored as the stuff class appearing is "other". images and their corresponding masks.</p><p>We fine-tune the linear and FCN head on a subset of 10% of the data i.e. 11829 images. We evaluate on the full 5000 validation images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 ADE20k</head><p>Overall, ADE20k features 3687 different objects that can act as parts. We constrain our evaluation to street scenes that contain parts annotations. This reduces our data to 1983 images and to the following 111 object parts: (a) ADE20k street scene images (b) ADE20k street scene parts masks <ref type="figure">Figure 13</ref>. ADE20k street scene images and masks data visualized as used for overclustering results reported in <ref type="table" target="#tab_0">Table 12</ref>. <ref type="figure">Figure 13</ref> shows some exemplary street scene images and their corresponding parts masks. During evaluation of our feature space clustering we ignore non-part pixels shown in dark green.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>DINO Attention masks vs. Leopart Cluster masks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Communities found in our cluster co-occurrence network constructed through self-supervision. Each node corresponds to a cluster in our learnt embedding space. The nodes are colored by community membership.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 12 .</head><label>12</label><figDesc>COCO-thing images and masks data visualized as used for head fine-tuning and evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>When finetuning on COCO-Stuff and COCO-Thing, we use a 10% split of the training sets. Evaluation results are computed on the full COCO validation data for COCO-Stuff and COCO-Thing and PVOC12 val. This setup up makes sure that Num. clusters mask LC 100 300 500 all 67.4 37.9 44.6 47.8 bg 64.7 28.1 39.0 41.4 fg 67.8 38.2 47.2 50.7 ] 66.1 33.0 42.5 45.0 [2,2] 67.7 37.8 45.4 49.3 [2,4] 67.8 38.2 47.2 50.7 Ablations of different design decisions for Leopart. Training data study for Leopart. We use the best performing model config from Table 1 and train on different datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Num. clusters</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">crops LC 100 300 500</cell></row><row><cell cols="3">(a) Focusing clustering on</cell><cell cols="2">[2(b) Local crops boost perfor-</cell></row><row><cell cols="2">foreground (fg) helps.</cell><cell></cell><cell>mance.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Num. clusters</cell></row><row><cell></cell><cell cols="2">Num. clusters</cell><cell cols="2">protos LC 100 300 500</cell></row><row><cell cols="3">tchr LC 100 300 500</cell><cell cols="2">100 67.7 36.8 45.4 49.2</cell></row><row><cell cols="3">? 67.6 34.6 44.3 47.9</cell><cell cols="2">300 67.8 38.2 47.2 50.7</cell></row><row><cell cols="3">? 67.8 38.2 47.2 50.7</cell><cell cols="2">500 67.4 35.8 44.8 49.1</cell></row><row><cell cols="3">(c) Using an EMA teacher helps.</cell><cell cols="2">(d) 300 prototypes work well.</cell></row><row><cell>Dataset</cell><cell>size</cell><cell>LC</cell><cell cols="2">K=500 K=300 K=100</cell></row><row><cell>IN-100</cell><cell cols="3">126k 67.8 50.7</cell><cell>47.2</cell><cell>38.2</cell></row><row><cell>COCO</cell><cell cols="3">118k 69.1 53.0</cell><cell>49.9</cell><cell>44.3</cell></row><row><cell cols="2">PASCAL 10k</cell><cell cols="2">64.5 50.7</cell><cell>47.8</cell><cell>38.2</cell></row></table><note>our representations are assessed under varying object def- initions (e.g. stuff vs. thing) and granularities. Further de- tails are provided in the Appendix.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Unsupervised semantic segmentation results. We outperform other state-of-theart methods by a large margin. ? indicates result taken from<ref type="bibr" target="#b55">[57]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>mIoU</cell></row><row><cell>Method</cell><cell>Train</cell><cell cols="5">PVOC12 COCO-Things COCO-Stuff LC K=500 LC K=500 LC K=500</cell><cell>Sup. ResNet Sup. ViT</cell><cell>18.5 21.1</cell></row><row><cell>Sup. ViT Sup. ResNet</cell><cell cols="4">IN + IN21 68.1 55.1 65.2 50.9 IN 53.8 36.5 57.8 44.2</cell><cell cols="2">49.0 35.1 44.4 30.8</cell><cell>DINO [6] SwAV [29] MoCo-v2 [29]</cell><cell>4.6 13.7 18.5</cell></row><row><cell>instance-level:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MaskContrast [57]</cell><cell>35.0  ?</cell></row><row><cell>MoCo-v2 [29]</cell><cell>IN</cell><cell cols="3">45.0  ? 39.1 47.5 36.2</cell><cell cols="2">32.6 28.3</cell><cell>Leopart (CBFE+CD) 41.7</cell></row><row><cell>DINO [6]</cell><cell>IN</cell><cell cols="3">50.6 17.4 50.6 23.5</cell><cell cols="2">47.7 32.1</cell></row><row><cell>SwAV [5]</cell><cell>IN</cell><cell cols="3">50.7  ? 35.7 56.7 37.3</cell><cell cols="2">46.0 33.1</cell></row><row><cell>pixel/patch-level:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IIC [37]</cell><cell>PVOC</cell><cell>28.0  ? -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">MaskContrast [57] IN+PVOC 49.2 45.4 47.5 37.0</cell><cell cols="2">32.0 25.6</cell><cell>mIoU</cell></row><row><cell>DenseCL [59] Leopart Leopart</cell><cell>IN IN IN+CC</cell><cell cols="3">49.0 43.6 53.0 41.0 68.0 50.5 62.5 49.2 69.3 53.3 67.6 55.9</cell><cell cols="2">40.9 30.3 51.2 43.8 53.5 43.6</cell><cell>K=150 DINO + Leopart 18.9 (+14.3%) 48.8 4.6 + CBFE 36.6 (+17.7%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+ CD</cell><cell>41.7 (+5.1%)</cell></row></table><note>Table 3. Transfer learning for semantic segmentation results. Best results are in bold and second best are underlined. 'IN', 'IN21', 'CC' and 'PVOC' indicate train- ing on ImageNet, ImageNet21k, CoCo and Pascal trainaug respectively.? indicates result taken from [57].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Component contributions. We show the gains that each individual component brings for PVOC segmentation and K=21.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>To show the generality and robustness of our approach, we fine-tune with Overclustering results on PVOC, COCO-Thing and COCO-Stuff. The results are comparable to Tab. 3 in the paper.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">K=500</cell></row><row><cell>arch</cell><cell cols="3">PVOC12 COCO-Thing COCO-Stuff</cell></row><row><cell cols="2">ViT-S/16 53.5</cell><cell>55.9</cell><cell>43.6</cell></row><row><cell>ViT-B/8</cell><cell>59.7</cell><cell>56.8</cell><cell>45.9</cell></row><row><cell cols="4">(a) ViT-S/16 ViT-B/8</cell></row><row><cell></cell><cell>DINO</cell><cell>4.6</cell><cell>5.3</cell></row><row><cell></cell><cell cols="2">+ Leopart 18.9</cell><cell>21.2</cell></row><row><cell></cell><cell>+ CBFE</cell><cell>36.2</cell><cell>43.3</cell></row><row><cell></cell><cell>+ CD</cell><cell>41.7</cell><cell>47.2</cell></row><row><cell cols="4">(b) Fully unsupervised semantic segmentation results on PVOC.</cell></row><row><cell cols="2">Method</cell><cell>arch</cell><cell>Jacc. (%)</cell></row><row><cell cols="4">Leopart IN CBFE ViT-S/16 58.6</cell></row><row><cell cols="4">Leopart CC CBFE ViT-S/16 59.6</cell></row><row><cell cols="3">Leopart CC CBFE ViT-B/8</cell><cell>63.5</cell></row></table><note>(c) Foreground extraction results on PVOC. The results are comparable to Table 7 in the paper.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .Table 10 .Table 11 .Table 12 .</head><label>9101112</label><figDesc>Comparison of ViT-S/16 and ViT-B/8 performances. We further improve state-of-the-art on all experiments by training a larger model with our loss and running CBFE and CD. Transfer learning results starting from various initializations. Leopart consistently improves upon the initialization (init.) and thus shows the generality of our method. Comparable to Tab. 3 in the paper.Leopart starting from a Moco-v3, MAE and supervised initialization. The results are shown inTable 10. Leopart is good at fine-tuning even more recent SSL methods and larger pretrained backbones like MAE (where our method adds +28% in K=500 performance). Our method is even able to boost the performance of a ViT pretrained with supervision showing the wide applicability of our dense loss. DenseCL with DINO init. For further comparison to our closest competitor in transfer learning, DenseCL<ref type="bibr" target="#b57">[59]</ref>, we trained a ViT with DINO initalization using their loss and following the setting of Tab. 3 for PVOC12. We find a performance of 54% and 17.1% for LC and K=500 evaluation respectively, i.e. fine-tuning with Leopart still outperforms by &gt;15% for LC and &gt;40% for K=500. These results indicate that DenseCL (perhaps due to its global-pooled loss term) does not seem apt for fine-tuning as it barely improves upon the DINO initialisation (+3.4% for LC and ?0.3% for K=500). Queue usage ablation. We show that the usage of a queue improves our results as shown inTable 11, comparable to the experiments ofTable 1in the main paper. This means Queue Ablation. A clustering queue improves performance. ADE20k overclustering results. Evaluated on 111 parts classes taken from ADE20k street scenes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Num. clusters</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">queue LC</cell><cell>100 300 500</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>67.2 35.0 45.7 48.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>67.8 38.2 47.2 50.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell>ADE20k-Street K=500 K=1000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Random ViT 1.5</cell><cell>2.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sup. ViT</cell><cell>5.4</cell><cell>7.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DINO [6]</cell><cell>5.7</cell><cell>7.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Leopart IN</cell><cell>6.9</cell><cell>9.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Leopart CC</cell><cell>7.6</cell><cell>10.0</cell></row><row><cell></cell><cell></cell><cell cols="2">At init.</cell><cell cols="2">After Leopart</cell></row><row><cell>Init</cell><cell>Arch</cell><cell>LC</cell><cell>K=500</cell><cell>LC</cell><cell>K=500</cell></row><row><cell>Superv.</cell><cell>ViT-S/16</cell><cell>68.1</cell><cell>55.1</cell><cell>72.5</cell><cell>61.6</cell></row><row><cell>MoCo-v3 [20]</cell><cell>ViT-S/16</cell><cell>13.4</cell><cell>5.8</cell><cell>42.0</cell><cell>31.2</cell></row><row><cell>MAE [19]</cell><cell cols="2">ViT-B/16 47.5</cell><cell>10.0</cell><cell>68.9</cell><cell>38.4</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>A.Z. is thankful for using compute resources while interning at Pina Earth. Y.M.A is thankful for MLRA funding from AWS.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cocostuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02057</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised object discovery and localization in images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on ubiquitous robots and ambient intelligence (URAI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised part discovery from contrastive reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Twins: Revisiting spatial attention design in vision trans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13840</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">formers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What is a good evaluation measure for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">France</forename><surname>Meylan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<meeting><address><addrLine>Bristol</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="10" to="5244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The mapequation software package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Edler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosvall</surname></persName>
		</author>
		<ptr target="http://www.mapequation.organdhttps://github.com/mapequation/infomap.12" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<idno>2021. 14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">An empirical study of training selfsupervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<idno>2021. 14</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Object categorization using co-occurrence, location and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Large-scale unsupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shang-Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Obow: Online bag-of-visual-words generation for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient visual pretraining with contrastive detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10957</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scops: Self-supervised co-part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Segsort: Segmentation by discriminative sorting of segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jyh-Jing</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tien-Ju</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Self-supervised feature learning by learning to spot artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dense semantic contrast for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoni</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiying</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07756</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Self-emd: Selfsupervised object detection without imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Peirce</surname></persName>
		</author>
		<title level="m">Reflections on real and unreal objects</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolina</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Imagenet-21k pretraining for the masses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS-Data</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Spatially consistent representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungseok</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuhyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1144" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rosvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergstrom</surname></persName>
		</author>
		<idno>arXiv preprint:0707.0609</idno>
		<title level="m">Maps of information flow reveal community structure in complex networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Localizing objects with selfsupervised transformers and no labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriane</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Huy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Roburin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14279</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised semantic segmentation by contrasting object mask proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Aet vs. aed: Unsupervised representation learning by autoencoding transformations rather than data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Self-supervised visual representation learning from hierarchical grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03044</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Object relational graph with teacher-recommended learning for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaya</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peijin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">blind&apos;, &apos;blinds&apos;, &apos;branch&apos;, &apos;bumper&apos;, &apos;chimney&apos;, &apos;cloud&apos;, &apos;clouds&apos;, &apos;column&apos;, &apos;columns&apos;, &apos;cornice&apos;, &apos;crosswalk&apos;, &apos;dome&apos;, &apos;door&apos;, &apos;door frame&apos;, &apos;doorbell&apos;, &apos;dormer&apos;, &apos;double door&apos;, &apos;drain pipe&apos;, &apos;eaves&apos;, &apos;entrance&apos;, &apos;entrance parking&apos;, &apos;exhaust pipe&apos;, &apos;face&apos;, &apos;fence&apos;, &apos;fender&apos;, &apos;fire bell&apos;, &apos;fire escape&apos;, &apos;garage door&apos;, &apos;garage doors&apos;, &apos;gas cap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="633" to="641" />
		</imprint>
	</monogr>
	<note>railing&apos;, &apos;rain pipe&apos;, &apos;revolving door&apos;, &apos;right arm&apos;, &apos;right foot&apos;, &apos;right hand&apos;, &apos;right leg&apos;, &apos;rim&apos;, &apos;road&apos;, &apos;roof&apos;, &apos;roof rack&apos;, &apos;rose window&apos;, &apos;saddle&apos;, &apos;shop window&apos;, &apos;shutter&apos;, &apos;shutters&apos;, &apos;sidewalk&apos;, &apos;sign&apos;, &apos;skylight. steering wheel&apos;, &apos;step&apos;, &apos;steps&apos;, &apos;taillight&apos;, &apos;terrace&apos;, &apos;torso&apos;, &apos;tower&apos;, &apos;tree&apos;, &apos;trunk&apos;, &apos;vent&apos;, &apos;wall&apos;, &apos;wheel&apos;, &apos;window&apos;, &apos;window scarf&apos;, &apos;windows&apos;, &apos;windshield&apos;, &apos;wiper&apos;, &apos;car&apos;, &apos;buildings&apos;, &apos;building&apos;</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
