<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Interventions for Causal Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzhi</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustine</forename><surname>Cha</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amogh</forename><surname>Gupta</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfeng</forename><surname>Yang</surname></persName>
							<email>junfeng@cs.columbia.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
							<email>vondrick@cs.columbia.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Rutgers University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Interventions for Causal Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a framework for learning robust visual representations that generalize to new viewpoints, backgrounds, and scene contexts. Discriminative models often learn naturally occurring spurious correlations, which cause them to fail on images outside of the training distribution. In this paper, we show that we can steer generative models to manufacture interventions on features caused by confounding factors. Experiments, visualizations, and theoretical results show this method learns robust representations more consistent with the underlying causal relationships. Our approach improves performance on multiple datasets demanding out-of-distribution generalization, and we demonstrate state-of-the-art performance generalizing from ImageNet to ObjectNet dataset. 0 The correct categories are clearly a broom, a tray, and a shoe.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual recognition today is governed by empirical risk minimization (ERM), which bounds the generalization error when the training and testing distributions match <ref type="bibr" target="#b51">[47]</ref>. When training sets cover all factors of variation, such as background context or camera viewpoints, discriminative models learn invariances and predict object category labels with the right cause <ref type="bibr" target="#b35">[33]</ref>. However, the visual world is vast and naturally open. Collecting a representative, balanced dataset is difficult and, in some cases, impossible because the world can unpredictably change after learning.</p><p>Directly optimizing the empirical risk is prone to learning unstable spurious correlations that do not respect the underlying causal structure <ref type="bibr" target="#b13">[11,</ref><ref type="bibr" target="#b10">8,</ref><ref type="bibr" target="#b26">24,</ref><ref type="bibr" target="#b46">43,</ref><ref type="bibr">4,</ref><ref type="bibr" target="#b37">35]</ref>. <ref type="figure">Figure 1</ref> illustrates the issue succinctly. In natural images, the object of interest and the scene context have confounding factors, creating spurious correlations. For example, ladle (the * Equal Contribution. Order by coin flip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Baseline Our Model Image Baseline Our Model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Plunger Tie</head><p>Television Tray <ref type="table">Table  Lamp</ref> Plunger Doormat Knife Ladle Television Shovel <ref type="figure">Figure 1</ref>: Top predictions from a state-of-the-art ImageNet classifier <ref type="bibr" target="#b23">[21]</ref>. The model uses spurious correlations (scene contexts, viewpoints, and backgrounds), leading to incorrect predictions. <ref type="bibr">0</ref> In this paper, we introduce a method to learn causal visual features that improve robustness of visual recognition models.</p><p>object of interest) often has a hand holding it (the scene context), but there is no causal relation between them. Several studies have exposed this challenge by demonstrating substantial performance degradation when the confounding bias no longer holds at testing time <ref type="bibr" target="#b42">[40,</ref><ref type="bibr" target="#b21">19]</ref>. For example, the ObjectNet <ref type="bibr" target="#b8">[6]</ref> dataset removes several common spurious correlations from the test set, causing the performance of state-of-the-art models to deteriorate by 40% compared to the ImageNet validation set. A promising direction for fortifying visual recognition is to learn causal representations (see <ref type="bibr" target="#b45">[42]</ref> for an excellent overview). If representations are able to identify the causal mechanism between the image features and the category labels, then robust generalization is possible. While the traditional approach to establish causality is through randomized control trials or interventions, natural images are passively collected, preventing the use of such procedures. This paper introduces a framework for learning causal visual representations with natural images. Our approach is based on the observation that generative models quantify nuisance variables <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b28">26]</ref>, such as viewpoint or back-   <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b28">26]</ref>, allowing us to manipulate images and construct interventions on nuisances. The transformations transfer across categories. Each column in the figure presents images with one consistent intervention direction.</p><p>ground. We present a causal graph that models both robust features and spurious features during image recognition. Crucially, our formulation shows how to learn causal features by steering generative models to perform interventions on realistic images, simulating manipulations to the camera and scene that remove spurious correlations. As our approach is model-agnostic, we are able to learn robust representations for any state-of-the-art computer vision model.</p><p>Our empirical and theoretical results show that our approach learns representations that regard causal structures. While just sampling from generative models will replicate the same training set biases, steering the generative models allows us to reduce the bias, which we show is critical for performance. On ImageNet-C <ref type="bibr" target="#b24">[22]</ref> benchmark, we surpass established methods by up to 12%, which shows that our method helps discriminate based on the causal features. Our approach also demonstrates the state-of-the-art performance on the new ObjectNet dataset <ref type="bibr" target="#b8">[6]</ref>. We obtain 39.3% top-1 accuracy with ResNet152 <ref type="bibr" target="#b23">[21]</ref>, which is over 9% gain over the published ObjectNet benchmark <ref type="bibr" target="#b8">[6]</ref> while maintaining accuracy on ImageNet and ImageNet-V2 <ref type="bibr" target="#b42">[40]</ref>. Our code is available at https://github.com/cvlabcolumbia/GenInt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Data augmentation: Data augmentation often helps learn robust image classifiers. Most existing data augmentations use lower-level transformations <ref type="bibr" target="#b31">[29,</ref><ref type="bibr" target="#b50">46]</ref>, such as rotate, contrast, brightness, and shear. Auto-data augmentation <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b56">52]</ref> uses reinforcement learning to optimize the combination of those lower-level transformations. Other work, such as cutout <ref type="bibr" target="#b17">[15]</ref> and mixup <ref type="bibr" target="#b55">[51]</ref>, develops new augmentation strategies towards improved generalization. <ref type="bibr" target="#b36">[34,</ref><ref type="bibr" target="#b58">54,</ref><ref type="bibr" target="#b21">19]</ref> explored style transfer to augment the training data, however, the transformations for training are limited to texture and color change. Adversarial training, where images are augmented by adding imperceptible adversarial noise, can also train robust models <ref type="bibr" target="#b54">[50]</ref>. However, both adversarial training <ref type="bibr" target="#b54">[50]</ref> and auto augmentation <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b56">52]</ref> introduce up to three orders of magnitude of computational overhead. In addition, none of the above methods can do high-level transformations such as changing the viewpoint or background <ref type="bibr" target="#b8">[6]</ref>, while our generative interventions can. Our method fundamentally differs from prior data augmentation methods because it learns a robust model by estimating the causal effects via generative interventions. Our method not only eliminates spurious correlations more than data augmentations, but also theoretically produces a tighter causal effect bound.</p><p>Causal Models: Causal image classifiers generalize well despite environmental changes because they are invariant to the nuisances caused by the confounding factors <ref type="bibr" target="#b10">[8]</ref>. A large body of work studies how to acquire causal effects from a combination of association levels and intervention levels <ref type="bibr" target="#b33">[31,</ref><ref type="bibr" target="#b13">11,</ref><ref type="bibr" target="#b34">32]</ref>. Ideally, we can learn an invariant representation across different environments and nuisances <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b37">35]</ref> while maintaining the causal information <ref type="bibr" target="#b7">[5]</ref>. While structural risk minimization, such as regularization <ref type="bibr" target="#b29">[27]</ref>, can also promote a model's causality, this paper focuses on training models under ERM <ref type="bibr" target="#b51">[47]</ref>.</p><p>Generative Models: Our work leverages recent advances in deep generative models <ref type="bibr" target="#b22">[20,</ref><ref type="bibr" target="#b30">28,</ref><ref type="bibr" target="#b27">25,</ref><ref type="bibr" target="#b12">10,</ref><ref type="bibr" target="#b41">39]</ref>. Deep generative models capture the joint distribution of the data, which can complement discriminative models <ref type="bibr" target="#b27">[25,</ref><ref type="bibr" target="#b39">37]</ref>. Prior work has explored adding data directly sampled from a deep generator to the original training data to improve classification accuracy on ImageNet <ref type="bibr" target="#b40">[38]</ref>. We denote it as GAN Augmentation in this paper. Other works im-proved classification accuracy under imbalanced and insufficient data by oversampling through a deep generator <ref type="bibr" target="#b18">[16,</ref><ref type="bibr" target="#b19">17,</ref><ref type="bibr" target="#b53">49,</ref><ref type="bibr" target="#b9">7]</ref>. However, sampling without intervention, the augmented data still follows the same training joint distribution, where unobserved confounding bias will continue to contaminate the generated data. Thus, the resulting models still fail to generalize once the spurious correlations changed. Ideally, we want to generate data independent of the spurious correlations while holding the object's causal features fixed.</p><p>Recent works analyzing deep generative models show that different variations, such as viewpoints and background, are automatically learned <ref type="bibr" target="#b28">[26,</ref><ref type="bibr" target="#b25">23]</ref>. We leverage deep generative models for constructing interventions in realistic visual data. Our work randomizes a large class of steerable variations, which shifts the observed data distribution to be independent of the confounding bias further. Our approach tends to manipulate high-level transformations orthogonal to traditional data augmentation strategies <ref type="bibr" target="#b14">[12,</ref><ref type="bibr" target="#b50">46,</ref><ref type="bibr" target="#b31">29]</ref>, and we obtain additional performance gains by combining them.</p><p>Domain Adaptation: Our goal is to train robust models that generalize to unforeseen data. Accessing the test data distribution, even unlabeled, could lead to overfitting and fail to measure the true generalization. Our work thus is trained with no access to the test data. Our setting is consistent with ObjectNet's policy prohibiting any form of learning on its test set <ref type="bibr" target="#b8">[6]</ref>, and ImageNet-C's policy discouraging training on the tested corruptions. On the other hand, domain adaptation <ref type="bibr">[3,</ref><ref type="bibr" target="#b43">41,</ref><ref type="bibr" target="#b52">48]</ref> needs access to the distributions of both the source domain and the target domain, which conflicts with our setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Causal Analysis</head><p>We quantify nuisances via generative models and propose the corresponding causal graph. We show how to train causal models via intervention on the nuisance factors. We theoretically show sufficient conditions for intervention strategy selection that promote causal learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Correlation Analysis</head><p>Nuisance factors do not cause the object label. If there is a correlation between the nuisance factors and the label in data, we cannot learn causal classifiers. While identifying such correlations is crucial, they are hard to quantify on large, real-world vision datasets, because nuisance factors such as viewpoint and backgrounds, are difficult and expensive to measure in natural images.</p><p>We propose to measure such nuisance factors via intervening on the conditional generative models. Prior work <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b28">26]</ref> shows that nuisance transformations automatically emerge in generative models ( <ref type="figure">Figure 17</ref>), which enables constructing desired nuisances via intervention. Given a  <ref type="figure">Figure 3</ref>: Do unwanted correlations exist between the nuisance factors (e.g. backgrounds, viewpoint) and labels on ImageNet? We measure correlation (y-axis) via how many times the classification accuracy is better than chance on the ImageNet validation set. The x-axis denotes the number of categories we select for prediction. To train causal models, nuisance factors should not be predictable for labels (chance). Our generative interventions (GenInt) reduce the unwanted correlations from the data better than existing data augmentation strategies <ref type="bibr" target="#b55">[51,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b40">38]</ref>.</p><p>category y and random noise vector h 0 , we first generate an exemplar image x = G(h 0 , y). We then conduct intervention z to get the intervened noise vector h * 0 , and the intervened image x * = G(h * 0 , y), which corresponds to changing the viewpoints, backgrounds, and scene context of the exemplar. We thus get data with both image x * and the corresponding nuisance manipulation z. Implementation details are in the supplementary.</p><p>We train a model that predicts the nuisances z from input image x * . This model can then predict nuisances z from natural images x. We read out the correlation between the nuisance z and label y by training a fully-connected classifier with input z and output y. We measure the correlations via the times the classifier outperforms random. Generative models may capture only a subset of the nuisances, thus our estimated correlations are lower bounds. The true correlations maybe even more significant.</p><p>In <ref type="figure">Figure 3</ref>, the training data of five established methods <ref type="bibr" target="#b23">[21,</ref><ref type="bibr" target="#b55">51,</ref><ref type="bibr" target="#b15">13,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b40">38]</ref> contains strong correlations that are undesirable. On the original ImageNet data, the undesirable correlation in the data is up to 8 times larger than chance. Our generative interventions reduce the unwanted correlations from the data significantly, naturally leading to robust classifiers that use the right cause.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Causal Graph</head><p>We build our causal graph based on the correlation analysis. We know that nuisances do not cause the label (context 'hand' does not cause the category 'ladle'), and there is no additional common outcome variable (collider) in our correlation prediction. If the correlation between the nuisances and the label is not chance, then there exists a confounder C that causes both Z and Y .  <ref type="figure" target="#fig_9">Figure 4</ref>: Causal graph for image classification. Gray variables are observed. (a) F is the variable that generates the object features. The unobserved confounder C causes both the background features Z and label Y , which creates a spurious correlation between the image X and label Y . (b) An ideal intervention blocks the backdoor path from Z to C, which produces causal models. (c) In practice, we cannot guarantee to intervene on all the Z variables. However, by properly intervening on even a small set of nuisance factors Z i , the confounding bias of the observed distribution is mitigated, which is theoretically proven by Theorem 3.</p><p>Figure 12(a) shows our causal graph for image recognition. We denote the unobserved confounder as C, which produces nuisance factors Z, and the corresponding annotated categorical label Y . Z produces the nuisance features X Z in images. There is another variable F that generates the core object features X F , which together with X Z constructs the pixels of a natural image X. There is no direct arrow from F to Y since Y ? ? F |X, i.e., image X contains all the features for predicting Y . We can observe only X but not X Z or Z F separately. We draw a causal arrow from X to Y . Since nuisances Z are spuriously correlated to the label but not causing the label Y , classifiers are not causal if they predict Y from the nuisances Z better than chance. Note that "while a directed path is necessary for a total causal effect, it is not sufficient <ref type="bibr" target="#b38">[36]</ref>." Thus, though there is a path Z ? X ? Y , Z does not cause Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Causal Discriminative Model</head><p>Generative interventions help in eliminating spurious correlations ( <ref type="figure">Figure 3</ref> and Section 3.1), leading to better generalization. We denote the causality from X to Y to be P (y|do(x)), which is the treatment effect of an input image X on label Y . To capture the right cause via correlations learned by empirical risk minimization, we need to construct data such that P (Y |do(X)) = P (Y |X).</p><p>Natural images are often biased by unobserved confounding factors that are common causes to both the image X and the label Y . A passively collected vision dataset only enables us to observe the variables X and Y . Theoretically, we cannot identify the causal effect P (Y |do(X)) in <ref type="figure" target="#fig_1">Figure  12</ref>(a) with only the observed joint distribution P (X, Y ) because there is an unobserved common cause.</p><p>We thus want to collect faithful data independent of the confounding bias, so that we can identify the causal effect with only the observed data. We need to intervene on the data-generation process for the nuisances Z to be independent to the confounders, while keeping the core object features F unchanged. In the physical world, such interventions correspond to actively manipulating the camera or objects in the scene. In our paper, we perform such interventions via steering the generative models. The outcome of this intervention on Z is visualized in <ref type="figure" target="#fig_1">Figure 12</ref>(b), which manipulates the causal graph such that dependencies arriving at Z are removed. Removing the backdoor, the correlation is now equal to the causality, i.e., P (Y |X) = P (Y |do(X)). While this result is intuitive, performing perfect intervention in practice is challenging due to the complexity of the natural image distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Causal Effect Bound</head><p>Imperfect interventions can eliminate only some spurious correlations. Though it is theoretically impossible to calculate the exact causal effect P (y|do(x)) when spurious correlations are not totally removed, we can still estimate the lower and upper bound for P (y|do(x)).</p><p>Given the observed joint distribution P (x, y), Pearl <ref type="bibr" target="#b35">[33]</ref> identified that P (y|do(x)) can be bounded by P (x, y) ? P (y|do(x)) ? P (x, y)+1?P (x), which can be estimated by existing discriminative models without interventions.</p><p>Prior work augments the data by sampling from the GANs without explicit intervention <ref type="bibr" target="#b53">[49,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b18">16,</ref><ref type="bibr" target="#b19">17]</ref>, which will yield the same causal bound as the original data. Since GANs capture the same distribution as the observational training set, the spurious correlations remain the same. The sampled transformations Z in <ref type="figure" target="#fig_1">Figure 12</ref> (a) are still dependent on the confounders C. Thus, augmenting training data with GANs <ref type="bibr" target="#b40">[38]</ref>, without intervention is not an effective algorithm for causal identification.</p><p>In this paper, we aim to identify a tighter causal effect bound for P (y|do(x)) using generative interventions. This is desirable for robustness because it removes or reduces the overlap between the causal intervals, promoting causal predictions. Section 3.3 establishes that perfect interventions eliminate all spurious correlation and leads to better generalization. In practice, our generative interventions may only eliminate a subset of spurious correlations Z i , while other nuisances Z U remain unobserved and untouched. The next question is then: what generative intervention strategy is optimal for tightening the causal effect bound? We derive the following theory:</p><p>Theorem 1 (Effective Intervention Strategy). We denote the images as x. The causal bound under intervention z i is thus P (y, x|z i ) ? P (y|do(x)) ? P (y, x|z i )+1?P (x|z i ). For two intervention strategies z 1 and z 2 , z 1 ? z, z 2 ? z, if P (x|z 1 ) &gt; P (x|z 2 ), then z 1 is more effective for causal identification.</p><p>Proof. <ref type="figure" target="#fig_1">Figure 12</ref>(c) shows the causal graph after interven-</p><formula xml:id="formula_0">tion Z i , where Z i ? ? Y |X.</formula><p>We add and remove the same term c P (y, x, c|z i ):</p><formula xml:id="formula_1">P (y|do(x)) = c P (y|x, zi, c)P (c) (Backdoor Criteria) = c P (y, x, c|zi) + c P (y|x, zi, c)(P (c) ? P (x, c|zi))</formula><p>Since 0 ? P (y|x, z i , c) ? 1, we have the lower and upper bounds. We denote ? 1 = P (x|z 1 ) ? P (x|z 2 ), thus ? 1 &gt; 0. In the causal graph ( <ref type="figure" target="#fig_1">Figure 12</ref>(c)), since we intervene on z i , all incoming edges to z i are removed; we then have z i ? ? y|x and P (x, y|z i ) = P (y|x, z i )P (x|z i ) = P (y|x)P (x|z i ). Therefore ? 2 = P (x, y|z 1 ) ? P (x, y|z 2 ) = ? 1 ? P (y|x). Since apparently 0 &lt; P (y|x) &lt; 1, we have that 0 &lt; ? 2 &lt; ? 1 .</p><p>Thus we obtain [P (y, x|z 1 ), P (y, x|z 1 ) + 1 ? P (x|z 1 )] ? [P (y, x|z 2 ), P (y, x|z 2 ) + 1 ? P (x|z 2 )], which means the intervention z 1 results in a tighter causal effect bound.</p><p>Our theorem shows that: the optimal intervention strategy should maximize P (x|z), which will tighten the causal effect bound P (y|do(x)). Also, the intervention strategy should be identically selected across all categories, so that they are independent of the confounding bias. While there are different choices of intervening on the generative model to create independence, we empirically select our generative intervention strategy that increases P (x|z), which we will discuss in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>We show how deep generative models can be used to construct interventions on the spuriously correlated features in the causal graph. We combine these results to develop a practical framework for robust learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Learning Objective</head><p>We minimize the following training loss on our intervened data:</p><formula xml:id="formula_2">L =L e (?(X), Y) + ? 1 L e (?(X int ), Y ) + ? 2 L e (?(X itr ), Y )<label>(1)</label></formula><p>where L e denotes the standard cross entropy loss and ? i ? R are hyper-parameters controlling training data choice. We denote the original data matrix as X with target labels Y; the generated data matrix as X int (Section 4.2) with target labels Y ; the transfered data as X itr (Section 4.3) with target labels Y ; and the discriminative classifier as ?.</p><p>The last two terms of this objective are the interventions. In the remainder of this section, we present two different ways of constructing these interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Generative Interventions</head><p>We construct interventions using conditional generative adversarial networks (CGAN). We denote the i-th layer's hidden representation as h i . CGAN learns the mapping x = G(h 0 , y), where h 0 ? N (0, I) is the input noise, y is the label, and x is a generated image of class y that lies in the natural image distribution. CGANs are trained on the joint data distribution P (x, y). While we can use any type of CGANs, we select BigGAN <ref type="bibr" target="#b12">[10]</ref> in this paper since it produces highly realistic images. In addition, generative models learn a latent representation h i equivariant to a large class of visual transformations and independent of the object category <ref type="bibr" target="#b25">[23,</ref><ref type="bibr" target="#b28">26]</ref>, allowing for controlled visual manipulation. For example, GANSpace <ref type="bibr" target="#b25">[23]</ref> showed that the principal components of h i correspond to visual transformations over camera extrinsics and scene properties. The same perturbations in the latent space will produce the same visual transformations across different categories. <ref type="figure">Figure 17</ref> visualizes this steerability for a few samples and different transformations. This property enables us to construct a new training distribution, where the nuisance features Z are not affected by the confounders.</p><p>Our generative intervention strategy follows the GANSpace <ref type="bibr" target="#b25">[23]</ref> method, which empirically steers the GAN with transformations independent of the categories. It contains three factors: the truncation value, the transformation type, and the transformation scale. The input noise h 0 is sampled from Gaussian noise truncated by value t <ref type="bibr" target="#b12">[10]</ref>. We define the transformations to be along the j-th principal directions r j in the feature space <ref type="bibr" target="#b25">[23]</ref>, which are orthogonal and captures the major variations of the data. We select the top-k significant ones {r 1 , r 2 , ..., r k } as the intervention directions. We then intervene along the selected directions with a uniformly sampled step size s from a range [?s, s]. We intervene on the generator's intermediate layers with</p><formula xml:id="formula_3">h * i = h i + ?s r j ? ?,</formula><p>where h * i are the features at layer i after interventions, ? is the standard deviation of noise on direction r, and ? is the offset term. After the intervention, we follow the method in GANSpace <ref type="bibr" target="#b25">[23]</ref> to recover h * 0 with regression and generate the new image x * = G(h * 0 , y). Using conditional generative models, we produce the causal features X F by specifying the category. Our intervention removes the incoming edge from C to Z i <ref type="figure" target="#fig_1">(Figure 12</ref> (c)). We denote the intervention procedure as function I, and rewrite the generative interventions as:</p><formula xml:id="formula_4">X int = I(t, s, k, Y )</formula><p>Based on our Theorem 3, we choose the hyper-parameters t, k, s for intervention Z that maximizes P (x|z). We show ablation studies in Section 5.4.  <ref type="table">Table 1</ref>: Accuracy on the ObjectNet test set versus training distributions. By intervening on the training distribution with generative models, we obtain the state-of-the-art performance on the ObjectNet test set, even though the model was never trained on ObjectNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Transfer to Natural Data</head><p>Maintaining the original training data X will add confounding bias to models. While our theory shows that our method still tightens the causal effect bound under the presence of spurious correlations, it is desirable to eliminate as many spurious correlations as possible. We will therefore also intervene on the original dataset.</p><p>One straightforward approach is to estimate the latent codes in the generator corresponding to the natural images, and apply our above intervention method. We originally tried projecting the images back to the latent space in the generative models <ref type="bibr" target="#b57">[53,</ref><ref type="bibr" target="#b1">2]</ref>, but this did not obtain strong results, because the projected latent code cannot fully recover the query images <ref type="bibr" target="#b11">[9]</ref>.</p><p>Instead, we propose to transfer the desirable generative interventions from X int to the original data X with neural style transfer <ref type="bibr" target="#b20">[18]</ref>. The category information is maintained by the matching loss while the intervened nuisance factors are transferred via minimizing the maximum mean discrepancy <ref type="bibr" target="#b32">[30]</ref>. Without projecting the images to the latent code, the transfer enables us to intervene on some of the nuisance factors z in the original data, such as the background. The transfer of the generative interventions I(t, k, s, Y ) to natural data X is formulated as:</p><formula xml:id="formula_5">X itr = T (I(t, k, s, Y ), X)</formula><p>where T denote the style transfer mapping. The corresponding label Y is the same label as for X. Please see supplemental material for visualizations of these interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We present image classification experiments on four datasets -ImageNet, ImageNet-V2, Imagenet-C, and Ob-jectNet -to analyze the generalization capabilities of this method and validate our theoretical results. We call our approach GenInt for generative interventions, and compare the different intervention strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>In our experiments, all the models are first trained on ImageNet <ref type="bibr" target="#b16">[14]</ref> (in addition to various intervention strategies). We train only on ImageNet without any additional data from other target domains. We directly evaluate the models on the following out-of-distribution testing sets:</p><p>ObjectNet <ref type="bibr" target="#b8">[6]</ref> is a test set of natural images that removes background, context, and camera viewpoints confounding bias. Improving performance on ObjectNet-without finetuning on it-indicates that a model is learning causal features. ObjectNet's policy prohibits any form of training on the ObjectNet data. We measure performance on the 113 overlapping categories between ImageNet and ObjectNet.</p><p>ImageNet-C <ref type="bibr" target="#b24">[22]</ref> is a benchmark for model generalization under 15 common corruptions, such as 'motion,' 'snow,' and 'defocus.' Each corruption has 5 different intensities. We use mean Corruption Error (mCE) normalized by AlexNet as the evaluation metric <ref type="bibr" target="#b24">[22]</ref>. Note that we do not train our model with any of these corruptions, thus the performance gain measures our model's generalization to unseen corruptions.</p><p>ImageNet-V2 <ref type="bibr" target="#b42">[40]</ref> is a new test set for ImageNet, aiming to quantify the generalization ability of ImageNet models. It contains three sampling strategies: MatchedFrequency, Threshold0.7, and TopImages. While current models are overfitting to the ImageNet test set, this dataset measures the ability to generalize to a new test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Baselines</head><p>We compare against several established data augmentation baselines:</p><p>Stylized ImageNet refers to training the model using style transferred dataset <ref type="bibr" target="#b21">[19]</ref>, which trains classifiers that are not biased towards texture.</p><p>Mixup <ref type="bibr" target="#b55">[51]</ref> does linear interpolation to augment the dataset. We use their best hyperparameters setup (? = 0.4).</p><p>AutoAug <ref type="bibr" target="#b15">[13]</ref> systematically optimizes the strategy for   <ref type="table">Table 3</ref>: Accuracy on ImageNet V2 validation set <ref type="bibr" target="#b42">[40]</ref> and original ImageNet validation set. Our method improves the performance upon the baselines, which suggests our causal learning approach does not hurt the performance on original test set while becoming robust.</p><p>data augmentation using reinforcement learning. GAN Augmentation refers to the method that augments the ImageNet data by directly sampling from the BigGAN <ref type="bibr" target="#b40">[38]</ref>. They provide an extensive study for hyper-parameter selection. We use their best setup as our baseline: 50% of synthetic data sampled from BigGAN with truncation 0.2.</p><p>ImageNet only refers to training the standard model on ImageNet dataset only <ref type="bibr" target="#b23">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Empirical Results</head><p>Our GenInt method demonstrates significant gains on four datasets over five established baselines. We report results for two different network architectures (ResNet18, ResNet152). All ResNet18 models are trained with SGD for 90 epochs, we follow the standard learning rate schedule where we start from 0.1, and reduce it by 10 times every 30 epochs. For ResNet152 models, we train "ImageNet only" models using the above mentioned method, and finetune all the other methods from the baseline for 40 epochs given that it is computationally expensive to train ResNet-152 models from scratch. For GenInt, we all use ? 1 = 0.05 and ? 2 = 0 for ResNet18 and ? 1 = 0.2 and ? 2 = 0 for ResNet152. For GenInt with Transfer, we use ? 1 = 0.02 and ? 2 = 1 for our experiments on Resnet18 with standard augmentation, ? 1 = 0.05 and ? 2 = 1 for our experiments on Resnet18 with additional augmentation, and ? 1 = 0.2 and ? 2 = 0.2 for our finetuning on ResNet152. We select hyperparameters of our intervention strategy in Section 5.4. Implementation details are in the supplementary.</p><p>ObjectNet: <ref type="table">Table 6</ref> shows that our model can learn more robust features, and consequently generalizes better to Ob-jectNet without any additional training. Our results consistently outperform the naive sampling from generative models <ref type="bibr" target="#b40">[38]</ref> and other data augmentation strategies <ref type="bibr" target="#b55">[51,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b15">13]</ref>   <ref type="figure">Figure 5</ref>: As the strength of the intervention increases, the value of log P (x|z) increases, which improves the performance of ResNet-18 model. for multiple metrics and network architectures, highlighting the difference between traditional data augmentation and our generative intervention. Our approach enjoys benefits by combining with additional data augmentations, demonstrated by the differences between the "Std. Augmentation" columns and the "Add. Augmentation" columns. <ref type="bibr" target="#b0">1</ref> This improvement suggests that our generative intervention can manipulate additional nuisances (viewpoints, backgrounds, and scene contexts) orthogonal to traditional augmentation, which complements existing data augmentation methods. Moreover, our results suggest that intervening on the generative model is more important than just sampling from it. ImageNet-C: To further validate that our approach learns causality, and not just overfits, we measure the same models' generalization to unseen corruptions on ImageNet-C. We evaluate performance with mean corruption error (mCE) <ref type="bibr" target="#b24">[22]</ref> normalized by CE of AlexNet. <ref type="table" target="#tab_2">Table 2</ref> shows that directly sampling from GAN as augmentation (GAN Augmentation) slightly improves performance (less than 1%). Stylized ImageNet achieves the best performance among all the baselines, but it is still worse than our approach in mCE. In addition, Stylized ImageNet hurts the performance on ObjectNet, which suggests its high performance on corruptions is overfitting to the correlations instead of learning the causality. Our approach outperforms baseline by up to 12.48% and 7.57% on ResNet18 and ResNet152 respectively, which validates that our generative interventions promote causal learning. <ref type="bibr" target="#b0">1</ref> Standard augmentation only uses random crop and horizontal flips <ref type="bibr" target="#b0">[1]</ref>. Additional augmentation method uses rotation and color jittering <ref type="bibr" target="#b50">[46]</ref>.  ImageNet and ImageNet-V2: <ref type="table">Table 3</ref> shows the accuracy on both validation sets. Some baselines, such as Stylized ImageNet, hurt the performances on the ImageNet validation set, while our approach improves the performance.</p><p>Overall, without trading-off the performance between different datasets, our approach achieves improved performance for all test sets, which highlights the advantage of our causal learning approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Analysis</head><p>Causal Bound and Performance: Does tighter causal bound lead to a better classifier? Following Theorem 3, we measure the tightness of causal bound after intervention, where we use the log likelihood log P (x|z) = i</p><p>x j log(P (x i |x j )P (x j |z)), where x i is the query image from the held out ImageNet validation set, and x j is the data generated by intervention z. We train ResNet18 on our generated data. <ref type="bibr" target="#b1">2</ref> By varying the intervention strength, we increase the value of P (x|z), which corresponds to a tighter causal bound. <ref type="figure">Figure 5</ref> shows that, as the causal bound getting tighter (left), performance steadily increases (right). Optimal Intervention Strategy: Since tighter causal bound produces better models, we investigate the optimal intervention strategy for tightening causal bounds. We study the effect of changing t, k, s for our intervention on the causal bound (Section 4.2). We conduct ablation studies and show the trend in <ref type="figure">Figure 6</ref>. We choose t = 1, k = 60, and s = 100% as our intervention strategy for tightest causal bound, which produces log P (x|z) = ?5.162 and yields the optimal accuracy of 45.06% <ref type="table" target="#tab_6">(Table 4</ref>) in practice.</p><p>Importance of Intervention: Our results show that creating interventions with a GAN is different from simply augmenting datasets with samples from a GAN. To examine this, <ref type="table" target="#tab_6">Table 4</ref> shows performance on ImageNet when the training sets only consist of images from the GAN. We use the best practices from <ref type="bibr" target="#b40">[38]</ref>, which comprehensively studies GAN image generation as training data. Our results show that creating interventions, not just augmentations, improves classification performance by 2.4%-6.0%.  <ref type="figure">Figure 7</ref>: We visualize the input regions that the model uses to make predictions. Blue implies the model ignores the region for discrimination, while red implies the region is very discriminative. The white text shows the model's top prediction. The baseline frequently latches onto spurious background context (e.g., hand spuriously correlated with ladle, chair spuriously correlated with tablelamp), and consequently makes the wrong prediction. Meanwhile, our model often predicts correctly for the right reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Model Visualization</head><p>By removing the confounding factors in the dataset, we expect the model to learn to attend tightly to the spatial regions corresponding to the object, and not spuriously correlated contextual regions. To analyze this, <ref type="figure">Figure 7</ref> uses GradCAM <ref type="bibr" target="#b47">[44]</ref> to visualize what regions the models use for making prediction. While the baseline often attends to the background or other nuisances for prediction, our method focuses on the spatial features of the object. For example, for the first 'Broom' image, the baseline uses spurious context 'hand,' leading to a misprediction 'Ladle,' while our model predicts the right 'Broom' by looking at its shape. This suggests that, in addition to performance gains, our model predicts correctly for the right reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Fortifying visual recognition for an unconstrained environment remains an open challenge in the field. We introduce a method for learning discriminative visual models that are consistent with causal structures, which enables robust generalization. By steering generative models to construct interventions, we are able to randomize many features without being affected by confounding factors. We show a theoretical guarantee for learning causal classifiers under imperfect interventions, and demonstrate improved performance on ImageNet, ImageNet-C, ImageNet-V2, and the systematically controlled ObjectNet. <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Motivating Experiment</head><p>We create a controlled experiment to show that generative models can construct interventions for causal learning. Our experiments demonstrate that generative models can inherently discover nuisances and intervene on the confounding factors, creating extrapolated data beyond the training distribution with confounding bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.A. Controlled Example: Colored MNIST</head><p>Dataset: We analyze our model on MNIST, which allows us to carefully control experiments. We use Colored MNIST <ref type="bibr" target="#b10">[8]</ref> where we explicitly introduce a confounding bias c that affects the background color b for the training set. This confounding bias does not exist in the test set. We set two different background colors for each digit category y i , i = 0 . . . 9. While the handwritten digit is the true cause, the background color is spuriously correlated to the target. A classifier that makes predictions based on the spurious background correlation will have P (y i |b) = 1 10 , while the causal classifier learns to be invariant to background-color P (y 1 |do(b)) = P (y 2 |do(b)) = <ref type="bibr">1 10</ref> . We show training examples for our manipulated colored MNIST in <ref type="figure">Figure 8</ref>. By training models under Empirical Risk Minimization, the model will learn the spurious correlations instead of the true causal relationship, thus not generalizing well once the spurious correlations change.</p><p>Experimental Setup: We validate this outcome by experiment. The baseline is trained only on the original colored data. For methods involving a generator, we train a conditional variational auto-encoder (CVAE) <ref type="bibr" target="#b49">[45]</ref> on the observed joint distribution. Observational CVAE denotes the classifier only trained on data sampled from an original CVAE without intervention, corresponding to the 'GAN Augmentation' method in our main paper. Our proposed method is labeled Interventional CVAE, corresponding to 'GenInt' in our main paper, where we train classifiers on data generated by intervening on the two principal directions in latent space. The intervention scale is uniformly sampled from a given interval to cut off the dependency on the category. Since the generative model captures the MNIST joint distribution well, we use only the generative interventional data without the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7.B. Results</head><p>Visualization Results: In <ref type="figure" target="#fig_4">Figure 9</ref>, we visualize data generated by observational CVAE. As we can see, the color produced by observational CVAE is the same as the training set-no new color emerges-which is due to observational CVAE capturing the same joint distribution as the training set. In <ref type="figure" target="#fig_5">Figure 10</ref>, we visualize what happens to the generated image background once we intervene on two major directions in the generative models. New colors emerge due to our intervention, thus we can randomize the features affected by the confounding bias through proper intervention. <ref type="figure" target="#fig_6">Figure 11</ref> also demonstrates the difference between the original dataset, the observational VAE, and the interventional VAE.</p><p>Invariant Risk Minimization: Recent work <ref type="bibr" target="#b10">[8]</ref> proposes an Invariant Risk Minimization(IRM) algorithm to estimate invariant correlations across multiple training distributions, which is related to causal learning and enables out-of-distribution generalization. IRM algorithm is shown to work on binary colored MNIST classification. We implement IRM on our more challenging 10-way colored MNIST dataset. <ref type="table" target="#tab_9">Table 5</ref> shows that our algorithm bypasses it by a big margin on the "Causal Test Accuracy," which demonstrates the effectiveness of our algorithm for causal learning in confounded data.</p><p>Quantative Results: <ref type="table" target="#tab_9">Table 5</ref> shows that the baseline model performs well when the confounding bias persists in the test set, but catastrophically fails (worse than chance) once the spurious colors are changed. The CVAE suffers from the same issue, demonstrating that data augmentation with a CVAE is insufficient to learn robust models. The state-of-the-art solution on colorful MNIST is the IRM, where the classifier is optimized across several differ-  ent environments. Our method, Interventional CVAE, doubles the accuracy on the causal test set without substantial decreases on the confounded test set, we also advance the state-of-the-art IRM method by more than 10%. The results show that using generative interventions, our approach can learn causal representations more effectively than noninterventional methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Proof for Theoretical Analysis</head><p>We formalize the framework with Structural Causal Models (SCMs) <ref type="bibr">(Pearl, 2000, pp. 204-207</ref>). An SCM con-tains &lt; U, V, ?, P (U ) &gt;, where U is a set of unobserved variables and V is the observed variables, ? is the set of dependence functions, and P (U ) encodes the uncertainty of the exogenous variables. In our paper, V = {X, Y }, U = {C, F, Z, U x , U y }, we do not plot U x , U y on the causal graph explicitly. We assume that U x , U y are exogenous variables that capture the uncertainty of variables X and Y , respectively. C is the unobserved confounding bias, which causes the object image X and its corresponding label Y . We validate the existence of C via the correlations analysis experiment in Section 3.1. We assume P (U ) satisfies the Gaussian distribution, but it can also be any other distribu- tion in our theory. We plot our causal graph in <ref type="figure" target="#fig_1">Figure 12</ref> . The functional relationship ? between the variables are as follows:</p><formula xml:id="formula_6">Z := ? z (C) X := ? x (U x , F, Z) Y := ? y (U y , X, C)</formula><p>Following <ref type="bibr" target="#b35">[33]</ref>, we define the causal effect of variable X on Y as follows: Definition 1. The causality of variable X on Y , denoted as P (Y |do(X)), is the effect of conducting X to Y while keeping all the other variables the same.</p><p>Note that P (Y |do(X)) is different from P (Y |X). Since P (Y |X) is the observational distribution, a change in X can suggest a change in unobserved confounding bias under our causal graph. It is possible that the observed change in X is due to changes in confounding variables. The observed P (Y |X) thus is not the same as P (Y |do(X)). To identify the causal effect of P (Y |do(X)), we need to observe the confounding bias, or intervene on X.</p><p>In <ref type="figure">Figure 1</ref> in the paper, by observing variables that block the back-doors from X to Y , we can learn the causal effect from X to Y . Theorem 2. One can identify the causal effect of X on outcome Y by observing the hidden factors C or Z.</p><p>Proof. We denote the parent node of variable X in the causal DAG graph as pa x .</p><p>Then for C: Given only the observational data, it is often impossible to identify the exact causal effect of data. Instead, <ref type="bibr" target="#b35">[33]</ref> proposed the natural bound for the causal effect to restrict the possible causal effect in a range. Lemma 1. Given the observed joint distribution, the natural bound for P (Y |do(x)) is bounded by</p><formula xml:id="formula_7">P</formula><formula xml:id="formula_8">[P (X, Y ), P (X, Y ) + 1 ? P (X)].</formula><p>Proof. We denote all the unobserved confounding factors as u. In practice, we cannot guarantee to intervene on all the Z variables. However, by properly intervening on even a small set of nuisance factors Z i , the confounding bias of the observed distribution is mitigated, which is theoretically proven by our theorem.</p><formula xml:id="formula_9">P (y|do(x)) ? u P (x, y, u) = P (x, y) (2) P (y|do(x)) ? u (P (x, y, u) + (P (u) ? P (u, x))) (3) = P (x, y) + (1 ? P (x))<label>(4)</label></formula><p>This illustrates the proof for natural bound.</p><p>Ideally, we desire a tighter causal bound. As we can see in <ref type="figure" target="#fig_10">Figure 13</ref>, a tighter bound reduces the overlap of bound intervals, and creates a margin between the intervals of the probability of predicting different objects. If we can separate the interval of the maximum category from the others, we can predict the causal results even under bounded causal effect.</p><p>Although observing all the confounding factors is impossible for most vision tasks, there are some confounding factors that can be captured by generative models. We assume that we can intervene on a subset of the features caused by the confounders. We can tighten the causal bound using our intervention using generative models, which helps to learn models that are more consistent with the true causal effect. We propose the following theorem in the main paper: Theorem 3. We denote the images as x. The causal bound under intervention z i is thus P (y, x|z i ) ? P (y|do(x)) ? P (y, x|z i ) + 1 ? P (x|z i ). For two intervention strategies z 1 and z 2 , z 1 ? z, z 2 ? z, if P (x|z 1 ) &gt; P (x|z 2 ), then z 1 is more effective for causal identification.</p><p>Proof. <ref type="figure">Figure 1b</ref> shows the causal graph after intervention where Z = Z i , Z i ? ? Y . We follow causal calculus rules by Pearl <ref type="bibr" target="#b35">[33]</ref> and add and remove the same term c P (y, x, c|z i ):</p><formula xml:id="formula_10">P (y|do(x)) = c P (y|x, z i , c)P (c) = c P (y, x, c|z i )+ c P (y|x, z i , c)(P (c) ? P (x, c|z i ))</formula><p>Since 0 ? P (y|x, z i , c) ? 1, we have the lower and upper bounds.</p><p>We denote ? 1 = P (x|z 1 ) ? P (x|z 2 ), thus ? 1 &gt; 0. Since we intervene on z i in the causal graph of <ref type="figure">Figure 8b</ref>, all incoming edges to z i are removed. Therefore, z i ? ? y|x and P (x, y|z i ) = P (y|x, z i )P (x|z i ) = P (y|x)P (x|z i ). Next, let ? 2 = P (x, y|z 1 ) ? P (x, y|z 2 ), then ? 2 = ? 1 ? P (y|x). Since 0 &lt; P (y|x) &lt; 1, then 0 &lt; ? 2 &lt; ? 1 . Thus we obtain [P (y, x|z 1 ), P (y, x|z 1 ) + 1 ? P (x|z 1 )] ? [P (y, x|z 2 ), P (y, x|z 2 ) + 1 ? P (x|z 2 )], which means the intervention z 1 results in a tighter causal effect bound.</p><p>Though deep learning methods are non-linear, to provide insights for the effect of intervening on a subset of variables, we analyze an example under a linear model. The following theorem 4 shows that, if the models are linear, then one can estimate the causal effect with intervened Z. Proof. Based on the linear assumption, Let Y = a 1 C + bX + u y , X = a 4 F + a 5 Z i + a 6 Z U + u x . Our goal is to estimate P (Y |do(X)) which we denote as b = P (Y |do(X)). The causal graph is shown in <ref type="figure" target="#fig_1">Figure 2b</ref> in the main paper. Under a linear model, we conduct linear regression to estimate the coefficient ? zy between Z i and Y .</p><formula xml:id="formula_11">? zy = a 5 b</formula><p>The confounding factors do not appear in this regression, because X is an unobserved collider in the regression, thus information cannot go to the confounders.</p><p>Then we conduct linear regression to estimate the coefficient ? zx between Z and X.</p><formula xml:id="formula_12">? zx = a 5</formula><p>where the path from z to x is the causal path. Thus we can estimate the causal effect P (Y |do(X)) = ?zy ?zx = b under linear model. This linear example shows that by intervening only on a subset of the confounding factors z, one can identify the causal effect from x to y under unobserved confounders. Compared with Section 3.1.1 in <ref type="bibr" target="#b26">[24]</ref>, we show that even under a much weaker assumption where we can intervene on only a subset of nuisance factors, we can learn causal effect in linear models. This linear example also motivates the reason for effectiveness of our method in a non-linear setting.</p><p>In our main paper, we mentioned that "Z produces the nuisance features X Z in images. There is another variable F that generates the core object features X F , which together with X Z constructs the pixels of a natural image X. There is no direct arrow from F to Y since Y ? ? F |X, i.e., image X contains all the features for predicting Y . We can observe X but not X Z or Z F separately. " We show the corresponding causal graph in <ref type="figure" target="#fig_9">Figure 14</ref>. If we can disentangle X Z and X F , we can estimate the causal effect by predicting XZ C Z XF Y F <ref type="figure" target="#fig_9">Figure 14</ref>: If we can disentangle the nuisances X Z and causal feature X F in the images, we get the above causal graph. We can identify causality by directly predicting Y from X F . We can measure the spurious correlations by predicting Y from X Z . However, disentangling X into the nuisances X Z and causal feature X F is still an open challenge.</p><p>Y from X F . However, given that the image pixels we observe are combinations of X F and X Z , we essentially use the causal graph in <ref type="figure" target="#fig_1">Figure 12</ref>.</p><p>9. Calculating P (x|z) For Causal Effect Bound</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.A. Causal Bound and Performance</head><p>In Section 5.4, we empirically calculate the P (x|z) for our causal effect bound. We describe the implementation details here. Following Theorem 3, we measure the effectiveness of our intervention using log P (x|z) given that log is a monotonic function, which will not change the relative ranking of different P (x|z). Our theorem suggests that a larger log P (x|z) results in a more effective intervention strategy for causal identification. To estimate log P (x|z), we sample x from the ImageNet validation set, which is a widely used benchmark and approximates the true unknown distribution. Other open-world data can also be used here in future work.</p><p>We estimate the likelihood to query validation set of images x given our intervention z. We denote the ith query image from the validation set as x i . The log likelihood of producing the test set data is log P (x|z) = i log P (x i |z) = i x j log(P (x i |x j )P (x j |z)), where x is the data we generate in the training set. P (x i |x j ) is calculated through the cross-entropy loss of the CNN features after normalization. Since the generator is a deterministic network given the intervention, we have P (x j |z) = 1. Thus P (x i |z) = x j P (x i |x j ). We approximate the marginalization of x j from generated dataset which is nearest to the query image. A larger value of log P (x|z) indicates the generated image under intervention z is closer to the real data distribution, which according to Theorem 3, tightens the causal identification bound. We randomly sample half of ObjectNet and ImageNet overlapping categories to calculate log P (x|z). For each category, we generate 1000 images for x j and use all the 50 validation data for x i . We use a pre-trained ResNet18 model for extracting the CNN features, where we use the features from the penultimate layer.</p><p>We first study the impact of log P (x|z) on the actual classifier generalization ability. We change the value of log P (x|z) using different interventional strengths. We set truncation value to be 0.5 for the experiments. We vary the strength of the intervention by controlling the scale of the randomness. For the listed datasets, we intervene with a different intervention scale and sample 2000 images for 1000 ImageNet categories each. Thus the intervention strength is controlled by how much we randomize the generation process: (1) For 'None' interventional data, we sample from the observational GAN. (2) For weak interventional data, for each category we sample 50 random seeds, and for each seed, we intervene on 4 randomly selected PCA directions from the top 20 PCA components, and generate 10 data points for each PCA transformation within s ? [?5, 5] uniformly. (3) For medium interventional data, we first sample 500 random noise vectors, then intervene for each noise with 2 randomly selected PCA directions among the top 20 PCA components. Then we randomly sample 2 images from s ? [?7, 7]. (4) For strong interventional data, we first sample 1000 random noise vectors, then intervene on each noise vector with 2 randomly selected PCA direction among the top 20 PCA components. Then we randomly sample one image from s ? [? <ref type="bibr" target="#b11">9,</ref><ref type="bibr" target="#b11">9]</ref>.</p><p>As shown in <ref type="figure">Figure 5</ref> in the main paper, P (x|z) and the model performance increases as the intervention is strengthened. It demonstrates that a proper intervention strategy that increases P (x|z) increases the models' performance, which also matches our Theorem 3. Moreover, in practice, the intervened GAN achieved much higher accuracy than the GAN method on both ImageNet and ObjectNet test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.B. Optimal Intervention Strategy</head><p>We conduct an ablation study for hyperparameters for intervention strategy, where we fix all the other variables and change only the hyperparameters we studied. Our paper finally uses the best setup as our intervention strategy.</p><p>Our intervention strategy contains three components: the truncation value for the BigGAN generator, the number of PCA direction selected, and the step size for the intervention scale. For all intervention strategies, we fix the number of generated data to be 500 for each category. For the interventional scale, we first construct a reference scale as a unit, then vary our intervention scale with relative percentage. We visualize each PCA component to specify a reference scale for interventional range, such that after the intervention in the given direction, the image still looks realistic. The intervention range is small for the largest PCA component, and large for the non-top PCA directions. For example, we use [?3, 3] for topmost PCA, and [?12, 12] for 40-th PCA. As we only visual check a subset of all the categories, this range value is just a rough estimate for our reference, not the true range. We refer to the intervention scale via the relative size to the reference intervention.</p><p>The setup for our three ablation study is as follows: Truncation Value We use the top 60 PCA with the 100% interventional scale to the reference size. We vary the truncation value from 0.125 to 1. We plot the calculated log P (x|z) value.</p><p>Top K PCA value selected. We use truncation 1 with 100% of the intervention range for this experiment. We experiment with intervention on 1 PCA to top 80 PCA intervention, and plot log P (x|z) value.</p><p>Interventional Scale s. We use truncation 1 with top 60 PCA component for this experiment. We vary the interventional scale based on their relative number to our predefined reference value, where the scale is selected from 10% to 130%. We find 100% to our reference intervention strength yield the highest value for log P (x|z).</p><p>The results are visualized in <ref type="figure">Figure 6</ref> in the main paper. We select the hyperparameters that produce the highest P (x|z) for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Setup for Correlation Analysis</head><p>In the main paper, we show that nuisance transformations such as viewpoints, backgrounds, and scene context automatically emerge in deep generative models. We hypothesise that there are spurious correlations between the nuisances and the object label. Given that nuisance factors are not the true cause for the label, simply predicting the label with images containing those spurious nuisances will result in non-causal models that are not robust.</p><p>We thus propose to quantify nuisances via generative models. We use the BigGAN <ref type="bibr" target="#b12">[10]</ref> [10] trained on ImageNet to perform the empirical analysis. BigGAN is a conditional generative model. Given a category y and random noise vector h 0 , BigGAN generates an image x = G(h 0 , y). The truncation factor in BigGAN controls the trade-off between quality and diversity. With small truncation 0.3, we find that BigGAN not only generates highly realistic images but highly discriminative viewpoint with representative backgrounds. We show examples in <ref type="figure" target="#fig_11">Figure 15</ref>. We treat the generated images as the exemplar for the given category.</p><p>We then conduct interventions z to get the intervened noise vector h * 0 , and the intervened image x * = G(h * 0 , y), which corresponds to changing the viewpoints, backgrounds, and scene context of the exemplar. Our goal is to construct a dataset that contains images and their corresponding interventions. We aim to exhaust all the interventions z on the given image, such that we can predict interventions well given an image. For each intervention z, we intervene along a given PCA direction with a scale sampled randomly from a given range. We exhaust the top 60 PCA directions but remove 5 redundant transformations. This setup makes sure each PCA component correspond to some visual transformation that aligns with human perception. We focus on understanding the interpretable transformations, but adding more PCA directions produces stronger correlations. We specify the scale for each PCA direction to be within 12. For each category, we generate 10 random exemplars h 0 . For each given h 0 , we exhaust top 60 studied PCA directions and we sample 5 random intervention scale for each PCA direction.</p><p>We thus get data with both image x * and the corresponding nuisance manipulation z. We use 80% of the data for training, and 20% of the data for validation. We train a ResNet34 <ref type="bibr" target="#b23">[21]</ref> model as backbone, which regresses the z intervention scale value for each PCA direction given an input image x. We use L1 loss. Our training achieves an error of 0.009 and a validation error of 0.36, while the random guessing L1 error is around 5.6. Our trained regression model is category agnostic, thus the produced information does not contain category information. We further validate this by training an MLP model to predict the category information from the output nuisances, the model can only produce random guessing results. Thus we conclude our model can predict the nuisances well despite some error.</p><p>We input ImageNet image to our trained nuisance predictor and output the corresponding nuisances z. We then train a 4 layer MLP model, with hidden units 512, to predict the ImageNet test label from the extracted nuisances factors. We find the learned MLP model can generalize well to the ImageNet test set, suggesting that nuisances factors are correlated with the label, which is shown in <ref type="figure">Figure 3</ref> in the main paper. We also input data processed with mixup, stylize transfer, auto augmentation method, and GAN augmentation method, we then compute their corresponding correlations between the predicted nuisances and the labels. Results are shown in <ref type="figure">Figure 3</ref> in the main paper, where we can see correlations are still there. However, after inputting our GenInt data, the correlations disappear, which suggests that our intervention removes the correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.">Experimental Setup and Ablation Study for Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11.A. Implementation details</head><p>We used Nvidia GeForce RTX 2080Ti GPU with 11 GB memory for experiments. As shown in <ref type="table">Table 6</ref>, we denote the model trained with only ImageNet data with as 'A,' the model trained with both ImageNet data and the interventional data X int as 'B,' the model trained with original Im-ageNet and Transferred interventional data X itr as 'C,' and the model trained with all data as type 'D'. We also provide code in our supplementary.</p><p>ResNet18 training details: For all A,B,C,D model type, we train the model with batch size 256 for ImageNet X. For B, we use ? 1 = 0.05 and batch size 64 for interventional data X int term. For the type C model, we train the Ima-geNet model with transferred interventional data X itr with ? 2 = 1, where we both use a batch size of 256. We use the same parameter setup for both the training with standard augmentation and the additional augmentation. For type D model, we directly fine-tune the pre-trained GenInt Transfer model with ? 1 = 0.05 for additional augmentation and ? 1 = 0.02 for standard augmentation, with batch size 64 and ? 2 = 1, batch size 256. We will provide an ablation study for the batch size and ? hyperparameters in the next subsection, which will show that our method constantly and robustly outperforms the baseline under a wide range of hyperparameters.</p><p>ResNet152 training details: We choose batch size as 256 for original ImageNet data X for models A,B,C. For model B and model C, we use ? 1 = 0.2, ? 2 = 0.2 and batch size with 64. For model D, due to the GPU memory limitation, we reduce the batch size of original ImageNet data X to be 192. We also use ? 1 = 0.2, ? 2 = 0.2 and batch size 64 for the interventional data. We finetune from model C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11.B. Ablation Study for Hyper-parameters</head><p>Besides the hyperparameters for intervention strategy (discussed in Section 5.4 in our main paper), our method also needs to specify the value for ? and the corresponding batch size for the data. We offer ablation studies for the ? and batch size here.</p><p>On ResNet18 models, we conduct an ablation study for batch size in <ref type="table">Table 7</ref>, and ablation study for the weight ? 1 in <ref type="table">Table 8</ref>. <ref type="table">Table 7</ref> varies the batch size of X int data from 0 to 256, fixing ? 1 = 0.05. Our models perform the best between batch size 64 to 128. <ref type="table">Table 7</ref> varies the value for ? 1 from 0 to 0.2, with fixed batch size 64. Model performs the best at ? 1 = 0.05. <ref type="table">Table 7</ref> and <ref type="table">Table 8</ref> demonstrate three major results: (1) our approach constantly and robustly outperforms the baseline under different setups, which suggests that one can achieve the state-of-the-art performance without much hyperparameter tuning (2) the performance of our method will change as the hyperparameters change (3) after grid search for hyperparameters, we demonstrate even higher performance than the one we reported in our main paper (which is a randomly chosen number without many trials), which suggests our method can be improved given more computational resources and hyperparameter tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.">Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12.A. Model Visualization: Which regions are used by the model To make predictions</head><p>By learning the right cause, we expect the models to learn to attend tightly to the spatial regions corresponding to the right object, and not spuriously correlated contextual regions. We use GradCAM to visualize all models' discriminative regions in <ref type="figure">Figure 16</ref>. The results demonstrate that, our model not only outperforms the the-state-of-theart ResNet152 model trained on ImageNet, but also outperforms other 4 established methods <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b55">51,</ref><ref type="bibr" target="#b21">19,</ref><ref type="bibr" target="#b40">38]</ref> for training robust classifiers with data augmentation. For example, for the first 'Sunglasses' image, the baseline models attend to the spurious floor background and viewpoint, then mispredict the 'Sunglasses' as 'Toilet Tissue' and 'Sandal'. However, our method learns the right causal features, which ignores the spurious background and viewpoint information, leading to the right prediction. The same mispredictions happen for 'Tie' and 'Envelope' as well: given a wooden background, the state-of-the-art classifiers tend to predict based on the background and context, getting mispredictions such as 'Cleaver' and 'Sandal'. This suggests that, in addition to performance gains upon the established 5 baseline methods, our model predicts the right category for the right reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12.B. Equivariance in Generative Models</head><p>In <ref type="figure">Figure 17</ref>, we show more examples of steering the transformation of images generated using BigGAN. The major data augmentations are often restricted to a few types of traditional transformations, such as rotation and color jit-  <ref type="table">Table 6</ref>: Ablation study for different generative interventions in our approach. On ResNet18, we experiment on training with different combinations of interventional data. The checkmark denote the data type is used in the training. We denote the model trained with only ImageNet data as 'A,' the model trained with both ImageNet data and the interventional data X int as 'B,' the model trained with ImageNet original and Transferred interventional data X itr as 'C,' and the model trained with all data as type 'D'. We show accuracy on the ObjectNet. With standard augmentation, simply training the original data with our interventional data X int improves performance on the ObjectNet and achieves the highest top 5 accuracy. By training on with both X int and transferred interventional data X itr , the classifier achieved the best on top 1 accuracy. For training with additional augmentation, training on our interventional data further increases performance, which demonstrates that our approach is complementary to standard data augmentation methods. Together with additional augmentation, we show combined training on both X int and X itr achieves the best performance on both top 1 and top 5 accuracy.  <ref type="figure">Figure 16</ref>: We visualize the input regions that are used to make predictions for 5 baseline models and our models. Blue indicates the model ignores the region for discrimination, while red indicates the region is the key for discriminative. The white text shows the model's top prediction. The baseline frequently latches onto spurious background context, resulting in wrong predictions. While the state-of-the-art data augmentation and robust learning methods cannot attend to the causal region, our model often predicts the right thing for the right reasons.  <ref type="table">Table 7</ref>: Ablation study for batch size in GenInt model. We investigate the effect of batch size when using our generative interventional data with the original data. We use ? 1 = 0.05 and change the batch size from 32 to 256. We observe that our method robustly outperforms the baseline under different hyperparameter setup.  <ref type="table">Table 8</ref>: Ablation study for the value of ? in GenInt model. We investigate the effect of ? 1 when using our generative interventional data with the original data. We use batch size 64 and vary the value of ? from 0 to 0.2. We observe our method robustly outperforms the baseline under different hyperparameter setup while performing best on ? 1 = 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>tering. Our intervention method can intervene on a larger number of nuisances with high-level transformations, such as stretch, which is orthogonal and complementary to existing augmentation strategies. Also, simply sampling from a generator will result in transformations that are spuriously correlated to the object category. With our generative intervention, we enable the generator to create transformations that relate less to the object category. Our interventional strategy increases the value of P (x|z), which tightens the causal bound according to our theory. Our generator sample a larger number of examples from the tail region of the distribution, which intuitively prevents the model from overfitting to certain biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12.C. Transferring generative interventions</head><p>Eliminating spurious correlations can promote causality, producing robust classifiers. We propose to intervene on not only the generative data, but also the natural data. As we discussed in the main paper, projecting natural images to the latent space in generative models is challenging, yielding inferior results. Instead, we directly transfer the desirable generative interventions to natural data. Leveraging the neural style transfer <ref type="bibr" target="#b32">[30,</ref><ref type="bibr" target="#b20">18]</ref>, we denote our generative interventional data as the 'style' images, and the natural data as the 'content' images. We use the VGG model as the backbone for style transfer, where we use the features at 1st, 2nd, 3rd, 4th, 5th convolutional layer as the style and match the feature at 4th convolutional layer as content. We weight the style loss with 1000000. We use the LBFGS optimizer with default parameter setup in Pytorch. We apply update steps with number uniformly sampled from 20 to 70. The results for transferred interventions are visualized in <ref type="figure">Figure 18</ref>. Our method can transfer the desirable interventions, such as the background and texture, to the target image, eliminating spurious correlations in the original natural data. We transfer the desirable intervention to our target images via the neural style transfer algorithm. The 'Raw Image' column shows the images that we want to intervene on. The 'Intervention to Transfer' column shows the result of applying our generative intervention to a random image generated by BigGAN, since intervention cannot exist itself, it needs to be realized on one image. For example, the first example has an intervention to whiten the background, where we realize it on a bagel image. Our goal is to apply this intervention to the natural image in the first column. The 'Intervention Result' column shows the outcome of our transferred intervention, where our 'Raw Image' is intervened with the intervention in the 'Intervention to transfer' column. For the first image, we can see the background for the hook is whitened. We can thus perform interventions on the natural data where no latent code is found in the generative models. While the interventions here are mostly restricted to the background and scene context changes without viewpoint rotation, it can remove spurious correlations from the raw images to enforce a tighter causal bound.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Generative adversarial networks are steerable</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>The background color for each class without intervention in the observational VAE model. The generator fails to generate digits with different background color from the training set, which demonstrates the importance of intervention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :</head><label>10</label><figDesc>The background color for each class in interventional CVAE. We intervene on two principal component directions in the latent space. Despite the dataset being created with only 2 colors per category, new background colors emerge in the generative model after interventions. This demonstrates the importance of intervening on generative models for creating unbiased data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of the background color for each class for the original dataset (A), the observational CVAE (B), and the interventional CVAE (C). New colors emerge after intervening on the generative model. The background color is randomized after interventions, so that it no long spuriously correlates with the target label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(y|do(x)) = pax P (y|x, pa x )P (pa x ) = pax,c P (y|x, pa x , c)p(e, c|x, pa x )P (pa x ) = pax,c P (y|x, c)P (e, c|pa x )P (pa x ) = c P (y|x, c)P (c) Thus one can identify the causal effect of X on Y by observing C. For Z: P (y|do(x)) = pax P (y|x, pa x )P (pa x ) = pax,z P (y|x, pa x , z)p(z|x, pa x )P (pa x ) = paz P (y|x, z)P (z|pa x )P (pa x ) = z P (y|x, z)P (z) Thus one can identify the causal effect of X on Y by observing Z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>PFigure 12 :</head><label>12</label><figDesc>(y|do(x)) = u P (y|x, u)P (u) = u P (y|x, u)(P (u, x) + P (u) ? P (u, x)) = u P (x, y, u) + u P (y|x, u)(P (u) ? P (u, x)) Since 0 ? P (y|x, u) ? 1, Causal graph for image classification. Gray variables are observed. (a) F is the unobserved variable that generates the object features. The unobserved confounder C causes both the background features Z and label Y , which creates a spurious correlation between the image X and label Y . (b) An ideal intervention blocks the backdoor path from Z to C, which produces causal models. (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Theorem 4 .</head><label>4</label><figDesc>If all the variables follow a Gaussian distribution, and the functional relationships between the variables are linear, then one can identify the causal effect by observing Z inFigure 4c, even though there remain unobserved confounding factors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Tightened identification bound reduces the overlap on the bound for probabilities of the predicted labels, which results in correct recognition when the bounds do not overlap.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 15 :</head><label>15</label><figDesc>The exemplars generated from the BigGAN<ref type="bibr" target="#b12">[10]</ref> model with truncation 0.3. From here we intervene the image with interventions of scale up to 40 times of the truncation value. While the initial images may contain minor nuisances variations, it is negligible compared with our intervention strength. We can thus infer the intervention value from the generated exemplars to the query images with reasonable precision (validation L1 error 0.36, while random model produces an error of 6).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 17 :Figure 18 :</head><label>1718</label><figDesc>Generative adversarial networks are steerable, allowing us to manipulate images and approximate interventions. Since the representations are equivariant, the transformations transfer across categories. Each row in the figure presents images from the same intervention direction. How to conduct interventions for natural images without knowing the corresponding latent code in a generator?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Transfer (ours) 22.34% 41.65% 27.03% 48.02% 34.69% 55.82% 39.38% 61.43%</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ResNet 18</cell><cell></cell><cell></cell><cell cols="2">ResNet 152</cell><cell></cell></row><row><cell></cell><cell cols="8">Std. Augmentation Add. Augmentation Std. Augmentation Add. Augmentation</cell></row><row><cell>Training Distribution</cell><cell>top1</cell><cell>top5</cell><cell>top1</cell><cell>top5</cell><cell>top1</cell><cell>top5</cell><cell>top1</cell><cell>top5</cell></row><row><cell>ImageNet Only [21, 6]</cell><cell cols="2">20.48% 40.64%</cell><cell>24.42%</cell><cell>44.39%</cell><cell cols="2">30.00% 48.00%</cell><cell>37.43%</cell><cell>59.10%</cell></row><row><cell>Stylized ImageNet [19]</cell><cell cols="2">18.39% 37.29%</cell><cell>22.81%</cell><cell>42.27%</cell><cell cols="2">31.64% 52.56%</cell><cell>36.17%</cell><cell>57.95%</cell></row><row><cell>Mixup [51]</cell><cell cols="2">19.12% 37.78%</cell><cell>24.05%</cell><cell>44.17%</cell><cell cols="2">34.27% 55.68%</cell><cell>38.61%</cell><cell>60.36%</cell></row><row><cell>AutoAug [13]</cell><cell cols="2">21.20% 41.26%</cell><cell>21.20%</cell><cell>41.26%</cell><cell cols="2">33.96% 55.81%</cell><cell>33.96%</cell><cell>55.81%</cell></row><row><cell>GAN Augmentation [38]</cell><cell cols="2">20.63% 39.77%</cell><cell>23.72%</cell><cell>43.67%</cell><cell cols="2">33.17% 54.59%</cell><cell>36.37%</cell><cell>58.88%</cell></row><row><cell>GenInt (ours)</cell><cell cols="3">22.07% 41.94% 25.71%</cell><cell>46.39%</cell><cell cols="2">34.47% 55.63%</cell><cell>39.21%</cell><cell>61.06%</cell></row><row><cell>GenInt with</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>40]</cell><cell>Original</cell></row></table><note>The mCE ? rate (the smaller the better) on ImageNet-C validation [22] set with 15 different corruptions. Our GenInt model, without training on any of the corruptions, reduces the mCE by up to 12.48%. From column 'Gauss.' to column 'JPEG,' we show individual Error Rate on each corruption method. Without adding similar corruptions in the training set, our generative causal learning approach learns models that naturally generalize to unseen corruptions.ImageNet-V2 Grouped by Sampling Strategy [</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>We show performance for ResNet50 trained only on BigGAN. Our intervention model surpasses performance of the best established benchmark<ref type="bibr" target="#b40">[38]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V. Le. Autoaugment: Learning augmentation policies from data, 2018. 2, 3, 6, 7, 19 [14] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. 2 [29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Yogesh Balaji, Carlos D. Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 3 Tseng, Geoffrey I. Webb, Bao Ho, Mohadeseh Ganji, and Lida Rashidi, editors, Advances in Knowledge Discovery and Data Mining, pages 349-360, Cham, 2018. Springer International Publishing. 2</figDesc><table><row><cell cols="2">CVPR09, 2009. 6 Chance Original Data IRM [8] [42] Bernhard Sch?lkopf. Causality for machine learning. arXiv Confounded Causal Test Accuracy Test Accuracy 10% 10% 99.45% 8.261% 87.32% 18.49% Observational CVAE [45] 59.949% 11.255%</cell><cell>[28] Imagenet classification with deep convolutional neural net-</cell></row><row><cell cols="2">[15] Terrance DeVries and Graham W. Taylor. Improved regular-preprint arXiv:1911.10500, 2019. 1 Interventional CVAE 58.478% 29.618%</cell><cell>works. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.</cell></row><row><cell cols="2">ization of convolutional neural networks with cutout, 2017. 2 [16] Fabio Henrique Kiyoiti dos Santos Tanaka and Claus [43] Patrick Schwab and Walter Karlen. Cxplain: Causal explana-tions for model interpretation under uncertainty. In Advances in Neural Information Processing Systems, pages 10220-</cell><cell>Weinberger, editors, Advances in Neural Information Pro-cessing Systems 25, pages 1097-1105. Curran Associates, Inc., 2012. 2, 3</cell></row><row><cell>Aranha. Data augmentation using gans, 2019. 3, 4 10230, 2019. 1</cell><cell></cell><cell>[30] Yanghao Li, Naiyan Wang, Jiaying Liu, and Xiaodi Hou.</cell></row><row><cell cols="2">[17] M. Frid-Adar, E. Klang, M. Amitai, J. Goldberger, and H. Greenspan. Synthetic data augmentation using gan for improved liver lesion classification. In 2018 IEEE 15th In-[44] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Ba-tra. Grad-cam: Visual explanations from deep networks via</cell><cell>Demystifying neural style transfer. In Proceedings of the 26th International Joint Conference on Artificial Intelli-gence, pages 2230-2236, 2017. 6, 21</cell></row><row><cell cols="2">ternational Symposium on Biomedical Imaging (ISBI 2018), gradient-based localization. International Journal of Com-</cell><cell>[31] Raha Moraffah, Kai Shu, Adrienne Raglin, and Huan Liu.</cell></row><row><cell>pages 289-293, 2018. 3, 4 puter Vision, 128(2):336-359, Oct 2019. 9</cell><cell></cell><cell>Deep causal representation learning for unsupervised do-</cell></row><row><cell></cell><cell></cell><cell>main adaptation, 2019. 2</cell></row><row><cell></cell><cell></cell><cell>[32] Suraj Nair, Yuke Zhu, Silvio Savarese, and Li Fei-Fei.</cell></row><row><cell></cell><cell></cell><cell>Causal induction from visual observations for goal directed</cell></row><row><cell></cell><cell></cell><cell>tasks, 2019. 2</cell></row><row><cell></cell><cell></cell><cell>[33] Judea Pearl. Causality: Models, reasoning, and inference,</cell></row><row><cell></cell><cell></cell><cell>2003. 1, 4, 14, 15</cell></row><row><cell></cell><cell></cell><cell>So-</cell></row><row><cell></cell><cell></cell><cell>ciety: Series B (Statistical Methodology), 78(5):947-1012,</cell></row><row><cell></cell><cell></cell><cell>Oct 2016. 1, 2</cell></row><row><cell></cell><cell></cell><cell>[36] Jonas Peters, Dominik Janzing, and Bernhard Schlkopf. Ele-</cell></row><row><cell></cell><cell></cell><cell>ments of Causal Inference: Foundations and Learning Algo-</cell></row><row><cell></cell><cell></cell><cell>rithms. The MIT Press, 2017. 4</cell></row><row><cell></cell><cell></cell><cell>[37] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-</cell></row><row><cell></cell><cell></cell><cell>vised representation learning with deep convolutional gen-</cell></row><row><cell>ing Representations, 2019. 2, 6, 7, 8</cell><cell>Learn-</cell><cell>erative adversarial networks. In Yoshua Bengio and Yann LeCun, editors, 4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May</cell></row><row><cell cols="2">[23] Erik H?rk?nen, Aaron Hertzmann, Jaakko Lehtinen, and Recognition,</cell><cell>2-4, 2016, Conference Track Proceedings, 2016. 2</cell></row><row><cell cols="2">Sylvain Paris. Ganspace: Discovering interpretable gan con-pages 819-828, 2020. 2 trols, 2020. 1, 2, 3, 5 [51] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and</cell><cell>[38] Suman Ravuri and Oriol Vinyals. Classification accuracy score for conditional generative models. In Advances in Neu-</cell></row><row><cell cols="2">[24] Maximilian Ilse, Jakub M. Tomczak, and Patrick Forr?. Designing data augmentation for simulating interventions, 2020. 1, 16 [25] Tommi Jaakkola and David Haussler. Exploiting generative models in discriminative classifiers. In In Advances in Neu-ral Information Processing Systems 11, pages 487-493. MIT David Lopez-Paz. mixup: Beyond empirical risk minimiza-tion. In International Conference on Learning Representa-tions, 2018. 2, 3, 6, 7, 19 [52] Xinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. In International Conference on Learning Representations, 2020. 2</cell><cell>ral Information Processing Systems 32, pages 12268-12279. Curran Associates, Inc., 2019. 2, 3, 4, 6, 7, 8, 19 [39] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generat-ing diverse high-fidelity images with vq-vae-2. In Advances in Neural Information Processing Systems, pages 14866-14876, 2019. 2</cell></row><row><cell cols="2">Press, 1998. 2 [26] Ali Jahanian*, Lucy Chai*, and Phillip Isola. On the "steer-ability" of generative adversarial networks. In International [53] Jun-Yan Zhu, Philipp Kr?henb?hl, Eli Shechtman, and Alexei A Efros. Generative visual manipulation on the nat-ural image manifold. In European conference on computer</cell><cell>[40] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to Im-</cell></row><row><cell cols="2">Conference on Learning Representations, 2020. 1, 2, 3, 5 vision, pages 597-613. Springer, 2016. 6</cell><cell></cell></row><row><cell cols="2">[27] Dominik Janzing. Causal regularization. In Advances</cell><cell></cell></row><row><cell cols="2">in Neural Information Processing Systems, pages 12704-</cell><cell></cell></row><row><cell>12714, 2019. 2</cell><cell></cell><cell></cell></row></table><note>5] Alessandro Achille and Stefano Soatto. Emergence of in- variance and disentanglement in deep representations. The Journal of Machine Learning Research, 19(1):1947-1980, 2018. 2 [6] Julian Alverio William Luo Christopher Wang Dan Gutfre- und Josh Tenenbaum Andrei Barbu, David Mayo and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In In Ad- vances in Neural Information Processing Systems 32, page 9448-9458, 2019. 1, 2, 3, 6 [7] Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks, 2017. 3, 4 [8] Martin Arjovsky, L?on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization, 2019. 1, 2, 12 [9] David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and Antonio Torralba. See- ing what a gan cannot generate. In Proceedings of the IEEE International Conference on Computer Vision, pages 4502- 4511, 2019. 6 [10] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthe- sis. In International Conference on Learning Representa- tions, 2019. 2, 5, 8, 18 [11] Krzysztof Chalupka, Pietro Perona, and Frederick Eberhardt. Visual causal feature learning. In Proceedings of the Thirty- First Conference on Uncertainty in Artificial Intelligence, UAI'15, page 181-190, Arlington, Virginia, USA, 2015. AUAI Press. 1, 2 [12] Shorten Connor and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal of Big Data, 6, 2019. 3 [13] Ekin D.[18] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2016. 6, 21 [19] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increas- ing shape bias improves accuracy and robustness. In Inter- national Conference on Learning Representations, 2019. 1, 2, 3, 6, 7, 19 [20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Commu- nications of the ACM, 63(11):139-144, 2020. 2 [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2016. 1, 2, 3, 6, 7, 18, 21 [22] Dan Hendrycks and Thomas Dietterich. Benchmarking neu- ral network robustness to common corruptions and perturba- tions. Proceedings of the International Conference on[34] Luis Perez and Jason Wang. The effectiveness of data aug- mentation in image classification using deep learning, 2017.2 [35] Jonas Peters, Peter B?hlmann, and Nicolai Meinshausen. Causal inference by using invariant prediction: identification and confidence intervals. Journal of the Royal StatisticalageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5389-5400, Long Beach, Califor- nia, USA, 09-15 Jun 2019. PMLR. 1, 2, 6, 7 [41] Swami Sankaranarayanan,[45] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional gen- erative models. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 3483-3491. Cur- ran Associates, Inc., 2015. 12 [46] Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenen- baum, and Phillip Isola. Rethinking few-shot image classifi- cation: a good embedding is all you need?, 2020. 2, 3, 8 [47] V. Vapnik. Principles of risk minimization for learning the- ory. In J. E. Moody, S. J. Hanson, and R. P. Lippmann, edi- tors, Advances in Neural Information Processing Systems 4, pages 831-838. Morgan-Kaufmann, 1992. 1, 2 [48] Hao Wang, Hao He, and Dina Katabi. Continuously indexed domain adaptation. In ICML, 2020. 3 [49] Yu-Xiong Wang, Ross Girshick, Martial Hebert, and Bharath Hariharan. Low-shot learning from imaginary data. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, Jun 2018. 3, 4 [50] Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan L Yuille, and Quoc V Le. Adversarial examples im- prove image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern[54] Xinyue Zhu, Yifan Liu, Jiahong Li, Tao Wan, and Zengchang Qin. Emotion classification with data augmentation using generative adversarial networks. In Dinh Phung, Vincent S.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">We show 10-way classification accuracy on the</cell></row><row><cell cols="2">Colored MNIST dataset. Color is a spurious correlation that</cell></row><row><cell cols="2">no longer holds during the causal test. Our generative inter-</cell></row><row><cell cols="2">vention strategy advances the state-of-the-art IRM method</cell></row><row><cell>by 11.12%</cell><cell></cell></row><row><cell>(a) Background 1</cell><cell>(b) Background 2</cell></row></table><note>Figure 8: Illustration of color MNIST dataset. For each digit category, we generate two different background colors. The feature background color is spuriously correlated to the category, where the confounder is us, the dataset creator. But the observed data is only color digits and corresponding targets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Type ImageNet Interventional Data Transferred Interventional Std. Augmentation Add. Augmentation</figDesc><table><row><cell></cell><cell>X</cell><cell>Xint</cell><cell>Data Xitr</cell><cell>top1</cell><cell>top5</cell><cell>top1</cell><cell>top5</cell></row><row><cell>Baseline</cell><cell>A</cell><cell></cell><cell></cell><cell cols="2">20.48% 40.64%</cell><cell>24.42%</cell><cell>44.39%</cell></row><row><cell>Ours</cell><cell>B</cell><cell></cell><cell></cell><cell cols="3">22.07% 41.94% 25.71%</cell><cell>46.39%</cell></row><row><cell>Ours</cell><cell>C</cell><cell></cell><cell></cell><cell cols="2">22.29% 41.76%</cell><cell>27.02%</cell><cell>47.51%</cell></row><row><cell>Ours</cell><cell>D</cell><cell></cell><cell></cell><cell cols="4">22.34% 41.65% 27.03% 48.02%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We sample an observational and intervention data from BigGAN with truncation t = 0.5<ref type="bibr" target="#b12">[10]</ref>. Please see supplementary material for full details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Pytorch imagenet tutorial</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Transforming and projecting images to class-conditional generative networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cycada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning and testing causal models with interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayadev</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantinos</forename><surname>Daskalakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saravanan</forename><surname>Kandasamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9447" to="9460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Transforming and projecting images to classconditional generative networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cycle consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cycada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Constantinos Daskalakis, and Saravanan Kandasamy. Learning and testing causal models with interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayadev</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnab</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9447" to="9460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Emergence of invariance and disentanglement in deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1947" to="1980" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian Alverio William Luo Christopher Wang Dan Gutfreund Josh Tenenbaum Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="9448" to="9458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Data augmentation generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4502" to="4511" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual causal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI&apos;15</title>
		<meeting>the Thirty-First Conference on Uncertainty in Artificial Intelligence, UAI&apos;15<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey on image data augmentation for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shorten</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">09</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Data augmentation using gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio Henrique Kiyoiti Dos</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santos</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Aranha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Synthetic data augmentation using gan for improved liver lesion classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frid-Adar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Klang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amitai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Greenspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="139" to="144" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ganspace: Discovering interpretable gan controls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Designing data augmentation for simulating interventions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forr?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Exploiting generative models in discriminative classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Haussler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 11</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the &quot;steerability&quot; of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12704" to="12714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<meeting><address><addrLine>Banff, AB, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-14" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deep causal representation learning for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raha</forename><surname>Moraffah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrienne</forename><surname>Raglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Causal induction from visual observations for goal directed tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Causality: Models, reasoning, and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The effectiveness of data augmentation in image classification using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>B?hlmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schlkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Classification accuracy score for conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="12268" to="12279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14866" to="14876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research. PMLR. 1, 2, 6</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on</title>
		<meeting>the IEEE Conference on</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Causality for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10500</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cxplain: Causal explanations for model interpretation under uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Karlen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10220" to="10230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grad-Cam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Rethinking few-shot image classification: a good embedding is all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Principles of risk minimization for learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>J. E. Moody, S. J. Hanson, and R. P. Lippmann</editor>
		<imprint>
			<publisher>Morgan-Kaufmann</publisher>
			<date type="published" when="1992" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Continuously indexed domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Katabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Low-shot learning from imaginary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adversarial examples improve image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="819" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Emotion classification with data augmentation using generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>Dinh Phung, Vincent S. Tseng, Geoffrey I. Webb, Bao Ho, Mohadeseh Ganji, and Lida Rashidi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="360" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
