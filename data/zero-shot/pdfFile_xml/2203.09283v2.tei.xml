<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PanoFormer: Panorama Transformer for Indoor 360?Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Shen</surname></persName>
							<email>zhjshen@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Lin</surname></persName>
							<email>cylin@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liao</surname></persName>
							<email>kangliao@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Nie</surname></persName>
							<email>nielang@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zishuo</forename><surname>Zheng</surname></persName>
							<email>zszheng@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
							<email>yzhao@bjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Information Science</orgName>
								<orgName type="institution">Beijing Jiaotong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Key Laboratory of Advanced Information Science and Network Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">PanoFormer: Panorama Transformer for Indoor 360?Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing panoramic depth estimation methods based on convolutional neural networks (CNNs) focus on removing panoramic distortions, failing to perceive panoramic structures efficiently due to the fixed receptive field in CNNs. This paper proposes the panorama transformer (named PanoFormer ) to estimate the depth in panorama images, with tangent patches from spherical domain, learnable token flows, and panorama specific metrics. In particular, we divide patches on the spherical tangent domain into tokens to reduce the negative effect of panoramic distortions. Since the geometric structures are essential for depth estimation, a self-attention module is redesigned with an additional learnable token flow. In addition, considering the characteristic of the spherical domain, we present two panorama-specific metrics to comprehensively evaluate the panoramic depth estimation models' performance. Extensive experiments demonstrate that our approach significantly outperforms the state-of-the-art (SOTA) methods. Furthermore, the proposed method can be effectively extended to solve semantic panorama segmentation, a similar pixel2pixel task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Depth information is important for computer systems to understand the real 3D world. Monocular depth estimation has attracted researchers' <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref> attention with its convenience and low cost, especially for panoramic depth estimation <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b33">34]</ref>, where the depth of the whole scene can be obtained from a single 360?image.</p><p>Since estimating depth from a single image is an ill-posed and inherently ambiguous problem, current solutions almost use powerful CNNs to extract explicitly or implicitly prior geometric to realize it <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>. However, when applied to panoramic tasks, these SOTA depth estimation solutions for perspective imagery <ref type="bibr" target="#b20">[21]</ref>   <ref type="figure">Fig. 1</ref>. We present PanoFomer to establish panoramic perception capability. The tangent-patch is proposed to remove panoramic distortions, and the token flows force the token positions to fit the structure of the sofa better. More details refer to Sec. 3 from panorama brings geometric distortions that challenge the structure perception. Specifically, distortions in panoramas (usually represented in equirectangular projection-ERP) increase from the center to both sides along the latitude direction, severely deforming objects' shapes. Due to the fixed receptive field, CNNs are inferior for dealing with distortions and perceiving geometric structures in panoramas <ref type="bibr" target="#b4">[5]</ref>. To deal with the distortions in panoramas, some researchers <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref> adopt the projection-fusion strategy. But this strategy needs to cover the domain gap between different projections, and the extra cross-projection fusion module increases computational burdens. Other researchers <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22]</ref> employ various distortion-aware convolution filters to make CNN-based depth estimation solutions adapt to 360?i mages. However, the fixed sampling positions still limit their performance. Pintore et al. <ref type="bibr" target="#b25">[26]</ref> focuses on the full geometric context of an indoor scene, proposing SliceNet but losing detailed information when reconstructing the depth map. We note that all the existing methods cannot perceive the distorted geometric structures with the fixed receptive field.</p><p>To address the above limitations, we propose the first panorama Transformer (PanoFormer) to enable the network's panoramic perception capability by removing distortions and perceiving geometric structures simultaneously (shown in <ref type="figure">Fig. 1</ref>). To make the Transformer suitable for panoramic dense prediction tasks (e.g., depth estimation and semantic segmentation), we redesign its structure. First, we propose a dense patches dividing method and handcrafted tokens to catch detailed features. Then, we design a relative position embedding method to reduce the negative effect of distortions, which utilizes a central token to locate the eight most relevant tokens to form a tangent patch (it differs from directly dividing patches on the ERP domain in traditional vision Transformers). To achieve this goal, we propose an efficient spherical token locating model (STLM) to guide the 'non-distortion' token sampling process on the ERP domain directly by building the Transformations among the three domains (shown in <ref type="figure" target="#fig_4">Fig. 3</ref>). Subsequently, we design a Panoramic Structure-guided Transformer (PST) block to replace the traditional block in a hierarchical architecture. Specifically, we redesign the self-attention module with additional learnable weight to push token flow, so as to flexibly capture various objects' structures. This module encourages the PanoFormer to further perceive geometric structures effec-tively. In this way, we establish our network's perception capability to achieve panoramic depth estimation. Moreover, the proposed PST block can be applied to other learning frameworks as well.</p><p>Furthermore, current evaluation metrics for depth estimation are suitable for perspective imagery. However, these metrics did not consider distortions and the seamless boundary property in panoramas. To comprehensively evaluate the depth estimation for panoramic images, we design a Pole Root Mean Square Error (P-RMSE) and Left-Right Consistency Error (LRCE) to measure the accuracy on polar regions and depth consistency around the boundaries, respectively.</p><p>Extensive experiments demonstrate that our solution significantly outperforms SOTA algorithms in panoramic depth estimation. Besides, our solution achieves the best performance when applied to semantic segmentation, which is also a pixel2pixel panoramic task. The contributions of this paper are summarized as follows:</p><p>-We present PanoFormer , the first panorama Transformer, to establish the panoramic perception capability by reducing distortions and perceiving geometric structures for the panoramic depth estimation task. -We propose a PST block that divides patches on the spherical tangent domain and reshapes the self-attention module with the learnable token flow. Moreover, the proposed block can be applied in other learning frameworks. -Considering the difference between Panorama and normal images, we design two new panorama-specific metrics to evaluate the panoramic depth estimation. -Experiments demonstrate that our method significantly outperforms the current state-of-the-art approaches on all metrics. The excellent panorama semantic segmentation results also prove the extension ability of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Panoramic Depth Estimation</head><p>There are two main fusion methods to reduce distortions while estimating depth on ERP maps. One is the equirectangular-cube fusion method represented by Bifuse <ref type="bibr" target="#b33">[34]</ref>, and the other is the dual-cube fusion approach described by Shen <ref type="bibr" target="#b27">[28]</ref>. Specifically, Bifuse <ref type="bibr" target="#b33">[34]</ref> propose a two-branch method of fusing equirectangular projection and cube projection, which improves the tolerance of the model to distortions. Moreover, UniFuse <ref type="bibr" target="#b16">[17]</ref> also uses a dual projection fusion scheme only at the encoding stage to reduce computation cost. Noting that the single-cube projection method produces significant discontinuities at the cube boundary, Shen et al <ref type="bibr" target="#b27">[28]</ref> proposed a dual-cube approach based on a 45?rotation to reduce distortions. This class of methods can attenuate the negative effect of distortions, but they need to repeatedly change the projection for fusion, increasing the model's complexity.   To apply depth estimation models of normal images to panoramas, Tateno et al. <ref type="bibr" target="#b32">[33]</ref> obtained exciting results by designing distortion-aware convolution filters to expand the perceptual field. Zioulis et al. <ref type="bibr" target="#b44">[45]</ref> demonstrated that monocular depth estimation models trained on conventional 2D images produce lowquality results, highlighting the necessity of learning directly on the 360?domain. Jin et al. <ref type="bibr" target="#b17">[18]</ref> demonstrated the effectiveness of geometric prior for panoramic depth estimation. Chen et al. <ref type="bibr" target="#b4">[5]</ref> used strip pooling and deformable convolution to design a new encoding structure for accommodating different degrees of distortions. Moreover, Pintore et al. <ref type="bibr" target="#b25">[26]</ref> proposed SliceNet, a network similar to HorizonNet <ref type="bibr" target="#b30">[31]</ref>, which uses a bidirectional Long Short-Term Memory (LSTM) to model long-range dependencies. However, the slicing method ignores the latitudinal distortion property and thus cannot accurately predict the depth near the poles. Besides, <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref> proved that on large-scale datasets, Transformer-based depth estimation for normal images are superior to CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformer</head><p>Unlike CNN-based networks, the Transformer has the nature to model longrange dependencies by global self-attention <ref type="bibr" target="#b26">[27]</ref>. Inspired by ViT <ref type="bibr" target="#b10">[11]</ref>, researchers have designed many efficient networks that have the advantages of both CNNs and Transformers. To enhance local features extraction, convolutional layers are added into muti-head self-attention (CvT <ref type="bibr" target="#b35">[36]</ref>) and feed-forward network (FFN) (CeiT <ref type="bibr" target="#b41">[42]</ref>, LocalViT <ref type="bibr" target="#b22">[23]</ref>) is replaced by locally-enhanced feed-forward network (LeFF) (Uformer <ref type="bibr" target="#b34">[35]</ref>). Besides, CvT <ref type="bibr" target="#b35">[36]</ref> demonstrates that the padding operation in CNNs implicitly encodes position, and CeiT <ref type="bibr" target="#b41">[42]</ref> proposes the imageto-tokens embedding method. Inspired by SwinT <ref type="bibr" target="#b23">[24]</ref>, Uformer <ref type="bibr" target="#b34">[35]</ref> proposes a shifted windows-based multi-head attention mechanism to improve the efficiency of the model. But all these solutions are developed based on normal FoV images, which cannot be applied to panoramic images directly. Based on these previous works, we further explore suitable Transformer structure for panoramic images and adapt it to the dense prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PanoFomer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture Overview</head><p>Our primary motivation is to make the Transformer suitable for pixel-level omnidirectional vision tasks by redesigning the standard components in conventional Transformers. Specifically, we propose a pixel-level patch division strategy, a relative position embedding method, and a panoramic self-attention mechanism. The proposed pixel-level patch division strategy is to enhance local features and improve the ability of Transformers to capture detailed features. For position embedding, we renounce the conventional absolute position embedding method and get the position of other related tokens on the same patch by the central token (described in 3.4). This method not only eliminates distortions, but also provides position embedding. Furthermore, we establish a learnable flow in the panorama self-attention module to perceive panoramic structures that are essential for depth estimation.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 2</ref>, the PanoFomer is a hierarchical structure with five major parts: input stem, output stem, encoder, decoder and bottleneck. For the input stem, a 3?3 convolution layer is adopted with size H?W to form the features with dimension C. Then the features are fed into the encoder. There are four hierarchical stages in encoder and decoder, and each of them contains a position embedding, two PST blocks (sharing the same settings), and a convolution layer. Specifically, a 4?4 convolution layer is adopted for increasing dimension and down-sampling in the encoder, while a 2?2 transposed convolution layer is used in the decoder for decreasing dimension and up-sampling. Finally, the output features from the decoder share the same resolution and dimension as the features from the input stem. Furthermore, the output stem, implemented by a 3?3 convolution, is employed to recover the depth map from features. More specifically, the number of heads is sequentially set as [Encoder:1, 2, 4, 8; Bottleneck: 16; Decoder: <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b1">2]</ref>. As for all padding operations in convolution layers, we utilize circular padding for both horizontal sides of the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer-customized Spherical Token</head><p>In vision Transformers, the input image is first divided into patches of the same size. For example, ViT <ref type="bibr" target="#b10">[11]</ref> divides the input image into patches with size of 16?16 to reduce the computational burden. Then, these patches are embedded as tokens in a learning-based way via a linear layer. However, this strategy loses much detailed information, which is a fatal drawback for dense prediction tasks, such as depth estimation. To overcome this issue, we propose a pixel-level patches dividing method.</p><p>First, the input features are divided into pixel-level patches, which means each sampling position in the features corresponds to a patch centered on it. Such a dense division strategy allows the network to learn more detailed features, which is beneficial for dense prediction tasks. Furthermore, we make each patch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relative Position Embedding</head><p>Inspired by the cube projection, we note that the spherical tangent projection can effectively remove the distortion (see Supplementary Materials for proof). Therefore, we propose STLM to initialize the position of related tokens. Unlike the conventional Transformers (e.g., ViT <ref type="bibr" target="#b10">[11]</ref>), which directly adds absolute position encoding to the features, we "embed" the position information via the central token. Firstly, the central token is projected from the ERP domain to the spherical domain; then, we use the central token to look up the position of eight nearest neighbors on the tangent plane; finally, these positions are all projected back to the ERP domain (the three steps are represented by yellow arrows in <ref type="figure" target="#fig_4">Fig. 3</ref>). We call patches formed in this way as tangent patches. To facilitate locating the related tokens in the ERP domain, we further establish the relationship among the three domains (illustrated in <ref type="figure" target="#fig_4">Fig. 3)</ref>. Tangent domain to spherical domain: Let the unit sphere be S 2 , and S(0, 0) = (? 0 , ? 0 ) ? S 2 is the spherical coordinate origin. ?S(x, y) = (?, ?) ? S 2 , we can obtain other 8 points (related tokens) around it (current token) on the spherical domain.</p><formula xml:id="formula_0">S(?1, 0) =(? ? ??, ?) S(0, ?1) =(?, ? ? ??) S(?1, ?1) =(? ? ??, ? ? ??)<label>(1)</label></formula><p>where (?, ?) denotes the unit spherical coordinates, and ? ? (??, ?), ? ? (? ? 2 , ? 2 ); ??,?? is the sampling step size. By the geocentric projection <ref type="bibr" target="#b24">[25]</ref>, we can calculate the local coordinates (T (x, y))of the sampling point in tangent domain <ref type="bibr" target="#b8">[9]</ref> (the current token in tangent domain is represented as T (0, 0) = T (?, ?) = (0, 0)):</p><formula xml:id="formula_1">T (? ? ??, ?) =(? tan ??, 0) T (?, ? ? ??) =(0, ? ? tan ??) T (? ? ??, ? ? ??) =(? tan ??, ? sec ?? tan ??)<label>(2)</label></formula><p>By applying the inverse projection described in <ref type="bibr" target="#b8">[9]</ref>, we can get the position of all tokens of a tangent patch in the spherical domain. Spherical domain to ERP domain: Furthermore, by utilizing the projection equation <ref type="bibr" target="#b27">[28]</ref>, we can get the position of each tangent patch in the ERP domain. This whole process is named Spherical Token Locating Model (STLM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Panorama Self-Attention with Token Flow</head><p>Based on the traditional vision Transformer block, we replace the original attention mechanism with panorama self-attention. To further enhance local features interaction, we replace FFN with LeFF <ref type="bibr" target="#b41">[42]</ref> for our pixel-level depth estimation task. Specifically, as illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>, when the features f ? R C?H?W with a height of H and a width of W are fed into PST block, they are flattened and reshaped as f ? ? R N ?C , where N = H ?W . Then a fully connected layer is applied to obtain query Q ? R N ?d and value V ? R N ?d , where d = C/M , and M is the head number. The Q and V will pass through three parallel branches for computing attention score (A ? R N ?9 ), token flows (?s ? R N ?18 ), and re-sampling features. In the top branch, a full connection layer is adopted to get attention weights W A ? R N ?9 from Q, and then softmax is employed to calculate the attention score A. In the middle branch, another fully connection layer is used to learn a token flow ?s and it is further reshaped to ?s ? ? R d?H?W ?9?2 , sharing the same dimension with? (the initialed position from the STLM). Moreover, ?s ? and? are added together to calculate the final token positions. In the bottom branch, the value V is reshaped to V ? ? R C?H?W and are sampled to form </p><formula xml:id="formula_2">PSA(f,?) = M m=1 W m * H?W q=1 9 k=1 A mqk ? W ? m f (? mqk + ?s mqk ) ,<label>(3)</label></formula><p>where? = ST LM (f ), and ST LM (?) denotes the spherical token locating model; m indexes the head of self-attention, M is the whole heads, q index the current point (token), k indexes the tokens in a tangent patch, ?s mqk is the learned flow of each token, A mqk represents the attention weight of each token, and W m and W ? m are normal learnable weights of each head. From the above process, we can see that the final positions of the tokens are determined by two steps: position initialization from STLM and additional learnable flow. Actually, the initialized position realizes the division of the tangent patch (described in 3.4) and removes the panoramic distortion. Furthermore, the learnable flow exhibits a panoramic geometry by adjusting the spatial distribution of tokens. To verify the effectiveness of the token flow, we visualize all tokens from the first PST block in <ref type="figure">Fig. 5</ref>. It can be observed that this additional flow provides the network with clear scene geometric information, which helps the network to estimate the panorama depth with the structure as a clue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Objective Function</head><p>For better supervision, we combine reverse Huber <ref type="bibr" target="#b13">[14]</ref> (or Berhu <ref type="bibr" target="#b20">[21]</ref>) loss and gradient loss <ref type="bibr" target="#b27">[28]</ref> to design our objective function as commonly used in previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref>. In our objective function, the Berhu loss ? ? can be written as:</p><formula xml:id="formula_3">? ? (g, p) = |g ? p| for |g ? p| ? ? |(g?p) 2 |+? 2 2? otherwise (4)</formula><p>where g, p denote the ground truth and predicted values, respectively. Similar to SliceNet <ref type="bibr" target="#b25">[26]</ref>, we apply gradient loss to Berhu loss. To obtain depth edges, we use two convolution kernels to obtain gradients in horizontal and vertical directions, respectively. They are represented as K h and K v , where K h = [-1 0 1, -2 0 2, -1 0 1], and K v = (K h ) T . Denote the gradient function as G, the horizontal gradient I h and vertical gradient I v of the input image I can be expressed as I h = G(K h , I) and I v = G(K v , I), respectively. In this paper, ? = 0.2 and the final objective function can be written as</p><formula xml:id="formula_4">? f inal = ? 0.2 (g, p) + ? 0.2 (G(K h , g), G(K h , p)) + ? 0.2 (G(K v , g), G(K v , p)), (5)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Panorama-specific Metrics</head><p>Rethinking the spherical domain, we note that two significant properties cannot be neglected: the spherical domain is continuous and seamless everywhere; the distortion in the spherical domain is equal everywhere. For the first issue, we propose LRCE to measure the depth consistency of left-right boundaries. For the second issue, since distortions on ERP maps vary in longitude, RMSE cannot visually reflect the model's ability to adapt to distortions. Therefore, we provide P-RMSE to focus on the regions with massive distortions to verify the model's panoramic perception capability. Pole Root Mean Square Error. Cube projection is a special spherical tangent projection format that projects the sphere onto the cube's six faces. The top and bottom faces correspond to the polar regions of the spherical domain, so we select the two parts to design P-RMSE (illustrated in <ref type="figure" target="#fig_7">Fig. 6</ref>). Define the function of converting ERP to Cube as E2C(?), the converted polar regions of the ERP image E can be expressed as Select(E2C(E), T, B), where T, B represent the top and bottom parts, respectively. The error C e between the ground truth GT and the predicted depth map P at the polar regions can be expressed as</p><formula xml:id="formula_5">C e = Select(E2C(GT ), T, B) ? Select(E2C(P ), T, B)<label>(6)</label></formula><p>The final P-RMSE can be written as <ref type="bibr" target="#b6">7)</ref> where N Ce is the number of values in C e .</p><formula xml:id="formula_6">P-RMSE = 1 N Ce N Ce i=1 |C i e |<label>(</label></formula><p>Left-Right Consistency Error. We can evaluate the depth consistency of the left-right boundaries by calculating the horizontal gradient between the both sides of the depth map. Define that the horizontal gradient G H E of the image E can be written as</p><formula xml:id="formula_7">G H E = E col f irst ?E col last ,</formula><p>where E col f irst / E col last represent the values in the first/last columns of the image E. But consider an extreme case where if the edge of an object in the scene happens to be on the edge of the depth map, then there is ambiguity in reflecting continuity only by G H E . We cannot tell whether this discontinuity is real or caused by the model. Therefore, we add ground truth to our design. The horizontal gradient of ground truth and the predicted depth map are denoted as G H GT and G H P (where G H GT = GT col f irst ? GT col last , G H P = P col f irst ? P col last ), respectively. The final expression can be as follows:</p><formula xml:id="formula_8">LRCE = 1 N error Nerror i=1 |error i |<label>(8)</label></formula><p>where error = G H GT ? G H P and N error is the number of values in error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In the experimental part, we compare the state-of-the-art approaches on four popular datasets and validate the effectiveness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Implementations</head><p>Four datasets are used for our experimental validation, they are Stanford2D-3D <ref type="bibr" target="#b0">[1]</ref>, Matterport3D <ref type="bibr" target="#b3">[4]</ref>, PanoSUNCG <ref type="bibr" target="#b28">[29]</ref> and 3D60 <ref type="bibr" target="#b44">[45]</ref>. Stanford2D3D and Matterport3D are two real-world datasets. They were rendered from a common viewpoint. Previous work used a dataset that was rendered only on the equator and its surroundings, ignoring the area near the poles, which undermined the integrity of the panorama. We strictly follow the previous works and employ the official datasets (Notice that the Stanford2D3D and Matterport3D that are contained in 3D60 have a problem that the light in the scenarios will leak the depth information). PanoSUNCG is a virtual panoramic dataset. And 3D60 is an updated version of 360D (360D is no longer available now). It consists of data from the above three datasets. There is a gap between the distributions of these three datasets, which makes the dataset more responsive to the model's generalizability. Note that we divide the dataset as the previous work and eliminate the samples that failed to render <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>In the implementation, we conduct our experiments on two GTX 3090 GPUs, and the batch size is set to 4. We choose Adam <ref type="bibr" target="#b19">[20]</ref> as the optimizer and keep the default settings. The initialized learning rate is 1 ? 10 ?4 . The number of parameters of our model is 20.37 M. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison Results</head><p>We selected the metrics used in previous work and the two proposed metrics for the quantitative comparison, including RMSE, ?(1.25, 1.25 2 , 1.25 3 ) and panorama-specific metrics, LRCE and P-RMSE (We cannot calculate the proposed new metrics due to limitation of the two real-world datasets). RMSE reflects the overall variability. ? exhibits the difference between ground truth and the predicted depth.</p><p>Quantitative Analysis.  <ref type="figure">Fig. 8</ref>. Visualization of the new metrics' comparison between our method and Unifuse <ref type="bibr" target="#b16">[17]</ref>. (a) We stitch the ERP results to observe the depth consistency. (b) We project the areas with massive distortions to cube face to compare the models' performance PanoSUNCG, there is a 42% improvement on RMSE. But there is just a 16% improvement on 3D60 dataset with RMSE. The improvement is not particularly significant compared to the other three datasets because 3D60 dataset is more extensive, the difference between the models is not obvious. The improvement on ? performance further demonstrates that our model can obtain more accurate prediction results. On the new metric P-RMSE, we achieved an average gain of about 40% on the other two virtual datasets. It indicates that our model is more resilient to the distortion in panoramas. In addition, on LRCE, our model outperforms 40% on PanoSUNCG and 12% on 3D60, showing that our model can better constrain the depth consistency of the left-right boundaries in panoramas, because our network fully considers the seamless property of the sphere. Qualitative Analysis. <ref type="figure">Fig. 7</ref> shows the qualitative comparison with the current SOTA approaches. From the figures, we can observe that SliceNet is relatively accurate in predicting regions without distortion. However, the model performance degrades dramatically in regions with distortions or large object deformations.</p><p>Although SliceNet can efficiently focus on the global panoramic structures, the depth reconstruction process cannot accurately recover the details, which affects the model's performance. UniFuse can deal with deformation effectively, but it still suffers from incorrect estimating and tends to lose detailed information. From <ref type="figure">Fig. 8</ref>, we can observe that our results are very competitive at boundary and pole areas. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation study</head><p>With the same conditions, we validated the key components of our model by ablation study on Stanford2D3D (real-world dataset, small-scale, challenging). As illustrated in <ref type="table" target="#tab_2">Table 2</ref>, a presents the baseline structure that we use convolutional layers to replace PST blocks; Our network with the traditional attention mechanism is expressed with b; c indicates our attention module without token flow; Our entire network is shown as d. Transformer vs. CNN. From <ref type="table" target="#tab_2">Table 2</ref>, we can observe that the Transformer gains 35% improvements over CNNs in terms of RMSE. Furthermore, qualitative results in b are more precise than the CNNs. Essentially, CNNs are a special kind of self-attention. Since the convolutional kernel is fixed, it requires various components or structures or even deeper networks to help the model learn the data patterns. On the other hand, the attention in Transformer is more flexible, and it is relatively easier to learn the patterns. Effectiveness of Tangent-patches for Transformer. To illustrate the effectiveness of the tangent-patch dividing method, we compared an alternative attention structure that currently performs SOTA in vision Transformers. From <ref type="table" target="#tab_2">Table 2</ref>, our network with tangent-patches (c) outperforms the attention mechanism (b) with 21% on RMSE, 10% on P-RMSE and 12% on LRCE. It proves that tangent-patch can help networks deal with panoramic distortions. Effectiveness of Token Flow. Since the geometric structures are essential for depth estimation, we add the additional token flows to perceive geometric structures. The results in <ref type="table" target="#tab_2">Table 2</ref> show that our model with the token flow can make P-RMSE more competitive. In <ref type="figure">Fig. 9</ref>, we can observe that the token flow allows the model to estimate the depth details more accurately.</p><p>RGB GT a b c d <ref type="figure">Fig. 9</ref>. Qualitative comparison of ablation study. a, b, c, d are the same as <ref type="table" target="#tab_2">Table 2</ref> 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>.4 Extensibility</head><p>We also validate the extensibility of our model by the panoramic segmentation that is also a pixel2pixel task. We did not change any structure of our network and strictly followed the experimental protocol in <ref type="bibr" target="#b31">[32]</ref>. As listed in <ref type="table" target="#tab_3">Table 3</ref>, the experimental results show that our model outperforms the current SOTA approaches. Due to page limitations, more qualitative comparisons and the results with a high resolution can be found in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose the first panorama Transformer (PanoFormer) for indoor panoramic depth estimation. Unlike current approaches, we remove the negative effect of distortions and further model geometric structures by using learnable token flow to establish the network's panoramic perceptions. Concretely, we design a PST block, which can be effectively extended to other learning frameworks. To comprehensively measure the performance of the panoramic depth estimation models, we propose two panorama-specific metrics based on the priors of equirectangular images. Experiments demonstrate that our algorithm significantly outperforms current SOTA methods on depth estimation and other pixel2pixel panoramic tasks, such as semantic segmentation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>show a dramatic degradation because the 360?field-of-view (FoV) arXiv:2203.09283v2 [cs.CV] 12 Jul 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Our PanoFormer takes a monocular RGB panoramic image as the input and outputs the high-quality depth map</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>consist of 9 features at different positions (one central position and eight Spherical Token Locating Model (STLM): locate related tokens on ERP domain. 1: tangential domain of unit sphere to spherical domain; 2: spherical domain to ERP domain surrounding positions, illustrated in Fig. 3 left) to balance the computational burden. Unlike standard Transformers that embed patches as tokens by a linear layer, our tokens are handcrafted. We define the features at the central position as the central token and those from the other 8 surrounding positions as the related tokens. The central token can determine the position of related tokens by looking up the eight most relevant tokens among the features. To remove distortion and embed position information for the handcrafted tokens, we propose a distortion-based relative position embedding method in Sec. 3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The proposed PST Block can remove the negative effect of distortions and perceive geometric structures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>2 RGBFig. 5 .</head><label>25</label><figDesc>Visualization of the token flows from the first PST block, which suggest the panoramic structures the divided patches (described in 3.1) by looking up the related tokens in the final token positions. Afterward, the PSA can be represented as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>P-RMSE: calculate the RMSE of the polar regions</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative comparisons on Matterport3D, Stanford2D3D, PanoSUNCG and 3D60 Datasets.</figDesc><table><row><cell>Classic metrics</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1shows the quantitative comparison results with the current SOTA monocular panoramic depth estimation solutions on the four popular datasets. As shown in the table, our model achieves the first place in all metrics. In particular, the RMSE metric of our model achieves a 16% improvement on Stanford2D3D, 26% on Matterport3D. Even on the virtual dataset</figDesc><table><row><cell></cell><cell>RGB</cell><cell>GT</cell><cell>Ours</cell><cell>UniFuse</cell><cell cols="2">SliceNet</cell></row><row><cell cols="7">Fig. 7. Qualitative results on Matterport3D, Stanford2D3D, PanoSUNCG, and 3D60.</cell></row><row><cell cols="5">More results can be found in Supplementary Materials</cell><cell></cell><cell></cell></row><row><cell>(a)</cell><cell>RGB</cell><cell>GT</cell><cell></cell><cell>Ours</cell><cell>Unifuse</cell><cell></cell></row><row><cell>Origin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Roll</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell>Top</cell><cell>Origin</cell><cell>Bottom</cell><cell>Top</cell><cell>Origin</cell><cell>Bottom</cell></row><row><cell>RGB</cell><cell></cell><cell></cell><cell>Unifuse</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GT</cell><cell></cell><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study. We trained on Stanford2D3D for 70 epochs. a is the baseline structure developed with CNNs</figDesc><table><row><cell cols="4">Index Transformer STLM Token Flow RMSE P-RMSE LRCE</cell></row><row><cell>a</cell><cell>0.6704</cell><cell>0.2258</cell><cell>0.2733</cell></row><row><cell>b</cell><cell>0.4349</cell><cell>0.2068</cell><cell>0.2155</cell></row><row><cell>c</cell><cell>0.3739</cell><cell>0.1825</cell><cell>0.1916</cell></row><row><cell>d</cell><cell cols="3">0.3366 0.1793 0.1784</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Quantitative comparison for semantic segmentation on Stanford2D3D. Results are averaged over the official 3 folds<ref type="bibr" target="#b31">[32]</ref> </figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">mIoU? mAcc?</cell></row><row><cell></cell><cell>TangentImg [12]</cell><cell>41.8</cell><cell>54.9</cell></row><row><cell>Stanford2D3D</cell><cell>HoHoNet [32]</cell><cell>43.3</cell><cell>53.9</cell></row><row><cell></cell><cell>Ours</cell><cell>48.9</cell><cell>64.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work was supported by the National Key R&amp;D Program of China (No.2021ZD0112100), and the National Natural Science Foundation of China (Nos. 62172032, U1936212, 62120106009).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Joint 2d-3d-semantic data for indoor scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01105</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adabins: Depth estimation using adaptive bins</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4009" to="4018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09402</idno>
		<title level="m">Monocular depth estimation: A survey</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Matterport3d: Learning from rgb-d data in indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="667" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Distortion-aware monocular depth estimation for omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="334" to="338" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cube padding for weakly-supervised saliency prediction in 360 videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Omnidirectional depth extension networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="589" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spherenet: Learning spherical representations for detection and classification in omnidirectional images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Coors</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Condurache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="518" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tangent images for mitigating spherical distortion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12426" to="12434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel approach to quantized matrix completion using huber loss measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Esmaeili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marvasti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="341" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niessner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02039</idno>
		<title level="m">Spherical cnns on unstructured grids</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unifuse: Unidirectional fusion for 360 panorama depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1519" to="1526" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Geometric structure based and regularized depth estimation from 360 indoor imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="889" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Geometry aware convolutional filters for omnidirectional images representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Khasanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spherephd: Applying cnns on a spherical polyhedron representation of 360deg images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9181" to="9189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">F</forename><surname>Pearson</surname></persName>
		</author>
		<title level="m">Map Projections: Theory and Applications</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Slicenet: deep dense depth estimation from a single indoor panorama using a slice-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pintore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Agus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gobbetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11536" to="11545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distortion-tolerant monocular depth estimation on omnidirectional images using dual-cubemap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1746" to="1754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kernel transformer networks for compact spherical convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9442" to="9451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Horizonnet: Learning room layout with 1d representation and pano stretch data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1047" to="1056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hohonet: 360 indoor holistic understanding with latent horizontal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2573" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Distortion-aware convolutional filters for dense prediction in panoramic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tateno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="707" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bifuse: Monocular 360 depth estimation via bi-projection fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="462" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">CVT: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Snap angle prediction for 360 panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Spherical dnns and their applications in 360?images and videos. IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Multi-modal masked pre-training for monocular panoramic depth completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.09855</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13802</idno>
		<title level="m">Rignet: Repetitive image guided network for depth completion</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Flat2sphere: Learning spherical convolution for fast features from 360 imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kristen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Neural Information Processing Systems (NIPS</title>
		<meeting>International Conference on Neural Information Processing Systems (NIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="579" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving 360 monocular depth estimation via non-local dense prediction transformer and joint supervised and self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rhee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="3224" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Omnidepth: Dense depth estimation for indoors spherical panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zioulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karakottas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zarpalas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="448" to="465" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
