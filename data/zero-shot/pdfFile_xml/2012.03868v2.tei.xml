<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Handwritten Paragraph Text Recognition Using a Vertical Attention Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Coquenet</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Chatelain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Paquet</surname></persName>
						</author>
						<title level="a" type="main">End-to-end Handwritten Paragraph Text Recognition Using a Vertical Attention Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Seq2Seq model</term>
					<term>Hybrid attention</term>
					<term>Segmentation-free</term>
					<term>Paragraph handwriting recognition</term>
					<term>Fully Convolutional Network</term>
					<term>Encoder-decoder</term>
					<term>Optical Character Recognition !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unconstrained handwritten text recognition remains challenging for computer vision systems. Paragraph text recognition is traditionally achieved by two models: the first one for line segmentation and the second one for text line recognition. We propose a unified end-to-end model using hybrid attention to tackle this task. This model is designed to iteratively process a paragraph image line by line. It can be split into three modules. An encoder generates feature maps from the whole paragraph image. Then, an attention module recurrently generates a vertical weighted mask enabling to focus on the current text line features. This way, it performs a kind of implicit line segmentation. For each text line features, a decoder module recognizes the character sequence associated, leading to the recognition of a whole paragraph. We achieve state-of-the-art character error rate at paragraph level on three popular datasets: 1.91% for RIMES, 4.45% for IAM and 3.59% for READ 2016. Our code and trained model weights are available at https://github.com/FactoDeepLearning/VerticalAttentionOCR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O FFLINE handwritten text recognition consists in recognizing the text in an image scanned from a document. An image of a word, a line or a paragraph of text, or even a full document is analyzed, and the sequence of characters that composes the text is expected as output. This paper focuses on paragraph text recognition based on an end-to-end segmentation-free neural network. Indeed, while line segmentation and handwriting recognition have been studied for decades now, they remain a challenging task. Moreover, they have rarely been studied and optimized together in one single trainable system.</p><p>Historically, early works have applied segmentation at character level and each character was then classified. Later on, segmentation was applied at word level first, then at line level. But whatever the segmentation level considered, the problem related to the definition of the individual entities to be segmented remains ambiguous. In this study, we go a step further and process a paragraph of text without any explicit segmentation step, neither during training nor during decoding. However, pretraining on line images is used to improve the convergence and the recognition accuracy.</p><p>When switching from line images to paragraph images, one faces new challenges. Indeed, the model must include a way to locate the text regions (word or line for example) and to order them. This brings several new difficulties: To carry out the recognition of whole page images, current approaches still rely on a two-step approach. In a first step, the document is segmented into text regions, which are then recognized in a second step. The recognition is performed applying an optical model, thus the name Optical Character Recognition (OCR). Taken separately, each of these two steps brings rather good results but if considered together they show three major drawbacks. Firstly, they require ground truth segmentation labels as well as transcription labels at line level, which is very costly to produce by hand. Secondly, this two-step approach accumulates the errors of each individual step: segmentation errors induce OCR errors, while the OCR stage produces its own errors. And finally, a two-step strategy implies that any modification of one stage should lead to retraining both stages so as to optimally combine both stages in the whole model. In addition, using a prior explicit segmentation step raises the question of the definition of a line. Baseline, X-height or bounding boxes are examples of target labels for segmentation that have been frequently used in the literature, all with their pros and cons <ref type="bibr" target="#b0">[1]</ref>. It is also frequent to find variability in labels from one annotator to another. We can also highlight that the reading order is defined by hand, based on the coordinates of the text regions; this could lead to some errors in the case of rather slanted lines.</p><p>This paper aims at providing a model freed from all of these constraints. We suggest using a segmentation-free model that processes whole handwritten paragraphs using an attention process. In this model, character recognition and implicit line segmentation are learned in an end-to-end fashion, so as to optimize both processes altogether. Most of the contributions of the literature have successfully used neural networks for line segmentation and text line recognition as well, reaching state-of-the-art results. In addition, Attention Neural Networks have been successfully applied for many other tasks such as translation <ref type="bibr" target="#b1">[2]</ref>, speech recognition <ref type="bibr" target="#b2">[3]</ref>, image captioning <ref type="bibr" target="#b3">[4]</ref> or even OCR applied at line level <ref type="bibr" target="#b4">[5]</ref>. This leads us to think that both tasks (segmentation and recognition) could be handled by a single neural network with attention as a control block.</p><p>In this paper, we propose an encoder-decoder architecture using a hybrid (content-based and location-based) attention mechanism to process whole paragraph images. The idea is to recurrently recognize the lines so as to remain in a one-dimensional sequence alignment problem between the recognized text lines and their ground truth transcription. This alignment is achieved during training using the standard Connectionist Temporal Classification (CTC) loss <ref type="bibr" target="#b5">[6]</ref>. To this end, we first use an encoder to extract two-dimensional features from the input paragraph images. These features account for the characters while preserving their location. The text line location and their reading order are both handled and learned by the attention module. This attention mechanism aims at focusing on specific feature rows to generate text line representations. They are generated recurrently, one at a time, from the first one to the last one, through the attention mechanism. The decoder recognizes a sequence of characters from each text line representation. A whitespace character is inserted between each recognized text line to get the final paragraph.</p><p>In brief, we make the following contributions:</p><p>? We propose the Vertical Attention Network: a novel encoder-decoder architecture using hybrid attention for text recognition at paragraph level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>The approach relies on an implicit line segmentation performed in the latent space of a deep model. <ref type="bibr">?</ref> We achieve state-of-the-art results on RIMES, IAM and READ 2016 datasets compared to paragraph-level approaches. <ref type="bibr">?</ref> We compare favorably this architecture with a standard two-step approach based on line segmentation followed by character recognition.</p><p>This paper is organized as follows. Related works are presented in Section 2. Section 3 is dedicated to the presentation of the proposed architecture. Section 4 is devoted to the experimental environment. It provides, inter alia, description of datasets, training and implementation details. Experiments and results are detailed in Section 5. We carried out extensive experiments in Section 6. Section 7 provides a discussion of the model and we draw conclusions in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>In the literature, only very few works have been devoted to multi-line text recognition, and most studies have concentrated on isolated lines recognition. We can classify these pioneer works into two categories: those using an explicit word/line segmentation, requiring segmentation and transcription labels, and those without any segmentation, only requiring transcription labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approaches using explicit line segmentation</head><p>Explicit line segmentation approaches are two-step methods that sequentially detect the text lines of a paragraph, and then proceed to their recognition. To our knowledge, <ref type="bibr" target="#b6">[7]</ref> is the only reference that gathers in the same study segmentation and text line recognition as two separate networks. However, a lot of works focus on one of these two tasks separately.</p><p>Early segmentation techniques <ref type="bibr" target="#b7">[8]</ref> can be classified in three categories as suggested in <ref type="bibr" target="#b8">[9]</ref>. First, the projection-based methods, which consider the boundaries between lines as valleys of vertical projection profile <ref type="bibr" target="#b9">[10]</ref>. Second, the grouping methods; it consists in grouping rows of connected components according to heuristic rules <ref type="bibr" target="#b10">[11]</ref>. The last category is the smearing methods which use blurring filters combined with binarization or active contours for example <ref type="bibr" target="#b11">[12]</ref>. It is now generally handled by a Fully Convolutional Network (FCN) as in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>Regarding handwritten lines recognition, it was first solved using handcrafted features and Hidden Markov Model (HMM) <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. However, those models lacked discriminative power and were limited when dealing with long term dependencies in sequences. Hybrid systems combining HMM with Neural Networks (NN) were proposed, leading to better results over standard HMM: HMM with MultiLayer Perceptron (HMM+MLP) <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, HMM with Convolutional Neural Network (HMM+CNN) <ref type="bibr" target="#b20">[21]</ref> or HMM with Recurrent Neural Network (HMM+RNN) <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. Currently, the state of the art is reached with neural networks.</p><p>Many kinds of architectures have been proposed: Multi-Dimensional Long-Short Term Memory (MDLSTM) <ref type="bibr" target="#b23">[24]</ref>, hybrid CNN and Bidirectional LSTM (BLSTM) <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> and more recently encoder-decoder with attention <ref type="bibr" target="#b26">[27]</ref>, Gated CCN (GCNN) <ref type="bibr" target="#b27">[28]</ref> and Gated FCN (GFCN) <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>. Except for attention-based character recognition models, which use cross-entropy loss, training an OCR model at line level was made possible thanks to the CTC. Indeed, it enables to align output sequences of characters with input sequences of features (or pixels) of different and variable lengths.</p><p>Recently, one can notice a trend toward gathering segmentation and recognition together. For the segmentation stage, we can distinguish two approaches: the first one comes from object detection methods and the second one is based on start-of-line prediction.</p><p>The models proposed in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref> follow the object detection approach: they are based on word (or line) bounding boxes prediction. They use a Region Proposal Network (RPN) combined with a non-maximal suppression process and Region Of Interest (ROI) pooling to obtain bounding boxes for each word or line in the input image. OCR is then applied on those boxes to recognize the text. In <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, the authors propose such a model and introduce a multi-task end-to-end architecture working at word level. In <ref type="bibr" target="#b6">[7]</ref>, the focus is on the modular approach. The authors propose a pipeline with four modules: a passage identification finds the handwritten text areas, then an object-detection-based word-level segmentation is applied and words are merged into lines afterwards. Finally, OCR is used on those lines, combined with a language model. Other works focus on predicting start-of-line coordinates and heights. In <ref type="bibr" target="#b32">[33]</ref>, a CNN+MDLSTM is used to predict start-of-line references and an MDLSTM is used for text lines recognition with a dedicated end-of-line token. This is particularly useful in the context of multi-column texts. In <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, a VGG-11-based CNN is used as start-of-line predictor. Then, a recurrent process predicts the next position based on the current one until the end of the line, generating a normalized line. A CNN+BLSTM is finally used as OCR on lines. Some examples with start-of-line, line segmentation and line transcription labels are needed to pretrain the different subnetworks individually; the network is then able to work only with transcription labels. The approach proposed in <ref type="bibr" target="#b34">[35]</ref> is similar to the one of <ref type="bibr" target="#b33">[34]</ref>, but it can handle transcriptions without line breaks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Segmentation-free approaches</head><p>Unlike explicit line segmentation approaches, segmentation-free approaches output the transcription result of a whole paragraph, performing recognition without a prior segmentation step. Among the segmentation-free approaches, one can notice two trends: a first one based on recurrent attention mechanisms and a second one exploiting the two-dimensional nature of the task, based on a one-step process.</p><p>To our knowledge, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> are the only works applying attention mechanisms to the task of multi-line text recognition, achieving a kind of implicit line/character segmentation. These works propose an encoder-decoder architecture with attention blocks at line and character levels, respectively. Both architectures use a CNN+MDLSTM encoder and an MDLSTM-based attention module. The encoder produces feature maps from the input image while the attention module recurrently generates line or character representations applying a weighted sum between the attention weights and the features. Finally, the decoder outputs character probabilities from this representation. These architectures require pretraining on line-level images, but they do not need line breaks to be included in the transcription labels.</p><p>Only two other works present segmentation-free approaches, focusing on the dimensional aspect to recognize the text without a recurrent process. They are <ref type="bibr" target="#b37">[38]</ref> and <ref type="bibr" target="#b38">[39]</ref>. In <ref type="bibr" target="#b37">[38]</ref>, the authors propose a two-dimensional version of the CTC: the Multi-Dimensional Connectionist Classification (MDCC). Using a Conditional Random Field (CRF), ground truth transcription sequences are converted into a two-dimensional model (a 2D CRF) able to represent multiple lines. A line separator label is introduced in addition to the CTC null symbol (also called blank label). The CRF graph enables to jump from one line to the following one, whatever the position in the current line. As for the CTC, repeating labels account for only one. An MDLSTM-based network is used to generate probabilities in two dimensions, preserving the spatial nature of the input image. Pretraining is also used beforehand, this time at word level, with the standard CTC.</p><p>Finally, in <ref type="bibr" target="#b38">[39]</ref> the authors focus on learning to unfold the input paragraph image i.e. into a single text line. The system is trained to concatenate text lines to obtain a single large line before character recognition takes place. This is mainly carried out with bi-linear interpolation layers combined with an FCN encoder. This transformation network enables to use the standard CTC loss and to process the image in a single step. Moreover, it neither needs pretraining nor line breaks in the transcriptions and achieves state-of-the-art results.</p><p>The approaches proposed in <ref type="bibr" target="#b36">[37]</ref> and <ref type="bibr" target="#b37">[38]</ref> are the first attempts reported in the literature towards end-to-end handwritten paragraph recognition, but they have remained below the state-of-the-art results on the IAM dataset <ref type="bibr" target="#b39">[40]</ref>. The interesting segmentation-free approaches presented in <ref type="bibr" target="#b35">[36]</ref> and <ref type="bibr" target="#b38">[39]</ref> reached competitive results.</p><p>The recurrent process used in <ref type="bibr" target="#b35">[36]</ref> enables to model dependencies between lines, but is computationally expensive due to the MDLSTM layers, which could lead to high training and prediction times. The fully convolutional model involved in <ref type="bibr" target="#b38">[39]</ref> enables high computation parallelization, but its recurrence-free process does not enable to model dependencies between lines. In addition, it implies a large number of parameters, of around 16.4 million.</p><p>In this work, we suggest combining the advantages of both approaches to design a fast, efficient and lightweight end-to-end model for paragraph recognition. The proposed model is based on an encoder-decoder architecture using attention as in <ref type="bibr" target="#b35">[36]</ref>, but we use an FCN encoder and an attention module without recurrent layers to reduce the computation time while implying few parameters at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ARCHITECTURE</head><p>We propose an end-to-end model, called Vertical Attention Network (VAN), following the encoder-decoder principles with an attention module. It takes as input an image X and recurrently recognizes the L text lines [y 1 , ..., y L ] present in it. Each text line y t is a sequence of tokens from an alphabet A, with |A| = N.</p><p>The overall model is presented in <ref type="figure" target="#fig_2">Figure 1</ref>. It first extracts features f from X. We wanted the encoder stage to be modular enough in order to be plugged in different architectures dedicated to text lines, paragraphs or documents recognition without needing any adaptation. To this end, we chose a Fully Convolutional Network encoder that can deal with input images of variable heights and widths, possibly containing multiple lines of text. The attention module is the main control block: it recurrently produces vertical attention weights that focus where to select the next features for the recognition of the next text line. This way, it recurrently generates as many text line features/representations l t as there are text lines in the input image. It also detects the end of the paragraph leading to the end of the whole process. Finally, the decoder stage produces character probabilities p t for each frame of each generated line features, and the best path decoding strategy is used. The model is trained using a combination of two losses: the CTC loss for the recognition task, through the line-by-line alignment between recognized text lines (through its probabilities lattice p t ) and ground truth line transcriptions y t , and the cross-entropy loss for the endof-paragraph detection. We now describe each module in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>An FCN encoder is used to extract features from the input paragraph images. It takes as input an image X ? R H?W ?C , H, W and C being respectively the height, the width and the number of channels (C=1 for a grayscale image, C=3 for a RGB image). Then, it outputs feature maps for the whole paragraph image:</p><formula xml:id="formula_0">f ? R H f ?W f ?C f with H f = H 32 , W f = W 8 and C f = 256.</formula><p>Those features must contain enough information to recognize the characters afterwards, while preserving the two-dimensional nature of the task.</p><p>As shown on <ref type="figure" target="#fig_3">Figure 2</ref> the encoder is made up of a large composition of Convolution Blocks (CB) and Depthwise Separable Convolution Blocks (DSCB) in order to enlarge the context that will serve at the final decision layer. Using our setup, the receptive field is 961 pixels high and 337 pixels wide.</p><p>CB and DSCB include Diffused Mix Dropout (DMD) layers which correspond to a new dropout strategy we propose. This strategy is detailed in Section 3.4.</p><p>A CB is a succession of two convolutional layers, followed by an Instance Normalization layer. A third convolutional layer is applied at the end of this block. Each convolution layer uses 3 ? 3 kernels and is followed by a ReLU activation function; zero padding is introduced to remove the kernel edge effect. While the first two convolutional layers have a 1 ? 1 stride, the third one has a 1 ? 1 stride for CB_1, 2 ? 2 stride for CB_2 to CB_4 and 2 ? 1 stride for CB_5 and CB_6. This enables to divide the height by 32 and the width by 8 so as to reduce the memory requirement. DMD is applied at three possible locations which are just after the activation layers of the three convolutional layers. Hidden state DSCB differs from CB in two respects. On the one hand, the standard convolutional layers are superseded by Depthwise Separable Convolutions (DSC) <ref type="bibr" target="#b40">[41]</ref>. The aim is to reduce the number of parameters while keeping the same level of performance. On the other hand, the third convolutional layer has a fixed stride of 1 ? 1. In this way, the shape is preserved until the last layer of the encoder.</p><formula xml:id="formula_1">h Wf ?(t?1) Decoder N+1 W 8 1 Char Probs p t 2 1 1 Stop Decision Probs d t L CT C L CT C L CE L CE</formula><p>Residual connections with element-wise sum operator are used between blocks when it is possible, that is to say when the shape is unchanged. This enables to strengthen the parameters update of the first layers of the network during back propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention</head><p>At this step, we have computed the features f . This is a static representation that preserves the two-dimensional aspect of the task: it never changes through the attention process. The purpose of the attention module is to recursively produce text line representations, in the desired reading order, from top to bottom for example. In addition, it has to decide when to stop generating a new line representation, i.e. to detect the end of the paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Line features generation</head><p>Given an input image with L text lines, the attention module successively produces L line features. In the following, the process of the t th line will be refered as attention step t. To produce the successive line features, we chose to use a soft attention mechanism, as proposed by Bahdanau in <ref type="bibr" target="#b1">[2]</ref>, and more specifically a hybrid attention. The soft attention is a way to focus on specific frames among a one-dimensional sequence of frames. It involves the computation of attention weights ? t,i for each frame i at attention step t. These attention weights sum to 1 and quantify the importance of each frame for the attention step t. We apply the attention on the vertical axis only, in order to focus on specific features row at each attention step t. This way, we need one weight per features row: we roughly expect one weight to be near 1, selecting the correct feature row and the others to be near 0. Each attention step t is dedicated to process the text line number t. The corresponding line features l t can be computed as a weighted sum between the feature rows and the attention weights:</p><formula xml:id="formula_2">l t = H f ? i=1 ? t,i ? f i .<label>(1)</label></formula><p>As one can note, this weighted sum enables to provide line features l t to be a one-dimensional sequence and thus to use the standard CTC loss for each recognized text line.</p><p>The hybrid aspect means that attention weights are computed from both the content and the location output layers. In our case, the attention weights computation combines three elements:</p><formula xml:id="formula_3">? f ? R H f ?W f ?C f : the features in which we want to select specific line features. ? ? t?1 ? R H f : the previous attention weights.</formula><p>To know which line to select, we must know which ones have already been processed. This is the location-based part.</p><p>? h W f ?(t?1) ? R C h : the decoder hidden state after processing the previous text line (each line features is made up of W f frames). It contains information about what has been recognized so far. This is the content-based part.</p><p>f , ? t?1 and h W f ?(t?1) cannot be combined directly due to dimension mismatching. Since we want to normalize the attention weights over the vertical axis, we must collapse the horizontal axis of the features. The idea is that we only need to know if there is at least one character in a features row to decide to process it as a text line. Thus, we can collapse this horizontal dimension without information loss. This collapse is carried out in two steps: we first get back to a fixed width of 100 (since inputs are of variable sizes) through AdaptiveMaxPooling. Then, a densely connected layer pushes the horizontal dimension to collapse. The remaining vertical</p><formula xml:id="formula_4">representation is called f ? R H f ?C f .</formula><p>Instead of using directly ? t?1 , we found that combining it with a coverage vector c t ? R H f is more beneficial. c t is defined as the sum of all previous attention weights:</p><formula xml:id="formula_5">c t = t ? k=0 ? k .<label>(2)</label></formula><p>c t enables to keep track of all previous line positions processed. c t is clamped between 0 and 1 for stability; we only need to know whether a row has already been processed or not. Bigger values would not have sens. We combined c t and ? t through concatenation over the channel axis, leading to i t ? R H f ?2 . Finally, we extract some context information from i t through a one-dimensional convolutional layer with C j = 16 filters of kernel size 15 and stride 1 with zero padding, followed by instance normalization.  This outputs j t ? R H f ?C j which contains all the contextualised spatial information from past attention steps.</p><p>We can now compute the attention weights ? t as follows:</p><p>? For each row i, we compute the associated multi-scale information s t,i , gathering all the elements we have seen previously:</p><formula xml:id="formula_6">s t,i = tanh(W f ? f i +W j ? j t,i +W h ? h W f ?(t?1) ).<label>(3)</label></formula><p>Indeed, s t,i contains both local and global information: information from features and previous attention weights can be considered as local since they are position-dependant; and information from the decoder hidden state can be seen as global since it is related to all characters recognized so far. W f , W j and W h are weights of densely connected layers unifying the number of channels of the elements to be summed to</p><formula xml:id="formula_7">C u = 256 (W f ? R C f ?C u , W j ? R C j ?C u and W h ? R C h ?C u ).</formula><p>? Score e t,i are then computed for each row i:</p><formula xml:id="formula_8">e t,i = W a ? s t,i ,<label>(4)</label></formula><p>where W a are weights of a densely connected layer reducing the channel axis to get a single value (W a ? R C u ?1 ).</p><p>? Attention weights are finally computed through softmax activation:</p><formula xml:id="formula_9">? t,i = exp(e t,i ) H f ? k=1 exp(e t,k ) .<label>(5)</label></formula><p>We emphasize that, although the attention module produces a vertical focus l t , the line recognizer has a broad view of the input signal due to the size of the receptive field. Therefore, it makes the method robust to inclined or non-straight lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">End-of-paragraph detection</head><p>One of the main intrinsic problem of paragraph recognition is the unknown number of text lines to be recognized. As the text recognition is processed sequentially, an end-of-paragraph detection is needed. To solve this problem, we compare three different approaches we named fixed-stop approach, early-stop approach and learned-stop approach.</p><p>For each of these approaches, we used the CTC loss to align the line prediction (probabilities lattice p t of length W f ), with the corresponding line transcription y t . It involves the addition of the null symbol (also known as blank), leading to a new alphabet A = A?{blank}. This loss is defined as the negative log probability of correctly labeling the sequence:</p><formula xml:id="formula_10">L CTC (p t , y t ) = ? ln p(y t |p t ).<label>(6)</label></formula><p>p(y t |p t ) corresponds to the sum of probabilities of all the paths ? leading to the target sequence y t through the reverse operations defined by automaton B:</p><formula xml:id="formula_11">p(y t |p t ) = ? ??B ?1 (y t ) p(?|p t ).<label>(7)</label></formula><p>B is the automaton that removes the identical successive tokens from a sequence, and then also removes the null symbols from it. Finally, the probability p(?|p t ) can be computed as:</p><formula xml:id="formula_12">p(?|p t ) = W f ? i=1 p i t ? i , ?? ? A W f ,<label>(8)</label></formula><p>where p i t ? i is the probability of observing label ? i at position i in the line prediction sequence p t .</p><p>We also defined a stopping criterion at evaluation time to avoid infinite loops. It consists in a constant l max large enough to cover the biggest paragraph of the dataset. This number can easily be set to match the datasets involved; in our case l max = 30.</p><p>The fixed-stop approach is the simplest way to handle the endof-paragraph detection issue. The model iterates l max times and stops, as proposed in <ref type="bibr" target="#b35">[36]</ref>. Additional fictive lines [y L+1 , ..., y l max ] are added to the ground truth as empty strings. The idea is that the extra iterations will focus on interlines, only predicting null symbols, to avoid recognizing the same line multiple times. During training the loss is defined as follows:</p><formula xml:id="formula_13">L fs = l max ? k=1 L CTC (p k , y k ).<label>(9)</label></formula><p>This approach leads to an additional processing cost during training and evaluation due to the extra iterations needed.</p><p>To alleviate this issue, we propose the early-stop approach. The idea is that we can consider that the lines have all been predicted as soon as the current prediction is an empty line (only null symbols predicted). This way, we only have to add one additional line to the ground truth: y L+1 . The loss is then defined this way:</p><formula xml:id="formula_14">L es = L+1 ? k=1 L CTC (p k , y k ).<label>(10)</label></formula><p>Finally, we propose the learned-stop approach as a more elegant way to solve this problem. It consists in learning when to stop recognizing a new text line i.e. to detect when the whole paragraph has been processed. This end-of-paragraph detection is performed at each iteration, computing the probability d t to stop or to continue the recognition. More specifically, d t determines whether p t should be considered or not. This way, the model iterates L + 1 times to learn to predict the end of the process.</p><p>We decided to compute this probability from two elements: the multi-scale information s t which brings some visual information about what has already been processed and what remains to be decoded, and the decoder hidden state h W f ?t which can contain information about what have already been recognized. Indeed, it is more likely to be the end of the paragraph if the last recognized character is a dot for example.</p><p>To this end, we first have to collapse the vertical dimension of s t for dimension matching purposes. This is carried out in the following way:</p><p>? A one-dimensional convolutional layer with kernel size 5, stride 1 and zero padding is applied on s t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>AdaptiveMaxPooling is used to reduce the height to a fixed value of 15 (since input are of variable sizes).</p><p>? A densely connected layer pushes the vertical dimension to collapse, leading to k t ? R C f .</p><p>Then, the produced tensor k t is combined with h W f ?t through concatenation over the channel axis, leading to b t .</p><p>Finally, the dimension is reduced to 2 in order to compute the probabilities d t ? R 2 through a densely connected layer of weights</p><formula xml:id="formula_15">W d ? R (C f +C h )?2 : d t = W d ? b t .<label>(11)</label></formula><p>This approach leads to the addition of a cross-entropy loss to the CTC loss, applied to the decision probabilities d t . The corresponding ground truth ? t is one-hot encoded, deduced from the line breaks. The cross-entropy loss is defined as follows:</p><formula xml:id="formula_16">L CE (d t , ? t ) = ? 2 ? i=1 ? t i log d t i .<label>(12)</label></formula><p>The final loss is then:</p><formula xml:id="formula_17">L ls = L ? k=1 L CTC (p k , y k ) + ? L+1 ? k=1 L CE (d k , ? k ),<label>(13)</label></formula><p>where ? is set to 1.</p><p>These three approaches are compared through experiments presented in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>The decoder aims at recognizing a sequence of characters from the current line features l t , i.e. a whole text line. To this end, we can process l t as we do in standard OCR applied to line images, since the vertical axis is already collapsed: l t is a one-dimensional sequence of features. First, we apply a single LSTM layer with C h = 256 cells that outputs another representation of same dimension r t which includes some context due to the recurrence over the horizontal axis. The LSTM hidden states h 0 are initialized with zeros for the first line; they are kept from one line to the next one to take advantage of the context at paragraph level. Then, a one-dimensional convolutional layer with kernel 1 going from 256 to N + 1 channels is applied in order to produce p t , the a posteriori probabilities of each character and the CTC null symbol, for each of the W f frames. N is the size of the character set. Best path decoding is used to get the final characters sequence. Successive identical characters and CTC null symbols are removed through the CTC decoding algorithm, leading to the final text. All the recognized text lines are concatenated with a whitespace character as separator to get the whole paragraph transcription p pg .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Diffused Mix Dropout</head><p>Dropout is commonly used as regularization during training to avoid over-fitting. We introduce Diffused Mix Dropout, a new dropout strategy that takes advantage of the two main modes proposed in the literature, namely standard (std.) dropout <ref type="bibr" target="#b41">[42]</ref> and spatial (or 2d) dropout <ref type="bibr" target="#b42">[43]</ref>. A Mix Dropout (MD) layer applies one of the two possible dropout modes randomly. It enables to take advantage of the two implementations in a single layer. Diffused Mixed Dropout (DMD) consists in randomly applying MD at different locations among a set of pre-selected locations. In the model, we use DMD with dropout probability of 0.5 and 0.25 respectively for the standard and the 2d modes. Both modes have equivalent probabilities to be chosen at each execution. The benefit of using this dropout strategy is discussed in Section 6.3 through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL CONDITIONS</head><p>This section is dedicated to the presentation of the experimental conditions: datasets, pre-processing, data augmentation strategy, post-processing, computed metrics and training details are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In this paper, we evaluate the Vertical Attention Network on three popular handwriting datasets: RIMES <ref type="bibr" target="#b43">[44]</ref>, IAM <ref type="bibr" target="#b39">[40]</ref> and READ 2016 <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">RIMES</head><p>RIMES is a popular handwriting dataset composed of gray-scale images of French handwritten text produced in the context of writing mails scenarios. The images have a resolution of 300 dpi. In the official split, there are 1,500 pages for training and 100 pages for the evaluation. To be comparable with other works, we took the last 100 training images for validation, as usually done. Segmentation and transcription are provided at paragraph, line and word levels. We used the first two segmentation levels in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">IAM</head><p>We used the handwriting IAM dataset which is made of handwritten copy of text passages extracted from the LOB corpus. It corresponds to gray-scale images of English handwriting with a resolution of 300 dpi. This dataset provides segmentation at page, paragraph, line and word levels with their corresponding transcriptions. In this work, we used the line and paragraph levels with the commonly used but unofficial split as detailed in <ref type="table" target="#tab_1">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">READ 2016</head><p>READ 2016 has been proposed in the ICFHR 2016 competition on handwritten text recognition. This dataset is composed of a subset of the Ratsprotokolle collection used in the READ project. Images are in color and represent Early Modern German handwriting. READ 2016 provides segmentation at page, paragraph and line levels. We ignored lines with null transcription in the ground truth leading to a small difference in the split compared to the official one. We removed the character "?" from the ground truth since it is not a real character. Sample images from these datasets are shown in <ref type="figure" target="#fig_4">Figure 3</ref>. As we can see, the number of lines per paragraph can vary a lot: from 2 to 18 for RIMES, from 2 to 13 for IAM and from 1 to 26 for READ 2016. RIMES exhibits more layout variability compared to the other datasets: a single paragraph image can contain multiple indents and variable interline heights. IAM layout is more structured and regular. READ 2016 sets oneself apart since its paragraph images can contain only the page number, few words or a large number of lines. In addition, these images have more noise in particular because of the bleed-through effects of handwriting occurring on the back of the pages. In Section 5.1, we show that the VAN is robust enough to handle such different datasets without adapting the model for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-processing</head><p>We use the following pre-processings that are applied similarly on every dataset: we downscale the input images by a factor of 2 through a bilinear interpolation. We are thus working with images with a resolution of 150 dpi for IAM and RIMES for example. For the VAN, we zero pad the input images to reach a minimum height of 480 px and a minimum width of 800 px when necessary. This assures that the minimum features width will be 100 and the minimum features height will be 15, which is required by the model as described previously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data Augmentation</head><p>In order to reduce over-fitting and to make the model more robust to fluctuations, we set up a data augmentation strategy, applied at training time only. We used the following augmentation techniques: resolution modification, perspective transformation, elastic distortion and random projective transformation (from <ref type="bibr" target="#b38">[39]</ref>), dilation and erosion, brightness and contrast adjustment and sign flipping. Each transformation has a probability of 0.2 to be applied. They are applied in the given order, and they can be combined except for perspective transformation, elastic distortion and random projective transformation which are mutually exclusive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Post-processing</head><p>We use the well known CTC best path decoding to get the final recognition from the character probabilities lattice. We do not use any external language model or lexicon constraint. The only postprocessing we used consists in keeping only one space character if several successive space characters are predicted, and removing space characters if they are predicted at the beginning or at the end of a line.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Metrics</head><p>In order to evaluate the quality of the recognition, we use the Character Error Rate (CER) and the Word Error Rate (WER). Both are computed with the Levenshtein distance (denoted as d lev ) between the ground truth y and the recognized text?, normalized by the length of the ground truth y len . To avoid that errors in the shortest lines have more impact on the metric than errors in the longest ones, we normalize by the total length of the ground truth:</p><formula xml:id="formula_18">CER = K ? i=1 d lev (? i , y i ) K ? i=1 y len i ,<label>(14)</label></formula><p>where K is the number of images in the data set. WER formula is exactly the same but at word level instead of character level. We considered punctuation characters as words, as in the READ 2016 competition <ref type="bibr" target="#b44">[45]</ref>.</p><p>For the evaluation of the segmentation task we present, as a comparative approach, we used two metrics: IoU and mAP. The segmentation is applied at pixel level with two classes, namely text and background. The IoU is defined as the intersection of text-classified pixels divided by the union of text-classified pixels between the ground truth and the prediction. We compute the global IoU over a set of images by weighting the image IoU by its number of pixels. We compute the mAP for an image as the average of AP computed for IoU thresholds between 50% and 95% with a step of 5%. Image mAPs are weighted by the number of pixels of the images to give the global mAP of a set.</p><p>Other metrics such as the number of parameters, the training time or the prediction time are useful to compare models. In the following experiments, models are trained during two days. The training time is computed as the time to reach 90% of the convergence. This is a more relevant value since tiny fluctuations can occur after numerous epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Training details</head><p>If not stated otherwise, the encoder and the last convolutional layer of the decoder of the VAN are pretrained on line-level images. It means that we transfer the weights from the architecture depicted in <ref type="figure" target="#fig_10">Figure 8</ref>, which is trained on the official line-level segmented images of the same dataset. The aim of training this intermediary architecture is to only focus on the recognition aspect of the task in the first place, in order to speed up the VAN convergence and improve the recognition performance, as demonstrated in section 5.2. Pretraining is carried out with the same data augmentation strategy and with the same pre-processing.</p><p>The VAN is then trained with whole paragraph images as inputs. We used the line break annotations to split the ground truth into a sequence of text line transcriptions. This enables us to use the CTC loss to train the VAN through a line-by-line alignment between the recognized text lines and the ground truth line transcriptions.</p><p>Learned-stop training and prediction processes are respectively detailed in Algorithm 1 and 2. The algorithms for the fixed-stop and early-stop approaches are similar. All the instructions related to the variables ? CE , d t and ? t must be removed. Also, the for loop iterates l max times instead of L + 1 times for the fixed-stop approach. </p><formula xml:id="formula_19">2 f = Encoder(X); 3 while t ? l max &amp; argmax(d t ) == 1 do 4 l t , ? t , d t = Attention( f , ? t?1 , h W f ?(t?1) ); 5</formula><p>if argmax(d t ) == 1 then 6 p t , h W f ?t = Decoder(l t , h W f ?(t?1) ); 7 p pg = concatenate(p pg ," ", ctc_decoding(p t )); 8 t = t + 1;</p><p>We can summarize those processes as follows. First, the input images are pre-processed and augmented (at training time only) as described previously. Then, the encoder extracts features f from them. The attention module recurrently generates line features l t until l max is reached or, for the learned-stop approach, until d t probabilities are in favor of stopping the process. The decoder outputs character probabilities from each line features l t , which are then decoded through the CTC algorithm, and merged together, separated by a whitespace character. We finally get the whole paragraph transcription p pg .</p><p>We used the Pytorch framework with the apex package to enable mixed precision training thus reducing the memory consumption. We used the Adam optimizer for all experiments with an initial learning rate of 10 ?4 . Trainings are performed on a single GPU Tesla V100 (32Gb). Models have been trained with mini-batch size of 16 for line-level model and mini-batch size of 8 for the segmentation model and for the <ref type="bibr">VAN.</ref> We use exactly the same hyperparameters from one dataset to another. Moreover, the model architecture is the same for each dataset: the last layer is the only difference since the datasets do not have the same character set size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>This section is dedicated to the evaluation of the Vertical Attention Network for paragraph recognition. We show that the VAN reaches state-of-the-art results on each dataset. We study the need for pretraining on isolated text lines of the target dataset. We show that this can be avoided by using a pretrained VAN on another dataset. It enables the model to be trained on the target dataset with the paragraph-level ground truth only, without the need for line segmentation ground truth, which is a considerable practical advantage. We study the different stopping strategies on the IAM dataset. We also provide a visualization of the attention process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Comparison with state-of-the-art paragraph-level approaches</head><p>The results presented in this section are given for the VAN, with pretraining on line images and using the learned-stop strategy. The following comparisons are made with approaches under similar conditions, i.e. without the use of external data (to model the language for example) and at paragraph level.</p><p>Comparative results with state-of-the-art approaches on the RIMES dataset are given in <ref type="table" target="#tab_2">Table 2</ref>. The VAN achieves better results on the test set compared to the other approaches with a CER of 1.91% and a WER of 6.72%.  <ref type="table" target="#tab_3">Table 3</ref> shows the results compared to the state of the art on the IAM dataset. As one can see, once again the VAN also achieves new state-of-the-art results with a CER of 4.45% and a WER of 14.55% on the test set. One can notice that we use more than 6 times fewer parameters than in <ref type="bibr" target="#b38">[39]</ref> with 2.7 M compared to 16.4 M. To our knowledge, there are no results reported in the literature on the READ 2016 dataset at paragraph or page level. The recognition results are presented in <ref type="table" target="#tab_4">Table 4</ref>. Our approach reaches a CER of 3.59% and a WER of 13.94%. The proposed Vertical Attention Network achieves new state-ofthe-art results on these three different datasets. For fair comparison with the other competitive approaches of the literature we highlight some comparative features in <ref type="table" target="#tab_5">Table 5</ref>. In this table, the approaches are analyzed with regards to the following features: 1-use of an explicit text region segmentation process 2-minimum segmentation level used (whether it is for pretraining, data augmentation or training itself) 3-number of hyperparameters adapted from one dataset to another (except for the last decision layer which is dependent of the alphabet size) 4-use of curriculum learning 5use of data augmentation. Curriculum learning, which consists in progressively increasing the number of lines in the input images, can be considered as data augmentation (crop technique). It is important to note that the use of line segmentation ground truth during training is costly due to the human effort involved to create them; this is even more costly for word segmentation. Data augmentation, for its part, does not require any human effort.</p><p>The approach proposed in <ref type="bibr" target="#b38">[39]</ref> is the only one that does not use any segmentation label neither at line nor at word level. However, it is also the only one that requires the architecture to be adapted to each dataset. More specifically, the height and width of the input and of two intermediate upsampling layers are tuned for each dataset. It means that those 6 hyperparameters must be tuned manually to reach the performance reported, which makes the architecture not generic at all. On the contrary, the VAN architecture remains the same for every dataset in all the experiments. Another important point is that the use of line-level segmentation ground truth is not inherent to our approach, as it was only introduced as a pretraining step. As we will see in the following section, pretraining the model at line level is not mandatory. Indeed, similar performance can be obtained with paragraph-level cross-dataset pretraining, without the need for line segmentation ground truth from the target dataset, as detailed in Section 5.2. Word <ref type="bibr" target="#b6">[7]</ref> Word <ref type="bibr" target="#b33">[34]</ref> Line <ref type="bibr" target="#b38">[39]</ref> Paragraph 6 ? <ref type="bibr" target="#b35">[36]</ref> Line <ref type="bibr" target="#b36">[37]</ref> Line Ours Line</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Impact of pretraining</head><p>In this section, we study the impact of pretraining on the VAN, using the learned-stop approach. The dataset used for pretraining is named source dataset, and the dataset on which we want to evaluate the model is named target dataset. We conducted three kinds of experiments. They are intended to evaluate the requirement of line-level segmentation labels for the target dataset to reach the best performance. A first training strategy consists in training the architecture at paragraph level directly on the target dataset, from scratch. A second strategy consists in training the architecture at paragraph level on the target dataset, with pretrained weights for the encoder and the convolutional layer of the decoder, as detailed in Section 4.6. In this case, pretraining is carried out on the isolated text line images of the target dataset, prior to train the VAN at paragraph level. Here, the source dataset and the target dataset are the same. This pretraining approach is referred to as line-level pretraining. A third training strategy consists in training the architecture at paragraph level on the target dataset with weights that are initialized with those of another VAN. This other VAN is trained on a source dataset, with the second strategy. The idea is not to use any segmentation label from the target dataset. In this case the training strategy is referred to as cross-dataset pretraining. <ref type="figure" target="#fig_6">Figure 4</ref> shows the evolution of the CTC loss on the IAM dataset during training from scratch and training with line-level pretraining. We also highlight the impact of dropout when training from scratch. The recognition performance are given on the test set in <ref type="table" target="#tab_6">Table 6</ref>. As was expected, we can clearly notice that training the model from scratch is feasible, but it takes much time to converge and it comes at the cost of an important increase of the CER, from 4.45% up to 7.06%. When training from scratch, the use of dropout highly slows down the convergence but it leads to a lower CER of 7.06% compared to 8.06%. This experiment shows us that state-of-the art results are reached with line-level pretraining.   <ref type="table" target="#tab_7">Table 7</ref> reports the recognition performance of the third training strategy, using cross-dataset pretraining. In this table we report the performance of the VAN on the three datasets when pretrained on one of the two other datasets. One can notice that this pretraining strategy performs almost similarly as line-level pretraining for every datasets, but without using any segmentation label of the target dataset. This is especially true with the two similar datasets RIMES and IAM, for which the two pretraining strategies reach very similar CER: 1.97% compared to 1.91% for RIMES and 4.55% compared to 4.45% for IAM. Although cross-dataset pretraining is a bit less efficient on READ 2016, it still leads to competitive results. This may be explained by the differences between this dataset (RGB color encoding, historical manuscripts and language) and the other datasets (RIMES or IAM) on which pretraining is performed. These experiments demonstrate the importance of pretraining for the VAN. However, we show that we can alleviate the need for line-level segmentation label of the target dataset through crossdataset pretraining. We assume that the important pretraining effect is mainly due to the attention mechanism. Indeed, the authors of <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, who also proposed attention-based models, used curriculum learning to tackle convergence issues. We assume this is due to the direct relation between recognition and implicit segmentation (through soft attention). When the encoder is pretrained, the VAN only has to learn the attention module and the decoder, as the pretrained features may contain all the information needed for the recognition of the characters. Considering random distribution of the attention weights over the vertical axis at first, training will progressively increases values of weights where the features correspond to the correct characters. But when the encoder is randomly initialized, the features extraction must also be learned, which slows the whole training. The use of dropout makes this phenomenon even worse, but it is essential in this architecture to avoid overfitting. It seems that cross-dataset pretraining skips this issue since the attention mechanism is already learned and the encoder only needs to be fine tuned.</p><p>Finally, we can notice that without introducing any pretraining, the VAN architecture is able to converge using the paragraph annotations only, but it is not competitive anymore compared to the state of the art. As a preliminary conclusion, we can highlight the capacity of the VAN to achieve state-of-the-art performance without no need to adapt the architecture to each dataset considered. Moreover, cross-dataset pretraining allows to reach similar results compared to line-level pretraining, without the need to use any line-level segmentation ground truth from the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Learning when to stop</head><p>We now compare the three stopping strategies mentioned in Section 3.2.2, namely fixed-stop, early-stop and learned-stop approaches.</p><p>Performance evaluation of these three methods are given in <ref type="table" target="#tab_8">Table 8</ref> for comparison purpose. Line-level pretraining is used for each approach, as detailed in Section 4.6. We define d mean , as the average of the absolute values of the differences between the actual number of lines in the image n i and the number of recognized lines n r . This metric is used to evaluate the efficiency of the early-stop and learned-stop approaches. For K images in the dataset:</p><formula xml:id="formula_20">d mean = 1 K K ? k=1 |n i k ? n r k |.<label>(15)</label></formula><p>As one can see, equivalent CER and WER are obtained for each stopping strategy. Moreover, the prediction time is not significantly impacted since data formatting, tensor initialization and encoderrelated computations take much longer than the recurrent process, which is made up of only a few layers at each iteration.  <ref type="figure">Figure 5</ref> illustrates the evolution of the CTC loss for the three approaches. One should keep in mind that the comparison is biased since the approaches do not iterate the same number of times for a same example. However, one can clearly notice that the early-stop and learned-stop approaches converge similarly, in contrast to the fixed-stop approach, which requires far more epochs to converge. But in the end, they reach almost identical CER. <ref type="figure">Figure 5</ref>: CTC training loss curves comparison for the VAN for each stopping approach on the IAM dataset. <ref type="figure" target="#fig_7">Figure 6</ref> compares the d mean for the early-stop and learned-stop approaches on the IAM validation dataset. For visibility, the fixedstop approach curve, which is a plateau at d mean = 20.32, is not depicted in this figure. The learned-stop approach leads to a faster convergence of d mean compared to the early-stop approach, whose curve is less stable. It results in a d mean of 0.03 for the learned-stop approach and 0.02 for the early-stop approach for the test set of IAM. With the learned-stop approach, the model successfully learns both tasks: it recognizes text lines with a state-of-the-art CER of 4.45% and it determines when to stop with a high precision since the d mean on the test set is only 0.03. It means that, on average, one line is missed or processed twice every 33 paragraph images. We choose to keep this stopping strategy since it slightly improves the stability of the convergence through the epochs while achieving nearly identical results. <ref type="figure" target="#fig_9">Figure 7</ref> shows the processing steps of an image from the RIMES validation set with a complex layout. Images from top to bottom represent the attention weights of the 5 attention iterations, each one recognizing a line. The intensity of the weights is encoded with the transparency of the red color. Given that attention weights are only computed along the vertical axis, the intensity is the same for every pixels at the same vertical position. Attention weights are rescaled to fit the original image height; indeed, attention weights are originally computed for the features height, which is 32 times smaller. The recognized text lines are given for each iteration, below the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Visualization of the vertical attention</head><p>As one can see, the VAN has learned the reading order, from top to bottom. The attention weights clearly focus on text lines following this reading order. Attention weights focus mainly on one features line, with smaller weights for the adjacent vertical positions.</p><p>One can notice that, sometimes, the focus is not perfectly centered on the text line. This may be due to rescaling, but this can also be a normal behavior due to the large size of the receptive field, which enables to manage slightly inclined lines. The second iteration shows this phenomenon very well with only one misrecognized character on an inclined line. However, processing inclined line is only possible when the lines, although inclined, do not share the same vertical position. The VAN cannot handle the case where lines would overlap vertically, because the attention weights would mix the two lines in this case.</p><p>Furthermore, one can notice that the attention is less sharp when the layout is more complex between two successive lines, as in the third image, but it does not disturb the recognition process however.</p><p>So far, we have provided strong results in favor of the Vertical Attention Network by achieving state-of-the-art performance on three datasets. We also evaluated the need for pretraining and the efficiency of the stopping strategies we propose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ADDITIONAL EXPERIMENTAL STUDIES</head><p>We now provide additional results which show the superiority of the VAN on many criteria compared with a standard two-step approach (line segmentation followed by recognition). We also highlight the positive contribution of the VAN applied to paragraph images compared with the single line recognition approaches. Finally, we highlight the positive effect of the proposed new dropout strategy compared to standard ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Comparison with the standard two-step approach</head><p>In this section, we compare the VAN with the standard two-step approach on the IAM dataset. In this respect, we introduce two new models: a first model performs line segmentation and a second one is in charge of OCR at line level. Both models are trained separately and they do not use any pretraining strategy since they are already working at line level. The line segmentation model follows a U-net shape architecture and is based on our FCN encoder. Indeed, the features f are  successively upsampled to match the features maps shapes of CB_5, CB_4, CB_3, CB_2 and CB_1 <ref type="figure" target="#fig_3">(Figure 2</ref>). This upsampling process is handled by Upsampling Blocks (UB). UB consists in DSC layer followed by DSC Transpose layer and instance normalization. This block also includes DMD layers. Each UB output is concatenated with the feature maps from its corresponding CB. A final convolutional layer output only two feature maps, to classify each pixel of the original image between text and background.</p><p>The OCR model for text line images is illustrated in <ref type="figure" target="#fig_10">Figure 8</ref>. It is made up of the FCN encoder, followed by an AdaptiveMax-Pooling layer, that pushes the vertical dimension to collapse. A final convolutional layer predicts the probability of the characters and of the CTC null symbol.</p><p>The line segmentation model is trained on the paragraph images at pixel level with ground truth of line bounding boxes. It is trained with the cross-entropy loss. The OCR is trained on the line-level images with the CTC loss. We used a mini-batch size of 8 for the segmentation task and of 16 for the OCR.</p><p>We now detail the two steps of this approach. In a first step, paragraphs are segmented into lines:</p><p>? Ground truth bounding boxes are modified in order to avoid overlaps: we divide their height by 2.</p><p>? A paragraph image is given as input of the network.</p><p>? A 2-class pixel segmentation (text or background) is output from the network.</p><p>? Adjacent text-labeled pixels are grouped to form connected components.</p><p>? Bounding boxes are created as the smallest rectangles containing each connected component; their height is multiplied by 2.</p><p>? The input image is cropped using those bounding boxes to generate lines.</p><p>In a second step, the OCR model, trained on the IAM dataset at line level, is applied on those cropped line images. The segmented lines are ordered by their vertical position (from top to bottom) and the recognized text lines are concatenated to compute the CER and the WER at paragraph level.</p><p>The performances of both tasks taken separately and together are shown in <ref type="table" target="#tab_9">Table 9</ref>. As one can see, the results are good for both tasks separately: we get 81.51% for the IoU and 85.09% for the mAP concerning the segmentation task; and a CER of 5.01% and a WER of 16.49% for the OCR. However, when we take the output of the segmentation as input for the OCR, it leads to a CER increase of 1.54 points. Indeed, the line segmentation errors induce recognition errors. We can now compare the Vertical Attention Network to the two-step approach. Comparison on the IAM test set is summarized in <ref type="table" target="#tab_1">Table 10</ref>. First, one can notice that the VAN reaches a better CER of 4.45% compared to 6.55%. Prediction time is computed as the average prediction time, on the test set, to process a paragraph image. As one can see, even though the segmentation step is without recurrence, it requires much more time for prediction due to the formatting of the input required for the OCR, including the bounding boxes extraction from the original images. Moreover, it cumulates prediction times of the two models involved. Despite its recurrent process, the total prediction time for the VAN is shorter than that of the two-step approach since one iteration is very fast. In addition, it implies fewer parameters, this is notably due to the two models required by the two-step approach. Except for the training time, which is a bit higher for the VAN, it only provides advantages compared to the two-step approach.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Line level analysis</head><p>In this section, we compare the results of the VAN with the state-ofthe-art approaches evaluated in similar conditions i.e. at line level and without using external data. We also provide results for the line-level OCR model <ref type="figure" target="#fig_10">(Figure 8</ref>), which has the same encoder, and for the VAN applied at paragraph level. The aim is to highlight the contribution of the VAN processing full paragraph images instead of isolated lines.</p><p>To have a fair comparison between the results obtained at paragraph and line level, one should consider the difference in the ground truth: the transcriptions of paragraph contain line breaks (or space characters in our case) between the different line transcriptions. <ref type="table" target="#tab_1">Table 11</ref> shows the VAN results difference at paragraph level when removing the interline characters from the metrics. As one can see, the removal of the interline character leads to an increase of the CER of 0.24, 0.09 and 0.24 on the test set of RIMES, IAM and READ 2016 respectively. The WER is not impacted by this modification. In the following tables of this section, we will use the results without considering interline characters for fair comparison with line-level approaches.  <ref type="table" target="#tab_1">Table 12</ref> shows state-of-the-art results on the RIMES dataset at line level. We report competitive results with a CER of 3.04% for the line-level model and 3.08% for the VAN on the test set compared to the model of <ref type="bibr" target="#b45">[46]</ref> which reached 2.3%. On should notice that Puigcerver et al. <ref type="bibr" target="#b45">[46]</ref> does not use exactly the same dataset split for training and validation. In conclusion we can highlight the performance of the VAN obtained at paragraph level which achieves a CER of 2.15% on the test set, which corresponds to decreasing the CER by 0.93 compared to processing isolated lines. Comparison with state-of-the-art results on the IAM dataset is presented in <ref type="table" target="#tab_1">Table 13</ref>. We reach competitive results with a CER of 4.97% on the test set. Models proposed in <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b26">[27]</ref> reach similar results but the former implies a large number of parameters compared to ours and the latter is more complex including a recurrent process with attention at character level. It should be noticed however that, in <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b45">[46]</ref>, the authors use a slightly different split from ours. As a matter of fact, since the VAN training implies pretraining at line level, it was not possible to use the same split since some lines for training and validation are extracted from the same paragraph image for example. On the IAM dataset, the VAN also reaches a better CER at paragraph level than at line level with 4.54% compared to 4.97%. The results on the READ 2016 dataset are gathered in <ref type="table" target="#tab_1">Table  14</ref>. We reached state-of-the-art CER on the test set with 4.10% compared to 4.66% for <ref type="bibr" target="#b26">[27]</ref>. Again, the paragraph-level VAN reaches better results than the VAN applied at line level with a CER of 3.83%.</p><p>In conclusion, one can notice that the VAN, applied on isolated lines, performs at least similarly as the line-level model, for each dataset (except for RIMES with a small CER increase of 0.04 points). It also achieves state-of-the art results at line level on the READ dataset.</p><p>The results also highlight the superiority of the Vertical Attention Network on the RIMES, IAM and READ 2016 datasets, applied to whole paragraph images, compared to isolated lines. Multiple factors can explain this result: segmentation ground truth annotations of text lines are prone to variations from one annotator to another, bringing variability that is not present when dealing with paragraph images directly. Indeed, the model implicitly learns to segment the lines so it does not have to adapt to pre-formatted lines; it uses more context (with a large receptive field) and uses it to focus on the useful information for the recognition purpose.</p><p>Moreover, the VAN decoder contains a LSTM layer that may have a positive impact acting as a language model without any loss of context when moving from one line to the next, when producing the output character sequence.</p><p>A key element to reach such results with a deep network is to use efficient regularization strategies. We discuss the new dropout strategy we propose in the following paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Dropout strategy</head><p>We use dropout to regulate the network training and thus avoid over-fitting. We defined Diffused Mix Dropout (DMD) in Section 3.4 to improve the results of the model. We carried out some experiments to highlight the contribution of DMD over commonly used standard and 2d dropout layers. Experiments are performed with the line-level model and the VAN on the IAM dataset; VAN is pretrained with weights from the corresponding line-level model. Results for the test set are shown in <ref type="table" target="#tab_1">Table 15</ref>. The columns from left to right correspond respectively to the number of dropout layers per block (CB and DSCB), the type of dropout layer used (mix, standard or spatial), the associated dropout probabilities, the use of the diffuse option (using only one or all dropout layers per block) and the CER and WER for both models. In <ref type="formula" target="#formula_2">(1)</ref> and <ref type="formula" target="#formula_5">(2)</ref>, Mix Dropout layers are respectively replaced by standard and 2d dropout layers, preserving their corresponding dropout probability. Using Mix Dropout leads to an improvement of 0.23 points of CER compared to standard dropout and of 0.37 compared to 2d dropout for the line-level model. These improvements are lower for the VAN with 0.01 and 0.27 points.</p><p>In <ref type="formula" target="#formula_6">(3)</ref>, only one Mix Dropout is used, after the first convolution of the blocks, leading to a higher CER than the baseline, with a difference of 0.34 points for the line-level model. The CER is decreased by 0.02 points for the VAN but the WER is increased by 0.70 points. In (4) and <ref type="bibr" target="#b4">(5)</ref>, we are in the same configuration as (3) i.e. with only one dropout layer per block. MixDropout is superseded by standard dropout in (4) and by 2d dropout in <ref type="bibr" target="#b4">(5)</ref> resulting in an increase of the CER compared to <ref type="bibr" target="#b2">(3)</ref>. This shows the positive impact of Mix Dropout layers in another configuration.</p><p>In <ref type="formula" target="#formula_10">(6)</ref> and <ref type="formula" target="#formula_11">(7)</ref>, Mix Dropout layers are set at each of the three positions i.e. they are all used at each execution, contrary to the baseline, which uses only one dropout layer per execution. While (6) keeps the same dropout probabilities, (7) divides them by 3. In both cases, the associated CER are higher than the baseline.</p><p>Finally, in <ref type="bibr" target="#b7">(8)</ref>, we are in the same context than the baseline, but dropout probabilities are divided by 3, leading to higher CER.</p><p>We can conclude that our dropout strategy leads to a CER improvement of 0.55 points for the line-level model and of 0.33 for the VAN, when compared to (4) and (5) that do not use Mix Dropout or diffuse option.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">DISCUSSION</head><p>As we have seen, the VAN achieves state-of-the-art results on multiple datasets at paragraph level. However, there is one point that should be notice. Modern deep neural systems involve many training strategies (hyperparameters, optimizer, regularization strategies, pre-processings, data augmentation techniques, transfer learning, curriculum learning, and many others). This makes the comparison between architectures very difficult as some training tricks are more suited for some architectures than some others. This is why one should be convinced that the state-of-the-art results obtained in this paper are due to the whole proposition, including training strategies, and not only to the VAN architecture. However, we have provided experimental results that show the interest of the proposed training strategies of the generic VAN architecture.</p><p>We compared different stopping strategies and showed that the VAN can learn to detect the end of paragraph. This additional task has no significant impact on the performance and slightly improves the stability through training. We also compared favorably the VAN to a standard two-step approach and showed the positive impact of processing paragraph-level images compared with line-level ones, for this architecture. The new dropout strategy we propose enabled to reach even better results.</p><p>Moreover, the VAN has multiple advantages. The VAN is robust: whether it is at paragraph or line level, and no matter the dataset used, we did not adjust any hyperparameter for each dataset. The VAN takes input of variable sizes, so it could handle whole page images without any modification. As mentioned previously, the VAN can handle slightly inclined lines. However, it is limited to layouts in which there is no overlap between lines on their horizontal projection. Indeed, this case remains to be solved. A standard n-gram language model could process the outputs of the VAN architecture but its impact on the performance remains to be determined through experiments.</p><p>However, there is still room for improvement. Notably, we showed that the VAN needs pretraining on isolated text lines of the target dataset to reach state-of-the-art results. But the need for line-level annotations is not inherent to the VAN, this is only related to this pretraining step. As a matter of fact, we demonstrate that pretraining on another dataset (cross-dataset pretraining) can alleviate this issue, even if the datasets are really different. Indeed, for the three datasets, cross-dataset pretraining leads to results similar to those from line-level pretraining, but without using any line-level annotation for the target dataset</p><p>The VAN should be considered to process single-column text document only. As a matter of fact, as it is the case for <ref type="bibr" target="#b35">[36]</ref> and supposedly <ref type="bibr" target="#b38">[39]</ref>, the models are designed and limited to process single-column multi-line text documents with relatively horizontal text lines. The next step would be to focus on processing images with more complex layout such as multi-column text images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we proposed the Vertical Attention Network: a novel end-to-end encoder-decoder segmentation-free architecture using hybrid attention. It handles paragraph images of variable sizes, and we showed its efficiency on the RIMES, IAM and READ 2016 datasets. Indeed, it achieves state-of-the-art results on these datasets at paragraph level. Its implicit line segmentation process enables to recognize complex layout including inclined lines. It could easily be used for recognition of whole single-column text page. The resulting unified model reaches a better CER than two-step approaches, for a shorter prediction time and a lighter architecture in terms of parameters. The proposed new dropout strategy, based on Diffused Mix Dropout layers, leads to an improvement of 0.33 points of CER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>??</head><label></label><figDesc>The number of text regions varies from one paragraph to another.The layout diversity can be important due to multiple factors such as line spacing, horizontal alignment or slant.? A reading order should be defined or learned in order to concatenate the recognized text regions together and obtain ? The authors are with the LITIS, France. E-mail: {denis.coquenet,clement.chatelain,thierry.paquet}@litislab.eu ? D. Coquenet is with Rouen University and Normandy University, France ? C. Chatelain is with the INSA of Rouen, France ? T. Paquet is with Rouen University, France Manuscript received ...; revised ... the final paragraph transcription.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Architecture overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>FCN Encoder overview. Specified dimensions are related to the output of the corresponding layer. CB: Convolution Block, DSCB: Depthwise Separable Convolution Block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Left to right: images from the RIMES, IAM and READ 2016 datasets at paragraph level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Algorithm 1 : 4 l 5 if t &lt; L + 1 6 p 7 ? 11 ? 2 : 1 , t = 1 ,</head><label>14516711211</label><figDesc>Training process. input : paragraph image X, ground truth transcription y composed of L lines [y 1 , ..., y L ] 1 ? 0 = 0, h 0 = 0, ? CTC = 0, ? CE = 0; // Initialization 2 f = Encoder(X); // Extract 2d features f // For each line of the paragraph 3 for t=1 to L+1 do / * Compute attention weights ? t for each features row to generate 1d line features l t . Alsocompute end-of-paragraph probabilities d t * / t , ? t , d t = Attention( f , ? t?1 , h W f ?(t?1) );/ * Determine ground truth for end-of-paragraph detection task * / then / * Compute character and CTC null symbol probabilities p t for each frame of l t * / t , h W f ?t = Decoder(l t , h W f ?(t?1) ); CTC += L CTC (p t , y t ); // Compute CTC loss 8 CE += L CE (d t , ? t ); // Compute cross-entropy loss 12 backward(? CTC +? CE ); // Backpropagation Algorithm Prediction process. input : paragraph image X 1 l max = 30, ? 0 = 0, h 0 = 0, d t = 0 p pg =" " ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>CTC training loss curves comparison for the VAN, with and without pretraining, on the IAM dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of the evolution of d mean (the mean difference between the true and estimated number of lines in the image ) on the validation set of IAM dataset for the early-stop and learned-stop approaches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Je me permet de vous ?crire cas je vMux augmentes mes quantit?s de CD vierges j'ai command? 50 CD et aduellement je voudrai en commander 100.Merci d'avance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Attention weights visualization on a sample of the RIMES validation set. Recognized text is given for each line and errors are shown in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Text line recognition architecture overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets split in training, validation and test sets and associated number of characters in their alphabet</figDesc><table><row><cell>Dataset</cell><cell>Level</cell><cell cols="2">Training Validation</cell><cell>Test</cell><cell>Charset size</cell></row><row><cell>RIMES</cell><cell>Line Paragraph</cell><cell>10,532 1,400</cell><cell>801 100</cell><cell>778 100</cell><cell>100</cell></row><row><cell>IAM</cell><cell>Line Paragraph</cell><cell>6,482 747</cell><cell>976 116</cell><cell>2,915 336</cell><cell>79</cell></row><row><cell>READ 2016</cell><cell>Line Paragraph</cell><cell>8,349 1,584</cell><cell>1,040 179</cell><cell>1,138 197</cell><cell>89</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Recognition results of the VAN and comparison with paragraph-level state-of-the-art approaches on the RIMES dataset.</figDesc><table><row><cell>Architecture</cell><cell cols="5">CER (%) WER (%) CER (%) WER (%) # Param. valid valid test test</cell></row><row><cell>[36] CNN+MDLSTM a</cell><cell>2.5</cell><cell>12.0</cell><cell>2.9</cell><cell>12.6</cell><cell></cell></row><row><cell>[34] RPN+CNN+BLSTM+LM</cell><cell></cell><cell></cell><cell>2.1</cell><cell>9.3</cell><cell></cell></row><row><cell>Ours</cell><cell>1.83</cell><cell>6.26</cell><cell>1.91</cell><cell>6.72</cell><cell>2.7 M</cell></row></table><note>a With line-level attention.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the VAN with the state-of-the-art approaches at paragraph level on the IAM dataset.</figDesc><table><row><cell>Architecture</cell><cell cols="5">CER (%) WER (%) CER (%) WER (%) # Param. valid valid test test</cell></row><row><cell>[37] CNN+MDLSTM a</cell><cell></cell><cell></cell><cell>16.2</cell><cell></cell><cell></cell></row><row><cell>[36] CNN+MDLSTM b</cell><cell>4.9</cell><cell>17.1</cell><cell>7.9</cell><cell>24.6</cell><cell></cell></row><row><cell>[31] RPN+CNN+BLSTM c</cell><cell>13.8</cell><cell></cell><cell>15.6</cell><cell></cell><cell></cell></row><row><cell>[7] RPN+CNN+BLSTM</cell><cell></cell><cell></cell><cell>8.5</cell><cell></cell><cell></cell></row><row><cell>[34] RPN+CNN+BLSTM+LM</cell><cell></cell><cell></cell><cell>6.4</cell><cell>23.2</cell><cell></cell></row><row><cell>[39] GFCN</cell><cell></cell><cell></cell><cell>4.7</cell><cell></cell><cell>16.4 M</cell></row><row><cell>Ours</cell><cell>3.02</cell><cell>10.34</cell><cell>4.45</cell><cell>14.55</cell><cell>2.7 M</cell></row><row><cell>a With character-level attention.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>b With line-level attention.c Results are given for page level.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>VAN results for the READ 2016 dataset at paragraph level.</figDesc><table><row><cell>Architecture</cell><cell>CER (%) validation</cell><cell cols="4">WER (%) CER (%) WER (%) # Param validation test test</cell></row><row><cell>Ours</cell><cell>3.71</cell><cell>15.47</cell><cell>3.59</cell><cell>13.94</cell><cell>2.7 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Training details of state-of-the-art approaches.</figDesc><table><row><cell>Approach</cell><cell>Explicit segmentation</cell><cell>Min. segment. Hyperparam. Curriculum label used adaptation learning</cell><cell>Data augmentation</cell></row><row><cell>[31]</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Impact of the pretraining on lines for the VAN. Results are given on the test set of the IAM dataset.</figDesc><table><row><cell cols="3">Pretraining Dropout CER (%) WER (%) Training time</cell></row><row><cell>8.06</cell><cell>25.38</cell><cell>1.39 d</cell></row><row><cell>7.06</cell><cell>22.65</cell><cell>1.77 d</cell></row><row><cell>4.45</cell><cell>14.55</cell><cell>0.63 d</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison between cross-dataset pretraining and linelevel pretraining for the VAN. Results are given on the test sets.</figDesc><table><row><cell>Source dataset</cell><cell cols="2">RIMES CER (%) CER (%) IAM</cell><cell>READ 2016 CER (%)</cell></row><row><cell>Cross-dataset pretraining</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RIMES</cell><cell></cell><cell>4.55</cell><cell>4.08</cell></row><row><cell>IAM</cell><cell>1.97</cell><cell></cell><cell>4.14</cell></row><row><cell>READ 2016</cell><cell>2.36</cell><cell>5.20</cell><cell></cell></row><row><cell>Line-level pretraining</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Target dataset</cell><cell>1.91</cell><cell>4.45</cell><cell>3.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison between fixed-stop, early-stop and learnedstop approaches with the VAN on the test set of the IAM dataset.</figDesc><table><row><cell cols="5">Stop method CER (%) WER (%) Train. time Pred. time</cell><cell>d mean</cell></row><row><cell>Fixed</cell><cell>4.41</cell><cell>14.69</cell><cell>1.20 d</cell><cell>33 ms</cell><cell>20.32</cell></row><row><cell>Early</cell><cell>4.41</cell><cell>14.39</cell><cell>0.41 d</cell><cell>32 ms</cell><cell>0.02</cell></row><row><cell>Learned</cell><cell>4.45</cell><cell>14.55</cell><cell>0.63 d</cell><cell>32 ms</cell><cell>0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Results of the two-step approach on the test set of IAM.</figDesc><table><row><cell>Architecture</cell><cell cols="4">IoU (%) mAP (%) CER (%) WER (%)</cell><cell># Param.</cell></row><row><cell>Line seg. model</cell><cell>81.51</cell><cell>85.09</cell><cell></cell><cell></cell><cell>1.8 M</cell></row><row><cell>OCR on lines</cell><cell></cell><cell></cell><cell>5.01</cell><cell>16.49</cell><cell>1.7 M</cell></row><row><cell>Two-step approach</cell><cell>81.51</cell><cell>85.09</cell><cell>6.55</cell><cell>18.54</cell><cell>1.8+1.7M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Comparison of the two-step approach with the Vertical Attention Network, results are given for the test set of the IAM dataset.</figDesc><table><row><cell cols="3">Architecture CER (%) WER (%)</cell><cell># Param.</cell><cell>Training time</cell><cell>Prediction time</cell></row><row><cell>Two-step</cell><cell>6.55</cell><cell>18.54</cell><cell cols="3">1.8+1.7 M 0.03+0.59 d 749+28 ms</cell></row><row><cell>VAN</cell><cell>4.45</cell><cell>14.55</cell><cell>2.7 M</cell><cell>0.59+0.63 d</cell><cell>32 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>VAN results at paragraph level with and without interline characters in ground truth for RIMES, IAM and READ 2016 dataset.</figDesc><table><row><cell>Dataset</cell><cell>Line break</cell><cell cols="4">CER (%) WER (%) CER (%) WER (%) valid valid test test</cell></row><row><cell>RIMES</cell><cell></cell><cell>1.83 2.10</cell><cell>6.26</cell><cell>1.91 2.15</cell><cell>6.72</cell></row><row><cell>IAM</cell><cell></cell><cell>3.02 3.07</cell><cell>10.34</cell><cell>4.45 4.54</cell><cell>14.55</cell></row><row><cell>READ 2016</cell><cell></cell><cell>3.71 4.01</cell><cell>15.47</cell><cell>3.59 3.83</cell><cell>13.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Comparison with the state of the art on the line-level RIMES dataset.</figDesc><table><row><cell>Architecture</cell><cell cols="5">CER (%) WER (%) CER (%) WER (%) # Param. valid valid test test</cell></row><row><cell>[24] 2D-LSTM+LM</cell><cell></cell><cell></cell><cell>2.8</cell><cell>9.6</cell><cell></cell></row><row><cell>[46] CNN+BLSTM a</cell><cell>2.2</cell><cell>9.6</cell><cell>2.3</cell><cell>9.6</cell><cell>9.6 M</cell></row><row><cell>Ours (line-level model)</cell><cell>2.20</cell><cell>6.26</cell><cell>3.04</cell><cell>8.32</cell><cell>1.7 M</cell></row><row><cell>Ours (VAN on lines)</cell><cell>1.97</cell><cell>6.09</cell><cell>3.08</cell><cell>8.14</cell><cell>2.7 M</cell></row><row><cell>Ours (VAN on paragraphs)</cell><cell>2.10</cell><cell>6.26</cell><cell>2.15</cell><cell>6.72</cell><cell>2.7 M</cell></row><row><cell cols="6">a This work uses a slightly different split (10,203 for training, 1,130 for validation and 778 for</cell></row><row><cell>test).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Comparison with the state of the art at the line level on the IAM dataset.</figDesc><table><row><cell>Architecture</cell><cell>CER (%) validation</cell><cell cols="4">WER (%) CER (%) WER (%) # Param. validation test test</cell></row><row><cell>[24] CNN+MDLSTM+LM</cell><cell>2.4</cell><cell>7.1</cell><cell>3.5</cell><cell>9.3</cell><cell>2.6 M</cell></row><row><cell>[46] CNN+BLSTM a</cell><cell>3.8</cell><cell>13.5</cell><cell>5.8</cell><cell>18.4</cell><cell>9.3 M</cell></row><row><cell>[29] GFCN a</cell><cell>3.3</cell><cell></cell><cell>4.9</cell><cell></cell><cell>&gt; 10 M</cell></row><row><cell>[27] Seq2seq (CNN+BLSTM) a</cell><cell></cell><cell></cell><cell>4.87</cell><cell></cell><cell></cell></row><row><cell>Ours (line-level model)</cell><cell>3.37</cell><cell>11.52</cell><cell>5.01</cell><cell>16.49</cell><cell>1.7 M</cell></row><row><cell>Ours (VAN on lines)</cell><cell>3.15</cell><cell>10.77</cell><cell>4.97</cell><cell>16.31</cell><cell>2.7 M</cell></row><row><cell>Ours (VAN on paragraphs)</cell><cell>3.07</cell><cell>10.34</cell><cell>4.54</cell><cell>14.55</cell><cell>2.7 M</cell></row><row><cell cols="6">a These works use a slightly different split (6,161 for training, 966 for validation and 2,915 for</cell></row><row><cell>test).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Comparison with the state-of-the-art line-level recognizers on READ 2016 dataset.</figDesc><table><row><cell>Architecture</cell><cell>CER (%) validation</cell><cell cols="4">WER (%) CER (%) WER (%) # Param. validation test test</cell></row><row><cell>[27] Seq2seq (CNN+BLSTM)</cell><cell></cell><cell></cell><cell>4.66</cell><cell></cell><cell></cell></row><row><cell>[45] a CNN+MDLSTM+LM</cell><cell></cell><cell></cell><cell>4.8</cell><cell>20.9</cell><cell></cell></row><row><cell>[45] b CNN+RNN</cell><cell></cell><cell></cell><cell>5.1</cell><cell>21.1</cell><cell></cell></row><row><cell>Ours (line-level model)</cell><cell>4.49</cell><cell>18.22</cell><cell>4.25</cell><cell>17.14</cell><cell>1.7 M</cell></row><row><cell>Ours (VAN on lines)</cell><cell>4.42</cell><cell>18.17</cell><cell>4.10</cell><cell>16.29</cell><cell>2.7 M</cell></row><row><cell>Ours (VAN on paragraphs)</cell><cell>4.01</cell><cell>15.47</cell><cell>3.83</cell><cell>13.94</cell><cell>2.7 M</cell></row><row><cell>a results from RWTH.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>b results from BYU.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 15 :</head><label>15</label><figDesc>Dropout strategy analysis. Results are given for the IAM test set.</figDesc><table><row><cell></cell><cell cols="2"># type</cell><cell>p</cell><cell>diffused</cell><cell cols="4">line-level model CER (%) WER (%) CER (%) WER (%) VAN</cell></row><row><cell cols="2">Baseline 3</cell><cell>mix</cell><cell>0.5/0.25</cell><cell></cell><cell>5.01</cell><cell>16.49</cell><cell>4.45</cell><cell>14.55</cell></row><row><cell>(1)</cell><cell>3</cell><cell>std.</cell><cell>0.5</cell><cell></cell><cell>5.24</cell><cell>17.23</cell><cell>4.46</cell><cell>14.88</cell></row><row><cell>(2)</cell><cell>3</cell><cell>2d</cell><cell>0.25</cell><cell></cell><cell>5.38</cell><cell>17.64</cell><cell>4.72</cell><cell>15.63</cell></row><row><cell>(3)</cell><cell>1</cell><cell>mix</cell><cell>0.5/0.25</cell><cell></cell><cell>5.33</cell><cell>17.70</cell><cell>4.43</cell><cell>15.25</cell></row><row><cell>(4)</cell><cell>1</cell><cell>std.</cell><cell>0.5</cell><cell></cell><cell>5.56</cell><cell>18.40</cell><cell>4.78</cell><cell>15.91</cell></row><row><cell>(5)</cell><cell>1</cell><cell>2d</cell><cell>0.25</cell><cell></cell><cell>5.70</cell><cell>18.92</cell><cell>4.93</cell><cell>16.80</cell></row><row><cell>(6)</cell><cell>3</cell><cell>mix</cell><cell>0.5/0.25</cell><cell></cell><cell>6.76</cell><cell>21.13</cell><cell>6.32</cell><cell>19.73</cell></row><row><cell>(7)</cell><cell>3</cell><cell>mix</cell><cell>0.16/0.08</cell><cell></cell><cell>6.71</cell><cell>20.91</cell><cell>4.64</cell><cell>15.36</cell></row><row><cell>(8)</cell><cell>3</cell><cell>mix</cell><cell>0.16/0.08</cell><cell></cell><cell>7.51</cell><cell>23.60</cell><cell>5.57</cell><cell>18.50</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The present work was performed using computing resources of CRIANN (Regional HPC Center, Normandy, France) and HPC resources from GENCI-IDRIS (Grant 2020-AD011012155). This work was financially supported by the French Defense Innovation Agency and by the Normandy region.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional network with dilated convolutions for handwritten text line segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Soullard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition, IJDAR</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="177" to="186" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ICML</title>
		<meeting>the 32nd International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An efficient end-to-end neural model for handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">202</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A computationally efficient pipeline approach to full page offline handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delteil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Learning, WML@ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Text line segmentation of historical documents: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Likforman-Sulem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zahour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taconet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition, IJDAR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="123" to="138" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Handwritten document image segmentation into text lines and words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Papavassiliou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katsouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carayannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="369" to="377" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Handwritten document offline text line segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weliwitage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Harvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Digital Image Computing: Techniques and Applications</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Line detection and segmentation in historical church registers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feldbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>T?nnies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Document Analysis and Recognition, ICDAR</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="743" to="747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-script iterative steerable directional filtering for handwritten text line extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Swaileh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ait-Mohand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Workshop on Multilingual OCR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1241" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">dhsegment: A generic deeplearning approach for document segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seguin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kaplan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th International Conference on Frontiers in Handwriting Recognition, ICFHR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A two-stage method for text line detection in historical documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gr?ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leifert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strau?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Labahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition, IJDAR</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="285" to="302" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Markov models for offline handwriting recognition: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pl?tz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition, IJDAR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="298" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An hmmbased approach for off-line unconstrained handwritten word modeling and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>El-Yacoubi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gilloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Suen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence, TPAMI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="752" to="760" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Lerec: a NN/HMM hybrid for on-line handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Nohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1289" to="1303" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A hybrid radial basis function network/hidden markov model handwritten word recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gilloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lemari?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leroux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Document Analysis and Recognition, ICDAR</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="394" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From characters to words: dynamical segmentation and predictive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garcia-Salicetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dorizzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wimmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gentric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="3442" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A neural network-hidden markov model hybrid for cursive word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Knerr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Augustin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition, ICPR</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="1518" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tandem HMM with convolutional neural network for handwritten word recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2390" to="2394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Forward-backward retraining of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 8, NIPS</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="743" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved handwriting recognition by combining two forms of hidden markov models and a recurrent neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Frinken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arti?res</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Analysis of Images and Patterns</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5702</biblScope>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Handwriting recognition with large multidimensional long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Frontiers in Handwriting Recognition, ICFHR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="228" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data augmentation for recognition of handwritten words and lines using a CNN-LSTM network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="639" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Handwriting recognition using Cohort of LSTM and lexicon verification with extremely large lexicon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stuner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia Tools and Applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluating sequenceto-sequence models for handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Labahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gr?ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Z?llner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition, ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1286" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Have convolutions already made recurrence obsolete for unconstrained handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coquenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Soullard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="65" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Accurate, data-efficient, unconstrained text recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">F</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">S</forename><surname>Mohammed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page">107482</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrence-free unconstrained handwritten text recognition using gated fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Coquenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paquet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th International Conference on Frontiers in Handwriting Recognition, ICFHR, 2020</title>
		<imprint>
			<biblScope unit="page" from="19" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">End-to-end handwritten text detection and transcription in full pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llad?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Machine Learning</title>
		<imprint>
			<publisher>WML@ICDAR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A neural model for text localization, transcription and named entity recognition in full pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forn?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Llad?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="219" to="227" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Full-page text recognition: Learning where to start and when to stop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Moysset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kermorvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="871" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Start, follow, read: End-to-end full-page handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11210</biblScope>
			<biblScope unit="page" from="372" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training full-page handwritten text recognition models without annotated line breaks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wigington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition, ICDAR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joint line segmentation and transcription for end-to-end handwritten paragraph recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="838" to="846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scan, attend and read: End-to-end handwritten paragraph recognition with MDLSTM attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Messina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1050" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-dimensional connectionist classification: Reading text in one step</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schambach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename><surname>Franz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th IAPR International Workshop on Document Analysis Systems</title>
		<imprint>
			<publisher>DAS</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="405" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Origaminet: Weakly-supervised, segmentation-free, one-step, full page text recognition by learning to unfold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yousef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition, CVPR, 2020</title>
		<imprint>
			<biblScope unit="page" from="14" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The iam-database: an english sentence database for offline handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Marti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Document Analysis and Recognition, IJDAR</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="46" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research, JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ICDAR 2011 -french handwriting recognition competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grosicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Abed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition, ICDAR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1459" to="1463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">ICFHR2016 competition on handwritten text recognition on the READ dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Toselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Frontiers in Handwriting Recognition, ICFHR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Are multidimensional recurrent layers really necessary for handwritten text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
