<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Document-level Relation Extraction by Entity Knowledge Injection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
							<email>sunweijian@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Huawei Technologies Co., Ltd</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">State Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">National Institute of Healthcare Data Science</orgName>
								<orgName type="institution" key="instit2">Nanjing University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Document-level Relation Extraction by Entity Knowledge Injection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>relation extraction ? knowledge injection ? knowledge graph</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction (RE) aims to identify the relations between entities throughout an entire document. It needs complex reasoning skills to synthesize various knowledge such as coreferences and commonsense. Large-scale knowledge graphs (KGs) contain a wealth of real-world facts, and can provide valuable knowledge to document-level RE. In this paper, we propose an entity knowledge injection framework to enhance current document-level RE models. Specifically, we introduce coreference distillation to inject coreference knowledge, endowing an RE model with the more general capability of coreference reasoning. We also employ representation reconciliation to inject factual knowledge and aggregate KG representations and document representations into a unified space. The experiments on two benchmark datasets validate the generalization of our entity knowledge injection framework and the consistent improvement to several document-level RE models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) aims to recognize the semantic relations between entities in texts, which is beneficial to a variety of AI applications such as language understanding and knowledge graph (KG) construction. Early methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref> mainly cope with sentence-level RE, which detects the relations in a single sentence. However, a large number of relations span across multiple sentences <ref type="bibr" target="#b31">[32]</ref>, which calls for document-level RE in recent years. Compared with sentence-level RE, document-level RE is more challenging. It needs the RE models to conduct complex reasoning, e.g., coreference reasoning, factual reasoning and logical reasoning, throughout an entire document. <ref type="figure">Figure 1</ref> shows a real example. A document-level RE model is asked to find the relations between three named entities IBM Research Brazil, S?o Paulo and South America. From S1, IBM Research Brazil is located in South America may [S1] IBM Research Brazil is one of twelve research laboratories comprising IBM Research , its first in South America .</p><p>[S2] It was established in June 2010 , with locations in S?o Paulo and Rio de Janeiro .</p><p>? coref. location location <ref type="figure">Fig. 1</ref>. An example of document-level RE excerpted from <ref type="bibr" target="#b31">[32]</ref> be first recognized by the model. Then, with the help of coreference knowledge that connects the pronoun It in S2 to IBM Research Brazil in S1, the model can recognize that IBM Research Brazil is located in S?o Paulo. Since the model may not know the exact types of entities, only with the aid of extra knowledge in KGs like S?o Paulo is a city and South America is a continent, then it can confidently determine that the relation between them is continent rather than others. The entire reasoning process demands the document-level RE model to synthesize various knowledge and have powerful reasoning capabilities.</p><p>Recent years have witnessed that large-scale KGs, e.g., Wikidata <ref type="bibr" target="#b25">[26]</ref> and DBpedia <ref type="bibr" target="#b0">[1]</ref>, become a valuable asset in information extraction <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. A KG contains a collection of real-world facts, in which a fact is structured in the form of a triple (entity, property, value). Property can be either an attribute or a relation, and value can be either a literal for attribute triple or an entity for relation triple. Particularly for the RE task, the works in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> exploit one or very few attribute and relation triples (e.g., rdfs:label ) in KGs to enhance their models. Furthermore, they overlook the heterogeneity between KG representations and document representations, and aggregate them in a simple way like vector concatenation.</p><p>In this paper, we propose a novel entity knowledge injection framework to enhance existing document-level RE models. Specifically, we introduce a general knowledge injection layer between the encoding layer and the prediction layer of popular RE models. Based on it, we focus on injecting various entity knowledge from KGs into the document-level RE models. We tackle two key challenges:</p><p>First, how to inject coreference knowledge into document-level RE models? Coreference resolution plays a vital role in RE. However, the coreferences derived from coreference resolution tools and aliases in KGs may contain errors. If we directly import them into an RE model as strong guidance information, such as the edges in a document graph <ref type="bibr" target="#b14">[15]</ref>, it is likely to bring a downside effect. Therefore, we present coreference distillation to distill knowledge from the coreferences and inject it into an RE model, so that the model can ultimately acquire generalized coreference knowledge.</p><p>Second, how to inject factual knowledge into document-level RE models? KG contains a wealth of facts related to entities, which we want to exploit for RE. However, the representations of entities in a KG and the text representations of a document are learned in two different spaces, which demand to be reconciled together. We present representation reconciliation to fuse KG representations and document representations into a unified space, endowing the RE model with the factual knowledge of entities.</p><p>In summary, our main contributions in this paper are twofold:</p><p>-We define a general knowledge injection framework KIRE and design various knowledge injection tasks for document-level RE, such as coreference distillation for coreference knowledge and representation reconciliation for factual knowledge. These knowledge injection and RE tasks are optimized together by multi-task learning. (Sections 3 and 4) -We perform the experiments on two benchmark datasets DocRED <ref type="bibr" target="#b31">[32]</ref> and DWIE <ref type="bibr" target="#b32">[33]</ref> for document-level RE. The result comparison between seven RE models and the models after knowledge injection validates the generalization and stable improvement of our framework. (Section 5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Document-level RE. Document-level RE has attracted vast attention in the past few years. A considerable number of studies have been conducted, which can be generally divided into graph-based models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31]</ref> as well as sequence-based models <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b29">30]</ref>. Graph-based models build document graphs to capture the semantic information in a document, and design various neural networks to carry out inference on the built document graphs. DISCREX <ref type="bibr" target="#b14">[15]</ref> models words in a document as nodes and intra/inter-sentential dependencies as edges. Following this idea, Peng et al. <ref type="bibr" target="#b12">[13]</ref> make use of graph LSTM while BRAN <ref type="bibr" target="#b23">[24]</ref> employs Transformer to encode document graphs. Recently, LSR <ref type="bibr" target="#b10">[11]</ref>, GAIN <ref type="bibr" target="#b34">[35]</ref> and GLRE <ref type="bibr" target="#b26">[27]</ref> define more sophisticated document graphs to reserve more dependency information in a document. Sequence-based models adopt neural encoders like BERT to implicitly capture dependencies in a document, instead of explicitly building document graphs. Wang et al. <ref type="bibr" target="#b27">[28]</ref> use BERT to encode a document and design a two-step pipeline, which predicts whether a relation exists between two entities first, and then predicts the specific relation types. HIN <ref type="bibr" target="#b17">[18]</ref> also makes use of BERT but design a hierarchical model that integrates the inference information from the entity, sentence and document levels. Huang et al. <ref type="bibr" target="#b6">[7]</ref> extract three types of paths which indicate how the head and tail entities can be possibly related in the context, and predict the relations based on the extracted evidence sentences. ATLOP <ref type="bibr" target="#b37">[38]</ref> proposes localized context pooling to transfer attentions from pre-trained language models and adaptive thresholding to resolve the multi-label and multientity problem. SSAN <ref type="bibr" target="#b29">[30]</ref> modifies the attention mechanism in BERT to model the coreference and co-occurrence structures between entities, to better capture the semantic information in the context.</p><p>In this paper, our focus is injecting knowledge into these document-level RE models. Our entity knowledge injection framework KIRE is applicable to various models as long as they fall into our framework formulation.</p><p>Knowledge injection. A few works have studied how to inject external knowledge such as a KG into the RE task for performance improvement. RE-SIDE <ref type="bibr" target="#b20">[21]</ref> uses entity types and aliases while BERT EM+TM <ref type="bibr" target="#b3">[4]</ref> only uses entity types. They both consider very limited features of entities. RECON <ref type="bibr" target="#b1">[2]</ref> proposes separate models to encode attribute triples and relation triples in a KG and obtain corresponding attribute context embeddings and relation context embeddings, which are combined into sentence embeddings. KB-both <ref type="bibr" target="#b24">[25]</ref> utilizes entity representations learned from either hyperlinked text documents (Wikipedia) or a KG (Wikidata) to raise the information extraction performance including document-level RE. Different from all above, we integrate more types of knowledge including coreferences, attributes and relations symbiotically with more effective knowledge injection methods to address the document-level RE task.</p><p>Additionally, a few studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37]</ref> explicitly exploit incorporating knowledge from various sources such as encyclopedia knowledge, commonsense knowledge and linguistic knowledge into pre-trained language models with different injection strategies to improve the performance of language models in downstream tasks. However, the goal of these studies is orthogonal to this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework Formulation</head><p>According to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>, we formulate the document-level RE task as a multiple binary classification problem. Given a document annotated with entities and their corresponding textual mentions, the task aims to predict the relations for each entity pair in the document, where a relation is either a predefined type (e.g., country) or N/A for no relation. Note that there may be more than one relation for an entity pair.</p><p>A basic neural network model <ref type="bibr" target="#b31">[32]</ref> for document-level RE contains an encoding layer and a prediction layer. The encoding layer encodes an input document to obtain the context-sensitive representations of tokens (words) in it, and the prediction layer generates entity representations and predicts relations using the entity representations. In this paper, we add a knowledge injection layer between the encoding layer and the prediction layer, and many document-level RE models such as <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref> can be used as the basic model.</p><p>We regard a KG as the knowledge source for injection. A KG is defined as a 7-tuple G = (U, R, A, V, X, Y, C), where U, R, A and V denote the sets of entities, relations, attributes and literal values, respectively. X ? U ? R ? U denotes the set of relation triples, Y ? U ? A ? V denotes the set of attribute triples, and C denotes the set of coreference triples derived from G. By the alias information (e.g., skos:altLabel ) in G, any two aliases of an entity can constitute one coreference triple (m s , m t , p cr ), where m s , m t are two alias mentions and p cr is the coreference probability. We employ off-the-shelf coreference resolution models to find more coreference knowledge for pronouns (e.g., it and he), possessives (e.g., herself ), noun phrases (e.g., this work ), etc., in the document. p cr is set to the resolution confidence. Due to the main scope of this paper, we follow <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> and reuse entity linking tools to link the entities in the document to those in the KG.</p><p>Framework. Given a document D = {w 1 , . . . , w J }, where w j denotes the j th token in D, and a KG G, the framework of document-level RE with entity (Zest Airways, Inc., Q196817) (Philippines, Q928) KG Input document: Zest Airways, Inc. operated as AirAsia Zest ? was a low-cost airline based at ? in Philippines . It operated ? MultiHead attn.</p><formula xml:id="formula_0">= [ 1 , 2 , ?] 2 ( +1) 1 ( +1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation reconciliation</head><p>MultiHead attn.</p><p>Relation triples:</p><formula xml:id="formula_1">(Q196817, P749, Q1319117) (Q196817, P159, Q17189) (Q1319117, P17, Q928)</formula><p>Attribute triples:</p><formula xml:id="formula_2">(Q196817, description, defunct Philippine airline) (Q196817, alias, AirAsia Zest) (Q928, description, sovereign state in Southeast Asia)</formula><p>Coreference triples:</p><p>(Zest Airways, Inc., AirAsia Zest, 1.0) (Zest Airways, Inc., It, 0.7) (Zest Airways, Inc., AirAsia Zest,</p><formula xml:id="formula_3">1.0) 4 ( +1) 3 ( +1) 2 ( +1) 1 ( +1) = 1. . aggregaters 4 ( ) 3 ( ) 2 ( ) 1 ( ) 2 ( ) 1 ( )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coreference distillation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context exchanging</head><p>Coref. encoding</p><formula xml:id="formula_4">[ 1 , 1 , 2 , 2 , ? ]</formula><p>Relation triple encoding Attribute triple encoding KG encoding </p><formula xml:id="formula_5">1 ( ) 2 ( ) 3 ( ) 4 ( ) Enhanced token reps.: [ 1 ? , 2 ? , ? ] [ 1 0 , 2 0 ? ] [ 1 ( ) ,</formula><formula xml:id="formula_6">H = [h w1 , . . . , h w J ] = Encode(D), H = KnowledgeInject(D, H, G), z = Predict(H ),<label>(1)</label></formula><p>where h wj denotes the hidden representation of w j , and z denotes the prediction probability distribution of relations. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Knowledge Injection</head><p>The architecture of the proposed knowledge injection framework KIRE is depicted in <ref type="figure">Figure 2</ref>, which accepts the document D, the hidden representation H of D and the relevant KG G as input. It injects the entity knowledge from the coreference triples, attribute triples and relation triples into an RE model, and outputs the final hidden representation H . Specifically, we inject the coreference triples into the basic document-level RE model with coreference distillation and context exchanging. Apart from this, the attribute triples are semantically encoded with AutoEncoder <ref type="bibr" target="#b15">[16]</ref>, and the encoded results are then reused to initialize the representations of relation triples. We use a relational graph attention network (R-GAT) <ref type="bibr" target="#b2">[3]</ref> to encode the relation triples and generate the KG representations of entities. Finally, the KG representations of entities and the token representations that have been enhanced by coreference knowledge are aggregated by representation reconciliation. Details are described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Coreference Encoding</head><p>This module leverages coreference triples to exchange the contextual information between aliases, and thus the representations of alias mentions can be closer.</p><p>Coreference Distillation. A simple method is to model the coreference triples as a new type of edges in the document graph and reuse graph-based models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b23">24]</ref>. However, such a method cannot be generalized to the sequencebased models since they do not construct document graphs. Furthermore, the accuracy of existing coreference resolution tools is still far from perfect, even they are trained on large corpora. To alleviate error accumulation, it is inappropriate to directly add the edges as strong guidance information in the RE models.</p><p>Knowledge distillation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>, as a model compression technique and a solution to integrate external knowledge into a target model, has been used in a wide range of NLP tasks. In this paper, we leverage the idea of knowledge distillation and propose coreference distillation to inject coreference triples into the RE models. Our main idea is to leverage a pre-trained coreference resolution model which has been trained on a large coreference dataset as the teacher model, and then force the student model (i.e., the RE model) to generate a prediction probability distribution that approximates the teacher model on the coreference triples. Finally, the student model learns the coreference knowledge and generalization ability in the teacher model. Formally, for a coreference triple (m s , m t , p cr ), its coreference probability generated by the teacher model is defined as</p><formula xml:id="formula_7">P tea (m s , m t ) = p cr .<label>(2)</label></formula><p>The student model generates the coreference probability with a multi-layer perceptron (MLP):</p><formula xml:id="formula_8">P stu (m s , m t ) = MLP m s ; m t ; ?(?(m s , m t )) ,<label>(3)</label></formula><p>where m s and m t denote the hidden representations of alias mentions m s and m t , respectively, which are calculated by averaging the hidden representations of tokens in m s and m t , that is, m s = avg wj ?ms (h wj ), m t = avg wj ?mt (h wj ). ";" is the concatenation operation, and ?(m s , m t ) denotes the shortest distance between m s , m t in the document. We divide the distance into {1, 2, . . . , 2 ? } bins, and associate each bin with a trainable distance vector. ?(?) associates each ? to the distance vector of relevant bin. Empirically, aliases with different distances should have different impacts on each other. Therefore, we propose trainable distance vectors to model and utilize such difference.</p><p>We enforce the student model to learn from the teacher model using the following coreference loss:</p><formula xml:id="formula_9">L cr = (ms,mt)?C KL P tea (m s , m t ) P stu (m s , m t ) ,<label>(4)</label></formula><p>where KL(?) is the Kullback-Leibler divergence.</p><p>Context exchanging. Based on the learned coreference knowledge, we further enable each alias mention to interact with its most similar counterpart, so as to exchange the semantic information between them. Specifically, given an alias mention m s , we update its hidden representation through m s = m s +m t * , where t * = arg max t P stu (m s , m t ) | (m s , m t ) ? C . In this way, the representations of the pronouns in particular can be enriched via their referents.</p><p>Finally, we obtain the token representations enhanced by coreference knowledge through the representations of alias mentions (if exists):</p><formula xml:id="formula_10">h wj = m s , if w j ? m s h wj , otherwise .<label>(5)</label></formula><p>In coreference encoding, the MLP contains d MLP (2d token + d dist ) + 2d token parameters, where d MLP , d token , d dist are the dimensions of MLP hidden layers, token representations and trainable distance vectors, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge Graph Encoding</head><p>This module aims to encode the attribute triples and the relation triples to generate the KG representations of entities.</p><p>Attribute triple encoding. A KG defines a set of common attributes, e.g., rdfs:label and schema:description, to describe its entities. We encode the attribute triples in the KG and generate the attribute representations for corresponding entities. For each attribute triple of an entity, we concatenate the attribute name a and attribute value v into a token sequence q = [a; v] = (w 1 , . . . , w M ). In order to cope with the out-of-vocabulary problem, we define a lookup function to convert each token to a token embedding:</p><formula xml:id="formula_11">LP(w j ) = WordEmb(w j ), if w j has word emb. CharEmb(w j ), otherwise ,<label>(6)</label></formula><p>where WordEmb(?) returns the word embedding in GloVe, and CharEmb(?) offers the average of character embeddings pre-trained with Skip-gram. Our method can work with other word or character embeddings easily. Next, we leverage AutoEncoder to encode a sequence of token embeddings into an attribute triple embedding in an unsupervised way:</p><formula xml:id="formula_12">q = AutoEncoder LP(w 1 ); . . . ; LP(w M ) ,<label>(7)</label></formula><p>where AutoEncoder is pre-trained on the attribute triples. We conduct selfsupervised training, and both encoder and decoder of AutoEncoder use BiLSTM. AutoEncoder has good capacity for feature extraction and compression. In our model, the input of AutoEncoder is a concatenation vector of an entity and its attributes. The reconstruction loss of AutoEncoder can help extract a better compressed feature representation while preserving the attribute knowledge.</p><p>Finally, we stack all attribute triple embeddings of an entity into a onedimensional CNN to obtain the attribute representation of the entity:</p><formula xml:id="formula_13">h 0 ei = MaxPooling CNN 1D ( j q j ) ,<label>(8)</label></formula><p>where denotes the stack operation, and h 0 ei is the attribute representation of entity e i , which would be used as the input representation for relation triple encoding below. Here, we choose CNN since the convolutional layer is a good feature extractor to learn high-level representations from value embeddings while reducing the dimension of output representations. Furthermore, we use the 1D convolution kernel as its invariance to the order of attribute embeddings.</p><p>Relation triple encoding. The relation triples present in the form of an entity-relation graph structure, and the topology and relation types are the key to encode such knowledge. Based on the attribute representations of entities, we employ a R-GAT <ref type="bibr" target="#b2">[3]</ref> with K layers to convolute the entity-relation graph. R-GAT incorporates relation types using different embeddings and calculates attention scores on all adjacent nodes based on entity embeddings and relation embeddings. Specifically, the node forward-pass update for the (k + 1) th layer is</p><formula xml:id="formula_14">e (k,b) ij = W (k,b) T out W (k,b) in h (k) i ; W (k,b) in h (k) j ; M(r ij ) , ? (k,b) ij = exp LeakyReLU(e (k,b) ij ) l?Ui exp LeakyReLU(e (k,b) il ) , h (k+1) i = 1 B B b=1 ? l?Ui ? (k,b) il W (k,b) in h (k) l ,<label>(9)</label></formula><p>where W are the node representations of entities e i and e j at the k th layer, respectively. M is a trainable mapping matrix corresponding to the relation types in the KG. r ij is the relation type between e i and e j . LeakyReLU(?) and ?(?) are the activation functions. U i is the neighbor set of e i . In this way, the entity representations are updated via their all adjacent entity embeddings and relation embeddings at the previous layer.</p><p>We refer to the representations of entities after graph convolution as the KG representations of entities, which encode the knowledge in both attribute triples and relation triples. We simply denote the KG representation of entity e i by h ei .</p><p>In the KG encoding, the attribute encoding has d Auto N max N kernel (d 2 kernel +1) parameters, where d Auto is the output dimension of AutoEncoder, N max is the maximum number of attributes that an entity has, N kernel is the number of kernels, and d kernel is the kernel size of CNN. The R-GAT network in the relation triple encoding contains 2(N layer ?1)N head d 2 RGAT +d RGAT d ent parameters, where N layer is the number of layers in R-GAT, N head is the number of attention heads at each layer of R-GAT, d RGAT is the hidden dimension of R-GAT hidden layers, and d ent is the dimension of entity representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Representation Reconciliation</head><p>The token representations and the KG representations of entities capture different knowledge in independent semantic spaces. Following ERNIE <ref type="bibr" target="#b36">[37]</ref> and K-BERT <ref type="bibr" target="#b9">[10]</ref>, we employ a representation reconciliation module to exchange the knowledge from entities in the KG with their linked tokens in the document.</p><p>The representation reconciliation module consists of N -stacked aggregators. For the n th (1 ? n ? N ) aggregator, given the token representations {h n?1 w1 , . . . , h n?1 w J } and the KG representations of entities {h n?1 e1 , . . . , h n?1 e I } from the preceding aggregator, the fusion phase is formulated as</p><formula xml:id="formula_15">h = ?(W (n) wh (n) wj +W (n) eh (n) ei +b (n) ), if w j , e i align ?(W (n) wh (n) wj +b (n) ), otherwise ,<label>(10)</label></formula><formula xml:id="formula_16">whereh (n) wj ,h (n)</formula><p>ei are the token representation and KG representation after the multi-head self-attention <ref type="bibr" target="#b21">[22]</ref>, respectively.W (n) w ,W (n) e ,b (n) are three trainable parameters. The information in the two semantic spaces is mutually integrated.</p><p>Then, the reconstruction phase leveragesh to refine the output representations of each token and entity in the aligned token-entity pairs:</p><formula xml:id="formula_17">h (n) wj = ?(W (n) wh + b (n) w ), h (n) ei = ?(W (n) eh + b (n) e ).<label>(11)</label></formula><p>Here, the aligned token representations and entity representations are updated and enhanced by the integrated information. Note that the representations of entities without aligned tokens would not be updated. Finally, we obtain the token representation sequence {h N w1 , . . . , h N w J } from the last aggregator, which would constitute H and be fed to the prediction layer.</p><p>To supervise the above process, we employ a token-entity alignment task. For each aligned token-entity pair (w j , e i ), we predict the aligned KG entity e i based on the token w j . We only ask the model to predict entities within a given entity candidate set. By default, all linked entities in the document form the candidate set. For the token sequence {w 1 , . . . , w J } and the corresponding candidate entities {e 1 , . . . , e I }, the token-entity alignment loss is</p><formula xml:id="formula_18">L kg = J j=1 I i=1 f * j,i * P (e i | w j ),<label>(12)</label></formula><p>where f * j,i ? {0, 1} is the true alignment label between w j and e i , and P (e i | w j ) = w j )?he l returns the probability that e i can be predicted by w j .</p><p>We optimize the RE loss, coreference loss and token-entity alignment loss with multi-task learning. The final loss is where ? 1 , ? 2 and ? 3 are the weight hyperparameters.</p><formula xml:id="formula_19">L = ? 1 ? L re + ? 2 ? L cr + ? 3 ? L kg ,<label>(13)</label></formula><p>In the representation reconciliation, for each aggregator, the multi-head selfattention networks contain 4d 2 token + 4d 2 ent parameters, the fusion phase contains d out (N token + N align ) + N token parameters, and the reconstruction phase contains 2N align (d out + 1) parameters, where d out is the output dimension of multi-head self-attention networks, N token is the number of tokens, and N align is the number of aligned token-entity pairs. Therefore, the parameters of representation reconciliation are N agg 4d 2 token + 4d 2 ent + d out (N token + N align ) + N token + 2N align (d out + 1) , where N agg is the number of aggregators.</p><p>Model complexity. The total parameter number of KIRE is</p><formula xml:id="formula_20">d MLP (2d token + d dist ) + 2d token + d Auto N max (d 2 kernel + 1) + 2(N layer ? 1)N head d 2 RGAT + d RGAT d ent + N agg 4d 2</formula><p>token + 4d 2 ent + d out (N token + N align ) + N token + 2N align (d out + 1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>We develop KIRE with PyTorch 1.7.1, and test on an X86 server with two Xeon Gold 5117 CPUs, 250 GB memory, two Titan RTX GPUs and Ubuntu 18.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Datasets. We select two benchmark datasets in our experiments: (1) DocRED <ref type="bibr" target="#b31">[32]</ref> is a crowdsourced dataset for document-level RE. The relation labels in its test set are not public. (2) DWIE <ref type="bibr" target="#b32">[33]</ref> is a new dataset for document-level multitask information extraction. We use the data relevant to RE only. Since DWIE does not have the validation set, we randomly split its training set into 80% for training and 20% for validation. <ref type="table" target="#tab_0">Table 1</ref> lists the statistical data. Knowledge graph. We select Wikidata (2020-12-01) as our KG due to its coverage and popularity <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21]</ref>. The numbers of its relation and attribute triples are 506,809,195 and 729,798,070, respectively. To prevent the test leakage, we filter out all the relation triples with entity pairs to be labeled in the test sets.</p><p>Evaluation metrics. We measure F1-score (F1) and Ignore F1-score (Ign F1) in our experiments. We repeat five times using the same hyperparameters but different random seeds, and report the means and standard deviations.</p><p>Implementation details. To achieve good generalization, we do not carry out excessive feature engineering on Wikidata. Numerical attributes are regarded as texts, and their semantics are captured by the word embeddings <ref type="bibr" target="#b13">[14]</ref>. We employ NeuralCoref 4.0 as our coreference resolution tool and also use the annotations provided in DocRED and DWIE. We use a two-stage method for training, which first trains a basic RE model and then fine-tunes this model to train the knowledge injection layer. The training procedure is optimized with Adam. Moreover, to compare fairly, the basic RE model and its corresponding KIRE adopt the same hyperparameter values. We set the batch size to 4 and the learning rate to 0.0005. We use three R-GAT layers and two aggregators. Moreover, ? 1 , ? 2 , ? 3 are 1, 0.01 and 0.01, respectively. The dimension of hidden layers in MLP is 256, the dimensions of GloVe and Skip-gram are 100, and the dimension of hidden layers in AutoEncoder is 50. See the source code for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>Improvement on baseline models. To validate the effectiveness and versatility of KIRE, we pick four baseline models in <ref type="bibr" target="#b31">[32]</ref>. The first three models directly employ CNN, LSTM and BiLSTM to encode documents, while the fourth model is called context-aware, which leverages the attention mechanism with BiLSTM. These four models are native to the DocRED dataset and widely chosen as the competitors in many RE studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>. <ref type="table" target="#tab_1">Table 2</ref> depicts the result improvement, and we observe that: (1) KIRE consistently improves the performance of all baselines on DocRED and DWIE, which demonstrates the good generalization of KIRE. Small standard deviations also tells the good stability of KIRE. (2) KIRE obtains a significant improvement of Ign F1/F1 up to 2.63/2.47 on DocRED and 2.03/1.95 on DWIE, respectively. This is mainly because the ways that the baseline models encode a document are too simple to capture some part of important contextual information in the document. External knowledge from KIRE makes up for this part, and therefore effectively improves the model performance. (3) CNN performs poorly, because the text order is important for RE while CNN cannot process such order well.</p><p>Comparison with existing knowledge injection models. We choose three recent models: RESIDE <ref type="bibr" target="#b20">[21]</ref>, RECON <ref type="bibr" target="#b1">[2]</ref> and KB-graph <ref type="bibr" target="#b24">[25]</ref>, which inject extra knowledge into RE models. Specifically, we use KB-graph instead of the full version KB-both since it selects Wikipedia as another knowledge source, which is unfair to other models. To compare fairly, we only adopt the knowledge injection modules of the above models to enhance the token representations in the documents, and the representations are used by the baseline RE models to predict the relation labels. <ref type="table" target="#tab_1">Table 2</ref> presents the comparison results, and we obtain several findings: (1) KIRE is consistently superior to RESIDE, RECON and KB-graph with an improvement of Ign F1/F1 up to 0.71/0.66 on DocRED and 0.39/0.43 on DWIE, respectively. Given that the test sets contain (ten) thousand relation instances, we think that the improvement makes sense. For example, on the validation set of DocRED, KIRE can correctly predict an average of 478 more instances than the second best method RECON. Such improvement brought by KIRE attributes to that KIRE absorbs more knowledge like coreferences and fuses the knowledge better. (2) The improvement brought by RESIDE is the lowest since it only injects limited knowledge like entity types and relation aliases. RE-CON and KB-graph explore more knowledge from the KG, but they still ignore the coreference knowledge. Besides, the methods that they employ to integrate knowledge are representation average or concatenation, which may lose part of semantic information in the injected knowledge.</p><p>Improvement on state-of-the-art models. We employ two sequencebased models, ATLOP <ref type="bibr" target="#b37">[38]</ref> and SSAN <ref type="bibr" target="#b29">[30]</ref>, as well as a graph-based model, GLRE <ref type="bibr" target="#b26">[27]</ref>, due to their good performance and open source. Enhancing these models is very challenging, since they have already explored various information in the documents and achieved state-of-the-art results. Due to the limit of GPU RAM, we use the BERT-base versions of ATLOP, SSAN and GLRE and re-run them according to the hyperparameters reported in their papers and source code.</p><p>The result improvement is shown in <ref type="table" target="#tab_2">Table 3</ref>, and we have several findings: (1) For the two sequence-based models, KIRE obtains an improvement of Ign F1/F1 up to 1.03/0.95 on DocRED and 3.08/1.81 on DWIE, respectively. This mainly attributes to the fact that the extra knowledge injected by KIRE can effectively help the models identify and capture more interactions between entity pairs especially across sentences. (2) For the graph-based model, KIRE obtains an improvement of Ign F1/F1 up to 1.14/1.09 on DocRED and 1.37/1.11, respectively. This is largely due to the fact that the extra knowledge injected by KIRE can enrich the representations of mention nodes and entity nodes in the document graphs for more accurate reasoning between entity pairs especially of longer distance. (3) This also verifies that our knowledge injection framework can be generalized to a broad range of document-level RE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Detailed Analysis</head><p>Ablation study. We conduct an ablation study on the four baseline models. For "w/o distill", we disable the coreference distillation module and directly use the original coreferences as the injected knowledge. For "w/o attr.", we initialize the relation triple representations by max pooling the word embeddings of entity labels. For "w/o rel.", we directly adopt the attribute representations of entities as KG representations. For "w/o KG", we disable the whole KG encoding module. Additionally, we replace KIRE with three simple variants for knowledge injection. For "w/ rep. avg", we average the hidden representations of alias mentions, and the token representations are averaged with KG representations of entities. For "w/ rep. concat", we concatenate the representations of alias mentions, and the KG representations of entities are concatenated after the aligned token representations. For "w/ MLP", we leverage two MLP layers to fuse the representations of alias mentions and the KG representations of entities with the aligned token representations, respectively.</p><p>From <ref type="figure">Figure 3</ref>, we can see that: (1) Ign F1/F1-scores reduce when we disable any modules, showing their contributions. <ref type="bibr" target="#b1">(2)</ref> The changes caused by removing one type of knowledge are not obvious, mainly due to the crossovers among the three types of knowledge in the information space. <ref type="formula" target="#formula_8">(3)</ref> The results decline if we disable the coreference distillation, due to the coreference errors in the injected knowledge. (4) If we remove the KG encoding, the results drop drastically, as the baseline models cannot generate extra relation and attribute knowledge. <ref type="bibr" target="#b4">(5)</ref> Compared to the three variants, the larger increase brought by KIRE validates the effectiveness of coreference distillation and representation reconciliation.</p><p>Influence of mention number. We measure the effectiveness of KIRE w.r.t. average mention number for each entity pair. For DocRED, we evaluate it on the validation set. The results are shown in <ref type="table" target="#tab_4">Table 4</ref>. We observe that KIRE gains higher performance for the entity pairs with more mentions, in particular when the average mention number &gt; 3. This is because KIRE injects knowledge into the RE models by updating the token representations of entity mentions, which has a greater impact on the entities with more mentions.</p><p>Comparison with alternative graph encoders. We compare R-GAT with GCN <ref type="bibr" target="#b8">[9]</ref>, GAT <ref type="bibr" target="#b22">[23]</ref> and R-GCN <ref type="bibr" target="#b16">[17]</ref>. We remove the coreference encoding and attribute triple encoding to eliminate their interference. From <ref type="figure">Figure 4,</ref>   <ref type="figure">Fig. 3</ref>. Results of ablation study performance of R-GCN and R-GAT is better than GCN and GAT, as they can capture the relation information in the entity-relation graphs. The results of GAT are greater than GCN, as GAT can selectively aggregate the neighboring information by self-attention. Similarly, R-GAT slightly outperforms R-GCN. Case study. We depict two successful cases and a failed case in <ref type="table">Table 5</ref>. We still use CNN, LSTM, BiLSTM and Context-aware as baselines. <ref type="table">Table 5</ref>. Case study. Target entities and related entities are colored.</p><p>[S1] The Waterloo Moraine ... was created as a moraine in the Regional Municipality of Waterloo, in Ontario, Canada.</p><p>[S2] It covers ... and some parts of the townships of Wellesley and North Dumfries. Case 1 Gold: P131 Baseline models: N/A + KIRE: P131</p><p>[S1] The news that British's Prince Harry is engaged to his partner Meghan Markle has attracted widespread attention from England, America and around the world.</p><p>[S10] Markle's parents Thomas Markle and Doria Ragland said in a statement: ... Case 2 Gold: citizen_of Baseline models: N/A + KIRE: citizen_of</p><p>[S1] Robert Kingsbury Huntington ... was a naval aircrewman and member of Torpedo Squadron 8 (or VT-8).</p><p>[S2] He was radioman/gunner to Ensign George Gay's TBD Devastator aircraft ...</p><p>[S4] Born in Los Angeles ... he was enlisted in the United States Navy 21 Apr. 1941. Case 3 Gold: P241 Baseline models: N/A + KIRE: N/A -Case 1. To identify the relation between North Dumfries in S2 and Regional Municipality of Waterloo in S1, we use the extra knowledge (Regional Municipality of Waterloo, instance of, regional municipality of Ontario) and the coreference of It and The Waterloo Moraine from NeuralCoref to correctly infer the relation P131 (located in the administrative territorial entity). -Case 2. With the aid of the extra knowledge (Meghan Markle, country of citizenship, United States of America) and the coreference of Meghan Markle and Markle from NeuralCoref, we successfully detect the relation citizen_of between Thomas Markle in S10 and America in S1. -Case 3. To recognize the relation between Ensign George Gay in S2 and the United States Navy in S4, we require a bridge entity Robert Kingsbury Huntington. Through the coreference of Robert and He from NeuralCoref, we identify that Robert and George are comrades. Then from S4, we find out that Robert is enlisted in the US Navy. According to this reasoning chain, we can see that the relation between George and the US Navy is P241 (military branch). KIRE fails to run such complex reasoning involving three sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose KIRE, an entity knowledge injection framework for enhancing document-level RE. Coreference knowledge is injected by coreference distillation, while factual knowledge is injected and fused with document representations via representation reconciliation. Our experiments validate the generalization and the stable performance increase of KIRE to various RE models. For future work, we plan to exploit other knowledge injection frameworks and integrate more knowledge sources.</p><p>Supplemental Material Statement: Source code for KIRE is available from Github at https://github.com/nju-websoft/KIRE. Datasets are available from <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>National</head><label></label><figDesc>Natural Science Foundation of China (No. 61872172). arXiv:2207.11433v1 [cs.CL] 23 Jul 2022</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 (Fig. 2 .</head><label>22</label><figDesc>Architecture of the knowledge injection layerknowledge injection is</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>out denote two trainable parameters of the b th attention head (1 ? b ? B) at the k th layer. h</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Dataset statistics. Inst. denotes relation instances excluding N/A relation.</figDesc><table><row><cell cols="2">Datasets</cell><cell cols="3">#Doc. #Rel. #Inst. #N/A Inst.</cell></row><row><cell></cell><cell>Training set</cell><cell cols="2">3,053 96 38,269</cell><cell>1,163,035</cell></row><row><cell>DocRED</cell><cell cols="3">Validation set 1,000 96 12,332</cell><cell>385,263</cell></row><row><cell></cell><cell>Test set</cell><cell cols="2">1,000 96 12,842</cell><cell>379,316</cell></row><row><cell></cell><cell>Training set</cell><cell cols="2">544 66 13,524</cell><cell>492,057</cell></row><row><cell>DWIE</cell><cell>Validation set</cell><cell>137 66</cell><cell>3,488</cell><cell>121,750</cell></row><row><cell></cell><cell>Test set</cell><cell>96 66</cell><cell>2,453</cell><cell>78,995</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of result improvement on baseline models 91?0.02 45.99?0.05 42.61?0.06 44.80?0.08 38.21?0.04 49.09?0.06 40.06?0.08 51.21?0.13 + KIRE 46.18?0.04 48.21?0.06 45.24?0.05 47.27?0.08 39.68?0.05 50.49?0.09 42.09?0.04 53.16?0.08 + RESIDE 45.03?0.06 47.12?0.08 43.79?0.05 45.96?0.09 39.24?0.04 50.13?0.07 41.31?0.06 52.47?0.11 + RECON 45.57?0.04 47.64?0.07 44.53?0.07 46.68?0.10 39.42?0.03 50.34?0.06 41.73?0.07 52.74?0.09 + KB-graph 45.49?0.03 47.58?0.08 44.46?0.06 46.61?0.09 39.34?0.06 50.26?0.09 41.65?0.08 52.63?0.12 LSTM 48.49?0.05 50.41?0.07 47.41?0.04 49.47?0.10 52.79?0.03 63.61?0.08 54.87?0.07 65.17?0.14 + KIRE 50.41?0.03 52.49?0.06 49.55?0.06 51.72?0.09 54.11?0.04 64.86?0.08 56.74?0.05 66.91?0.07 + RESIDE 49.58?0.04 51.49?0.08 48.52?0.06 50.51?0.09 53.87?0.02 64.56?0.06 55.96?0.06 66.29?0.12 + RECON 50.03?0.03 51.98?0.08 49.07?0.07 51.12?0.12 53.98?0.03 64.69?0.07 56.35?0.04 66.51?0.08 + KB-graph 49.94?0.04 51.89?0.07 48.98?0.05 51.04?0.09 53.91?0.05 64.61?0.08 56.27?0.06 66.43?0.09 BiLSTM 48.51?0.04 50.54?0.08 47.58?0.05 49.66?0.11 53.95?0.05 63.96?0.07 54.91?0.09 65.39?0.11 + KIRE 50.46?0.02 52.65?0.05 49.69?0.04 51.98?0.07 55.86?0.05 65.77?0.09 56.88?0.05 67.02?0.08 + RESIDE 49.64?0.03 51.59?0.06 48.62?0.04 50.71?0.10 55.04?0.06 65.01?0.09 56.16?0.05 66.47?0.12 + RECON 49.97?0.04 52.06?0.07 49.14?0.06 51.32?0.09 55.42?0.04 65.38?0.08 56.51?0.06 66.63?0.09 + KB-graph 49.89?0.03 51.98?0.07 49.05?0.05 51.26?0.08 55.35?0.03 65.31?0.09 56.42?0.07 66.55?0.11 Context-aware 49.79?0.03 51.84?0.04 48.73?0.07 50.91?0.12 54.68?0.04 64.29?0.06 56.53?0.07 65.91?0.09 + KIRE 51.07?0.03 53.25?0.07 50.43?0.05 52.75?0.10 56.58?0.03 65.62?0.07 58.41?0.04 67.37?0.08 + RESIDE 50.43?0.04 52.59?0.07 49.58?0.05 51.86?0.09 55.74?0.03 65.11?0.07 57.64?0.05 66.78?0.08 + RECON 50.78?0.03 52.89?0.06 49.97?0.04 52.27?0.08 56.12?0.05 65.48?0.08 58.02?0.06 66.94?0.10 + KB-graph 50.69?0.05 52.81?0.07 49.88?0.06 52.19?0.11 56.03?0.04 65.39?0.09 57.94?0.05 66.89?0.11</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DocRED</cell><cell></cell><cell></cell><cell></cell><cell>DWIE</cell></row><row><cell>Models</cell><cell cols="2">Validation set</cell><cell>Test set</cell><cell></cell><cell cols="2">Validation set</cell><cell>Test set</cell></row><row><cell></cell><cell>Ign F1</cell><cell>F1</cell><cell>Ign F1</cell><cell>F1</cell><cell>Ign F1</cell><cell>F1</cell><cell>Ign F1</cell><cell>F1</cell></row><row><cell>CNN</cell><cell>43.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Result improvement on state-of-the-art models 25?0.03 61.14?0.07 58.32?0.05 60.44?0.08 69.12?0.04 76.32?0.09 73.85?0.08 80.38?0.12 + KIRE 59.58?0.04 61.45?0.09 59.35?0.06 61.39?0.11 69.75?0.05 76.75?0.08 74.43?0.07 80.73?0.15 SSAN 56.68?0.03 58.95?0.04 56.06?0.05 58.41?0.06 51.80?0.05 62.87?0.10 57.49?0.09 67.77?0.12 + KIRE 57.29?0.05 59.31?0.06 56.31?0.06 58.65?0.08 52.67?0.06 63.64?0.10 60.57?0.09 69.58?0.12 GLRE 56.57?0.06 58.43?0.09 55.40?0.07 57.40?0.13 63.11?0.03 71.21?0.06 62.95?0.05 72.24?0.09 + KIRE 57.31?0.05 59.45?0.10 56.54?0.09 58.49?0.14 65.17?0.05 71.68?0.09 64.32?0.06 73.35?0.11</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DocRED</cell><cell></cell><cell></cell><cell></cell><cell>DWIE</cell></row><row><cell>Models</cell><cell cols="2">Validation set</cell><cell>Test set</cell><cell></cell><cell cols="2">Validation set</cell><cell>Test set</cell></row><row><cell cols="2">Ign F1</cell><cell>F1</cell><cell>Ign F1</cell><cell>F1</cell><cell>Ign F1</cell><cell>F1</cell><cell>Ign F1</cell><cell>F1</cell></row><row><cell>ATLOP 59.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>/o distill w/o attr. w/o rel. w/o KG w/ rep. avg w/ rep. concat w/ MLP</figDesc><table><row><cell>51.72 + KIRE w47.27 46.51 50.72 46.42 46.58 46.04 46.43 46.51 46.53 (ii) F1 44.32 44.24 44.39 43.98 44.26 44.35 44.39 45.24 49.55 48.49 (i) Ign F1</cell><cell>50.43 48.29</cell><cell>50.61 48.44</cell><cell>50.08 47.99</cell><cell>50.63 48.55</cell><cell>50.69 48.62</cell><cell>50.71 48.66</cell><cell>51.98 49.69</cell><cell>50.97 48.79</cell><cell>50.58 48.44</cell><cell>50.75 48.59</cell><cell>50.29 48.14</cell><cell>50.72 48.74</cell><cell>50.85 48.83</cell><cell>50.89 48.87</cell><cell>52.75 50.43</cell><cell>51.95 49.63</cell><cell>51.89 49.58</cell><cell>52.01 49.69</cell><cell>51.42 49.26</cell><cell>51.85 49.56</cell><cell>51.94 49.62</cell><cell>51.98 49.67</cell><cell>53.16 (iv) F1 52.21 52.14 52.25 51.72 42.09 41.07 40.98 41.09 40.69 (iii) Ign F1 51.95 52.04 40.89 40.92</cell><cell>52.12 41.01</cell><cell>66.91 56.74</cell><cell>66.03 55.86</cell><cell>65.85 55.64</cell><cell>65.99 55.78</cell><cell>65.63 55.41</cell><cell>65.76 55.54</cell><cell>65.81 55.59</cell><cell>65.89 55.67</cell><cell>67.02 56.88</cell><cell>65.99 55.77</cell><cell>65.96 55.72</cell><cell>66.08 55.87</cell><cell>65.57 55.36</cell><cell>65.79 55.58</cell><cell>65.85 55.69</cell><cell>65.99 55.81</cell><cell>67.37 58.41</cell><cell>66.25 57.13</cell><cell>66.21 57.03</cell><cell>66.35 57.27</cell><cell>65.97 56.69</cell><cell>66.05 56.83</cell><cell>66.12 56.95</cell><cell>66.25 57.09</cell></row><row><cell>CNN</cell><cell cols="5">LSTM</cell><cell></cell><cell></cell><cell cols="6">BiLSTM</cell><cell></cell><cell></cell><cell cols="6">Context</cell><cell></cell><cell>CNN</cell><cell></cell><cell></cell><cell></cell><cell cols="5">LSTM</cell><cell></cell><cell></cell><cell cols="6">BiLSTM</cell><cell></cell><cell></cell><cell cols="6">Context</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="10">(a) DocRED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">(b) DWIE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Results w.r.t. average mention number 44.27 44.42 46.45 45.38 47.35 39.35 50.44 40.81 51.96 41.94 53.05 + KIRE ?1.64 ?1.75 ?2.71 ?2.89 ?3.21 ?3.43 ?1.52 ?1.49 ?2.35 ?2.32 ?2.95 ?2.98 LSTM 47.39 49.35 48.59 50.53 50.09 52.08 54.14 64.41 55.97 66.14 57.21 67.55 + KIRE ?1.76 ?1.87 ?2.65 ?2.82 ?3.37 ?3.51 ?1.31 ?1.26 ?1.87 ?1.89 ?2.35 ?2.29 BiLSTM 47.35 49.32 48.61 50.54 50.21 52.28 54.07 64.61 56.01 66.25 57.30 67.79 + KIRE ?1.83 ?1.95 ?2.73 ?2.91 ?3.31 ?3.43 ?1.49 ?1.12 ?1.91 ?1.86 ?2.34 ?2.16 Context-aware 48.33 50.19 49.63 51.64 51.10 53.34 55.98 65.16 58.02 67.01 58.72 68.41 + KIRE ?1.43 ?1.56 ?2.35 ?2.47 ?2.87 ?2.98 ?1.35 ?0.91 ?1.92 ?1.57 ?2.32 ?1.95</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">DocRED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DWIE</cell></row><row><cell>Models</cell><cell></cell><cell>1</cell><cell>(1, 3]</cell><cell>&gt; 3</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(1, 3]</cell><cell>&gt; 3</cell></row><row><cell></cell><cell></cell><cell cols="9">Ign F1 F1 Ign F1 F1 Ign F1 F1 Ign F1 F1 Ign F1 F1 Ign F1 F1</cell></row><row><cell>CNN 45.58 45.79 (ii) F1 45.96 GCN 46.23 43.49 43.68 43.84 44.12 (i) Ign F1</cell><cell>49.72 47.61</cell><cell cols="5">65.24 R-GCN 65.35 52.23 55.02 55.14 41.07 (iii) Ign F1 51.55 51.71 (iv) F1 42.24 51.44 49.82 51.01 49.84 49.99 51.09 50.02 50.18 51.28 50.34 50.49 51.57 GAT 40.35 40.48 40.67 47.69 48.85 47.74 47.84 48.98 47.89 48.04 49.12 48.16 48.33 49.44</cell><cell>65.42 55.31</cell><cell>65.93 55.72</cell><cell>65.42 55.11</cell><cell>65.49 55.28</cell><cell>65.56 55.39</cell><cell>66.03 55.83</cell><cell>65.94 R-GAT 66.01 66.06 66.39 56.58 56.69 56.81 57.36</cell></row><row><cell>CNN</cell><cell></cell><cell>LSTM</cell><cell>BiLSTM</cell><cell>Context</cell><cell>CNN</cell><cell cols="3">LSTM</cell><cell cols="2">BiLSTM</cell><cell>Context</cell></row><row><cell></cell><cell></cell><cell cols="2">(a) DocRED</cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) DWIE</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="6">Fig. 4. Result comparison of graph encoders</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">RECON: Relation extraction using knowledge graph context in a graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">O</forename><surname>Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ACM</publisher>
			<biblScope unit="page" from="1673" to="1685" />
			<pubPlace>Online</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Relational graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Busbridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sherburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cavallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
		<idno>abs/1904.05811</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Enhancing online knowledge graph population with semantic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fern?ndez-Ca?ellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marco Rimmek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Espadaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Garolera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Codina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sastre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Riveiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bou-Balust</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="183" to="200" />
			<pubPlace>Athens, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language-agnostic relation extraction from wikipedia abstracts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="383" to="399" />
			<pubPlace>Vienna, Austria</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno>CoRR abs/1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three sentences are all you need: Local path enhanced document relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="998" to="1004" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Distant supervision for relation extraction with sentence-level attention and entity descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="3060" to="3066" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">K-BERT: Enabling language representation with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="2901" to="2908" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sekulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="1546" to="1557" />
			<date type="published" when="2020" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Entity enabled relation linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">V</forename><surname>Harmelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="523" to="538" />
			<pubPlace>Auckland, New Zealand</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. pp</title>
		<imprint>
			<biblScope unit="page" from="1532" to="1543" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction beyond the sentence boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="1171" to="1182" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="593" to="607" />
			<pubPlace>Heraklion, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">HIN: Hierarchical inference network for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="197" to="209" />
			<pubPlace>Singapore</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving event detection via open-domain trigger knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="5887" to="5897" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Weakly supervised short text categorization using world knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>T?rker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sack</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="584" to="600" />
			<pubPlace>Athens, Greece</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">RE-SIDE: improving distantly-supervised neural relation extraction using side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Prayaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="1257" to="1266" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="872" to="884" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Injecting knowledge base information into end-to-end joint entity and relation extraction and coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verlinden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zaporojets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of ACL</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1952" to="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kr?tzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. pp</title>
		<imprint>
			<biblScope unit="page" from="3711" to="3721" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fine-tune Bert for DocRED with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Knowledge enhanced pretrained language models: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Arnold</surname></persName>
		</author>
		<idno>abs/2110.08455</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="14149" to="14157" />
			<pubPlace>Online</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Document-level relation extraction with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="14167" to="14175" />
			<pubPlace>Online</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="764" to="777" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DWIE: An entity-centric dataset for multi-task document-level information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zaporojets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Develder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Demeester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">102563</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="2335" to="2344" />
			<date type="published" when="2014" />
			<publisher>COLING</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="1630" to="1640" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Graph convolution over pruned dependency trees improves relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP. pp</title>
		<imprint>
			<biblScope unit="page" from="2205" to="2215" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced language representation with informative entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="page" from="1441" to="1451" />
			<date type="published" when="2019" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI Press</publisher>
			<biblScope unit="page" from="14612" to="14620" />
			<pubPlace>Online</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
