<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WAVEMIX: RESOURCE-EFFICIENT TOKEN MIXING FOR IMAGES PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jeevan</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
							<email>asethi@iitb.ac.in</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering Indian Institute of Technology Bombay Mumbai</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WAVEMIX: RESOURCE-EFFICIENT TOKEN MIXING FOR IMAGES PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Wavelet transform ? Image classification ? Attention ? Efficient training</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although certain vision transformer (ViT) and CNN architectures generalize well on vision tasks, it is often impractical to use them on green, edge, or desktop computing due to their computational requirements for training and even testing. We present WaveMix as an alternative neural architecture that uses a multi-scale 2D discrete wavelet transform (DWT) for spatial token mixing. Unlike ViTs, WaveMix neither unrolls the image nor requires self-attention of quadratic complexity. Additionally, DWT introduces another inductive bias -besides convolutional filtering -to utilize the 2D structure of an image to improve generalization. The multi-scale nature of the DWT also reduces the requirement for a deeper architecture compared to the CNNs, as the latter relies on pooling for partial spatial mixing. WaveMix models show generalization that is competitive with ViTs, CNNs, and token mixers on several datasets while requiring lower GPU RAM (training and testing), number of computations, and storage. WaveMix have achieved State-of-the-art (SOTA) results in EMNIST Byclass and EMNIST Balanced datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most of the neural architectures that generalize well (infer accurately) on vision applications require power-hungry and expensive GPUs to train in a reasonable time, which is a bottleneck for many practical applications. This is especially true of vision transformer (ViT) architectures due to their use of self-attention Dosovitskiy et al. [2021], but is also a concern for convolutional neural networks (CNNs). Our objective was to propose and test neural architectures for image classification that use significantly fewer computational resources (GPU RAM for a fixed batch size, images per second, and model storage size) for training and testing, and yet generalize competitively with the state-of-the-art ViTs and CNNs.</p><p>The self-attention mechanism in the transformers <ref type="bibr" target="#b1">Vaswani et al. [2017]</ref> model long-range relationships between tokens and gives state-of-the-art generalization in NLP and image recognition Dosovitskiy et al. [2021]. However, the quadratic complexity of self-attention with respect to the sequence length (number of pixels in an unrolled image) creates a computational challenge for training ViT models. To some extent, this challenge has been alleviated by the use of sparse and linear attention mechanisms Tay et al. [2020]. However, we believe that the computational burden can be further reduced by using an appropriate inductive bias for images, which the transformers lack.</p><p>On the other hand, convolutional neural networks (CNNs) have the inductive bias to handle 2D images, such as the translational equivariance of convolutional layers and partial scale invariance of the pooling layers, which enables them to generalize well with smaller image datasets, while also consuming fewer computational resources. However, CNN layers are not well-structured to capture the long-range dependencies compared to self-attention models due to the local scope of convolutional and pooling operations. As a result, CNNs require more layers to increase their receptive fields (spatial token mixing) compared to ViTs.</p><p>More recently, hybrid vision X-formers Jeevan and Sethi [2022] that combine inductive priors of convolutional layers with relatively efficient long-range token mixing of linear attention mechanisms have been proposed Choromanski et al.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>[2021], <ref type="bibr" target="#b5">Xiong et al. [2021]</ref>. However, their data and computational requirements still remain impractical for many applications. By proposing WaveMix 1 , we take a step in the search for novel hybrid architectures for vision and further reduce the data and computational requirements for generalization comparable to ViTs and CNNs.</p><p>We replace the learnable attention mechanism in hybrid transformers with predefined (unlearnable) multi-level DWT layers for spatial token mixing. Our motivation is to utilize the well-researched multi-scale analysis properties of wavelet decomposition for image processing <ref type="bibr" target="#b6">Kingsbury [1997]</ref>. However, unlike previous works we do not simply use wavelet transforms to extract image features that are passed to a machine learning model. Instead, our architecture starts with a convolutional layer for short-range feature extraction (image-specific inductive bias), and then alternate between multi-level DWT for long-range spatial token mixing and convolutional layers for channel (and some spatial) token mixing. We thus obviate the need for image unrolling and even linear attention mechanisms. Additionally, wavelet decomposition introduces another form of image-specific inductive bias, that is hitherto unused in popular neural network architectures.</p><p>WaveMix achieves state-of-the-art (SOTA) generalization on EMNIST Balanced and Byclass datasets and performs better than transformers, ResNets and other token mixers in all the other datasets. It consumes orders of magnitude less GPU RAM than transformer models. When compared to CNNs, WaveMix performs on par with the deeper ResNets while using fewer parameters, layers, and GPU RAM. All of our experiments were done on a single GPU with 16GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Token mixing for images: Experiments have shown that replacing the self-attention in transformers with fixed token mixing mechanisms, such as the Fourier transform (FNet), achieves comparable generalization with lower computational requirements Lee- <ref type="bibr">Thorp et al. [2021]</ref>. Other token-mixing architectures have also been proposed that use standard neural components, such as convolutional layers and multi-layer perceptrons (MLPs) for mixing visual tokens. MLPmixer <ref type="bibr" target="#b7">Tolstikhin et al. [2021]</ref> uses two MLP layers (cascade of 1x1 convolutions) applied first to image patch sequence and then to the channel dimension to mix tokens. ConvMixer <ref type="bibr" target="#b8">Trockman and Kolter [2022]</ref> uses standard convolutions along image dimensions and depth-wise convolutions across channels to mix token information. These token mixing models perform well with lower computational costs compared to transformers without compromising generalization. The quadratic complexity with respect to the sequence length (number of pixels) for vanilla transformers has also led to the search for other linear transforms to efficiently mix tokens <ref type="bibr" target="#b3">Jeevan and Sethi [2022]</ref>.</p><p>Wavelets for images: Extensive prior research has uncovered and exploited various multi-resolution analysis properties of wavelet transforms on image processing applications, including denoising <ref type="bibr" target="#b9">Ruikar and Doye [2010]</ref>, superresolution <ref type="bibr" target="#b10">Guo et al. [2017]</ref>, recognition <ref type="bibr" target="#b11">Mahmood et al. [2018]</ref>, and compression <ref type="bibr" target="#b12">Lewis and Knowles [1992]</ref>. Features extracted using wavelet transforms have also been used extensively with machine learning models <ref type="bibr" target="#b13">Mowlaei et al. [2002]</ref>, such as support vector machines and neural networks Ranaware and Deshpande <ref type="bibr">[2016]</ref>, especially for image classification Nayak et al. <ref type="bibr">[2016]</ref>. Instances of integration with neural architectures include the following. ScatNet architecture cascades wavelet transform layers with non-linear modulus and average pooling to extract a translation invariant feature that is robust to deformations and preserves high-frequency information for image <ref type="bibr">classification Bruna and Mallat [2013]</ref>. WaveCNets replaces max-pooling, strided-convolution, and average-pooling of CNNs with DWT for noise-robust image classification <ref type="bibr" target="#b17">Li et al. [2020a]</ref>. Multi-level wavelet CNN (MWCNN) has been used for image restoration as well with U-Net architectures for better trade-off between receptive field size and computational efficiency <ref type="bibr" target="#b18">Liu et al. [2018]</ref>. Wavelet transform has also been combined with a fully convolutional neural network for image super resolution <ref type="bibr" target="#b19">Kumar et al. [2017]</ref>.</p><p>We propose using the two-dimensional discrete wavelet transform (2D DWT) for long-range token mixing. Among the different types of mother wavelets available, we used the Haar wavelet (a special case of the Daubechies wavelet <ref type="bibr" target="#b20">Daubechies [1990]</ref>) also known as Db1), which is frequently used due to its simplicity and faster computation. Haar wavelet is both orthogonal and symmetric in nature, and has been used to extract basic structural information from images <ref type="bibr" target="#b21">Porwik and Lisowska [2004]</ref>.  that scale invariance can be better modeled by wavelet decomposition due to its natural multi-resolution analysis properties. Additionally, the finer scale of a multi-level wavelet decomposition also incorporates the idea of linear space-invariant feature extraction using convolutional filters of small support; albeit it uses predefined weights. The basic idea, therefore, behind our proposed architecture is to alternate between spatially repeated (convolutional) learnable feature extraction and fixed multi-resolution token mixing using DWT for a few layers. Injecting learnability is key to improving the utility of the wavelet transform, while convolutional kernels allow parameter-efficient learning suitable for the location-invariant statistics of images. This combination requires far fewer layers and parameters than using only convolutional layers with pooling. On the other hand, while transformers and other token mixers have very large effective receptive fields right from the first few layers, they do not utilize inductive priors that are suitable for images. This is where the wavelet transform plays its role.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overall Architecture</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, in our models the input image is first passed through a convolutional layer that creates feature maps of the image. The use of trainable convolutions before the wavelet transform is a key aspect of our architecture, as it allows the extraction of only those feature maps that are suitable for the chosen wavelet family. This is followed by a series of WaveMix blocks, an MLP head, and global average pooling layer, and an output layer for classification. The global average pooling imparts a certain degree of size invariance to our architecture.</p><p>At no point in the model do we unroll the image into a sequence of pixels. So, we have developed a model that can exchange information between pixels which are separated by long distances without using self-attention, thereby escaping the quadratic complexity bottleneck of self-attention. WaveMix even eliminates the learning mechanism required for linear approximations to the quadratic attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">WaveMix Block</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, inside the WaveMix block, the input channels are decomposed into multiple levels of 2D DWT, which produces four output channels (one approximation and three details) for each input channel per DWT level. Channel mixing and reduction is performed by the MLP (two 1x1 convolutional layers separated by a GELU nonlinearity). Channel size reconciliation between multiple levels of DWT is performed using transposed convolutions (up-convolutions). The kernel size and stride of deconvolutional layers were chosen such that all the different sized outputs from different levels of DWT were brought back to the same size as the original image. We chose deconvolutional layer rather than an inverse DWT because the former is much faster and consumes less GPU than the latter. The outputs from the deconvolutional layers are then concatenated together (depth or channel-wise) and this output has the same number of channels as the input to the WaveMix block (embedding dimension). The concatenated output is then passed through a depth-wise convolutional layer with a kernel size of 5, followed by GELU activation and batch normalization. A residual connection <ref type="bibr" target="#b23">He et al. [2015]</ref> is provided within each WaveMix block so that the model can be made deeper with a larger number of blocks, if necessary.</p><p>The approximation and detail coefficients are extracted from the input using multi-level 2D DWT, as shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>. We used Haar wavelet (Db1) for generating the 2D DWT output 2 . In addition to the simplicity of implementation, an additional advantage of the Haar wavelet is that it reduces the size of a feature map exactly by a factor of 2?2, which makes the design of the deconvolution layer to increase the size back to the original easier. The number of levels of wavelet decomposition needed is decided based on the image size. Each level reduces the DWT output size by half. Therefore, we use as many levels as necessary till the input size is reduced to 2?2 to ensure token mixing over long spatial distances. For example, a 32?32 image requires a 4-level 2D DWT, which creates 16?16, 8?8, 4?4 and 2?2 sized outputs, respectively at different levels. To the the low-resolution image generated at each level (approximation sub-band) we concatenate in the channel dimension the corresponding three sets of detail coefficients from the same level. Hence, each level will output different sized images (16?16, 8?8, 4?4 and 2?2) each having 4 times more feature maps than the input embedding dimension.</p><p>We conjecture that the lower levels of the DWT capture the finer details of the image while higher levels capture more global information. The feed-forward (MLP or 1x1 convolutional) sub-layers immediately following this DWT have access only to the outputs at the corresponding level to learn the features. Once the features learned from each resolution level are passed through transposed convolutions, where all the different low-resolution images are up-sampled to full image size and concatenated along channel dimension, the succeeding depth-wise convolutional layer will have full access to all the local and global information carried by the tokens. Transposed convolution of the lower resolution DWT outputs will spread the global information to all regions of the image which helps the succeeding sub-layers model relationships between tokens both locally and globally. The depth-wise convolutional block processes the combined concatenated feature maps containing information in multiple resolutions of the image. This enables the model to mix information from different resolutions of the image along with mixing of global information from different spatial locations.</p><p>The presence of normalization and residual connections enable the construction of deeper models that can handle larger images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">WaveMix-lite Block</head><p>We created a lighter and faster version of WaveMix block as shown in <ref type="figure">Fig. 2</ref> to fit models with larger embedding dimensions into a single GPU with 16 GB RAM. First, the input to the WaveMix-Lite is passed through a convolutional layer which decreases the embedding dimension by four, so that the concatenated output after 2D DWT has the same dimension as input. To reduce the parameters and computations, we only use 1 level 2D DWT in WaveMix-Lite. The concatenated output from 2D DWT is passed to an MLP layer with GELU non-linearity having a multiplication factor more than one. This output is then passed through resizing deconvolutional sub-layer and then through a batch normalization sub-layer. A residual connection <ref type="bibr" target="#b23">He et al. [2015]</ref> is also provided within each WaveMix-Lite block. We remove the GELU non-linearity and depth-wise convolutional sub-layers used at the end of WaveMix block. These changes significantly reduce the number of parameters and GPU footprint of the model and increase its training speed, which are extremely useful while training large datasets. WaveMix-Lite is mostly used when we need high embedding dimensions. The larger embedding dimension available will ensure that all information about long-range global dependencies are passed to subsequent layers in one level of 2D DWT, without the need for multiple levels. We replace the WaveMix block with WaveMix-Lite block when using models with embedding dimensions larger than 64. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modifications made to 1D Token Mixers</head><p>Recently, it has been shown that 1-D Fourier transform of FNet and 1-D MLP-Mixer can be used for image classification without much modifications to give better results than a ViT of a comparable size <ref type="bibr" target="#b3">Jeevan and Sethi [2022]</ref>. However, in both those models the image had to be unrolled as a sequence of patches. We wanted to see if using a 2-D version of these models, where image can be processed in its 2-D form, can provide an advantage. That is, we compared with token mixing architectures that used a linear transform without learnable parameters (FNet) or a nonlinear transform with learnable parameters (MLP-Mixer) to understand if the wavelet transform is better suited for images than these other transforms. For a comparison of the performance of WaveMix architecture with the appropriate alternatives, we re-designed other 1D-token mixing architectures to suit image data. Since ConvMixer architecture was designed to handle image data in 2D form, no modifications were made to it.</p><p>The FNet Lee-Thorp et al. <ref type="bibr">[2021]</ref> architecture was built to handle long 1D sequence and not 2D images. So the model used by <ref type="bibr" target="#b3">Jeevan and Sethi [2022]</ref> in their experiments, which unrolled the image into pixel sequences, did not perform well. We modified the 1D FNet by replacing 1D Fourier transform with a 2D one, and then applied a 1D Fourier transform across the channel dimension and took the real parts. This enabled the information to mix across all the three dimensions of the image. The feed-forward layers were implemented using 1?1 kernel convolutional layers along with 2D batch normalisation. A residual connection <ref type="bibr" target="#b23">He et al. [2015]</ref> was added across the 2D FNet block. This 2D FNet block replaces the WaveMix block in the WaveMix architecture in our experiments.</p><p>The MLP-Mixer <ref type="bibr" target="#b7">Tolstikhin et al. [2021]</ref> was also redesigned to handle 2D image data by applying two MLPs along both height and width dimensions and another MLP along the channel dimension. We used layer normalization before the MLPs acting along height and width dimensions and a 2D batch normalization before the MLP along channel dimension. The three MLPs enable the mixing of tokens along the three dimensions of an image. A residual connection <ref type="bibr" target="#b23">He et al. [2015]</ref> was added across the 2D MLP-Mixer block. This 2D MLP-Mixer block replaces the WaveMix block in the WaveMix architecture in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training and Architectural Details</head><p>We trained models using Adam optimizer (? = 0.001, ? 1 = 0.9, ? 2 = 0.999, = 10 ?8 ) with a weight decay coefficient of 0.01. We used automatic mixed precision in PyTorch during training to optimize speed and memory consumption. Experiments were done with a 16 GB Tesla V100-SXM2 GPU available in Google Colab Pro Plus. No image augmentations were used while training the models. GPU usage for a batch size of 64 was reported along with top-1% accuracy from best of three runs with random initialization based on prevailing protocols <ref type="bibr" target="#b34">Hassani et al. [2021]</ref>. Maximum number of epochs in all experiments was set as 120.</p><p>Patch size of 1 was chosen for all the transformer models that unrolled the images as a sequence of pixels, such as the ViT. ConvMixer with kernel size of 8 and patch size of 1 was used for 32 ? 32 images and patch size of 2 for 64 ? 64 images. We used 64 benchmark points in the Nystr?mformer used in hybrid ViN. A dropout of 0.5 was used across all models.</p><p>In WaveMix, we applied two layers of 3?3 convolutions to the input image. These layers increased the channel dimension from three to the set embedding dimension in two stages. We observed that since we are constrained to   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Notation</head><p>We use the format Model Name-Embedding Dimension/Layers ? Heads for transformer based models and same notation without heads for other architectures. For example, CCT with embedding dimension of 128 having 4 layers and 4 heads is labelled as CCT-128/4 ? 4. ConvMixer performed better than FNet and MLP-Mixer because, just like wavelet transforms, convolutions also possess an inductive prior for exploiting spatial invariance in 2D image data. However, the higher accuracy obtained by WaveMix is due its ability to process an image at multiple resolutions in parallel, where it can learn image features obtained at different scales. This ability is absent in convolutional layers, which require pooling for large-scale information mixing, followed by quadratic attention or its linear approximations. The low GPU usage of WaveMix is due to the linearity of wavelet transforms which is easy to compute compared to convolutions and expensive self-attention matrices. The low GPU consumption of WaveMix is noteworthy when we add the fact that our model computes not just one, but four stages of wavelet transforms and processes them in parallel to get the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Main Results</head><p>The 2D FNet and 2D MLP-Mixer that use Fourier transforms and MLP, respectively, for token-mixing, could not match the generalization of WaveMix. This is due the ability of wavelet transform to better handle multi-resolution token-mixing for images, which is absent in these other two models. Although the Fourier transform is also in some sense a multi-resolution transform, it suffers from non-local analysis even for fine details, which is precisely why wavelets and cosine transform had replaced it for various image analysis tasks, such as image compression. This comparison with FNet and MLP-Mixer confirms that the presence of wavelet transform in our architecture is essential for the improved accuracy we observe in our models, since the other network components, such as feed-forward layers are present in all three token-mixing models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">In-depth Comparison with ResNets</head><p>Even though convolution has been widely regarded as a GPU-efficient operation, the need for deeper layers have necessitated the use of networks having over tens to hundreds of layers for achieving high generalization. Even though a single convolutional operation is comparatively cheaper than a 2D DWT, we can achieve generalization comparable to deep convolutional networks with very few layers of wavelet transforms. This ability of wavelet transform to provide competitive performance without needing large number of layers helps in improving the efficiency of the network by consuming much lesser GPU RAM than deep convolutional models like ResNets. We can see from <ref type="table" target="#tab_3">Table 2</ref> that WaveMix models outperforms ResNets on all the EMNIST datasets (28 ? 28). WaveMix achieved state-of-the-art (SOTA) results in EMNIST Balanced dataset (0.01 percentage points more than VGG-5 network with SpinalNet classification layers <ref type="bibr" target="#b36">Kabir et al. [2020]</ref>) and EMNIST Byclass dataset (0.31 percentage points more than the previous best <ref type="bibr" target="#b25">Cohen et al. [2017]</ref>).</p><p>Since the 2D DWT is a linear transformation, no learnable parameters are needed for token mixing in WaveMix. Hence, we observe that WaveMix requires significantly fewer parameters than ResNets.</p><p>We see from <ref type="table" target="#tab_4">Table 3</ref> that even when we test on larger datasets with millions of images and datasets having image sizes greater than 64 ? 64, the performance of the WaveMix model is still better than the ResNets. Datasets having hundreds of million images pose real challenges to the hardware available to most researchers. Even though image resolution had to be downscaled to 64 ? 64 using the initial convolutional layers, it does not affect the performance of the WaveMix models and they perform competitively with ResNets.</p><p>We have also experimented with ImageNet-1k, although the image size was downscaled to 64 ? 64 due to computational and storage constraints. We see from <ref type="table" target="#tab_6">Table 4</ref> that WaveMix can outperform the deep ResNet models like ResNet-50 and ResNet-101 while using less GPU RAM. The results from training large datasets including ImageNet-1k, iNaturalist-10k, and Places-365 shows that WaveMix is resource-efficient and performs on par with the ResNets even for datasets with large image sizes. Thus, WaveMix can allow practitioners to pre-train models with large datasets with millions of images using less GPUs, thus opening more possibilities for their applications. . These additional methods improve the results of the core architectures by a few percentage points each. However, experimenting with these additional training methods requires extensive hyperparameter tuning. On the other hand, by excluding these methods, we were able to compare the contribution of the base architectures in a uniform manner. Even though the accuracy obtained in our experiments for the other architectures are thus slightly lower than the previously reported numbers, the results are still within the expected range when such additional training tricks are not used. Another reason for the lower performance of these models is due to small number of epochs we used for training due to resource constraints. Running the models for hundreds and thousands of epochs will give better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Training and Inference Speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Discussion</head><p>Training architectures that require large GPU RAMs for any workable batch size, whose hyperparameters have to be tuned on large clusters, have become out-of-reach for most researchers who depend on affordable GPU servers or cloud services. We re-emphasize that our objective was to propose an innovative neural architecture that can be trained on affordable hardware (e.g., a single GPU) without compromising much on classification accuracy. It was neither our objective nor within our means to pursue a singular focus on beating the state-of-the-art accuracy while disregarding the computational effort and GPU RAM required for training.</p><p>We proposed an attention-less WaveMix architecture for token mixing for images by using 2D wavelet transform. The WaveMix architecture offers the best of both self-attention networks and CNNs by combining long distance token mixing of attention; and low GPU RAM consumption, efficiency, and speed of CNNs. It is better tailored for computer vision applications as it handles the data in 2D format without unrolling it as a sequence unlike the transformer models, such as ViT, CCT, CvT and hybrid xformers. Our experiments on image classification show that WaveMix achieves competitive accuracy with orders of magnitude lower GPU RAM consumption compared to transformer and convolutional models.</p><p>This work can be extended in several directions. Variants of the proposed architecture, such as those inspired from U-Net <ref type="bibr" target="#b43">Ronneberger et al. [2015]</ref> and <ref type="bibr">YOLO Redmon et al. [2016]</ref>, will need to be tried for other computer vision tasks, such as semantic segmentation and object detection. While we tested the simplest wavelet family (Haar), other wavelet families might give better results. It can also be tested whether the wavelet family itself needs to vary with the layer depth. Alternatively, the mother wavelet itself can be learned at different levels in an end-to-end manner. The role of the sparseness of the wavelet response at different levels can also be examined, as has been done for image compression. An additional redundancy that is not fully exploited by wavelets either is the rotational invariance, for which other mechanisms are needed.</p><p>The high accuracy of image classification by transformers and CNNs comes with high costs in terms of training data, computations, GPU RAM, hardware costs, form factors, and power consumption <ref type="bibr" target="#b45">Li et al. [2020b]</ref>, while in several practical situations there are tight constraints on these factors. Overall, our research suggests that alternatives to convolutional or attention-based architectures for vision need to be explored to better exploit image redundancies to reduce these requirements, while still generalizing well. Neural architectures that exploit domain-specific inductive biases have previously (and in the present study) resulted in such improvements, and this search for alternative architectural innovations must continue.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>WaveMix Architecture: (a) Overall architecture with initial convolutional layer, WaveMix blocks, global average pooling, and the final classification head; (b) details of the WaveMix block used in the overall architecture; and (c) representation of the multiple levels of the wavelet transform used in a WaveMix block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: WaveMix-Lite Block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Datasets and models comparedTo demonstrate the general applicability of WaveMix, we used multiple types of image datasets based on the number of images and image size. Small datasets of smaller image sizes included CIFAR-10, CIFAR-100<ref type="bibr" target="#b24">Krizhevsky [2009]</ref>,EMNIST Cohen et al. [2017], FashionMNIST Xiao et al. [2017], andSVHN Netzer et al. [2011]. Small datasets of larger image sizes included STL-10 Coates et al.[2011], Caltech-256<ref type="bibr" target="#b29">Griffin et al. [2007]</ref> and Tiny ImageNet<ref type="bibr" target="#b30">Le and Yang [2015]</ref>. We also used larger datasets with reduced image size (e.g., 64x64), such as Places-365<ref type="bibr" target="#b31">Zhou et al. [2017]</ref>, ImageNet-1k<ref type="bibr" target="#b32">Deng et al. [2009]</ref>, and iNaturalist 2021-10k (iNAT mini) Horn et al.[2021]. For comparison, we choseResNet-18 and ResNet-34, ResNet-50 and ResNet-101 He et al. [2015]  as convolutional models; FNet Lee-Thorp et al.[2021],MLP-Mixer Tolstikhin et al. [2021], and ConvMixerTrockman and Kolter [2022] as token-mixers; and  ViT Dosovitskiy et al. [2021], Hybrid ViN<ref type="bibr" target="#b3">Jeevan and Sethi [2022]</ref> CCT 3<ref type="bibr" target="#b34">Hassani et al. [2021]</ref>, and CvT<ref type="bibr" target="#b35">Wu et al. [2021]</ref> as transformer models. The 2D versions of FNet and MLP-Mixer were also used for the experiments, as explained next.</figDesc><table><row><cell>4 Experimental Settings</cell></row><row><cell>4.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison of top-1 accuracy (without data augmentation) and computational requirements of various models for image classification</figDesc><table><row><cell>Models</cell><cell>#Param (Million)</cell><cell>GPU (GB)</cell><cell>CIFAR -10 acc. (%)</cell><cell>CIFAR-100 acc. (%)</cell><cell>Tiny ImageNet acc. (%)</cell></row><row><cell>Convolutional Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet-18</cell><cell>11.2</cell><cell>1.2</cell><cell>86.29</cell><cell>59.15</cell><cell>48.11</cell></row><row><cell>ResNet-34</cell><cell>21.3</cell><cell>1.4</cell><cell>87.97</cell><cell>57.79</cell><cell>45.60</cell></row><row><cell>ResNet-50</cell><cell>25.2</cell><cell>3.3</cell><cell>86.21</cell><cell>58.48</cell><cell>48.77</cell></row><row><cell>Token Mixers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>2D FNet-64/5</cell><cell>0.10</cell><cell>1.0</cell><cell>64.60</cell><cell>29.32</cell><cell>15.53</cell></row><row><cell>2D FNet-128/5</cell><cell>0.41</cell><cell>1.6</cell><cell>70.52</cell><cell>32.13</cell><cell>26.56</cell></row><row><cell>2D MLP-Mixer-64/5</cell><cell>0.15</cell><cell>1.8</cell><cell>47.81</cell><cell>20.76</cell><cell>7.78</cell></row><row><cell>2D MLP-Mixer-128/5</cell><cell>0.45</cell><cell>3.7</cell><cell>55.02</cell><cell>22.81</cell><cell>11.12</cell></row><row><cell>ConvMixer-256/8</cell><cell>0.67</cell><cell>3.7</cell><cell>85.41</cell><cell>57.29</cell><cell>43.15</cell></row><row><cell>ConvMixer-256/16</cell><cell>1.3</cell><cell>7.0</cell><cell>88.46</cell><cell>61.80</cell><cell>45.39</cell></row><row><cell>Transformer Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-128/4 ? 4</cell><cell>0.53</cell><cell>13.8</cell><cell>56.81</cell><cell>30.25</cell><cell>26.43</cell></row><row><cell>Hybrid ViN-128/4 ? 4</cell><cell>0.62</cell><cell>4.8</cell><cell>75.26</cell><cell>51.44</cell><cell>34.05</cell></row><row><cell>CCT-128/4 ? 4</cell><cell>0.90</cell><cell>15.8</cell><cell>82.23</cell><cell>57.09</cell><cell>39.05</cell></row><row><cell>CvT-128/4 ? 4</cell><cell>1.10</cell><cell>15.4</cell><cell>79.93</cell><cell>48.29</cell><cell>40.69</cell></row><row><cell>WaveMix Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WaveMix-16/5</cell><cell>0.18</cell><cell>0.2</cell><cell>78.04</cell><cell>34.32</cell><cell>26.96</cell></row><row><cell>WaveMix-32/5</cell><cell>0.72</cell><cell>0.2</cell><cell>81.47</cell><cell>45.70</cell><cell>29.97</cell></row><row><cell>WaveMix-64/5</cell><cell>2.88</cell><cell>0.3</cell><cell>86.16</cell><cell>56.20</cell><cell>38.19</cell></row><row><cell>WaveMix-128/7</cell><cell>2.42</cell><cell>1.3</cell><cell>91.08</cell><cell>68.40</cell><cell>52.03</cell></row><row><cell>WaveMix-256/7</cell><cell>9.62</cell><cell>2.3</cell><cell>90.72</cell><cell>70.20</cell><cell>51.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Top-1 accuracy of WaveMix compared to ResNets on different EMNIST datasets GB GPU, we could not send image of size larger than 64 ? 64 into the WaveMix-Lite block for a batch size of 64. For images with resolutions larger than 64 ? 64, we adjusted the stride so that the output from the convolutional layers reduced the image resolution to 64 ? 64. Stride of 2 was used in both layers when we input 256 ? 256 images. For images of size less than 64 ? 64, we set the stride to 1. Unless otherwise stated, all WaveMix models with embedding dimension of 128 and 256 used WaveMix-Lite blocks.</figDesc><table><row><cell>Models</cell><cell cols="8">#Params Byclass Bymerge Letters Digits Balanced MNIST Fashion</cell></row><row><cell>ResNet-18</cell><cell>11.2</cell><cell>87.98</cell><cell>91.09</cell><cell>94.76</cell><cell>99.67</cell><cell>89.00</cell><cell>99.69</cell><cell>93.35</cell></row><row><cell>ResNet-34</cell><cell>21.3</cell><cell>88.10</cell><cell>91.13</cell><cell>95.04</cell><cell>99.68</cell><cell>89.17</cell><cell>99.67</cell><cell>93.34</cell></row><row><cell>ResNet-50</cell><cell>23.6</cell><cell>88.18</cell><cell>91.29</cell><cell>94.64</cell><cell>99.62</cell><cell>89.76</cell><cell>99.56</cell><cell>93.30</cell></row><row><cell>WaveMix-128/7</cell><cell>2.4</cell><cell>88.43</cell><cell>91.52</cell><cell>95.78</cell><cell>99.77</cell><cell>91.06</cell><cell>99.71</cell><cell>93.91</cell></row><row><cell>WaveMix-256/7</cell><cell>9.6</cell><cell>88.42</cell><cell>91.59</cell><cell>95.56</cell><cell>99.70</cell><cell>90.36</cell><cell>99.65</cell><cell>93.78</cell></row><row><cell>use a single 16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Top-1 accuracy of WaveMix compared to ResNets on datasets of different image resolutions</figDesc><table><row><cell>Models</cell><cell>STL-10 96 ? 96</cell><cell>SVHN 32 ? 32</cell><cell>Caltech-256 256 ? 256</cell><cell>Places-365 256 ? 256</cell><cell>iNAT-2021 256 ? 256</cell></row><row><cell>ResNet-18</cell><cell>70.41</cell><cell>97.40</cell><cell>52.97</cell><cell>48.74</cell><cell>26.35</cell></row><row><cell>ResNet-34</cell><cell>68.07</cell><cell>97.47</cell><cell>50.92</cell><cell>49.02</cell><cell>31.02</cell></row><row><cell>ResNet-50</cell><cell>66.04</cell><cell>97.32</cell><cell>49.97</cell><cell>49.80</cell><cell>33.14</cell></row><row><cell>WaveMix-256/7</cell><cell>70.88</cell><cell>97.61</cell><cell>54.62</cell><cell>49.83</cell><cell>33.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1</head><label>1</label><figDesc>shows the performance of WaveMix compared to other architectures on image classification using supervised learning on different datasets. WaveMix models outperform ResNets, transformers, hybrid xformers, and other tokenmixing models while requiring the least GPU RAM for the same batch size.</figDesc><table><row><cell>WaveMix-128 and WaveMix-256 achieve</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Generalization and GPU RAM usage for a batch size of 64 and maximum batch size possible in one 16 GB GPU for WaveMix and ResNets on ImageNet-1k dataset with image size downscaled to 64 ? 64</figDesc><table><row><cell>Models</cell><cell cols="5">GPU(GB) Top-1 Acc. (%) Top-5 Acc. (%) Max Batch Size</cell></row><row><cell>ResNet-18</cell><cell>2.6</cell><cell>50.67</cell><cell></cell><cell>75.07</cell><cell>384</cell></row><row><cell>ResNet-34</cell><cell>3.5</cell><cell>55.04</cell><cell></cell><cell>76.95</cell><cell>288</cell></row><row><cell>ResNet-50</cell><cell>11.3</cell><cell>55.66</cell><cell></cell><cell>78.40</cell><cell>96</cell></row><row><cell>ResNet-101</cell><cell>15.1</cell><cell>56.05</cell><cell></cell><cell>79.43</cell><cell>64</cell></row><row><cell>WaveMix-256/7</cell><cell>9.0</cell><cell>56.66</cell><cell></cell><cell>80.04</cell><cell>112</cell></row><row><cell cols="6">Table 5: Training (one forward and backward passes) and inference speeds (images/s) of various models for images</cell></row><row><cell cols="2">sizes of 32 ? 32 and 64 ? 64 on a 16GB GPU</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Models</cell><cell>Train 32 ? 32</cell><cell>Infer 32 ? 32</cell><cell>Train 64 ? 64</cell><cell>Infer 64 ? 64</cell></row><row><cell cols="2">ResNet-18</cell><cell>1571</cell><cell>3436</cell><cell>467</cell><cell>1389</cell></row><row><cell cols="2">ConvMixer-256/16</cell><cell>166</cell><cell>451</cell><cell>120</cell><cell>445</cell></row><row><cell cols="3">Hyb. ViN-128/4 ? 4 243</cell><cell>801</cell><cell>240</cell><cell>773</cell></row><row><cell cols="2">CCT-128/4 ? 4</cell><cell>149</cell><cell>521</cell><cell>149</cell><cell>514</cell></row><row><cell cols="2">WaveMix-16/5</cell><cell>3230</cell><cell>4310</cell><cell>797</cell><cell>2208</cell></row><row><cell cols="2">WaveMix-32/5</cell><cell>1834</cell><cell>3802</cell><cell>460</cell><cell>1623</cell></row><row><cell cols="2">WaveMix-128/7</cell><cell>1279</cell><cell>3401</cell><cell>276</cell><cell>488</cell></row><row><cell cols="2">WaveMix-256/7</cell><cell>495</cell><cell>1028</cell><cell>159</cell><cell>408</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>shows that WaveMix is significantly faster in training and inference than ConvMixer and transformers as it does not have the complexity of self-attention. WaveMix's speed is comparable to shallower ResNets, which can be attributed to WaveMix's ability to learn useful image representations with just few layers compared to CNNs. The lack of significant differences in training and inference speeds of ConvMixer and transformer models between the 2 image sizes is due to the variation in patch sizes and strides which essentially reshapes the 64 ? 64 image to 32 ? 32.We attribute the higher accuracy for the other architectures reported in their original papers to the the effects of various well-intentioned incremental training methods (tips and tricks), including RandAugment Cubuk et al.[2019], mixup<ref type="bibr" target="#b38">Zhang et al. [2017]</ref>, CutMix<ref type="bibr" target="#b39">Yun et al. [2019]</ref>, random erasing<ref type="bibr" target="#b40">Zhong et al. [2017]</ref>, gradient norm clippingZhang  et al. [2020], learning rate warmup Gotmare et al.[2019]  and cooldown, and timm augmentations<ref type="bibr" target="#b42">Wightman [2019]</ref> </figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Maximum train and test batch size possible for various models on a 16 GB GPU for training on CIFAR-100 datset</figDesc><table><row><cell>Model</cell><cell cols="2">Max batch size for training Max batch size for inference</cell></row><row><cell>ResNet-50</cell><cell>320</cell><cell>960</cell></row><row><cell>WaveMix-128</cell><cell>768</cell><cell>2176</cell></row><row><cell>WaveMix-256</cell><cell>448</cell><cell>768</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">WaveMix ArchitectureImage pixels have several interesting co-dependencies. The localized and stationary nature of certain image features (e.g., edges) has been exploited using linear space-invariant filters (convolutional kernels) of limited size. Scale invariance of natural images has been exploited to some extent by pooling<ref type="bibr" target="#b22">LeCun et al. [1998]</ref>. However, we think 1 Our code is available at https://github.com/pranavphoenix/WaveMix</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Base code: https://pytorch-wavelets.readthedocs.io/en/latest/readme.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Base code: https://github.com/lucidrains/vit-pytorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Resource-efficient hybrid x-formers for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Jeevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2022-01" />
			<biblScope unit="page" from="2982" to="2990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image processing with complex wavelets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with fourier transforms</title>
		<meeting><address><addrLine>James Lee-Thorp, Joshua Ainslie</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">357</biblScope>
			<biblScope unit="page" from="2543" to="2560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Patches are all you need?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asher</forename><surname>Trockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=TVHS5Y4dNvM" />
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image denoising using wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ruikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D D</forename><surname>Doye</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICMET.2010.5598411</idno>
	</analytic>
	<monogr>
		<title level="m">2010 International Conference on Mechanical and Electrical Technology</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="509" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep wavelet prediction for image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiep Huu</forename><surname>Hojjat Seyed Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monga</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPRW.2017.148</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1100" to="1109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facial expression recognition in image sequences using 1d transform and gabor wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmad</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hawke</forename><forename type="middle">A</forename><surname>Evans</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICAEM.2018.8536280</idno>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Applied and Engineering Mathematics (ICAEM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image compression using the 2-d wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Knowles</surname></persName>
		</author>
		<idno type="DOI">10.1109/83.136601</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="244" to="250" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature extraction with wavelet transform for recognition of isolated handwritten farsi/arabic characters and numerals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mowlaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Faez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Haghighat</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDSP.2002.1028240</idno>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Digital Signal Processing Proceedings. DSP 2002 (Cat. No.02TH8628)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="923" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detection of arrhythmia based on discrete wavelet transform using artificial neural network and support vector machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Preeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohini</forename><forename type="middle">A</forename><surname>Ranaware</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deshpande</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCSP.2016.7754470</idno>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Communication and Signal Processing (ICCSP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1767" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Brain mr image classification using two-dimensional discrete wavelet transform and adaboost with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ratnakar</forename><surname>Deepak Ranjan Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Banshidhar</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Majhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="188" to="197" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Mallat</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2012.230</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Wavelet integrated cnns for noise-robust image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiufu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linlin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multi-level wavelet-cnn for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengju</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for wavelet domain super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neeraj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruchika</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="65" to="71" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The wavelet transform, time-frequency localization and signal analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Daubechies</surname></persName>
		</author>
		<idno type="DOI">10.1109/18.57199</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="961" to="1005" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The haar-wavelet transform in digital image processing: Its status and achievements. Machine graphics &amp; vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Porwik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Lisowska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="79" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
		<ptr target="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.7665" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Emnist: Extending mnist to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="DOI">10.1109/IJCNN.2017.7966217</idno>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. CoRR, abs/1708.07747</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1708.07747" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v15/coates11a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>Geoffrey Gordon, David Dunson, and Miroslav Dud?k</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-04" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Benchmarking representation learning for natural world image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elijah</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimberly</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin Mac</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aodha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Escaping the big data paradigm with compact transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abulikemu</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Spinalnet: Deep neural network with gradual input. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Kabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moloud</forename><surname>Abdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed Mohammad Jafar</forename><surname>Jalali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeid</forename><surname>Atiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti</forename><surname>Nahavandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates training: A theoretical justification for adaptivity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Random erasing data augmentation</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Akhilesh Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno>abs/1810.13243</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Train big, then compress: Rethinking model size for efficient training and inference of transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuohan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5958" to="5968" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
