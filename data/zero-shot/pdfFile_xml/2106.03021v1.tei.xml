<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 SADRNet: Self-Aligned Dual Face Regression Networks for Robust 3D Dense Face Alignment and Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Ruan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Changqing</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhai</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Gangshan</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Limin</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 SADRNet: Self-Aligned Dual Face Regression Networks for Robust 3D Dense Face Alignment and Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Three-dimensional face dense alignment and reconstruction in the wild is a challenging problem as partial facial information is commonly missing in occluded and large pose face images. Large head pose variations also increase the solution space and make the modeling more difficult. Our key idea is to model occlusion and pose to decompose this challenging task into several relatively more manageable subtasks. To this end, we propose an end-to-end framework, termed as Self-aligned Dual face Regression Network (SADRNet), which predicts a posedependent face, a pose-independent face. They are combined by an occlusion-aware self-alignment to generate the final 3D face. Extensive experiments on two popular benchmarks, AFLW2000-3D and Florence, demonstrate that the proposed method achieves significant superior performance over existing state-of-the-art methods.</p><p>Index Terms-Three-dimensional deep face reconstruction, Dense face alignment, occlusion-aware attention.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Monocular 3D face reconstruction recovers 3D facial geometry from a single-view image. Dense face alignment (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>) locates all facial vertices of a face model. They are closely related to each other, and both play important roles in broad applications such as face recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, normalization <ref type="bibr" target="#b4">[5]</ref>, tracking <ref type="bibr" target="#b5">[6]</ref>, swapping <ref type="bibr" target="#b6">[7]</ref>, and expression recognition <ref type="bibr" target="#b7">[8]</ref> in computer vision and graphics.</p><p>The existing methods that simultaneously address 3D dense face alignment and face reconstruction (3D-DFAFR, for short) can be roughly grouped into two categories: model-based category and model-free category. Model-based methods infer the parameters of a parametric model <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, such as a 3D morphable model (3DMM <ref type="bibr" target="#b10">[11]</ref>), by solving a nonlinear optimization problem or directly regressing with convolutional neural networks (CNNs) <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Recent work <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b17">[18]</ref> achieves high-fidelity face shape and dense face alignment by using nonlinear 3DMM decoders to improve its representation power. Rather than using a parametric model, model-free methods obtain unrestricted 3D face structure and alignment information by directly inferring the 3D position of face vertices represented in specific forms (e.g., UV map <ref type="bibr" target="#b1">[2]</ref>, volume <ref type="bibr" target="#b20">[21]</ref>). Z. Ruan, G. Wu, L. Wang are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China (e-mail: mg1833060@smail.nju.edu.cn, gswu@nju.edu.cn, lmwang@nju.edu.cn).</p><p>C. Zou is with the School of Data and Computer Science, Sun Yat-sen University, Guangzhou, 510006, China (e-mail: aaronzou1125@gmail.com).</p><p>L. Wu is with the Samsung Electronics (China) R&amp;D Centre, Nanjing, 210012, China (e-mail: longhai.wu@samsung.com).</p><p>Although significant improvements have been achieved on the problem of 3D-DFAFR in a controlled setting during the past few years, 3D-DFAFR under unconstrained conditions is still yet to be well addressed. Specifically, under unconstrained conditions, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, self occlusions caused by large pose orientation and inter-object occlusion of hair and glasses could significantly reduce the useful information in an image, making it challenging to generate good results. Meanwhile, large pose diversity will make it hard to distinguish the shape variations and increase the modeling difficulty. Previous works mainly tackle these problems by increasing the data size and diversity of training data <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref> or performing strong regularization on the shape <ref type="bibr" target="#b22">[23]</ref>.</p><p>To addresses the challenges of 3D-DFAFR in the wild, this paper proposes an effective solution based on the following three motivations: <ref type="bibr" target="#b0">(1)</ref> an occluded region in the image does not contain any face information but may negatively affect the network prediction. (2) the occluded part of a face can only be inferred through the global facial structure or prior knowledge. (3) disentangling the face pose and face shape will significantly reduce the complexity of the problem of 3D-DFAFR and thus make it more tractable. Based on the above motivations, we believe occlusion and pose are two critical factors in achieving a robust 3D-DFAFR. Unfortunately, there are few 3D-DFAFR works explicitly handle with the face occlusions. The face pose estimation is also rarely discussed deeply in existing 3D-DFAFR works. Therefore, in this work, we propose to explicitly model these two factors to build a robust method for 3D-DFAFR.</p><p>We propose a self-aligned dual face regression network (SADRNet) for robust 3D dense face alignment and face reconstruction based on the above analysis. In particular, we present a dual face regression framework to decouple face pose estimation and face shape prediction in a low computational cost manner. These two regression networks share the same encoder-decoder backbone. They are equipped with its task-specific head design for predicting pose-dependent face and pose-independent face, respectively. The pose-independent shape regression could relieve the difficulty of directly predicting the original complex face shape under various poses and thus improve the shape prediction accuracy. Besides, to tackle the occlusion issue, we devise a supervised attention mechanism to enhance the discriminative features in visible areas while suppressing the occluded region's influence. The attention mechanism could be plugged into our dual face arXiv:2106.03021v1 [cs.CV] 6 Jun 2021 regression framework to improve prediction accuracy. Finally, we propose an occlusion-aware self-alignment module to combine the pose-dependent and pose-independent faces to yield the final face reconstruction. In this alignment module, we only use the visible and sparse face landmarks to estimate the pose parameters, which could further improve the robustness of our SADRNet. Our solution significantly improves the robustness toward face occlusions in the wild. It achieves a considerable margin on both face reconstruction and dense face alignment. In summary, the main contributions of this paper are:</p><p>? We propose a self-aligned dual faces regression framework, which is robust to face pose variation and occlusion, for the problem of 3D-DFAFR. <ref type="bibr">?</ref> We propose an attention-aware mechanism for visible face region regression, which can improve the regression accuracy and robustness under various situations of face occlusion. ? The proposed end-to-end architecture is efficient and it can run at 224 ? 224/70 FPS on a single GTX 1080 Ti GPU. ? The proposed method achieves a considerable margin on the challenging AFLW2000-3D <ref type="bibr" target="#b0">[1]</ref> dataset and Florence 3D Faces <ref type="bibr" target="#b23">[24]</ref> over the state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Face Reconstruction</head><p>Blanz and Vetter <ref type="bibr" target="#b10">[11]</ref> proposed 3DMM to represent a face model by a linear combination of orthogonal bases obtained by PCA. In this way, 3D face reconstruction can be formulated as 3DMM parameters regression problems. Later, Paysan et al. <ref type="bibr" target="#b24">[25]</ref> extended the model by adding more scans and decomposing the expression bases from shape bases. A lot of earlier methods regressed the 3DMM parameters by solving a nonlinear optimization function <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Thanks to the development of deep learning, some methods <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> started to use deep convolutional neural network (DCNN) architectures to learn 3DMM parameters and largely replaced traditional optimization-based methods with more accurate results and shorter running time. The self-supervised training of DCNNbased methods was implemented by exploiting a differentiable renderer <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, which alleviated the lack of 3D-supervised data and improved the generalization of networks. However, these model-based methods' reconstruction geometry is constrained by the linear bases with limited representation power.</p><p>Some works proposed to break the limitation by using nonlinear models <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b43">[44]</ref>. In <ref type="bibr" target="#b18">[19]</ref>, a DCNN was used as a nonlinear 3DMM decoder. Ranjan et al. <ref type="bibr" target="#b44">[45]</ref> used spectral graph convolutions to learn 3D faces. Zhou et al. <ref type="bibr" target="#b17">[18]</ref> presented a nonlinear 3DMM using colored mesh decoding. Guo et al. <ref type="bibr" target="#b45">[46]</ref> learn a more powerful nonlinear 3DMM from different data sources: scanned 3D face, RGB-D images, and RGB images.</p><p>Some other works directly obtained the full 3D geometry to avoid the restriction of parametric models and difficulty in pose estimation <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b44">[45]</ref>. Jackson et al. <ref type="bibr" target="#b20">[21]</ref> proposed to use a volumetric representation of 3D face shape instead of the previously used point cloud or mesh and directly regressed the voxels. Feng et al. <ref type="bibr" target="#b1">[2]</ref> mapped the mesh of a face geometry into UV position maps and then trained a light-weighted network that obtains the 3D facial geometry along with its correspondence information.</p><p>Some works also combine the 3DMM-based regression and direct 3D geometry regression to improve the reconstruction performance. Chen et al. <ref type="bibr" target="#b47">[48]</ref> used a 3DMM-based coarse model and a displacement map in UV space to represent a 3D face, and utilize the input image as supervision to effectively learn the facial details. Huber et al. <ref type="bibr" target="#b11">[12]</ref> proposed to estimate an intermediate volumetric geometry and finetune it with 3DMM parameters' regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Alignment</head><p>In the beginning, face alignment works aimed to locate a set of 2D facial landmarks in the image plane. Traditional works were mainly based on Active Appearance Models (AMM) and Active Shape Models (ASM) <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref> and considered face alignment as a model parameter optimization problem. Then cascaded regression methods <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref> became popular. They iteratively refined the predictions and reached higher accuracy. With the development of deep learning, CNNs were wildly used to regress the landmarks' positions directly or predict heat maps and largely improved the performance <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>. Since the 2D face alignment methods have limitations on detecting invisible landmarks, the 3D face alignment problem has been widely researched in recent years. There are two major strategies for 3D face alignment: (1) separately detecting 2D landmarks and their depth <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr">[?]</ref>, and (2) fitting a certain 3D face model <ref type="bibr" target="#b62">[63]</ref>, <ref type="bibr" target="#b63">[64]</ref> to obtain the full 3D structure to guide the 3D landmark localization.</p><p>Since the methods mentioned above can handle only a limited quantity of landmarks, which is far from enough in some applications, 3D dense face alignment <ref type="bibr" target="#b64">[65]</ref> started to be researched. It requires methods to offer pixel-wise facial region correspondence between two face images. As the prediction target changes from a sparse set of facial landmarks to a dense set of tens of thousands of points, it is natural to solve this problem by fitting a registered 3D face model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Dense Face Alignment and Face Reconstruction</head><p>Most model-based reconstruction approaches can be applied to dense face alignment if the face model is well registered and provides a dense correspondence <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b11">[12]</ref>. Explicit facial pose estimation is needed in these works. Zhu et al. proposed 3DDFA <ref type="bibr" target="#b0">[1]</ref>, which is a representative work in this task. They used a cascaded CNN framework to regress the 3DMM parameters, including the pose parameters. The rotation angles are represented by a 4D unit quaternion for less difficulty in learning. However, their output faces are sensitive to the fluctuation of every parameter and hard to reach high precision. In <ref type="bibr" target="#b11">[12]</ref>, an ICP post-processing is incorporated to refine the regressed pose parameters, but it takes enormous extra computation. Zhou et al. learned a nonlinear 3DMM by directly using graph convolutions on face meshes and reached an extremely fast decoding speed. Nevertheless, the face pose is still obtained by direct regression like 3DDFA.</p><p>Some model-free methods that directly predict 3D coordinates of facial points are also applicable to the task of dense face alignment with their face representation registered with fixed semantic meaning <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>. However, it is difficult for these model-free methods to handle severe occlusions and large poses since no prior knowledge or constraint is provided.</p><p>Our method's final output face geometry is represented by a UV position map, which possesses a dense correspondence between the face shape and the input image. Unlike the methods mentioned above, we explicitly deal with object occlusions by leveraging an attention mechanism on network features. We predict a pose-independent face model to avoid the large variance in face shape brought by large poses. Instead of directly regressing the pose parameters, we perform a visibility-aware self-alignment between a pose-dependent face model and a pose-independent face model to estimate the pose of a nonlinear 3DMM. The alignment process is stable as the error caused by a single outlier is apportioned by all of the landmarks. In contrast, the error of every pose parameter is accumulated in parameter-regressing methods. Besides, the alignment process is based on two regressed faces with similar shapes, rather than the matching of landmarks and a fixed face template. Therefore, the change of face shape is taken into account in pose estimation. In this way, our method is more robust and accurate than previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>In this section, we detail the SADRNet. We first introduce the dual face representation used in this work and the network architecture overview. We then present the occlusion-aware attention mechanism and the face fusion module. After that, we introduce the loss functions and implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dual Face Regression Framework</head><p>Facial geometry representation. We assume the projection from the 3D face geometry to the 2D image is a weak perspective projection:</p><formula xml:id="formula_0">V = Pr * G (1)</formula><p>where V is the projected geometry on the 2D plane, Pr = 1 0 0 0 1 0 is the projection matrix, G ? R 3?n is the 3D mesh of a specific face with n vertices. We separate the 3D face geometry into pose, mean shape, and deformation as:</p><formula xml:id="formula_1">G = f * R * S + t,<label>(2)</label></formula><formula xml:id="formula_2">S = (S + D),<label>(3)</label></formula><p>S ? R 3?n represents the pose-independent (i.e., posenormalized) face shape,S ? R 3?n is the mean shape template provided by <ref type="bibr" target="#b24">[25]</ref> and D ? R 3?n is the deformation between S andS. The pose parameters consist of the scale factor f , the 3D rotation matrix R ? R 3?3 and the 3D translation t ? R 3 . Dual face regression and analysis. We propose to jointly regress two face models: a pose-independent face (the face shape) S and a pose-dependent face P. And then, we use a self-alignment post-process ? to estimate face pose from P, S and facial vertices' visibility information Vis:</p><formula xml:id="formula_3">?(P, S, Vis) = f, R, t.<label>(4)</label></formula><p>The visibility information Vis is explained in Sec.III-B. Based on the estimated face pose ?(P, S, Vis) and pose-independent face S, we could reconstruct our final face shape G via transformation defined in Eq.2. In fact, G and P have the same physical meaning, which means the ground truth of them are the same:</p><formula xml:id="formula_4">P =?.<label>(5)</label></formula><p>The difference is that P is obtained by direct network regression, while G is obtained by applying Eq.2. To distinguish them, we term P as pose-dependent face and G as face geometry. The learning of pose-dependent face is easy to overfit to pose and under-fit to shape as the orientation variations bring much greater point-to-point distances than the shape variations, resulting in some implausible face shape under large pose cases. By contrast, the pose-independent face S does not change with the pose. Thus the network would focus on the shape characteristics and learn more details. S is disentangled into the mean face shape templateS and the deformation D between the actual shape and the mean shape.</p><p>Only the zero-centered D needs to be predicted. It further reduces the fitting difficulty. The mean face shape also serves as prior knowledge to keep the invisible facial parts plausible. By combining the shape of S with the pose ?(P, S, Vis) estimated from P and S, we are able to get a much better G, as demonstrated in experiments. UV map representation. The face geometry G, poseindependent face S, mean faceS, deformation D, and posedependent face P are transformed into UV space <ref type="bibr" target="#b1">[2]</ref> as UV maps. UV map U is a 2D representation of 3D vertices on the face mesh model. It can be expressed as</p><formula xml:id="formula_5">U(u i , v i ) = (x i , y i , z i ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_6">(x i , y i , z i ) is the 3D coordinate of vertex i on the 3D face mesh and (u i , v i ) is the corresponding 2D UV coordinate.</formula><p>The mapping relationship between the 3D object coordinate</p><formula xml:id="formula_7">(X i , Y i , Z i ) and the UV coordinate (u i , v i ) of a facial vertex i can be formulated as u i ? ? 1 ? Y i + ? 1 ,<label>(7)</label></formula><formula xml:id="formula_8">v i ? ? 2 ? arctan( X i Z i ) + ? 2 ,<label>(8)</label></formula><p>where ? 1 , ? 2 , ? 2 , ? 1 are scaling and translation constants.</p><p>The mapping relationship is computed on the mean face mesh from the Basel Face Model (BFM) <ref type="bibr" target="#b24">[25]</ref> and applied to all the face meshes. In this way, the points in the UV map are registered to the face mesh model. <ref type="figure">Fig. 3</ref> illustrates the mapping for a better understanding. This representation guarantees the spatial consistency between face model and UV map, i.e., spatially neighboring points on the face model are neighboring in the UV map. Since 2D UV maps can be processed by sophisticated CNNs, this ensures a great potential of the representation in applications of unconstrained situations. In the remainder of this paper, the face geometry, pose-independent face, mean face, deformation, and posedependent face are represented as UV maps. For clarity, the same notation is used for a thing in the two spaces (e.g., G is used to denote both the face geometry and the UV map of it).</p><formula xml:id="formula_9">(a) (b) (c) (d) (e)</formula><p>Network architecture. Our self-aligned dual face regression network is an encoder-decoder-based architecture that regresses the deformation D and infers the pose parameters f , R and t to reconstruct the 3D face geometry from a single 2D face image. It consists of three sub-networks: encoder, attention side branch, and decoder.</p><p>The encoder network contains a sequence of residual blocks that first extract low-level features, which are then fed into the attention side branch. The attention network generates an attention mask A with the visible face region highlighted. Then encoder network further encodes the attended features into high-level features. The decoder then decodes them into the pose-dependent face P and the deformation D. The face shape S is obtained by Eq. 3. The pose parameters f , R and t are then obtained from P, S and A in the self-alignment module. The final face geometry G is generated by applying Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Occlusion-Aware Attention Mechanism</head><p>To deal with the occlusions, we adopt an attention mechanism to extract features mainly from the visible face regions in the input images. This sub-network is a side branch consisting of five convolutional layers. It outputs a soft attention mask assigning high values to the pixels corresponding to the visible regions and low values to the pixels in the occluded regions and background region. The attention is applied to the lowlevel features to make a trade-off between resolution and accuracy.</p><p>Considering the attention mask may be inaccurate in some cases, we do not discard the information of the regions with low attention entirely. We instead highlight the features of the visible face regions according to the attention values. This operation can be formulated as</p><formula xml:id="formula_10">F a = F l exp(A),<label>(9)</label></formula><p>where F a denotes the obtained weighted feature map, F l is the low-level feature map, and A is the attention mask.</p><p>Once we get the attention mask, we are able to estimate the visibility of the vertices in the pose-dependent face through</p><formula xml:id="formula_11">Vis(i) = 0 if n z &lt; 0 A( x i , y i ) if n z ? 0,<label>(10)</label></formula><p>where the 3D position of vertex i is (x i , y i , z i ). The normal direction of vertex i that can be obtained from its adjacent vertices is (n x , n y , n z ). The vertex visibility is employed in the self-alignment module and is discussed detailedly in Sec. III-C.</p><p>Since no database has the ground truth for face occlusion annotation, we simulate occlusions through data augmentation. Specifically, we project the 3D face geometry to the image plane to generate a binary map that indicates the full face region. Then we overlay patterns with random shapes to the input image and use them as the occlusions as shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We use the resulting binary image as the ground truth of the attention mask. In this way, we can obtain the ground truth data for the training of the attention branch. Although occlusions in the real world can be very diverse, our experiments find the proposed method has the ability to predict real-world occlusions as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Self-Alignment Module</head><p>This module aims to extract the pose information from the pose-dependent face P and the shape information from the pose-independent face S (see <ref type="figure">Fig. 2</ref> for the two faces P and S). It is achieved by estimating the similarity transformation matrices between P and S (i.e., f , R, and t in Eq. 2). The similarity transformation matrix is estimated using two sets of correspondent landmarks, K S and K P , extracted from S and P, respectively. The pixels with the same coordinates in the UV maps of S and P are semantically correspondent (e.g., the nose tip corresponds to two locations with the same coordinates in the UV maps of S and P). Thus, in our method, K S and K P are extracted based on the same UV coordinates set. Specifically, we extract the 68 landmarks as the same as <ref type="bibr" target="#b0">[1]</ref> for K S and K P .</p><p>We do not simply use all the 68 landmarks. We propose to use more reliable facial vertices, i.e., the visible landmarks, for the alignment. We define a diagonal matrix W ? R k?k that represents the weight of each landmark with the visibility. The weight of the ith landmark is formulated as</p><formula xml:id="formula_12">W(i, i) = Vis(i) + eps,<label>(11)</label></formula><p>where Vis(i) denotes the visibility of the ith landmark as mentioned earlier. eps is set to 0.1 in our implementation to avoid the divide-by-zero error caused by the estimated visibility of all landmarks being zero. The estimation of the similarity transformation goes as the following steps: we first estimate f , and then estimate R and t using the singular value decomposition method. Specifically, We first compute the two weighted centroids M S and M P of the two sets of landmarks K S and K P by</p><formula xml:id="formula_13">M S = k i=1 W(i, i) * K S (i) |W| ,<label>(12)</label></formula><formula xml:id="formula_14">M P = k i=1 W(i, i) * K P (i) |W| .<label>(13)</label></formula><p>Then we obtain f by</p><formula xml:id="formula_15">f = k i=1 K P (i) ? M P k i=1 K S (i) ? M S .<label>(14)</label></formula><p>After that, we normalize K S and K P as bellow:</p><formula xml:id="formula_16">K S = f * (K S ? M S ),<label>(15)</label></formula><formula xml:id="formula_17">K P = K P ? M P .<label>(16)</label></formula><p>Performing SVD to H = K S * W * K T P , we have [U, ?, P ] = SVD(H), the rotation matrix R between K S and K P can be obtained by</p><formula xml:id="formula_18">R = P * U T ,<label>(17)</label></formula><p>and t can be obtained by</p><formula xml:id="formula_19">t = M P ? R * M S .<label>(18)</label></formula><p>The pose estimation from landmarks requires only a small amount of computation and has high accuracy because the estimation is performed on a number of reliable landmarks on the two regressed faces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Functions</head><p>The loss of our SADRNet consists of 6 components, the face geometry loss L G , the deformation loss L D , the posedependent face loss L P , the attention mask loss L A , the edge length loss L E , and the normal vector loss L V . Since the significance of the vertices in different face regions may differ considerably, we adopt a weight mask M as <ref type="bibr" target="#b1">[2]</ref> to L D , L P , and L G . We adjust the weight ratio of the four sub-regions for better training results. We use 16:12:3:0 for sub-region1 (the landmarks): sub-region2 (the eyes, nose, and mouth): sub-region3 (the cheek, chin and forehead): sub-region4 (the neck). L G , L D , and L P can be obtained by the weighted average Euclidean distance between the estimated value and the ground truth, i.e., generalized as</p><formula xml:id="formula_20">L N = h u=1 w v=1 N(u, v) ?N(u, v) 2 ? M(u, v).<label>(19)</label></formula><p>N denotes any one of the face geometry G, the posedependent face P, and the shape deformation D. h and w are the height and width of the maps. N(u, v) denotes the estimated 3D coordinates of the vertex at the UV location (u, v) in the UV maps of G, P, or D. The symbol?denotes the corresponding ground truth. M(u, v) denotes the weight value at the pixel location (u, v) in M. We use binary cross entropy (BCE) between the predicted attention mask A and the ground truth? to compute L A .</p><p>The edge length loss L E is defined based on the lengths of the edges in the pose-independent face S as</p><formula xml:id="formula_21">E ij = S(u i , v i ) ? S(u j , v j ),<label>(20)</label></formula><formula xml:id="formula_22">L E = (i,j)?E | E ij 2 ? ? ij 2 |.<label>(21)</label></formula><p>E is the edges set. It is composed of every pair of adjacent vertices (i, j) in the UV map. (u i , v i ) is the UV coordinates of vertex i. E ij is the edge vector. The normal vector loss L V is defined as</p><formula xml:id="formula_23">n ijk = E ij ? E jk E ij ? E jk 2 ,<label>(22)</label></formula><formula xml:id="formula_24">L V = (i,j,k)?T E ij E ij 2 , n ijk + E jk E jk 2 , n ijk + E ki E ki 2 , n ijk ,<label>(23)</label></formula><p>where T is the triangle facets set, n i,j,k is the normal vector of the triangle facet (i, j, k). The edge length loss and the normal vector loss are used to generate better-looking face models. The entire loss function of SADRNet is given by</p><formula xml:id="formula_25">L = ? G L G +? D L D +? P L P +? A L A +? E L E +? V L V ,<label>(24)</label></formula><p>where ? G , ? D , ? P , ? A , ? E , ? V are respectively set to 0.1, 0.5, 1, 0.05, 1, 0.1 in our implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Implementation Details</head><p>Our network takes cropped-out 256 ? 256 ? 3 images as the input, regardless of the effect of the face detector. The network starts with a single convolution layer followed by a low-level feature extractor that consists of 6 residual blocks <ref type="bibr" target="#b65">[66]</ref> and outputs a 32?32?128 feature map. The attention sub-network contains 5 convolution layers and a sigmoid activation. The high-level feature extractor contains 4 residual blocks and outputs a 8 ? 8 ? 512 feature map. The decoder starts with 10 transpose convolution layers that up-sample the feature map to 64 ? 64 ? 64, followed by 7 transpose convolution layers to output a UV map of the face shape with size 256?256?3 and another 7 layers to output a UV map of the pose-dependent face with size 256 ? 256 ? 3.</p><p>We follow <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref> and use the full 300W-LP [1] as the training set. 300W-LP contains 122, 450 face images generated from 300W [1] by 3D rotation around the y axis and horizontal flipping. Since there is no sample with a larger than 90 degrees yaw angle in 300W-LP, we generate 5,000 samples with yaw angles range from 90 to 105 degrees by 3D rotation to supplement the dataset. Similar to <ref type="bibr" target="#b1">[2]</ref>, we augment the training data by randomly rotating the image from -90 to 90 degrees, translating in the range of 0 to 10 percent of input size, scaling the image size from 0.9 to 1.1, and scaling the color channel separately from 0.6 to 1.4. We also generate synthetic occlusions as explained in Sec. III-B. We use Adam optimizer with a gradual warm-up strategy <ref type="bibr" target="#b66">[67]</ref>. We start from a learning rate of 1e-5 and increase it by a constant amount at each iteration. After 4 epochs, the learning rate reaches 1e-4. Then we use an exponential scheduler that decays the learning rate by 0.85 every epoch. The batch size is set to 16. We train our network for 25 epochs.</p><p>The original annotations of 300W-LP are 3DMM parameters, so the ground truth of D can be generated by computing D = i ? i A i , where (? 1 , ? 2 , . . . ) are the shape parameters, (A 1 , A 2 , . . . ) are the shape basis vectors (including the identity and expression basis vectors).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate the performance of the proposed method in terms of 3D face reconstruction, dense face alignment and head pose estimation. Our SADRNet is quantitatively compared with state-of-the-art methods including CMD (2019 <ref type="bibr" target="#b17">[18]</ref>), SPDT (2019 <ref type="bibr" target="#b22">[23]</ref>), PRN (2018 <ref type="bibr" target="#b1">[2]</ref>) and 3DDFAv2 (2020 <ref type="bibr" target="#b15">[16]</ref>). The qualitative comparison of PRN, MGCNet <ref type="bibr" target="#b39">[40]</ref> and our method are demonstrated in <ref type="figure" target="#fig_4">Fig. 6</ref>. <ref type="bibr" target="#b0">[1]</ref> is an in-the-wild dataset with large variations in pose, illumination, expression, and occlusion. There are 2,000 images annotated with 68 3D landmarks and fitted 3DMM parameters to recover the ground truth face model. We evaluate both face alignment and 3D face reconstruction performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AFLW2000-3D</head><p>Florence <ref type="bibr" target="#b23">[24]</ref> is a publicly available database of 53 subjects. The ground truth annotations are meshes scanned by a structured-light system. Similar to <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b11">[12]</ref>, each face mesh is rendered in 20 poses: a pitch angle of -15, 0, 20, or 25 degrees and a yaw angle of -80, -40, 0, 40, and 80 degrees to generate the face images. We evaluate the 3D face reconstruction performance on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Face Alignment</head><p>We employ normalized mean error (NME) as the evaluation metric. It is defined as the normalized mean Euclidean distance between each pair of corresponding points in the predicted result p and the ground truthp:</p><formula xml:id="formula_26">NME = 1 N N i=1 p i ?p i 2 d .<label>(25)</label></formula><p>For a fair comparison, the normalization factor d of the compared methods is computed in the same way. For sparse and dense alignment, the normalization factor of NME is defined as ? h * w where h and w are the height and width of the bounding box of all the evaluated points.</p><p>We evaluate the performance of 2D and 3D sparse alignment on the point set of 68 landmarks. We evaluate the performance of 2D and 3D dense alignment on the point set of around 45K points selected from the largest common face region of different methods as in <ref type="bibr" target="#b1">[2]</ref>. Following the settings of previous works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b17">[18]</ref>  The quantitative results on AFLW2000-3D are shown in <ref type="table" target="#tab_1">Table I</ref>. The results of other methods are from published papers or produced by the official open-source codes. We can see our method is superior to the other methods on most metrics. Especially on 3D and dense alignment tasks, our method has taken a clear lead. SPDT <ref type="bibr" target="#b22">[23]</ref> generates rendered large pose training samples and uses a CycleGAN <ref type="bibr" target="#b21">[22]</ref> to transform the rendering style. The large pose samples in our training database 300W-LP are mostly obtained by 3D image rotation with large distortion. We argue that they outperform us for sparse alignment in-between 60 and 90 degrees is because of the quality gap of training data.</p><p>Visualized results of challenging samples in AFLW2000-3D are demonstrated in <ref type="figure" target="#fig_4">Fig. 6</ref>, in which various degrees of occlusion and different face orientations are evaluated. We compare our method with two representative methods: MGCNet <ref type="bibr" target="#b39">[40]</ref> and PRN <ref type="bibr" target="#b1">[2]</ref>. MGCNet is a model-based method that fits the shape and the pose parameters of 3DMM by CNN. PRN is a model-free method that directly infers 3D coordinates of face mesh vertices with the UV position map. Our pose estimation method based on the alignment using visibility is more accurate and robust than previous works.</p><p>On the samples where the faces are partially occluded, as shown in <ref type="figure" target="#fig_4">Fig. 6c and Fig. 6d</ref>, the inaccuracy of the estimated poses of MGCNet leads to misalignment. And under severe occlusion, e.g., <ref type="figure" target="#fig_4">Fig. 6e</ref>, MGCNet fails to obtain a reasonable result, while our method is still able to precisely estimate the head pose. PRN jointly obtains pose and shape and works well for the visible facial region. However, PRN tends to give results with larger errors for invisible regions (see the misaligned landmarks in <ref type="figure" target="#fig_4">Fig. 6b and Fig. 6g</ref>). It is worth noting that AFLW2000-3D <ref type="bibr" target="#b0">[1]</ref> is semi-automatically annotated and has some inaccurate annotations in some cases. In <ref type="figure" target="#fig_5">Fig. 7</ref>, we demonstrate some examples from AFLW2000-3D that our predictions have relatively larger NME but are apparently more accurate than the ground truth. This may narrow our method's superiority margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. 3D Face Reconstruction</head><p>On this task, we use NME normalized by 3D outer interocular distance as the evaluation metric. We follow the settings of <ref type="bibr" target="#b1">[2]</ref> to evaluate our method on AFLW2000-3D <ref type="bibr" target="#b0">[1]</ref> and Florence <ref type="bibr" target="#b23">[24]</ref>. For AFLW2000-3D, we use the same set of points as we do for dense face alignment. The results under different yaw angles are also reported. As for Florence, we choose the face region containing 19K points. We use iterative closest point (ICP) to align the results to the corresponding ground truth as in <ref type="bibr" target="#b1">[2]</ref>. <ref type="table" target="#tab_1">Table II</ref> gives the quantitative comparison. Our method is robust to pose variations and achieves around 13% improvement over the state-of-the-art method on both datasets.</p><p>As our method employs a nonlinear shape deformation decoder that is more powerful than the linear bases based representation of MGCNet, our network can reconstruct more shape variations. It is worth mentioning that the output space of PRN is also nonlinear, but its large space, caused by entangled shape and pose, increases the learning difficulty for the high-frequency details. It is shown in <ref type="figure" target="#fig_4">Fig. 6</ref> that eyes, lips, and noses of the face models reconstructed by our method have more details than PRN. Our attention-aware mechanism also contributes to the robustness toward the occlusions. In <ref type="figure">Figs.</ref> 6b-e, our reconstructed faces maintain natural appearances while those from PRN are distorted in occluded areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Head Pose Estimation</head><p>On this task, we use MAE (mean absolute error) of the head pose parameters as the evaluation metric:</p><formula xml:id="formula_27">MAE = 1 N N i=1 |p i ?p i | ,<label>(26)</label></formula><p>where p represents the predicted pose parameters andp represents the ground truth.</p><p>In <ref type="table" target="#tab_1">Table III</ref>, we compare our proposed self-aligned module with the SOTA head pose estimation methods. The actual      output of our method is the transformation matrix. So we convert the rotation matrix to Euler angles to calculate MAE with the ground truth. However, when the yaw angle is close to 90 degrees, the angle conversion suffers a serious gimbal lock problem. Some examples are shown in <ref type="figure" target="#fig_6">Fig.8</ref>. Their yaw angles' errors are low, and the faces' orientations seem nice, but the errors of the pitch and roll angles are extremely high. It will make the quantitative evaluation result worse than the actual performance. Thus, besides the initial evaluation results, we also report a result marked as SADRNet-fix that ignores the samples with more than 20 degrees pitch or roll error and less than 5 degrees yaw error. However, the negatively biased performance of our method (SADRNet) is still equivalent to the best standalone head pose estimation method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We conduct ablation experiments on AFLW2000-3D to analyze the effectiveness of the following three modules: 1) the attention mechanism, 2) the self-alignment module, and 3) the regression via the shape deformation. The experiments are conducted on three tasks: sparse alignment, dense alignment, and 3D face reconstruction. The results are summarized in <ref type="table" target="#tab_1">Table IV</ref>. Attention mechanism. To study the contribution of the proposed attention mechanism, we remove the attention side branch from the proposed SADRNet (denoted as SADRNet-D; see <ref type="figure" target="#fig_7">Fig. 9c</ref> for the structure.) and compare it with SADRNet. The results of the two networks are respectively summarized in the 3rd and the bottom rows in <ref type="table" target="#tab_1">Table IV</ref>. We can see that the proposed attention mechanism benefits all three tasks. In <ref type="figure" target="#fig_0">Fig. 10</ref>, we visualize the comparison. SADRNet-D is more  <ref type="table" target="#tab_1">Table IV, respectively.</ref> easily affected by the occlusions and has lower reconstruction accuracy in the occlusion areas. In addition, SADRNet-D cannot estimate a face orientation as accurately as SADRNet. We can see the evidence from the bottom row of <ref type="figure" target="#fig_0">Fig. 10</ref>. We also investigate the attention mechanism on the basic encoder-decoder architecture as shown in <ref type="figure" target="#fig_7">Figs. 9 a and d</ref>. The corresponding results in <ref type="table" target="#tab_1">Table IV</ref> confirm the conclusion we draw from the comparison between SADRNet-D and SADRNet. Dual face regression and self alignment. The distinguishing features of the self-alignment module lie in two points. One is that we obtain the target 3D face geometry through a twostage refinement process (i.e., first pose-dependent face P and then the final 3D face model G as shown in <ref type="figure">Fig. 2</ref>) rather than a direct regression as shown in <ref type="figure" target="#fig_7">Fig. 9a</ref>. Another feature is that we estimate the pose information by aligning two reconstructed faces: the pose-dependent face P and the pose-independent face S, rather than direct regression using the image features as in <ref type="bibr" target="#b0">[1]</ref> (shown in <ref type="figure" target="#fig_7">Fig. 9b</ref>). By comparing the first and the third result rows in <ref type="table" target="#tab_1">Table IV</ref>, we can observe that the two-stage refinement process brings significant gains for all of the three tasks. The comparison between the second and third result rows demonstrates that the self-alignmentbased face alignment is more reliable than that based on direct pose regression from image features. However, <ref type="figure" target="#fig_7">Fig. 9b</ref> has a slightly better 3D face reconstruction than <ref type="figure" target="#fig_7">Fig. 9c</ref>. This may be because regressing S and P together may slightly affect the estimation of S. Shape deformation. This paper regresses the poseindependent face S through the shape deformation, which estimates the differential 3D geometry relative to the mean face template. An alternative solution to regress S is to directly perform the estimation in UV space from scratch by the decoder layers as shown in <ref type="figure" target="#fig_7">Fig. 9d</ref>. By comparing the results in the 5th and bottom result rows in <ref type="table" target="#tab_1">Table IV</ref>, it is easy to find that the shape regression based on deformation provides more accurate results than direct regression from scratch on all three tasks. Moreover, quantitatively comparing the improvement provided by using deformation (i.e., the bottom row over the 5th row) and that provided by using attention mechanism (i.e., the bottom row over the 3rd row), we can see the contribution of the deformation regression. Its improvement is more than that made by the attention mechanism in terms of 3D face reconstruction. Mesh loss. Besides the losses that directly supervise the 3D coordinates of P, S, and G, inspired by <ref type="bibr" target="#b73">[74]</ref>, we also adopt losses defined on the face mesh structure, i.e., the edge length loss and the normal vector loss. They do not improve the quantitative results, but help to reconstruct the face details for better visualization. In <ref type="figure" target="#fig_0">Fig.11</ref>, we demonstrate some reconstruction results of the model trained with and without the mesh loss. In the demonstrated cases, the estimated landmarks are almost the same. However, the model with mesh loss can better reflect the difference between identities. In the first example with the mesh loss, the mouth's openness is more suitable. In the second example, the curvature of the cheek is better, and the reconstructed model is more recognizable. In the third example, the hollow is more obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Running Time and Model Size</head><p>Our method has a model size of 60MB (including the mean face template parameters). In <ref type="table" target="#tab_1">Table IV</ref>-F we compare the model sizes of the proposed method and the baseline methods. Our model is lighter than all other deep-learningbased methods. The network inference for one face image takes 12ms on a GTX 1080 Ti GPU. The self-alignment post  process (i.e., the step generating G from S and P) takes 4ms on GPU or 1.5ms on an Intel Xeon E5-2690 CPU @ 2.60GHz. The fastest implementation of our method reconstructs the 3D face model from a cropped image in up to 13.5 ms.</p><p>The backbone of our method, (i.e. the framework in <ref type="figure" target="#fig_7">Fig.9</ref>.a) has a model size of 52MB. The extra size introduced by the attention side branch is 7MB. The parameters of the 7-layer pose-dependent face decoder and the 7-layer pose-independent face decoder add up to 1MB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Limitations</head><p>We note the following limitations of our work:</p><p>? Our network only regresses the shape geometry and pose, but does not reconstruct the facial texture from the input image. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Size VRN <ref type="bibr" target="#b20">[21]</ref> 1.5GB PRN <ref type="bibr" target="#b1">[2]</ref> 153MB Nonlinear-3DMM <ref type="bibr" target="#b16">[17]</ref> 152MB CMD <ref type="bibr" target="#b17">[18]</ref> 93MB SADRNet (ours) 60MB</p><p>? The learning of the proposed SADRNet is fully supervised and depends on the costly face mesh annotation. Designing a weakly supervised architecture that can utilize additional data modalities (e.g., facial keypoint detection data, silhouette data, segmentation data) may improve the application potential. ? There is still much room for improvement in the reconstruction of high-frequency facial details (i.e. the pores, blemishes, and wrinkles) in our work. We believe the improvements can be made in two aspects. First, the training dataset we currently use is labeled with the parameters of 3DMM. The ground truth face models lack high-frequency details. Using datasets with highfidelity ground truth models may help the detail synthesis.</p><p>Considering that such training data is expensive, a selfsupervised architecture that utilizes the facial details in the input image as supervision may be another feasible way. Second, although we have regressed the face shape deformation separately in the framework, the highfrequency details are relatively minor compared to the deformation and easy to ignore in learning. Further decomposing the shape and details or iteratively updating the face shape with cascaded regressors can more adequately supervise the learning of high-frequency details. We leave these limitations as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we have proposed a self-aligned dual face regression network (SADRNet) to solve the problem of 3D face reconstruction and dense alignment under unconstrained conditions. We decouple the framework of face reconstruction to two regression modules for pose-dependent and poseindependent face shape estimation, respectively. Then, a novel self-alignment module is presented to transform the detailed and more accurate face shape into its corresponding pose view to yield the final face reconstruction. To make our method robust to occlusion, we incorporate an attention module to enhance the visible facial information and estimate the transformation matrix only with visible landmarks. We evaluate our network on the AFLW2000-3D database and the Florence database. With the power of robustness to occlusions and large pose variation, our proposed method outperforms the state-ofthe-art methods by a notable margin on both face alignment and 3D reconstruction.</p><p>Lacking high-frequency facial details is the main drawback of our method and we consider to improve our method in the future from two aspects. First, we could finetune our SADRNet on a high-fidelity 3D scanned dataset or design a self-supervised learning framework to train the network with high-frequency details in the input images. Second, we plan to present a cascade reconstruction pipeline to regress our face shape in a coarse-to-fine manner and focus on more detailed face shape regression in the latter stages.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of the challenges of large poses (the 1st row) and occlusions (the 2nd row). The results of both face alignment and reconstruction are demonstrated. Only 68 landmarks are plotted for better view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>The framework of our proposed self-aligned dual face regression network (SADRNet). A is the attention mask. P is the pose-dependent face. D is the face shape deformation (visualized in UV space).S is the mean face template. S is the pose-independent face. G is the output face model. Illustration the UV map of a 3D face model. Given a textured face model (a), we can compute the corresponding unwrapped texture in UV space (b), and the corresponding UV position map (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>An example of the synthetic attention mask annotation. (a) is the original face image. (b) is the ground truth face geometry projected on the image plane. (c) is the binary image of the projected face. (d) is the face image augmented with synthetic occlusions. (e) is the ground truth attention mask corresponding to (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The top row shows the input images and the bottom row shows the attention masks predicted by our attention branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The qualitative comparison on AFLW2000-3D dataset. The estimated landmarks are in blue and the ground truth landmarks are in red. NME(%) is shown at the bottom right of each result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Results on AFLW2000-3D from our SADRNet. Our results are more accurate than the ground truth. From the top row to the bottom row are the input images, the sparse alignment results of SADRNet and the corresponding ground truth (blue for our method and red for the ground truth), the reconstructed face models, and the ground truth face models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Some samples in AFLW2000-3D, in which our estimated pitch angle and roll angle have an error of more than 20 degrees, while the yaw angle errors are less than 5 degrees.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Alternative frameworks of baselines in ablation study. The figures (a)-(e) correspond to the 1st-5th result rows in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>SADRNet vs. SADRNet-D; results for sparse alignment and face reconstruction are demonstrated. Facial regions around occlusions are zoomed in for better visual comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11 .</head><label>11</label><figDesc>With mesh loss vs. without mesh loss; results for sparse alignment and face reconstruction are demonstrated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>on 2D sparse alignment, we also test the images with yaw angles in [0 ? , 30 ? ) (1,306 samples), [30 ? , 60 ? ) (462 samples) and [60 ? , 90 ? ] (232 samples) separately, as well as the balanced subsets of each angle interval.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I PERFORMANCE</head><label>I</label><figDesc>COMPARISON ON AFLW2000-3D ON THE TASK OF SPARSE ALIGNMENT (68 LANDMARKS) AND DENSE ALIGNMENT (45K POINTS). THE NME (%) ARE REPORTED. IMAGES WITH DIFFERENT YAW ANGLE RANGES ARE ALSO EVALUATED SEPARATELY. "BALANCED" DENOTES THE RESULTS ON THE SUBSET WITH BALANCED DISTRIBUTION OF THE YAW ANGLES. "MEAN" DENOTES THE AVERAGE RESULTS ON THE ENTIRE DATASET. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">68 points</cell><cell></cell><cell></cell><cell cols="2">45k points</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell>2D</cell><cell></cell><cell></cell><cell>3D</cell><cell>2D</cell><cell>3D</cell></row><row><cell></cell><cell cols="3">0 to 30 30 to 60 60 to 90</cell><cell cols="5">Balanced Mean Mean Mean Mean</cell></row><row><cell>3DDFA [1]</cell><cell>3.78</cell><cell>4.54</cell><cell>7.93</cell><cell>5.42</cell><cell>6.03</cell><cell>7.50</cell><cell>5.06</cell><cell>6.55</cell></row><row><cell>3DFAN [60]</cell><cell>2.77</cell><cell>3.48</cell><cell>4.61</cell><cell>3.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DeFA [65]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.50</cell><cell>4.36</cell><cell>6.23</cell><cell>4.44</cell><cell>6.04</cell></row><row><cell>3DSTN [68]</cell><cell>3.15</cell><cell>4.33</cell><cell>5.98</cell><cell>4.49</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Nonlinear 3DMM [19]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>PRN [2]</cell><cell>2.75</cell><cell>3.51</cell><cell>4.61</cell><cell>3.62</cell><cell>3.26</cell><cell>4.70</cell><cell>3.17</cell><cell>4.40</cell></row><row><cell>DAMDN [14]</cell><cell>2.90</cell><cell>3.83</cell><cell>4.95</cell><cell>3.89</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CMD [18]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.90</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SPDT [23]</cell><cell>3.56</cell><cell>4.06</cell><cell>4.11</cell><cell>3.88</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>3DDFAv2 [16]</cell><cell>2.63</cell><cell>3.42</cell><cell>4.48</cell><cell>3.51</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.18</cell></row><row><cell>SADRNet (ours)</cell><cell>2.66</cell><cell>3.30</cell><cell>4.42</cell><cell>3.46</cell><cell>3.05</cell><cell>4.33</cell><cell>2.93</cell><cell>4.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>COMPARISON ON 3D FACE RECONSTRUCTION. EVALUATIONS ARE CONDUCTED ON THE AFLW2000-3D DATASET AND FLORENCE DATASET. AROUND 45K POINTS ARE USED ON AFLW2000-3D AND 19K POINTS ARE USED ON FLORENCE. THE NME (%) NORMALIZED BY OUTER INTEROCULAR DISTANCE ARE REPORTED. THE BEST RESULTS ARE HIGHLIGHTED IN BOLD.</figDesc><table><row><cell>Method</cell><cell>0 30</cell><cell cols="2">AFLW2000-3D 30 60 60 90</cell><cell>Mean</cell><cell>Florence Mean</cell></row><row><cell>3DDFA [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.36</cell><cell>6.38</cell></row><row><cell>DeFA [65]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.64</cell><cell>-</cell></row><row><cell>VRN -Guided [21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.26</cell></row><row><cell>PRN [2]</cell><cell>3.72</cell><cell>4.04</cell><cell>4.45</cell><cell>3.96</cell><cell>3.75</cell></row><row><cell>SPDT [23]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.70</cell><cell>3.83</cell></row><row><cell>3DDFAv2 [16]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.56</cell></row><row><cell>SADRNet (ours)</cell><cell>3.17</cell><cell>3.42</cell><cell>3.36</cell><cell>3.25</cell><cell>3.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON ON HEAD POSE ESTIMATION. EVALUATIONS ARE CONDUCTED ON THE AFLW2000-3D DATASET. THE MAES OF HEAD POSE PARAMETERS ARE REPORTED. THE BEST RESULTS ARE</figDesc><table><row><cell cols="3">HIGHLIGHTED IN BOLD.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>yaw</cell><cell cols="2">AFLW2000-3D pitch roll</cell><cell>Mean</cell></row><row><cell>FSANet [69]</cell><cell>4.50</cell><cell>6.08</cell><cell>4.64</cell><cell>5.07</cell></row><row><cell>GLDL [70]</cell><cell>3.02</cell><cell>5.06</cell><cell>3.68</cell><cell>3.92</cell></row><row><cell>QuatNet [71]</cell><cell>3.97</cell><cell>5.61</cell><cell>3.92</cell><cell>4.50</cell></row><row><cell>FDN [72]</cell><cell>3.78</cell><cell>5.61</cell><cell>3.88</cell><cell>4.42</cell></row><row><cell>MNN [73]</cell><cell>3.34</cell><cell>4.69</cell><cell>3.48</cell><cell>3.83</cell></row><row><cell>SADRNet (ours)</cell><cell>2.93</cell><cell>5.00</cell><cell>3.54</cell><cell>3.82</cell></row><row><cell cols="2">SADRNet-fix (ours) 2.93</cell><cell>4.43</cell><cell>2.95</cell><cell>3.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV ABLATION</head><label>IV</label><figDesc>STUDY ON AFLW2000-3D. THE "AT" COLUMN INDICATES USING THE ATTENTION MECHANISM OR NOT. THE "SA" COLUMN INDICATES USING THE SELF ALIGNMENT MODULE OR NOT. THE "DF" COLUMN INDICATES WHETHER 3D-DFAFR IS ACHIEVED BY REGRESSING SHAPE DEFORMATION "D" OR NOT.</figDesc><table><row><cell cols="3">Applied approaches</cell><cell cols="2">Sparse</cell><cell cols="2">Dense</cell><cell>Rec.</cell></row><row><cell cols="2">AT SA</cell><cell>DF</cell><cell>2D</cell><cell>3D</cell><cell>2D</cell><cell>3D</cell><cell>3D</cell></row><row><cell>? ? ?</cell><cell>? ? ?</cell><cell>? ? ?</cell><cell>3.31 3.43 3.18 3.16 3.24 3.05</cell><cell>4.74 4.94 4.51 4.53 4.59 4.33</cell><cell>3.10 3.21 3.03 3.03 3.14 2.93</cell><cell>4.33 4.50 4.17 4.21 4.31 4.02</cell><cell>4.07 3.37 3.39 4.02 3.49 3.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF MODEL SIZE.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Joint 3d face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Joint face alignment and 3d face reconstruction with application to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">High-fidelity pose and expression normalization for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Face swapping: Realistic image synthesis based on facial landmarks alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Problems in Engineering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Going deeper in facial expression recognition using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 26th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mmface: A multi-metric regression network for unconstrained face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Extreme 3d face reconstruction: Seeing through occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nirkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual attention mobdensenet(damdnet) for robust 3d face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards high-fidelity 3d face reconstruction from in-the-wild images using graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3d dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dense 3d face decoding over 2500fps: Joint texture and shape convolutional mesh decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On learning 3d face morphable model from inthe-wild images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards high-fidelity nonlinear 3d face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large pose 3d face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semi-supervised monocular 3d face reconstruction with end-to-end shape-preserved domain transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The florence 2d/3d hybrid face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding</title>
		<meeting>the 2011 Joint ACM Workshop on Human Gesture and Behavior Understanding</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A 3d face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single view-based 3d face reconstruction robust to self-occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Advances in Signal Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fitting 3d morphable face models using local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>R?tsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Estimating 3d shape and texture using pixel intensity, edges, specular highlights, texture constraints and a prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Large-pose face alignment via cnn-based dense 3d model fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regressing robust and discriminative 3d morphable models with a very deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Photo-realistic facial details synthesis from single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Avatarme: Realistically renderable 3d facial reconstruction &quot;in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lattas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Triantafyllou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised training for 3d morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to regress 3d face shape and expression from an image without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ploumpis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mvf-net: Multi-view 3d face morphable model regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">N</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fml: Face model learning from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selfsupervised monocular 3d face reconstruction by occlusion-aware multiview geometry consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Disentangling features in 3d face shapes for joint face reconstruction and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Modeling facial geometry using compositional vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bagautdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for 3d face shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Production-level facial performance capture using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generating 3d faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3d face from x: Learning face shape from diverse sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dual neural networks coupling data regression with explicit priors for monocular 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-supervised learning of detailed 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.</forename><forename type="middle">C T</forename><surname>Timothy Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gareth</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Active shape models-their training and application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Underst</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Accurate regression procedures for active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adaptive active appearance model with incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Lett</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Nonlinear Discriminative Approach to AAM Fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Wing loss for robust facial landmark localisation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Awais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Facial landmark machines: A backbone-branches architecture with progressive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A recurrent encoder-decoder network for sequential face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Joint voxel and coordinate regression for accurate 3d facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Convolutional aggregation of local evidence for large pose face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Two-stage convolutional part heatmap regression for the 1st 3d face alignment in the wild (3dfaw) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment with a deformable hough transform model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">3d face alignment without correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>S?nta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Faster than real-time facial alignment: A 3d spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Fsa-net: Learning fine-grained structure aggregation for head pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Facial pose estimation by deep learning from label distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Quatnet: Quaternion-based head pose estimation with multiregression loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Fdn: Feature decoupling network for head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Multi-task head pose estimation in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baumela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pixel2mesh: Generating 3d mesh models from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
