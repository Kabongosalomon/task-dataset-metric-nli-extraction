<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Situation Recognition with Graph Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
							<email>ryli@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<addrLine>3 Youtu Lab</addrLine>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<addrLine>3 Youtu Lab</addrLine>
									<region>Tencent</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<addrLine>3 Youtu Lab</addrLine>
									<region>Tencent</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Uber Advanced Technologies Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
							<email>fidler@cs.toronto.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<addrLine>3 Youtu Lab</addrLine>
									<region>Tencent</region>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Situation Recognition with Graph Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of recognizing situations in images. Given an image, the task is to predict the most salient verb (action), and fill its semantic roles such as who is performing the action, what is the source and target of the action, etc. Different verbs have different roles (e.g. attacking has weapon), and each role can take on many possible values (nouns). We propose a model based on Graph Neural Networks that allows us to efficiently capture joint dependencies between roles using neural networks defined on a graph. Experiments with different graph connectivities show that our approach that propagates information between roles significantly outperforms existing work, as well as multiple baselines. We obtain roughly 3-5% improvement over previous work in predicting the full situation. We also provide a thorough qualitative analysis of our model and influence of different roles in the verbs. VERB: REPAIRING PROBLEM Part Pipe Wheel ? ITEM AC Wall Car ? TOOL Screwdriver Wrench Knife ? PLACE Outdoors Bathroom Field</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref>, action <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b40">41]</ref>, and scene classification <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref> have come a long way, with performance in some of these tasks almost reaching human agreement. However, in many real world applications such as robotics we need a much more detailed understanding of the scene. For example, knowing that an image depicts a repairing action is not sufficient to understand what is really happening in the scene. We thus need additional information such as the person repairing the house, and the tool that is used.</p><p>Several datasets have recently been collected for such detailed understanding <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, the Visual Genome dataset was built containing detailed relationships between objects. A subset of the scenes were further annotated with scene graphs <ref type="bibr" target="#b16">[17]</ref> to capture both unary (e.g. attributes) and pairwise (e.g. relative spatial info) object relationships. Recently, Yatskar et al. <ref type="bibr" target="#b47">[48]</ref> extended this idea to actions by labeling action frames where a frame consists of a fixed set of roles that define the action. <ref type="figure">Fig. 1</ref> shows a frame for action repairing. The challenge then consists ? AGENT Worker Plumber Soldier ? <ref type="figure">Figure 1</ref>. Understanding an image involves more than just predicting the most salient action. We need to know who is performing this action, what tools (s)he may be using, etc. Situation recognition is a structured prediction task that aims to predict the verb and its frame that consists of multiple role-noun pairs. The figure shows a glimpse of our model that uses a graph to model dependencies between the verb and its roles.</p><p>of assigning values (nouns) to these roles based on the image content. The number of different role types, their possible values, as well as the number of actions are very large, making it a very challenging prediction task. As shown in <ref type="figure">Fig. 2</ref>, the same verb can appear in very different image contexts, and nouns that fill the roles are vastly different.</p><p>In <ref type="bibr" target="#b47">[48]</ref>, the authors proposed a Conditional Random Field (CRF) to model dependencies between verb-rolenoun pairs. In particular, a neural network was trained in an end-to-end fashion to both, predict the unary potentials for verbs and nouns, and to perform inference in the CRF. While their model captured the dependency between the verb and role-noun pairs, dependencies between the roles were not modeled explicitly.</p><p>In this paper, we aim to jointly reason about verbs and their roles using a Graph Neural Network (GNN), a generalization of graphical models to neural networks. A GNN defines observation and output at each node in the graph, <ref type="bibr">Figure 2</ref>. Images corresponding to the same verb can be quite different in their content involving verb roles. This makes situation recognition difficult. and propagates messages along the edges in a recurrent manner. In particular, we exploit the GNNs to also model dependencies between roles and predict a consistent structured output. We explore different connectivity structures among the role nodes, and show that our approach significantly improves performance over existing work. In addition, we compare with strong baseline methods using Recurrent Neural Networks (RNNs) that have been shown to work well on joint prediction tasks, such as semantic <ref type="bibr" target="#b49">[50]</ref> and object instance <ref type="bibr" target="#b2">[3]</ref> segmentation, as well as on group activity recognition <ref type="bibr" target="#b7">[8]</ref>. We also visualize the learned models to further investigate dependencies between roles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Situation recognition generalizes action recognition to include actors, objects, and location in the activity. There has been work to combine activity recognition with scene or object labels <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref>. In <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref>, visual semantic role labeling tasks were proposed where datasets are built to study action along with localization of people and objects. In another line of work, Yatskar et al. <ref type="bibr" target="#b47">[48]</ref> created the imSitu dataset that uses linguistic resources from FrameNet <ref type="bibr" target="#b9">[10]</ref> and WordNet <ref type="bibr" target="#b28">[29]</ref> to associate images not only with verbs, but also with specific role-noun pairs that describe the verb with more details. As a baseline approach, in <ref type="bibr" target="#b47">[48]</ref>, a Conditional Random Field (CRF) jointly models prediction of the verb and verb-role-noun triplets. Further, considering that the large output space and sparse training data could be problematic, a tensor composition function was used <ref type="bibr" target="#b46">[47]</ref> to share nouns across different roles. The authors also proposed to augment the training data by searching images using query phrases built from the structured situation.</p><p>Different from these methods, our work focuses on explicitly modeling dependencies between roles for each verb through the use of different neural architectures.</p><p>Understanding Images. There is a surge of interest in joint vision and language tasks in recent years. Visual Question Answering in images and videos <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">38]</ref> aims to answer questions related to image or video content. In image captioning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b25">26]</ref>, a natural language sentence is generated to describe the image. Approaches for these tasks often use the CNN-RNN pipelines to provide a caption, or a correct answer to a specific question. Dependencies between verbs and nouns are typically being implicitly learned with the RNN. An alternative is to list all important objects with their attributes and relationships. Johnson et al. <ref type="bibr" target="#b16">[17]</ref> created scene graphs, which are being used for visual relationship detection <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b48">49]</ref> tasks. In <ref type="bibr" target="#b24">[25]</ref>, the authors exploit scene graphs to generate image captions.</p><p>In Natural Language Processing (NLP), semantic role labeling <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53]</ref> involves annotating a sentence with thematic or semantic roles. Building upon resources from NLP, and leveraging collections such as FrameNet <ref type="bibr" target="#b9">[10]</ref> and WordNet <ref type="bibr" target="#b28">[29]</ref>, visual semantic role labeling, or situation recognition, aims to interpret details for one particular action with verb-role-noun pairs.</p><p>Graph Neural Networks. There are a few different ways for applying neural networks to graph-structured data. We divide them into two categories. The first group defines convolutions on graphs. Approaches like <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref> utilized the graph Laplacian and applied CNNs to spectral domain. Differently, Duvenaud et al. <ref type="bibr" target="#b8">[9]</ref> designed a special hash function such that a CNN can be used on the original graphs.</p><p>The second group applies feed-forward neural networks to every node of the graph recurrently. Information is propagated through the network by dynamically updating the hidden state of each node based on their history and incoming messages from their neighborhood. The Graph Neural Network (GNN) proposed by <ref type="bibr" target="#b33">[34]</ref> utilized multi-layer perceptrons (MLP) to update the hidden state. However, their learning algorithm is restrictive due to the contraction map assumption. In the following work, the Gated Graph Neural Network (GGNN) <ref type="bibr" target="#b22">[23]</ref> used a recurrent gating function <ref type="bibr" target="#b3">[4]</ref> to perform the update, and effectively learned model parameters using back-propagation through time (BPTT).</p><p>Other work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b36">37]</ref> designed special update functions based on the LSTM <ref type="bibr" target="#b15">[16]</ref> cell and applied the model to treestructured or general graph data. In <ref type="bibr" target="#b27">[28]</ref>, knowledge graphs and GGNNs are used for image classification. Here we use GGNNs for situation recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph-based Neural Models for Situation Recognition</head><p>Task Definition. Situation recognition as per the imSitu dataset <ref type="bibr" target="#b47">[48]</ref> assumes a discrete set of verbs V, nouns N , roles R, and frames F. The verb and its corresponding frame that contains roles are obtained from FrameNet <ref type="bibr" target="#b9">[10]</ref>, while nouns come from WordNet <ref type="bibr" target="#b28">[29]</ref>. Each verb v ? V is associated with a frame f ? F that contains a set of semantic roles E f . Each role e ? E f is paired with a noun value  <ref type="figure">Figure 3</ref>. The architecture of fully-connected roles GGNN. The undirected edges between all roles of a verb-frame allows to fully capture the dependencies between them. n e ? N ? {?}. Here, ? indicates that the noun is unknown or not applicable. A set of semantic roles and their nouns is called a realized frame, denoted as</p><formula xml:id="formula_0">R f = {(e, ne) : e ? E f }, where each role is with a noun.</formula><p>Given an image, the task is to predict the structured situation S = (v, R f ), specified by a verb v ? V and its corresponding realized frame R f . For example, as shown on the right of <ref type="figure">Fig. 2</ref>, the verb riding is associated with three role-noun pairs, i.e., {agent:dog, vehicle:surfboard, place:sidewalk}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Graph Neural Network</head><p>The verb and semantic roles of a situation depend on each other. For example, in the verb carrying, the roles agent and agent-part are tightly linked with the item being carried. Small items can be carried by hand, while heavy items may be carried on the back. We propose modeling these dependencies through a graph G = (A, B). The nodes in our graph a ? A are of two types of verb or role, and take unique values of V or N , respectively. Since each image in the dataset is associated with one unique verb, every graph has a single verb node. Edges in the graph b = (a , a) encode dependencies between role-role or verbrole pairs, and can be directed or undirected. <ref type="figure">Fig. 1</ref> shows an example of such a graph where verb and role nodes are connected to each other.</p><p>Background. Modeling structure and learning representation on graphs have prior work. Gated Graph Neural Networks (GGNNs) <ref type="bibr" target="#b22">[23]</ref> is one approach that learns the representation of a graph, which is then used to predict nodeor graph-level output. Each node of a GGNN is associated with a hidden state vector that is updated in a recurrent fashion. At each time step, the hidden state of a node is updated based on its history and incoming messages from its neighbors. These updates are applied simultaneously to all nodes in the graph at each propagation step. The hidden states after T propagation steps are used to predict the output. In contrast, a standard unrolled RNN only moves information in one direction and updates one "node" per time step.</p><p>GGNN for Situation Recognition. We adopt the GGNN framework to recognize situations in images. Each image i is associated with one verb v that corresponds to a frame f with a set of roles E f . We instantiate a graph G f for each image that consists of one verb node, and |E f | (number of roles associated with the frame) role nodes. To capture the dependency between roles to the full extent, we propose creating undirected edges between all pairs of roles. <ref type="figure">Fig. 3</ref> shows two example graph structures of this type. We explore other edge configurations in the evaluation.</p><p>To initialize the hidden states for each node, we use features derived from the image. In particular, for every image i, we compute representations ? v (i) and ? n (i) using the penultimate fully-connected layer of two convolutional neural network (CNN) pre-trained to predict verbs and nouns, respectively. We initialize the hidden states h ? R D of the verb node a v and role node a e as</p><formula xml:id="formula_1">h 0 av = g(W iv ? v (i)) (1) h 0 ae = g(W in ? n (i) W e e W vv ) ,<label>(2)</label></formula><p>wherev ? {0, 1} |V| corresponds to a one-hot encoding of the predicted verb and e ? {0, 1} |R| is a one-hot encoding of the role that the node a e corresponds to. W v ? R D?|V| is the verb embedding matrix, and W e ? R D?|R| is the role embedding matrix. W iv and W in are parameters that transform image features to the space of hidden representations. corresponds to element-wise multiplication, and g(?) is a non-linear function such as tanh(?) or ReLU (g(x) = max(0, x)). We normalize the initialized hidden states to unit-norm prior to propagation.</p><p>For any node a, at each time step, the aggregation of incoming messages at time t is determined by the hidden states of its neighbors a :</p><formula xml:id="formula_2">x t a = (a ,a)?B W p h t?1 a + b p .<label>(3)</label></formula><p>Note that we use a shared linear layer of weights W p and biases b p to compute incoming messages across all nodes. After aggregating the messages, the hidden state of the node is updated through a gating mechanism similar to the Gated Recurrent Unit <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23]</ref> as follows:</p><formula xml:id="formula_3">z t a = ?(W z x t a + U z h t?1 a + b z ) , r t a = ?(W r x t a + U r h t?1 a + b r ) , h t a = tanh(W h x (t) a + U h (r t a h t?1 a ) + b h ) , h t a = (1 ? z t a ) h t?1 a + z t a h t a .<label>(4)</label></formula><p>This allows each node to softly combine the influence of the aggregated incoming message and its own memory. W z ,  <ref type="figure">Figure 4</ref>. The architecture of chain RNN for verb riding. The time-steps at which different roles are predicted needs to be decided manually, and has an influence on the performance. to predict the verb and nouns. Specifically, for each image, we predict the verb and a set of nouns for each role associated with the verb frame using a softmax layer:</p><formula xml:id="formula_4">U z , b z , W r , U r , b r , W h ,</formula><formula xml:id="formula_5">p v = ?(W hv h av + b hv ) (5) p e:n = ?(W hn h ae + b hn ) .<label>(6)</label></formula><p>Note that the softmax function ? is applied across the class space for verbs V and nouns N . p e:n can be treated as the probability of assigning noun n to role e. Each image i in the imSitu dataset comes with three sets of annotations (from three annotators) for the nouns. During training, we accumulate the cross-entropy loss at verb and noun nodes for every annotation as</p><formula xml:id="formula_6">L = i 3 j=1 y v log(p v ) + 1 |E f | e</formula><p>y e:n log(p e:n ) , <ref type="bibr" target="#b6">(7)</ref> where y v and y e:n correspond to the ground-truth verb for image i and the ground-truth noun for role e of the image, respectively. Different to the Soft-OR loss in <ref type="bibr" target="#b47">[48]</ref>, we encourage the model to predict all three annotations for each image. We use back-propagation through time (BPTT) <ref type="bibr" target="#b41">[42]</ref> to train the model.</p><p>Inference. At test time, our approach first predicts the verbv = arg max v p v to choose a corresponding frame f and obtain the set of associated roles E f . We then propagate information among role nodes and choose the highest scoring nounn e = arg max n p e:n for each role. Thus our predicted situation i?</p><formula xml:id="formula_7">S = (v, {(e,n e ) : e ? E f }) .<label>(8)</label></formula><p>To reduce reliance on the quality of verb prediction, we explore beam search over verbs as discussed in Experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Simpler Graph Architectures</head><p>An alternative to model dependencies between nodes is to use recurrent neural networks (RNN). Here, situation recognition can be considered as a sequential prediction problem of choosing the verb and corresponding noun-role pairs. The hidden state of the RNN carries information across the verb and noun-role pairs, and the input at each time-step dictates what the RNN should predict. Chain RNN. An unrolled RNN can be seen as a special case of a GGNN, where nodes form a chain with directed edges between them. However, there are a few notable differences, wherein the nodes receive information only once from their (left) neighbor. In addition, the nodes do not perform T steps of propagation among each other and predict output immediately after the information arrives.</p><p>In the standard chain structure of a RNN, we need to manually specify the order of the verb and roles. As the choice of the verb dictates the set of roles in the frame, we predict the verb at the first time step. We observe that the imSitu dataset and any verb-frame in general, commonly consist of place and agent-like roles (e.g. semantic role teacher can be considered as the agent for the verb teaching). We thus predict place and agent roles as the second and third roles in the chain <ref type="bibr" target="#b0">1</ref> . We make all other roles for the frame to follow subsequently in descending order of the number of times they occur across all verbframes. <ref type="figure">Fig. 4</ref> shows an example of such a model.</p><p>For a fair comparison to the fully connected roles GGNN, we employ the GRU update in our RNN. The input to the hidden states matches node initialization (Eqs. 1 and 2). We follow the same scheme for predicting the output (linear layer with softmax), and train the model with the same cross-entropy loss.</p><p>Tree-structured RNN. As mentioned above, the place and agent semantic roles occur more frequently. We propose a structure where they have a larger chance to influence prediction of other roles. In particular, we create a tree-structured RNN <ref type="bibr" target="#b36">[37]</ref> where the hidden states first predict the verb, followed by agent and place, and all other roles. <ref type="figure" target="#fig_1">Fig. 5</ref> shows examples of resulting structures.</p><p>The tree-structured RNN can be deemed as a special case of GGNN, where nodes have the following directed edges: <ref type="bibr" target="#b8">(9)</ref> where Z = {agent, place}, and E f \Z represents all roles in that frame other than agent and place. Similar to the chain RNN, we use GRU update and follow the same learning and inference procedures.  <ref type="table">Table 1</ref>. Situation prediction results on the development set. We compare several variants of our fully-connected roles model to show the improvements achieved at every step. T refers to the number of time-steps of propagation in the fully connected roles GGNN (FC Graph).</p><formula xml:id="formula_8">B = {(av, a ) : a ? Z} ? {(a , a) : a ? Z, a ? E f \Z} ,</formula><p>BS=10 indicates the use of beam-search with beam-width of 10. vOH (verb, one-hot) is included when the embedding of the predicted verb is used to initialize the hidden state of the role nodes. g=ReLU refers to the non-linear function used after initialization. All other rows use g=tanh(?). Finally, Soft-OR refers to the loss function used in <ref type="bibr" target="#b47">[48]</ref>. Best performance is in bold and second-best is italicized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evaluation</head><p>We evaluate our methods on the imSitu dataset <ref type="bibr" target="#b47">[48]</ref> and use the standard splits with 75k, 25k, and 25k images for the train, development, and test subsets, respectively. Each image in imSitu is associated with one verb and three annotations for the role-noun pairs.</p><p>We follow <ref type="bibr" target="#b46">[47]</ref> and report three metrics: (i) verb: the verb prediction performance; (ii) value: the semantic verbrole-value tuple prediction performance that is considered to be correct if it matches any of the three ground truth annotators; and (iii) value-all: the performance when the entire situation is correct and all the semantic verb-role-value pairs match at least one ground truth annotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Image Representations. We adopt two pre-trained VGG-16 CNNs <ref type="bibr" target="#b35">[36]</ref> for extracting image features by removing the last fully-connected and softmax layers, and fine-tuning all weights. The first CNN (? v (i)) is trained to predict verbs, and second CNN (? n (i)) predicts the top K most frequent nouns (K = 2000 cover about 95% of nouns) in the dataset.</p><p>Unaries. Creating a graph with no edges, or equivalently with T = 0 steps of propagation corresponds to using the initialized features to perform prediction. We refer to this approach as Unaries, which will be used as the simplest baseline to showcase the benefit of modeling dependencies between the roles.</p><p>Learning. We implement the proposed models in Torch <ref type="bibr" target="#b4">[5]</ref>. The network is trained using RMSProp <ref type="bibr" target="#b14">[15]</ref> with mini-batches of 256 samples. We choose the hidden state dimension D = 1024, and train image (W iv , W in ), verb (W v ) and role (W e ) embeddings. The image features are extracted before training the GGNN or RNN models.</p><p>The initial learning rate is 10 ?3 and starts to decay after 10 epochs by a factor of 0.85. We use dropout with a prob-ability of 0.5 on the output prediction layer (c.f . Eqs. 5 and 6) and clip the gradients to range (?1, 1).</p><p>Mapping agent Roles. The imSitu dataset <ref type="bibr" target="#b47">[48]</ref> has situations for 504 verbs. Among them, we notice that 19 verbs do not have the semantic role agent but instead with roles of similar meaning (e.g. verb educating has role teacher). We map these alternative roles to agent when determining their position in the RNN architecture. Such a mapping is not used for the fully connected GGNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variable Number of Roles.</head><p>A verb has a maximum of 6 roles associated with it. We implement our proposed model with fixed-size graphs involving 7 nodes. To deal with verbs with less than 6 roles, we zero the hidden states at each time-step of propagation, making them not receive or send any information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We first present a quantitative analysis comparing different variants of our proposed model. We then evaluate the performance of different architectures, and compare results with state-of-the-art approaches.</p><p>Ablative Analysis A detailed study of the GGNN model with fully connected roles (referred to as FC Graph) is shown in <ref type="table">Table 1</ref>. An important hyper-parameter for the GGNN model is the number of propagation steps T . We found that the performance increases by a small amount when increasing T , and saturates soon (in rows 3, 4, and 5). We believe that this is due to the use of a fully-connected graph, and all nodes sharing most of the information at the first-step propagation. Nevertheless, the propagation is important, as revealed in the comparison between Unaries (T = 0) from row 1 and T = 1 in row 3. We obtain a mean improvement of 3.8% in all metrics.</p><p>During test we have the option of using beam search, where we hold B best verb predictions and compute the  <ref type="table">Table 3</ref>. We compare situation prediction results on the development and test sets against state-of-the-art models. Each model was run on the test set only once. Our model shows significant improvement in the top-1 prediction on all metrics, and performs better than a baseline that uses data augmentation. The performance improvement on the value-all metric is important for applications, such as captioning and QA. Best performance is highlighted in bold, and second-best is italicized.</p><p>role-noun predictions for each of the corresponding graphs (frames). Finally, we select the top prediction using the highest log-probability across all B options. We use a beam width of B = 10 in our experiments, which yields small improvement. Rows 1 and 2 of <ref type="table">Table 1</ref> show the improvement using beam search on a graph without propagation. Rows 5 and 6 show the benefit after multiple steps of propagation. Rows 6 and 7 of <ref type="table">Table 1</ref> demonstrate the impact of using embeddings of the predicted verb (vOH) to initialize the role nodes' hidden states in Eq. (2). Notable improvement is obtained when using the ground-truth verb (3-4%). The value-all for the top-1 predicted verb increases from 17.70% to <ref type="bibr" target="#b18">19</ref>.15%. We also tested different non-linear functions for initialization, i.e., tanh (row 7) or ReLU (row 8), however, the impact is almost negligible. We thus use tanh for all experiments.</p><p>Finally, comparing rows 7 and 9 of <ref type="table">Table 1</ref> reveals that our loss function to predict all annotations in Eq. (7) performs slightly better than the Soft-OR loss that aims to fit at least one of the annotations <ref type="bibr" target="#b47">[48]</ref>.</p><p>Baseline RNNs. <ref type="table" target="#tab_2">Table 2</ref> summarizes the results with different structures on the dev set. As expected, Unaries perform consistently worse than models with information prop- agation between nodes on the value and value-all metrics.</p><p>The Tree-structured RNN provides a 2% boost in value-all for top-1 predicted verb, while the Chain RNN provides a 3.9% improvement. Owing to the better connectivity between the roles in a Chain RNN (especially place and agent), we observe better performance compared to the Tree-structured RNN. Note that as the RNNs are trained jointly to predict both verbs and nouns, and as the noun gradients dominate, the verb prediction takes a hit.</p><p>Different Graph Structures. We can also use chain or tree-structured graphs in GGNN. Along with the FC graph in row 6 of <ref type="table" target="#tab_2">Table 2</ref>, rows 4 and 5 present the results for different GGNN structures. They show that connecting roles with each other is critical and sharing information helps. Interestingly, the Chain GGNN needs more propagation steps (T =8), as it takes time for the left-most and right-most nodes to share information. Smaller values of T are possible when nodes are well-connected as in Treestructured (T =6) or FC Graph (T =4). <ref type="figure" target="#fig_2">Fig. 6</ref> presents prediction from all models for two images. The FC Graph is able to reason about associating cheese and pizza rather than sprinkling meat or food on it.</p><p>Comparison with State-of-the-art. We compare the performance of our models against state-of-the-art on both the dev and test sets in <ref type="table">Table 3</ref>. Our CNN predicts the verb well. Beam search leads to even better performance (2-4% higher) in verb prediction. We note that Tensor Composition + DataAug actually uses more data to train models. Nevertheless, we achieve the best performance on all metrics when using the top-1 predicted verb.</p><p>Another advantage of our model is in improvement for the value-all metric. It yields +8% when using the groundtruth verb, +6% with top-5 predicted verbs, and +4.5% with top-1 predicted verb, compared with the baseline without data augmentation. Interestingly, even with data augmentation, we outperform <ref type="bibr" target="#b46">[47]</ref> by 3-4% in value-all for top-1 predicted verb. This property attributes to information sharing between role nodes, which helps in correcting errors and better predicts frames. Note that value-all is an important metric to measure a full understanding of the image. Models with higher value-all will likely lead to better captioning or question-answering results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Further Discussion</head><p>We delve deeper into our model and discuss why the FC Graph outperforms baselines.</p><p>Learned Structure. A key emphasis of this model is on information propagation between roles. In <ref type="figure">Fig. 7</ref>, we present the norms of the propagation matrices. Each element in the matrix P (a , a) is the norm of the incoming message from role a to a averaged across all images (in dev set) at the first time-step, i.e., x t=1 (a ,a) regarding Eq. (3). In this example, tool is important for the verb fastening and influences all other roles, while agent and obstacle influence roles in jumping.  <ref type="figure">Figure 7</ref>. We present the "amount" of information that is propagated between roles for two verbs along with sample images. Blue corresponds to high, and green to zero. Each element of the matrix corresponds to the norm of the incoming message from different roles (normalized column sum to 1). Left: verb fastening needs to pay attention to the tool used. Right: important components to describe jumping are the agent and obstacles along the path. Ground-truth (GT) nouns are in yellow and predicted (PRED) nouns with green when correct, or red when wrong. Although the predicted verb is different from the ground-truth, it is very plausible. Some of the verbs refer to the same frame (e.g. sitting and slouching), and contain the same set of roles, which our model is able to correctly infer. <ref type="figure" target="#fig_4">Fig. 8</ref>. Note that in fact these predicted verbs are plausible options for the given images. The metric value treats them as wrong, and yet we can correctly predict the rolenoun pairs. One example is the middle one of slouching vs. sitting. <ref type="figure" target="#fig_4">Fig. 8 (bottom)</ref> shows that choosing a different verb might lead to the selection of different roles (goalitem vs. item, destination). Nevertheless, predicting book for browsing is a good choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Wrong Verb Predictions. We present a few examples of top scoring results where the verb prediction is wrong in</head><p>Predictions with Correct Verb. <ref type="figure">Fig. 9</ref> shows several examples of prediction obtained by FC Graph, where the predicted verb matches the ground-truth one. The top row corresponds to samples where the metric value-all scores correctly as all role-noun pairs are correct. Note that the roles are closely related (e.g. (agent, clungto) and (material, dye)) and help each other choose the correct nouns. In the bottom row, we show some failure cases in predicting role-noun pairs. First, the model favors predicting place as outdoor (a majority of place is outdoor in the training set). Second, for the sample with verb picking, we predict the crop as apple, which appears 79 times in the dataset compared with cotton that appears 14 times. Providing more training samples (e.g. <ref type="bibr" target="#b46">[47]</ref>) could help remedy such issues.</p><p>In the latter three samples of the bottom row, although the model makes reasonable predictions, they do not match the ground-truth. For example, the ground-truth annotation for the verb taxiing is agent:jet and for the verb camping is agent:persons. Therefore, even though each image comes with three annotations, synonymous nouns and verbs make the task still challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented an approach for recognizing situations in images that involves predicting the correct verb along with its corresponding frame consisting of role-noun pairs. Our Graph Neural Network (GNN) approach explicitly models dependencies between verb and roles, allowing nouns to inform each other. On a benchmark dataset imSitu, we achieved ?4.5% accuracy improvement on a metric that evaluates correctness of the entire frame (value-all). We presented analysis of our model, demonstrating the need to capture the dependencies between roles, and compared it with RNN models and other related solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We present additional analysis and results of our approach in the supplementary material. First, we analyze the verb prediction performance in Sec. A. In Sec. B, we present t-SNE <ref type="bibr" target="#b38">[39]</ref> plots to visualize the verb and role embeddings. We present several examples of the influence of different roles on predicting the verb-frame correctly. This is visualized in Sec. C through propagation matrices similar to <ref type="figure">Fig. 7</ref> of the main paper. Finally, in Sec. D we include several example predictions that our model makes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Verb Prediction</head><p>We present the verb prediction accuracies for our fully-connected model on the development set in <ref type="figure" target="#fig_5">Fig. 10</ref>. The random performance is close to 0.2% (504 verbs). About 22% of all verbs are classified correctly over 50% of the time. These include taxiing, erupting, flossing, microwaving, etc. On the other hand, verbs such as attaching, making, placing can have very different image representations, and show prediction accuracies of less than 10%.</p><p>Our model helps improve the role-noun predictions by sharing information across all roles. Nevertheless, if the verb is predicted incorrectly, the whole situation is treated as incorrect. Thus, verb prediction performance plays a crucial role. Confusion between similar verbs. We analyze the confusion between similar verbs, that according to the metrics, leads to incorrect situation recognition. In the main paper, <ref type="figure" target="#fig_4">Fig. 8</ref> presents a few examples where we are able to correctly predict the roles, but the situation is classified as wrong since the verb is incorrect.</p><p>The imSitu dataset consists of 504 verbs, and while we do have a complete 504 ? 504 confusion matrix, visualizing the results is hard. As explained in the dataset <ref type="bibr" target="#b47">[48]</ref>, the verb frames were obtained using FrameNet. We notice that the 504 verbs from the imSitu dataset are grouped into 161 FrameNet verbs <ref type="bibr" target="#b9">[10]</ref>. For example, several verbs such as walking, climbing, skipping, prowling and 26 others are clustered together to the FrameNet verb: self motion. The clusters need not be large, and 73 of 161 clusters consist of just one verb.</p><p>We use this as a clustering, and present several confusion matrices for verb clusters in <ref type="figure">Fig. 11</ref>. All verb predictions that do not belong to the cluster are grouped as others. While, the others column does collect most of the predictions, there is significant confusion between similar verbs. <ref type="figure">Figure 11</ref>. Confusion matrices for verb prediction. Each row indicates the expected ground-truth, and the columns are predictions (each row sums to 100%). As it is not possible to show all 504 verbs, we pick verb clusters based on their FrameNet labels (shown in the title). Confusion between remaining verbs not in the cluster is grouped in the last column as others. The examples show significant confusion between verbs which are hard to differentiate visually: colliding-crashing-ramming, or crying-giggling-laughing-weeping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Verb and Role Embeddings</head><p>We initialize the hidden states of our role nodes (c.f . Eq. 2 of the main paper) with</p><formula xml:id="formula_9">h 0 ae = g(W in ? n (i) W e e W vv ) ,<label>(10)</label></formula><p>where, W v and W e are verb and role embeddings respectively, and e ? R 190 andv ? R 504 are one-hot vectors representing the noun for a specific role, and the predicted verb. ? n (i) is the image representation using the noun-prediction CNN. Note that both verbs and roles are embedded to a R 1024 space.</p><p>Verbs. The dataset consists of 504 verbs. We first show a plot depicting all verbs in <ref type="figure" target="#fig_6">Fig. 12</ref>. Owing to the number of verbs, this is quite hard to see, nevertheless, we can still observe clusters of similar verbs (e.g. dusting-cleaningscrubbing-wiping, recording-singing-performing, etc.). Additionally, we use the verb clustering afforded by the FrameNet verb associations, and select a set of 196 verbs from the 11 largest clusters (cluster size ? 8). We present their embeddings in <ref type="figure">Fig. 13</ref>. The learned embeddings not only discover the clustering, but are also able to associate across clusters. For example, (in the top-left corner), applying and smearing belong to the Placing FrameNet verb, while spreading and buttering correspond to Filling in FrameNet. Nevertheless, our model is able to learn that these verbs may have similar context (e.g. buttering bread), and brings their representations close.</p><p>Roles. The dataset comes with 190 roles, however, 139 of them are unique to one verb. For example, the roles top and bottom appear only once, in the frame for the verb stacking. Similarly, roles shape and cloth appear only when the verb is folding. We present two-dimensional t-SNE <ref type="bibr" target="#b38">[39]</ref> representations of the learned role embeddings in <ref type="figure">Fig. 14.</ref> We  <ref type="figure">Figure 13</ref>. 2D t-SNE representation of the learned verb embeddings of the verbs belonging to 11 largest clusters (using FrameNet verb clustering). The clusters are: attaching, body movement, cause harm, cause motion, closure, filling, manipulation, operate vehicle, placing, removing, self motion. Each cluster is assigned a unique color from the jet colormap. Our model is even able to learn to embed similar verbs across these FrameNet groupings. For example, it brings together whirling (FrameNet: cause motion) and dancing (FN: self motion); raking (FN: cause motion) and shoveling (FN: removing); packing (FN: placing) and unpacking (FN: filling); throwing (FN: cause motion) and kicking (FN: cause harm); and many others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualizing the propagation matrices.</head><p>We visualize the propagation matrix for 15 more verbs (extending <ref type="figure">Fig. 7</ref> of the main paper). Note that, even though we choose the verbs randomly, we see that many verbs do have dominant roles that influence others. Each row consists of the matrix, and 4 randomly chosen images corresponding to the verb.  <ref type="figure">Figure 14</ref>. 2D t-SNE representation of the learned role embeddings. Note how semantic roles capturing similar themes are brought together. For example, blocked-blocker, or recipients-distributed, or payment-goods. Additionally, related semantic roles that apply across verbs are also brought together. For example, components-instrument-object-part, or liquiddrencheditem, foodcontainer-glue-connector. As most roles do not present a natural clustering, we are unable to color all roles, and they are shown in black. Colored roles are associated with one unique verb.</p><p>Our model propagates information between all roles, and we present the norm of the message sent by each role to the other in the propagation matrix. The verb and list of roles is displayed at the beginning of each row for simplicity. The rows and columns of the propagation matrix follow this ordering of roles.  <ref type="figure" target="#fig_1">Figure 15</ref>. Images with top-1 predictions from the development set. For all samples, the predicted verb is correct, and is shown below the image in bold. Roles are marked with a blue background, and predicted nouns with green when correct, and red when wrong. We are able to correctly predict the situation (verb and all role-noun pairs) for all example images shown here.  <ref type="figure" target="#fig_2">Figure 16</ref>. Images with top-1 predictions from the development set. For all samples, the predicted verb is correct, and is shown below the image in bold. Roles are marked with a blue background, and predicted nouns with green when correct, and red when wrong. We show examples with genuine errors in prediction (e.g. the telephone for the verb pressing is clearly a remote control). However, some examples are marked wrong due to the lack of matching ground-truth annotations (e.g. the woman smelling the flower is outdoors (GT: field)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DANCING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FRYING</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>The architecture of tree-structured RNN. Like the Chain RNN, verb prediction is at the root of the tree, and semantic roles agent-like and place are parents of all other roles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>SPRINKLINGFigure 6 .</head><label>6</label><figDesc>AGENT PLACE ITEM SOURCE DEST. Unaries PERSON KITCHEN MEAT HAND HAND RNN PERSON KITCHEN FOOD FINGER PIZZA FC Graph PERSON KITCHEN CHEESE HAND PIZZA Example images with their predictions listed from all methods. Roles are marked with a blue background, and predicted nouns are in green boxes when correct, and red when wrong. Using the FC Graph corrects mistakes made by the Unaries or Chain RNN prediction models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Images with ground-truth and top-1 predictions from the development set. Roles are marked with blue background.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>ta x ii n g la p p in g re tr ie v in g fl ic k in g m in in g w a x in g ju g g li n g c u rt s y in g c o m m u ti n g d a n c in g c ru s h in g re a d in g e x a m in in g d o u s in g d e c o m p o s in g c h o p p in g d ra w in g c ry in g c a lm in g s n if fi n g m o u rn in g s u b m e rg in g tw is ti n g c a rv in g ru b b in g a Verb prediction accuracy on the development set. Some verbs such as taxiing typically have a similar image (a plane on the tarmac), while verbs such as rubbing or twisting can have very different corresponding images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 .</head><label>12</label><figDesc>2D t-SNE representation of the all the learned verb embeddings. While the number of labels is quite large, it is still possible to see small clusters of verbs forming at the periphery of the figure. top: farming-harvesting, pouring-emptying-milking, slicing-choppingpeeling. top-right: carting-wheeling-heaving, pinching-poking. right: providing-giving, offering-begging-serving, reading-squintingstaring. bottom-right: betting-gambling, grieving-mourning, baptizing-praying. bottom: glowing-flaming, bubbling-overflowing, sniffing-smelling. bottom-left: landing-taxiing, dialing-calling-phoning-typing, boating-rowing. left: drinking-lapping, microwavingbaking, mining-climbing-descending. top-left: dusting-scrubbing-cleaning-wiping, drying-hanging, repairing-fixing-installing.associate same colors with role pairs that are associated with only one verb (there are only 12 such pairs, accounting for 24 of 190 roles). All other roles are shown in black. In theFig. 14,we see that the strongly related pairs that are unique to one verb (and colored) are very close to each other. Additionally, other semantic roles that are related, e.g. food, heatsource, container (right side of figure) are also close together.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>U h , and b h are the weights and biases of the update function.</figDesc><table><row><cell>RIDING</cell><cell>OUTSIDE</cell><cell>MAN</cell><cell>BIKE</cell><cell></cell></row><row><cell>h 0</cell><cell>h 1</cell><cell>h 2</cell><cell>h 3</cell><cell>?</cell></row><row><cell>image</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>feature</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>PLACE</cell><cell>AGENT</cell><cell>VEHICLE</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Output and Learning. We run T propagation steps. Af-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ter propagation, we extract node-level outputs from GGNN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>=4, BS=10, vOH, g=ReLU 36.26 27.22   </figDesc><table><row><cell>Method</cell><cell cols="7">top-1 predicted verb verb value value-all verb value value-all value value-all mean top-5 predicted verbs ground truth verbs</cell></row><row><cell>1 Unaries</cell><cell>36.32 23.74</cell><cell>13.86</cell><cell>61.51 38.57</cell><cell>20.76</cell><cell>58.32</cell><cell>27.57</cell><cell>35.08</cell></row><row><cell>2 Unaries, BS=10</cell><cell>36.39 23.74</cell><cell>14.01</cell><cell>61.65 38.64</cell><cell>20.96</cell><cell>58.32</cell><cell>27.57</cell><cell>35.16</cell></row><row><cell>3 FC Graph, T =1</cell><cell>36.25 25.99</cell><cell>17.02</cell><cell>61.60 42.91</cell><cell>26.44</cell><cell>64.87</cell><cell>35.52</cell><cell>38.83</cell></row><row><cell>4 FC Graph, T =2</cell><cell>36.43 26.08</cell><cell>17.22</cell><cell>61.52 42.86</cell><cell>26.38</cell><cell>65.31</cell><cell>35.86</cell><cell>38.96</cell></row><row><cell>5 FC Graph, T =4</cell><cell>36.46 26.26</cell><cell>17.48</cell><cell>61.42 43.06</cell><cell>26.74</cell><cell>65.73</cell><cell>36.43</cell><cell>39.19</cell></row><row><cell>6 FC Graph, T =4, BS=10</cell><cell>36.70 26.52</cell><cell>17.70</cell><cell>61.63 43.34</cell><cell>27.09</cell><cell>65.73</cell><cell>36.43</cell><cell>39.39</cell></row><row><cell>7 FC Graph, T =4, BS=10, vOH</cell><cell>36.93 27.52</cell><cell>19.15</cell><cell>61.80 45.23</cell><cell>29.98</cell><cell>68.89</cell><cell>41.07</cell><cell>41.32</cell></row><row><cell cols="3">8 FC Graph, T 19.10</cell><cell>62.14 45.59</cell><cell>30.32</cell><cell>69.35</cell><cell>41.71</cell><cell>41.46</cell></row><row><cell cols="2">9 FC Graph, T =4, BS=10, vOH, Soft-OR 36.75 27.33</cell><cell>18.94</cell><cell>61.69 44.91</cell><cell>29.41</cell><cell>68.29</cell><cell>40.25</cell><cell>40.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>top-1 predicted verb top-5 predicted verbs ground truth verbs verb value value-all verb value value-all value value-all mean Situtation prediction results on the development set for models with different graph structures. All models use beam search, predicted verb embedding, and g = tanh(?). Best performance is highlighted in bold, and second-best in each table section is italicized.</figDesc><table><row><cell></cell><cell>1 Unaries</cell><cell>36.39 23.74</cell><cell>14.01</cell><cell>61.65 38.64</cell><cell>20.96</cell><cell>58.32</cell><cell>27.57</cell><cell>35.16</cell></row><row><cell></cell><cell>2 Chain RNN</cell><cell>34.62 24.67</cell><cell>17.94</cell><cell>61.09 41.67</cell><cell>27.80</cell><cell>62.58</cell><cell>36.57</cell><cell>38.36</cell></row><row><cell></cell><cell>3 Tree-structured RNN</cell><cell>34.62 24.24</cell><cell>16.04</cell><cell>58.86 39.15</cell><cell>23.65</cell><cell>60.44</cell><cell>30.91</cell><cell>35.98</cell></row><row><cell></cell><cell>4 Chain GGNN, T =8</cell><cell>36.63 27.27</cell><cell>19.03</cell><cell>61.88 44.97</cell><cell>29.44</cell><cell>68.20</cell><cell>40.21</cell><cell>40.95</cell></row><row><cell></cell><cell cols="2">5 Tree-structured GGNN, T =6 36.78 27.48</cell><cell>19.54</cell><cell>61.75 45.12</cell><cell>30.11</cell><cell>68.54</cell><cell>41.01</cell><cell>41.29</cell></row><row><cell></cell><cell cols="2">6 Fully-connected GGNN, T =4 36.93 27.52</cell><cell>19.15</cell><cell>61.80 45.23</cell><cell>29.98</cell><cell>68.89</cell><cell>41.07</cell><cell>41.32</cell></row><row><cell></cell><cell></cell><cell cols="2">top-1 predicted verb</cell><cell cols="2">top-5 predicted verbs</cell><cell cols="3">ground truth verbs</cell></row><row><cell></cell><cell></cell><cell cols="7">verb value value-all verb value value-all value value-all mean</cell></row><row><cell></cell><cell>CNN+CRF [48]</cell><cell>32.25 24.56</cell><cell>14.28</cell><cell>58.64 42.68</cell><cell>22.75</cell><cell>65.90</cell><cell>29.50</cell><cell>36.32</cell></row><row><cell>dev</cell><cell cols="2">Tensor Composition [47] Tensor Composition + DataAug [47] 34.20 26.56 32.91 25.39</cell><cell>14.87 15.61</cell><cell>59.92 44.50 62.21 46.72</cell><cell>24.04 25.66</cell><cell>69.39 70.80</cell><cell>33.17 34.82</cell><cell>38.02 39.57</cell></row><row><cell></cell><cell>Chain RNN</cell><cell>34.62 24.67</cell><cell>17.94</cell><cell>61.09 41.67</cell><cell>27.80</cell><cell>62.58</cell><cell>36.57</cell><cell>38.36</cell></row><row><cell></cell><cell>Fully-connected Graph</cell><cell>36.93 27.52</cell><cell>19.15</cell><cell>61.80 45.23</cell><cell>29.98</cell><cell>68.89</cell><cell>41.07</cell><cell>41.32</cell></row><row><cell></cell><cell>CNN+CRF [48]</cell><cell>32.34 24.64</cell><cell>14.19</cell><cell>58.88 42.76</cell><cell>22.55</cell><cell>65.66</cell><cell>28.96</cell><cell>36.25</cell></row><row><cell>test</cell><cell cols="2">Tensor Composition [47] Tensor Composition + DataAug [47] 34.12 26.45 32.96 25.32</cell><cell>14.57 15.51</cell><cell>60.12 44.64 62.59 46.88</cell><cell>24.00 25.46</cell><cell>69.20 70.44</cell><cell>32.97 34.38</cell><cell>37.97 39.48</cell></row><row><cell></cell><cell>Chain RNN</cell><cell>34.63 24.65</cell><cell>17.89</cell><cell>61.06 41.73</cell><cell>28.15</cell><cell>62.94</cell><cell>37.32</cell><cell>38.54</cell></row><row><cell></cell><cell>Fully-connected Graph</cell><cell>36.72 27.52</cell><cell>19.25</cell><cell>61.90 45.39</cell><cell>29.96</cell><cell>69.16</cell><cell>41.36</cell><cell>41.40</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Predicting place requires a more global view of the image compared to agent. Changing the order to verb ? agent ? place ? . . . results in 1.9% drop of performance.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is in part supported by a grant from the Research Grants Council of the Hong Kong SAR (project No. 413113). We also acknowledge support from NSERC, and GPU donations from NVIDIA.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 17</ref><p>. Images with ground-truth and top-1 predictions from the development set. Roles are marked with blue background. Groundtruth (GT) nouns with yellow, and predicted (PRED) nouns with green when correct, or red when wrong. Although the predicted verb is different from the ground-truth, it is very plausible. Some of the verbs refer to the same frame (e.g. sprinting, racing), and contain the same set of roles, which our model is able to correctly infer.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Annotating object instances with a polygon-rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dzmitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Recognizing human actions in still images: a study of bag-of-features and partbased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structured Inference Machines: Recurrent Neural Networks for Analyzing Relations in Group Activity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Background to FrameNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R L</forename><surname>Petruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Lexicography</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph Alignment for Semi-Supervised Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>F?rstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond Nouns: Exploiting Prepositions and Comparative Adjectives for Learning Visual Classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual Semantic Role Labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image Retrieval using Scene Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Speech and Language Processing, chapter 22</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Semantic Role Labeling. 3, draft edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Visual-Semantic Alignments for Generating Image Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">From TreeBank to PropBank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<title level="m">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gated Graph Sequence Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generating multisentence lingual descriptions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Teaching machines to describe images via natural language feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual Relationship Detection with Language Priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The more you know: Using knowledge graphs for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Phrase Localization and Visual Relationship Detection with Comprehension Linguistic Cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06641</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Describing Common Human Visual Actions in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural Semantic Role Labeling with Dependency Path Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ImageNet Large Scale Visual Recognition Challenge. IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding Stories in Movies through Question-Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Visualizing High-Dimensional Data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J P</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Show and Tell: A Neural Image Caption Generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Generalization of backpropagation with application to a recurrent gas market model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="339" to="356" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attend and Tell: Neural Image Caption Generation with Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Show</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Grounded Semantic Role Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grouplet: A Structured Image Representation for Recognizing Human and Object Interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Human Action Recognition by Learning Bases of Action Attributes and Parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Commonly Uncommon: Semantic Sparsity in Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Situation Recognition: Visual Semantic Role Labeling for Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Visual Translation Embedding Network for Visual Relation Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08319</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional Random Fields as Recurrent Neural Networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Places: An Image Database for Deep Scene Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02055</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning Deep Features for Scene Recognition using Places Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end Learning of Semantic Role Labeling using Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">BRUSHING Roles: agent, place, target, tool, substance Verb: BURNING Roles: agent, place, target Verb: CHECKING Roles: agent, place, patient, aspect, tool Verb: CRAFTING Roles: agent, place, created, instrument Verb: DIPPING Roles: agent, place, item, substance Verb: DISTRIBUTING Roles: agent, place, tool, distributed, recipients Verb: EXAMINING Roles: agent, place, item, tool Verb: GIVING Roles: agent, place, item, recipient Verb: HUNCHING Roles: agent, place, surface Verb: KISSING Roles: agent, place, coagent, coagentpart, agentpart Verb: MILKING Roles: agent, place, source, tool, destination Verb: PACKING Roles: agent, place, item</title>
	</analytic>
	<monogr>
		<title level="m">Verb: ADJUSTING Roles: agent, place, item, feature, tool Verb: AUTOGRAPHING Roles: agent, place, item, receiver Verb</title>
		<imprint/>
	</monogr>
	<note>container Verb: PERFORMING Roles: agent, place, event, stage, tool</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Prediction Results We round up the supplementary material with several more example predictions from our model. Fig. 15 shows predictions that are completely correct. Fig. 16 shows examples where we are able to predict the correct verb, but not all the role-noun pairs. Such examples are counted towards the value metric, but not value-all. Finally, Fig. 17 shows top-scoring (logprobability) examples where the verb is wrongly predicted</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>but is mostly plausible (the correct noun predictions are not captured by any metric). The role-noun pairs here are often correct</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
