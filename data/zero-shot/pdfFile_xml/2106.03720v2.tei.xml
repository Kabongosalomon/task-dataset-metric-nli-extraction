<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Person Re-Identification with a Locally Aware Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
							<email>charus2@umbc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Contributed Equally Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland Baltimore County</orgName>
								<address>
									<postCode>21250</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><forename type="middle">R</forename><surname>Kapil</surname></persName>
							<email>skapil1@umbc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Contributed Equally Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland Baltimore County</orgName>
								<address>
									<postCode>21250</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chapman</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Contributed Equally Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland Baltimore County</orgName>
								<address>
									<postCode>21250</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Person Re-Identification with a Locally Aware Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person Re-Identification is an important problem in computer vision-based surveillance applications, in which the same person is attempted to be identified from surveillance photographs in a variety of nearby zones. At present, the majority of Person re-ID techniques are based on Convolutional Neural Networks (CNNs), but Vision Transformers are beginning to displace pure CNNs for a variety of object recognition tasks. The primary output of a vision transformer is a global classification token, but vision transformers also yield local tokens which contain additional information about local regions of the image. Techniques to make use of these local tokens to improve classification accuracy are an active area of research. We propose a novel Locally Aware Transformer (LA-Transformer) that employs a Parts-based Convolution Baseline (PCB)-inspired strategy for aggregating globally enhanced local classification tokens into an ensemble of ? N classifiers, where N is the number of patches. An additional novelty is that we incorporate blockwise fine-tuning which further improves re-ID accuracy. LA-Transformer with blockwise fine-tuning achieves rank-1 accuracy of 98.27% with standard deviation of 0.13 on the Market-1501 and 98.7% with standard deviation of 0.2 on the CUHK03 dataset respectively, outperforming all other state-of-the-art published methods at the time of writing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, Person Re-Identification(re-ID) has gained a lot of attention due to its foundational role in computer vision based video surveillance applications. Person re-ID is predominantly considered as a feature embedding problem. Given a query image and a large set of gallery images, person re-ID generates the feature embedding of each image and then ranks the similarity between query and gallery image vectors. This can be used to re-identify the person in photographs obtained by nearby surveillance cameras.</p><p>In the words of <ref type="bibr" target="#b1">Beal et al. [2020]</ref>, "The remaining tokens in the sequence are used only as features for the final class token to attend to. However, these unused outputs correspond to the input patches, and in theory, could encode local information useful for performing object detection". <ref type="bibr" target="#b1">Beal et al. [2020]</ref> observed that the local tokens, although theoretically influenced by global information, also have substantial correspondence to the original input patches. One might therefore consider the possibility of using these local tokens as an enhanced feature representation of the original image patches to more strongly couple vision transformer encoders to fully connected (FC) classification techniques. This coupling of local patches with FC classification techniques is the primary intuition behind the LA-Transformer architectural design.</p><p>Part-based Convolutional Baseline (PCB) ] is a strong convolutional baseline technique for person re-ID and has inspired many state-of-the-art models <ref type="bibr" target="#b27">[Yao et al., 2018</ref>. PCB partitions the feature vector received from the backbone network into six vertical regions and constructs an ensemble of regional classifiers with a voting strategy to determine the predicted class label. A limitation of PCB is that each regional classifier ignores the global information which is also very important for recognition and identification. Nevertheless, PCB has achieved much success despite this limitation, and as such the design of LA-Transformer uses a PCB-like strategy to combine globally enhanced local tokens.</p><p>Our work also improves on the recent results of <ref type="bibr" target="#b9">He et al. [2021]</ref>, who was the first to employ Vision Transformers to person re-ID and achieved results comparable to the current state-of-the-art CNN based models. Our approach extends <ref type="bibr" target="#b9">He et al. [2021]</ref> in several ways but primarily because we aggregate the globally enhanced local tokens using a PCB-like strategy that takes advantage of the spatial locality of these tokens. Although <ref type="bibr" target="#b9">He et al. [2021]</ref> makes use of fine-grained local tokens, it does so with a ShuffleNet  like Jigsaw shuffling step which does not take advantage of the 2D spatial locality information inherent in the ordering of the local tokens. LA-Transformer overcomes this limitation by using a PCB-like strategy to combine the globally enhanced local tokens while first preserving their ordering in correspondence with the image dimension.</p><p>An additional novelty of our approach is the use of blockwise fine-tuning which we find is able to further improve the classification accuracy of LA-Transformer for person re-ID. Blockwise fine-tuning is viable as a form of regularization when training models with a large number of parameters over relatively small in-domain datasets. <ref type="bibr" target="#b11">Howard and Ruder [2018]</ref> advocate for blockwise fine-tuning or gradual unfreezing particularly when training language models due to a large number of fully connected layers. As vision transformers also have high connectivity, we find that this approach is able to further improve the classification accuracy for LA-Transformer. This paper is organized as follows: Firstly, we discuss related work involving Transformer architectures and other related methodologies in person re-ID. Secondly, we describe the architecture of LA-Transformer, including the novel locally aware network and blockwise fine-tuning techniques. Finally, we present quantitative results of the person re-ID including mAP and rank-1 analysis on the market-1501 and CUHK03 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>For many years CNN based models have dominated image recognition tasks including person re-ID. A vast body of research has been performed to determine the best strategy to extract features using CNNs to address issues like appearance ambiguity, background perturbance, partial occlusion, body misalignment, viewpoint changes, and pose variations, etc. <ref type="bibr" target="#b17">Sarfraz et al. [2018]</ref> proposed a Pose-Sensitive Embedding to incorporate information associated with poses of a person in the model, <ref type="bibr">Yu et al. [2020] used a Graph Convolution Network [Kipf and</ref><ref type="bibr" target="#b13">Welling, 2017]</ref> to generate a conditional feature vector based on the local correlation between image pairs, <ref type="bibr" target="#b10">Herzog et al. [2021]</ref> used global channel-based and part-based features,  used global pooling to extract global features and horizontal pooling followed by 1 ? 1 CNN for local features. CNN based methods have led to many advances in recent years and are continuing to be developed for person re-ID.</p><p>Another branch of techniques for person re-ID focuses on the development of highly engineered network designs that incorporate additional domain knowledge to improve re-ID performance. <ref type="bibr">Ding et al. [2020]</ref> used a part-aware approach for which the model performs the main task as well as auxiliary tasks for each body part. <ref type="bibr" target="#b34">Zhou and Shao [2018]</ref> and <ref type="bibr" target="#b35">Zhu et al. [2019]</ref> use viewing angles as additional features. <ref type="bibr" target="#b26">Yao et al. [2017]</ref> introduced the idea of calculating part loss and  (Part-based Convolutional Backbone a.k.a. PCB) improved on it. Even current top-performing models like <ref type="bibr" target="#b27">Yao et al. [2018]</ref> used PCB along with domain-specific Spatio-temporal distribution information to achieve good results on the Market-1501 dataset. In our work we incorporate PCB-like local classifiers with Vision Transformers, and furthermore we find that our model performs better if we pass global information along with local features. LA-Transformer achieves results with comparable and slightly higher rank-1 accuracy than the reported results of <ref type="bibr" target="#b27">Yao et al. [2018]</ref> over Market-1501 and does so without the use of additional Spatio-temporal information.</p><p>Interest in Vision Transformers grew initially from attention mechanisms which were first employed for language translation problems in NLP <ref type="bibr" target="#b0">[Bahdanau et al., 2016]</ref>, Attention mechanisms have been employed to great effect in image recognition. <ref type="bibr" target="#b21">Wang et al. [2018a]</ref> introduced parameter-free spatial attention to integrating spatial relations to Global Average Pooling (GAP). <ref type="bibr" target="#b25">Xie et al. [2020]</ref> used Spatial Attention Module (SAM), and Channel Attention Module (CAM) to deliver prominent spatial and channel information. <ref type="bibr" target="#b4">Chen et al. [2019]</ref> propose Position Attention Module (PAM) for semantically related pixels in the spatial domain along with CAM. Attention mechanisms continue to be an active area of research for many problems related to object detection and recognition.</p><p>Transformers were first introduced in NLP problems by <ref type="bibr" target="#b20">Vaswani et al. [2020]</ref>, and now Transformers are contributing to many new developments in machine learning. <ref type="bibr" target="#b6">Dosovitskiy et al. [2020]</ref> introduced transformers to images by treating a 16x16 patch as a word and treating image classification as analogous to text classification. This approach showed promising results on ImageNet and it was soon adopted in many image classification problems <ref type="bibr" target="#b15">[Parmar et al., 2018</ref><ref type="bibr" target="#b23">, Wang et al., 2018b</ref>. Object detection is another highly related problem for which vision transformers have been recently applied <ref type="bibr" target="#b2">[Carion et al., 2020</ref><ref type="bibr" target="#b1">, Beal et al., 2020</ref>. <ref type="bibr" target="#b1">Beal et al. [2020]</ref> described a correspondence between local tokens and input patches and combined local tokens to create spatial feature maps. At present, this observation of the correspondence between local tokens and input patches has yet to be applied to a wide variety of computer vision problems, nor has it been previously explored in the context of person re-ID. One exception is in the area of image segmentation, for which recent works are beginning to take advantage of the 2D ordering of the local tokens in order to produce more accurate predicted masks <ref type="bibr" target="#b3">, Chen et al., 2021</ref>. Our approach builds upon the recent work of <ref type="bibr" target="#b9">He et al. [2021]</ref> who was the first to apply vision transformers to object and person re-ID. Although the approach of <ref type="bibr" target="#b9">He et al. [2021]</ref> makes use of global and local tokens, <ref type="bibr" target="#b9">He et al. [2021]</ref> combines the local tokens using a jigsaw classification branch which shuffles the ordering of the local features. Shuffling the order of local features does not take advantage of the observation of <ref type="bibr" target="#b1">Beal et al. [2020]</ref> in that local features correspond strongly with input patches and therefore have a natural ordering in the form of a 2D image grid. Conversely, LA-Transformer takes advantage of the spatial locality of these local features by combining globally enhanced local tokens with a PCB-like strategy . Furthermore, LA-Transformer incorporates the blockwise fine-tuning strategy as described by <ref type="bibr" target="#b11">Howard and Ruder [2018]</ref> as a form of regularization for high-connectivity pre-trained language models. As such LA-Transformer builds upon recent advances in the application of vision transformers in tandem with novel training techniques to achieve state of the art accuracy in person re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>LA-Transformer combines vision transformers with an ensemble of FC classifiers that take advantage of the 2D spatial locality of the globally enhanced local tokens. Section 3.1 describes the overall architecture including the backbone vision transformer (section 3.1.1), as well as the PCB inspired classifier network ensemble (section 3.1.2). The blockwise fine-tuning strategy is described in section 3.2. As such, these sections describe the major elements of the LA-Transformer methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Locally Aware Transformer</head><p>LA-Transformer (figure 3) consists of two main parts: a backbone network and a locally aware network. Both components are interconnected and trained as a single neural network model.</p><p>The backbone network is the ViT architecture as proposed by <ref type="bibr" target="#b6">Dosovitskiy et al. [2020]</ref>. ViT generates tokens F = f 0 , f 1 , .., f N . The token f 0 , also known as the global classification token and we refer to this token as the global token G. Supplementary outputs f 1 ..f 196 are referred to as local tokens which we denote to collectively as Q. Globally Enhanced Local Tokens (GELT) are obtained by combining global tokens and local tokens (G and Q) using weighted averaging and are arranged into a 14 ? 14 2D spatial grid as seen in <ref type="figure">Figure 3(a)</ref>. The row-wise averaged GELTs are then fed to the locally aware classification ensemble as seen in <ref type="figure">Figure 3</ref>(b) to classify during the training process and to generate feature embedding (by concatenating L) during the testing process. These steps are described in greater detail in the following sections 3.1.1 and 3.1.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">LA-Transformer Backbone</head><p>The backbone network of LA-Transformer is the ViT vision transformer <ref type="bibr" target="#b6">[Dosovitskiy et al., 2020]</ref>. ViT requires extensive training data on the order of 14M ? 300M images to train effectively, but the Market1501 and CUHK-03 datasets are relatively small <ref type="table">(Table 4</ref>.1) in comparison on the order of 10's of thousands of images. As such we employed a pre-trained ViT model, and further made use of blockwise fine-tuning to improve accuracy as described in section 3.2</p><p>Embeddings The backbone ViT architecture takes images of size 224 ? 224 as input, and as such the Market1501 and CUHK-03 images are re-sampled to this resolution during training. First, the image is converted into N number of patches x i p |i = 1, .., N . Each patch is then linearly projected into D dimensions using the patch embedding function (E(x i p )|i = 1, .., N ) (eq. 2), which is obtained using a convolution layer with a kernel size of 16 ? 16. For non-overlapping patches, a stride equal to 16 is used. D is the number of channels and is set to 768 which represents the size of the embedding. The total number of patches N depends on kernel size, stride, padding, and size of the image. N can be easily calculated using the eq. 1. Assuming padding is 0, and H, W are height and width of an image, K H , K W are height and width of the kernel and S is kernel stride.</p><formula xml:id="formula_0">N = H ? K H S + 1 ? W ? K W S + 1<label>(1)</label></formula><p>Afterward, the learnable class embedding x class is prepended with the patch embedding (E(x i p )) whose output state keeps the information of the entire image and serves as the global vector. The resulting vectors are then added with position embeddings P to preserve the positional information. Subsequently, the final sequence of vectors z 0 (eq. 2) is fed into the transformer encoder (figure3) to generate N + 1 feature vectors where N is the number of patches plus class embedding.</p><formula xml:id="formula_1">z 0 = [x class ; E(x 1 p ); E(x 2 p ); ....; E(x N p )] + P<label>(2)</label></formula><p>Transformer Encoder The transformer encoder consist of total B = 12 blocks. Each block contains alternating MSA (Multiheaded Self-Attention) introduced by Vaswani et al.</p><p>[2020] and MLP blocks. The Layernorm (LN) is applied before MSA and MLP blocks and a residual connections is applied after each encoder block. The output of transformer encoder F described in eq. 5 passes through all the B blocks (eq. 3 and 4 ).</p><formula xml:id="formula_2">z b = z b?1 + M SA(LN (z b?1 )) (3) z b = z b + M LP (LN (z b )) (4) F = LN (z B )<label>(5)</label></formula><p>While the seminal work of <ref type="bibr" target="#b6">Dosovitskiy et al. [2020]</ref> only uses classification token z 0 B for classification, LA-Transformer makes use of all of the features z B eq. 5. Though the class embedding can be removed from the backbone network, our experiments show promising results with class embedding serving as a global vector <ref type="table" target="#tab_2">(Table 2)</ref>. From our experiments, it is clear that ViT as a backbone network is a good choice for person re-ID based problems. Further, we believe that any transformer based model like Diet by <ref type="bibr" target="#b19">Touvron et al. [2020]</ref>, or DeepViT by <ref type="bibr" target="#b3">Zhou et al. [2021]</ref> can be used as a backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Locally Aware Network</head><p>The Locally Aware Network is a classifier ensemble similar to the PCB technique of  but with some differences. Firstly, in  the input features are purely local, whereas in LA-Transformer, we find that the inclusion of global vectors along with local vectors via weighted averaging can increase the network accuracy. Secondly, although in  the image is divided into six input regions, we divide the 2D spatial grid of tokens into ? N = 14 regions as seen in <ref type="figure">Figure 3</ref>. Finally, while PCB uses a convolutional backbone, LA-Transformer uses the ViT backbone.</p><p>In <ref type="figure">Figure 3</ref>, the transformer encoder outputs N + 1 feature vectors. The global tokens G = f 0 and local tokens Q = [f 1 , f 2 , f 3 , ..., f N ] are obtained for which N is number of patches. N R is defined as the total number of patches per row and N C as the total number of patches per column. In our case, N R = N C = ? N . Then we define L as the averaged GELT obtained after average pooling of Q and G as follows,</p><formula xml:id="formula_3">L i = 1 N R (i+1) * N R j=i * N R +1 (Q j + ?G) (1 + ?) i = 0...N C ? 1<label>(6)</label></formula><p>In eq. 6 all the patches in a row are averaged to create one local vector per row . The total number of FC classifiers is equal to N C . Each FC classifier contains two fully connected layers with RELU and Batch Normalization. We define y as the output of LA-Transformer as follows, </p><formula xml:id="formula_4">y i = F C i (L i ) i = 1...N C<label>(7)</label></formula><p>The outputs y are passed through softmax and the softmax scores are summed together. The argument of the maximum score represents the ID of the person as follows.</p><formula xml:id="formula_5">score = N C i=0 sof tmax(y i ) (8) prediction = argmax(score)<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning Strategies</head><p>According to the recent studies of <ref type="bibr" target="#b19">Touvron et al. [2020]</ref> and <ref type="bibr" target="#b6">Dosovitskiy et al. [2020]</ref>, training a vision transformer from scratch requires about 14M-300M images. Person re-ID datasets are known for their small size and training a transformer on these datasets can quickly lead to overfitting. As such, ViT was pre-trained on ImageNet <ref type="bibr" target="#b16">(Ridnik et al. [2021]</ref>), and then fine-tuned on person re-ID datasets. Blockwise fine-tuning was applied which is highly similar to the gradual unfreezing method described by <ref type="bibr" target="#b11">Howard and Ruder [2018]</ref> for the purposes of training large language models in the event of limited training data from a target domain.</p><p>Blockwise Fine-tuning In blockwise fine-tuning, all transformer blocks are frozen in the start except for the bottleneck model. After every t epochs (where t is a hyper-parameter), one additional transformer encoder block is unfrozen and the learning rate is reduced as described by Alg3.2. Blockwise fine-tuning helps in mitigating the risk of catastrophic forgetting of the pre-trained weights <ref type="bibr" target="#b11">[Howard and Ruder, 2018]</ref>. The learning rate decay helps in reducing the gradient flow in the subsequent layers hence prevent abrupt weight updates.</p><p>Algorithm 1 Blockwise Fine-tuning 1: Freeze all the transformer blocks B 2: Initialize parameters: t = 2, b = 12, lr = 3e ? 4, lr ? decay = 0.  Evaluation protocol By convention, re-ID is evaluated over two standard evaluation metrics; Cumulative Matching Characteristics (CMC) and Mean Average Precision (mAP). We apply these metrics to assess the performance of the LA-Transformer and other experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Implementation Details</head><p>ViT was pre-trained on ImageNet-21K and used as a backbone network as well as a baseline model <ref type="bibr" target="#b6">[Dosovitskiy et al., 2020</ref><ref type="bibr" target="#b16">, Ridnik et al., 2021</ref>. All the images are resized into 224 ? 224 as this resolution is compatible with the backbone network. The model is trained over 30 epochs with a batch size of 32. We used the Adam optimizer with an initial learning rate of 3e ? 5, step decay of 0.8, t = 2 and ? = 0.8. For testing, we concatenated all of the averaged GELTs L to generate the feature embedding. To efficiently calculate the Euclidean norm between the query and gallery vectors, we use the FAISS library <ref type="bibr" target="#b12">Johnson et al. [2017]</ref>. All the models are trained and tested on a single GPU machine with an Nvidia RTX2080 Ti with 11 GB VRAM, and 64 GB RAM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation study of LA Transformer</head><p>The table 2 compares the performance of variations of LA-Transformer versus the same variations of baseline ViT using the Market-1501 dataset. All six experiments are performed with and without blockwise fine-tuning. Experiment 1 is the baseline model that uses only the global token to generate feature embedding. Experiment 2 uses only the local tokens of the transformer encoder and exhibits the lowest rank-1 accuracy (95.1) and mAP score (86.5) out of all of the variations of ViT. Experiment 3 combines the the first and second experiments by utilizing the globally enhanced local tokens. The impact of global and local features is also compared using LA-Transformer via three variations: global, local, and globally enhanced local tokens. All of the experiments with LA-Transformer perform better than the baseline ViT and its variations. LA-Transformer increases the rank-1 accuracy Ablation study of Blockwise fine-tuning Blockwise fine-tuning achieves higher rank-1 and mAP scores in all experiments as compared against similar experiments without blockwise fine-tuning over the Market-1501 dataset. As seen in <ref type="table" target="#tab_2">Table 2</ref>, blockwise fine-tuning increases the rank-1 accuracy by +0.6% and mAP score by +0.77 on average across all of the experiments in this Ablation study. During blockwise fine-tuning, the hyperparameter t is set to 2 which means, after every 2 epochs one additional block is unfrozen. The baseline ViT model has 12 blocks. Therefore, it takes 22 epochs to unfreeze and train on all the layers. However, for most models, we found the best validation score is reached before the 22nd epoch, but rather after the 18th epoch yielding 10 trainable blocks during fine tuning. <ref type="figure">Figure 4</ref>.2 shows the comparison of validation results for LA-Transformer trained with and without blockwise fine-tuning. It can be clearly seen that blockwise fine-tuning leads to faster convergence and better results than the training model without blockwise fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with state of the art</head><p>To evaluate the performance of LA-Transformer, it is trained and evaluated five times on Market-1501 and CUHK03 and the mean results are reported in <ref type="table" target="#tab_3">Table 3</ref>. On Market-1501, the rank-1 accuracy of LA-Transformer is 98.27% with standard deviation of 0.13 with blockwise fine-tuning and 97.55 with standard deviation of 0.49 without blockwise fine-tuning. On CUHK03, the rank-1 accuracy of LA-Transformer is 98.7% with a standard deviation of 0.2 with blockwise fine-tuning. <ref type="table" target="#tab_3">Table 3</ref> compares LA Transformer with the state-of-the-art (SOTA) models on two benchmarks of person re-ID; Market-1501 and CUHK-03. On the Market-1501 dataset, LA-transformer achieves the highest reported rank-1 accuracy of all models in this comparison, and outperforms the rank-1 accuracy of the next highest SOTA model by +0.27%. On Market-1501, the mAP score lies among the top five SOTA models. In case of CUHK-03, LA-Transformer achieves both the highest rank-1 accuracy as well as the highest mAP score, and outperforms the next highest SOTA models score by +0.9% (rank-1) and +5.1 (mAP) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We present a novel technique for person re-ID called Locally Aware Transformer (LA-Transformer) which achieves state of the art performance on the Market-1501 and CHUK-03 datasets. This approach makes two contributions toward solving the person re-ID problem. First, we show that the global token and local token outputs of vision transformers can be combined with a PCB-like strategy to improve re-ID accuracy. Secondly, we incorporate blockwise fine-tuning to regularize the fine tuning of a pre-trained vision transformer backbone network. We believe that vision transformers will continue to have a major positive impact in the field of computer vision, and we are hopeful that the architectural design of LA-transformer will lead to further innovation and the development of new and novel techniques to advance our understanding of person re-ID.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of LA-Transformer. Part (a) -shows the backbone architecture. The input image is converted into patch embedding using 2D convolution. The class embedding (cls embedding) is prepended to the patch embedding. Then the position embeddings are added and this resulting sequence is fed to the transformer encoder. F = f 0 , f 1 , ..., f N is the output of the transformer encoder where f 0 is the global vector G and remaining tokens from f 1 to f N are local tokens Q. G and Q are then combined using weighted averaging and are called Globally Enhanced Local Tokens (GELT). GELT are then arranged into a 2D spatial grid. Part (b) -shows the Locally Aware Network. L is the row-wise average of GELTs and is performed using average pooling. L is then fed to the locally aware classification ensemble. Part (c) and (d) describes the architecture of transformer encoder and FC classifier respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Part(a)-compares the validation loss of LA-Transformer with and without Blockwise Fine-tuning (BWFT). Part(b) -compares validation accuracy of LA-Transformer with and without BWFT. Both graphs show results using Market-1501.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">: Datasets Overview</cell><cell></cell></row><row><cell>Dataset</cell><cell cols="3">Classes Train Query Gallery</cell></row><row><cell cols="2">Market-1501 751</cell><cell>12192 3368</cell><cell>19744</cell></row><row><cell>CUHK-03</cell><cell>1367</cell><cell>13131 965</cell><cell>965</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation result of the influence of global and local features on baseline ViT and LA-Transformer with and without blockwise fine-tuning over Market-1501</figDesc><table><row><cell>Without BW-FT</cell><cell>With BW-FT</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with State of the Art methods by +2.3% and mAP score by +2.6 on an average versus ViT over the experiments in table 2. Similar to ViT with local features, LA-Transformer with only local features achieves the lowest accuracy of the LA-Transformer only experiments. Therefore, we conjecture that using only local vectors to predict the output and generate the final embedding is not sufficient. Nevertheless, using globally enhanced local tokens outperforms the local only results by +1.31% rank-1 and +0.265 mAP and improves over the global only results by +0.62% rank-1 and +0.5 mAP. Therefore LA-Transformer using globally enhanced local tokens achieves the highest rank-1 and mAP scores of all technique + feature embedding designs in this comparison.</figDesc><table><row><cell cols="2">Market-1501</cell><cell></cell><cell cols="2">CUHK-03</cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Rank-1 mAP Model</cell><cell cols="2">Rank-1 mAP</cell></row><row><cell>PCB</cell><cell>92.3</cell><cell cols="2">77.4 UTAL</cell><cell>56.3</cell><cell>42.3</cell></row><row><cell>AANet</cell><cell>93.9</cell><cell cols="2">83.4 k-reciprocal 46</cell><cell>61.6</cell><cell>67.6</cell></row><row><cell>IANet</cell><cell>94.4</cell><cell cols="2">83.1 DG-Net 61.1 65.6</cell><cell></cell><cell></cell></row><row><cell>DG-Net</cell><cell>94.8</cell><cell>86</cell><cell>OIM</cell><cell>77.5</cell><cell>72.5</cell></row><row><cell>TransReID</cell><cell>95.2</cell><cell cols="2">88.9 VI+LSRO</cell><cell>84.6</cell><cell>87.6</cell></row><row><cell>Flip reid</cell><cell>95.8</cell><cell cols="2">94.7 TriNet</cell><cell>89.63</cell><cell>-</cell></row><row><cell>VA-reid</cell><cell>96.79</cell><cell cols="2">95.4 FD-GAN</cell><cell>92.6</cell><cell>91.3</cell></row><row><cell>st-Reid</cell><cell>98</cell><cell cols="2">95.5 DCDS</cell><cell>95.8</cell><cell>-</cell></row><row><cell>CTL Model</cell><cell>98</cell><cell cols="2">98.3 AlignedReID</cell><cell>97.8</cell><cell>-</cell></row><row><cell cols="2">LA-Transformer 98.27</cell><cell cols="2">94.46 LA-Transformer</cell><cell>98.7</cell><cell>96.4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Toward transformer-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<editor>Dong Huk Park, Andrew Zhai, and Dmitry Kislyuk</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Abd-net: Attentive but diverse person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-task learning with coarse priors for robust part-aware person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08069</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (cs.CV), 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (cs.CV)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Training vision transformers for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Beyond human parts: Dual part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinge</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transreid: Transformer-based object re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuting</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Lightweight multi-branch network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Herzog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunbo</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torben</forename><surname>Teepe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>H?rmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Gilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Rigoll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Fine-tuned language models for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno>abs/1801.06146</idno>
		<ptr target="http://arxiv.org/abs/1801.06146" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image transformer</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Ben-Baruch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Imagenet-21k pretraining for the masses</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A pose-sensitive embedding for person re-identification with expanded cross neighborhood re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Saquib</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arne</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Eberle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>abs/2012.12877</idno>
		<ptr target="https://arxiv.org/abs/2012.12877" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Parameter-free spatial attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Visual transformers: Token-based image representation and processing for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning diverse features with part-level resolution for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep representation learning with part loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00798</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (cs.CV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial-temporal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hantao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03282</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (cs.CV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Devil&apos;s in the details: Aligning visual clues for conditional embedding in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fufu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pyramidal person re-identification via multi-loss dynamic training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqiao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Scalable person reidentification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.133</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deepvit: Towards deeper vision transformer. CoRR, abs/2103.11886, 2021</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2103.11886" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Viewpoint-aware attentive multi-view inference for vehicle re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (cs.CV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Viewpointaware loss with angular regularization for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weishi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Suno</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01300</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (cs.CV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
