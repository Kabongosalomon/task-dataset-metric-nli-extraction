<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Classifying Textual Data with pretrained Vision Models through Transfer Learning and Data Transformations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charaf</forename><forename type="middle">Eddine</forename><surname>Benarab</surname></persName>
							<email>charafeddineben@std.uestc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Science and Technology</orgName>
								<orgName type="department" key="dep2">School of Computer Science and Engineering Chengdu</orgName>
								<orgName type="institution">University of Electronics</orgName>
								<address>
									<country>China, China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Classifying Textual Data with pretrained Vision Models through Transfer Learning and Data Transformations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Natural language processing</term>
					<term>Text Classification</term>
					<term>Image Classification</term>
					<term>t-SNE</term>
					<term>BERT</term>
					<term>Transfer Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Domain Adaptation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge is acquired by humans through experience, and no boundary is set between the kinds of knowledge or skill levels we can achieve on different tasks at the same time. When it comes to Neural Networks, that is not the case. The breakthroughs in the field are extremely task and domain-specific. Vision and language are dealt with in separate manners, using separate methods and different datasets. Current text classification methods, mostly rely on obtaining contextual embeddings for input text samples, then training a classifier on the embedded dataset. Transfer learning in Language-related tasks in general, is heavily used in obtaining the contextual text embeddings for the input samples. In this work, we propose to use the knowledge acquired by benchmark Vision Models which are trained on ImageNet to help a much smaller architecture learn to classify text. A data transformation technique is used to create a new image dataset, where each image represents a sentence embedding from the last six layers of BERT, projected on a 2D plane using a t-SNE based method. We trained five models containing early layers sliced from vision models which are pretrained on ImageNet, on the created image dataset for the IMDB dataset embedded with the last six layers of BERT. Despite the challenges posed by the very different datasets, experimental results achieved by this approach which links large pretrained models on both language and vision, are very promising, without employing compute resources. Specifically, Sentiment Analysis is achieved by five different models on the same image dataset obtained after BERT embeddings are transformed into gray scale images.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. I</head><p>Attention-based architectures and specifically the Transformer <ref type="bibr" target="#b23">[24]</ref> sparked a revolution in the world of Natural Language Processing and Deep Learning in General. BERT-representations <ref type="bibr" target="#b4">[5]</ref> opened a lot of new goals and challenges for the Machine Learning Community, because of their semantically rich embeddings-and the kind of knowledge they encode given textual data. Being pretrained to Masked Language and Next Sentence prediction and on a very large Wikipedia Corpus, makes it a powerful benchmark for Natural Language Processing and a current standard for word and sentence representations. Computer Vision made very remarkable achievements <ref type="bibr" target="#b27">[28]</ref>. The groundbreaking work introduced in AlexNet <ref type="bibr" target="#b13">[14]</ref> and the parallel structure of Convolutional Neural Networks enabled the use of GPUs, making it a standard for training Neural Networks for Visual Recognition. Different architectures have emerged <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b19">[20]</ref>  <ref type="bibr" target="#b8">[9]</ref> [26] since then using CNN's as a building block and achieving very high accuracies on different datasets. Transfer Learning <ref type="bibr" target="#b0">[1]</ref> [2] opened the doors for a very wide range of applications, allowing for the exchange of previously acquired knowledge from a large dataset on a certain task to another task with a much smaller dataset. This philosophy is often an essential practice in both academia and industry as it helps avoid a very long and computationally demanding procedure. In visual understanding tasks, transfer learning in the last decade heavily relied on ImageNet <ref type="bibr" target="#b3">[4]</ref> as a base or source dataset, ImageNet <ref type="bibr" target="#b3">[4]</ref> is a large dataset containing over 14 million images with annotations from one-thousand class labels and is often selected for pre-training Vision Models. The work described in this paper aims to use knowledge acquired by vision models to train text-classifiers for Sentiment Analysis, using a t-SNE based transformation method brought about in <ref type="bibr" target="#b17">[18]</ref> to transform IMDB Text Embeddings from BERT into gray scale images. The main objective of this paper is to bring language and vision a step closer, and to harness the power of transfer learning from large image datasets in Natural Language Processing and vice-versa. We hope this will encourage further work on the topic using appropriate resources and datasets since none were available during the conception of this paper. We extend the relevant body of work on the integration of language and vision covered in <ref type="bibr" target="#b17">[18]</ref> with a complete Transfer Learning-based fusion of these modalities. The main components of our study include:</p><p>? Using the BERT <ref type="bibr" target="#b4">[5]</ref> embeddings for the IMDB-Dataset <ref type="bibr" target="#b15">[16]</ref> to create an IMDB-Image Dataset using the method described in <ref type="bibr" target="#b17">[18]</ref>  The contributions of this paper are stated as follows:</p><p>? Exchanging knowledge between language and vision models through transfer learning and data transformations. ? Generating an image dataset for IMDB textual dataset, avoiding domain shifts between source and target datasets (ImageNet <ref type="bibr" target="#b3">[4]</ref> and IMDB-Images) with pixel normalization. ? Harnessing the pre-training of vision models on a large image dataset in text classification, and promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. R W</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transfer Learning</head><p>The modern learning paradigm for most Vision models, is mostly based on extracting features from the ImageNet dataset <ref type="bibr" target="#b3">[4]</ref>, then finetuning certain layers on a new smaller dataset concerned with a new task. The concept of T ransf erLearning evolved from when it was first introduced by Stevo Bozinovski and Ante Fulgosi [1] <ref type="bibr" target="#b1">[2]</ref>. Due to the revolution in hardware allowing training on very large datasets, most training paradigms transfer knowledge obtained from a large dataset to solve a target task with a target dataset. Authors in <ref type="bibr" target="#b31">[32]</ref> provide a complete survey on T ransf erLearning, its applications, types, methods, and challenges. Yosinski et al. <ref type="bibr" target="#b26">[27]</ref> conducted a detailed study on what kind of features different layers in a Neural Network learn about the source dataset, and how T ransf erLearning can make the most of the acquired knowledge to solve a target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transfer Learning in Language and Vision</head><p>Transfer Learning in Computer Vision and Natural Language Processing as two separate sub-fields, is the current standard, as it allows large pretrained models to be used in multiple tasks after acquiring a certain understanding of features in a large dataset.</p><p>1) Transfer Learning in Language: BERT <ref type="bibr" target="#b4">[5]</ref> representations and Fine-Tuning, allowed for state-of-the-art results in tasks such as Text Classification, following different approaches mentioned in <ref type="bibr" target="#b20">[21]</ref>, commonly using the [CLS] representations from certain layers as an input to a classifier, freezing the BERT model or finetuning it depending on the task and size of task dataset. The current approaches mentioned in <ref type="bibr" target="#b20">[21]</ref> achieved very low error rates. T ransf erLearning in this case, is mostly learning contextualized representations for the input text, in a self-supervised manner, where the generated embeddings can be directly used as an input to another model to solve a target task.</p><p>2) Transfer Learning in Vision: Vision Models, more specifically <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b19">[20]</ref> [9] <ref type="bibr" target="#b25">[26]</ref> are pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>. Considered benchmarks for image classification, they achieved very high accuracies for smaller datasets through T ransf erLearning and DomainAdaptation <ref type="bibr" target="#b2">[3]</ref>. Being pretrained on a very large dataset (ImageNet <ref type="bibr" target="#b3">[4]</ref>), the features they can extract are useful in Machine Learning tasks on different datasets. Finetuning pretrained features on a target task often gives optimal results as discussed by Yosinski et al. <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Text Classification using Convolutional Neural Networks</head><p>The work conducted by Yoon Kim <ref type="bibr" target="#b11">[12]</ref> suggested a CNN based architecture with multiple filter widths and feature maps, and a fully connected layer, using padding to produce input vectors of a fixed length. A similar approach is taken in <ref type="bibr" target="#b29">[30]</ref>, where sentence classification is achieved, using W ord2V ec <ref type="bibr" target="#b16">[17]</ref> embeddings, 3 filter region sizes, with 2 filters for each region size, which generates 6 variable size feature maps, forming a feature vector after concatenation, then fed into a Softmax layer. The mentioned approach, treats text embeddings as fixed length inputs, with no regard to which order the tokens in the sentence appear, and no consideration of the geometrical abilities and nature of kernels in a Convolutional Neural Network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. P A. IMDB Dataset</head><p>IMDB <ref type="bibr" target="#b15">[16]</ref> is a Polarity Dataset for Sentiment Analysis or Text Classification in broader terms, it contains 50000 Sentences and their binary class labels, being either "Positive" or "Negative", IMDB is a relatively small dataset that provides a level of flexibility and suitable testing for the study this paper is concerned with, due to the computational resources both Data Generation and Training of different models require.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. BERT</head><p>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding <ref type="bibr" target="#b4">[5]</ref>, is a language representation model with the Transformer <ref type="bibr" target="#b23">[24]</ref> as its building block, pretrained on very large unlabelled textual data for two main tasks: Masked Language Modelling, where the model is required to predict words intentionally masked in a multi-layered context. The Second Task is Next Sentence Prediction, also in a self-supervised manner, BERT is trained to classify a sentence as "Next" or "Not Next" for a previous sentence as input. BERT pre-training takes the possible relationships between two sentences into consideration, especially when being trained on the entire Wikipedia English Corpus and the Books-Corpus <ref type="bibr" target="#b30">[31]</ref>. The Attention mechanism employed in all layers of BERT produces semantically rich, and context-sensitive representations, where a token might have many representations depending on the context it is being used in. For the sake of this study, A pretrained BERT model provided by HuggingFace <ref type="bibr" target="#b24">[25]</ref> is used, containing twelve layers, 768 hidden-size (Each Layer produces a 768 sized vector for each word), 12 attention heads, and 110M parameters. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DeepInsight and t-SNE</head><p>Sharma et al in <ref type="bibr" target="#b17">[18]</ref>, proposed a methodology to transform non-image data to an image, using t-SNE <ref type="bibr" target="#b22">[23]</ref> or K-PCA <ref type="bibr" target="#b5">[6]</ref> to project the transpose of a dataset creating a set of features related on a 2D plane according to their similarity, re-transposing the set to obtain the original set size with [NxNx3] images as new samples. The method was heavily used in Cancer Detection and related medical applications. t-SNE <ref type="bibr" target="#b22">[23]</ref> (t-distributed Stochastic Neighbor Embedding), is a similarity measuring and dimensionality reduction technique, developed in 2008 by Geoffrey Hinton and Laurens Van Der Maaten. What separates t-SNE <ref type="bibr" target="#b22">[23]</ref> from classical dimensionality reduction techniques, is that it is a non-linear method, meaning that datasets with a very large number of dimensions can be easily viewed or projected on a 2D or 3D dimensional space, even when features are not related linearly. The mapping from the high dimensional space to the lower dimensional space (2D or 3D), happens according to the similarity between features and data points, where samples with similar features are clustered together. The similarity between two points is computed as probabilities based on Euclidean Distances between pairs of data points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. M</head><p>In this section, the steps taken to generate the dataset used in our experimental part are explained. Going through obtaining BERT-embeddings from the pretrained BERT <ref type="bibr" target="#b4">[5]</ref> model for the original IMDB dataset <ref type="bibr" target="#b15">[16]</ref>, transforming the embeddings into images, and visualization of some obtained IMDB-Image Dataset samples, and normalizing the pixel space. Along with an analysis of Source and T arget domains for the T ransf erLearning conducted in this paper, and the architectures trained on the generated IMDB-Image dataset, providing sample feature maps produced by the pretrained layers from <ref type="bibr" target="#b8">[9]</ref> [15] <ref type="bibr" target="#b19">[20]</ref>.</p><p>A. Generating the IMDB-Image Dataset 1) BERT Embeddings Generation: A very special feature of BERT is the [CLS] token that is added at the beginning of a sentence embedding at each layer output, which indicates the beginning of a sentence and is also a unique Sentence Representation for classification purposes. Since our procedure requires a high Dimensional Space because we attempt to obtain images after a t-SNE <ref type="bibr" target="#b22">[23]</ref> projection of different features representing each input from IMDB <ref type="bibr" target="#b15">[16]</ref>, as it will be further discussed in following sections. The study conducted in <ref type="bibr" target="#b10">[11]</ref>, demonstrates the semantic nature of the output from the higher layers of BERT, thus, the [CLS] embeddings from the last six layers are concatenated for each input sentence from the IMDB-Dataset <ref type="bibr" target="#b15">[16]</ref>, giving a [6x768] sized vector for each input sample as <ref type="figure" target="#fig_0">Fig.1</ref> depicts. 2) Transforming BERT Embeddings into Images: After stacking 6 vectors with 768 features for each sample in the original IMDB <ref type="bibr" target="#b15">[16]</ref> dataset, our dataset is now of shape [50000, 4608]. Following the pipeline introduced in <ref type="bibr" target="#b17">[18]</ref>, We have n = 50000 samples, with d = 4608 features for each sample, thus our dataset can be defined as</p><formula xml:id="formula_0">D = {x 1 , x 2 , ...., x n }, where each feature vector x is defined as x = {f 1 , f 2 , ..., f d }, our feature set is then defined as F = {x 1 , x 2 , ..., x n },</formula><p>where each feature f is a row with n elements representing the number of samples containing that feature. In short terms F = D T , and transposing the instance-feature matrix allows for features to be treated as elements that can be projected on a 2D plane according to similarities between samples in the dataset. The obtained 2D plane demonstrated in <ref type="figure">Fig.2</ref>, represents the location of features according to t-SNE projection of the feature-instance matrix. A Convex Hull algorithm is used to isolate the rectangle containing all the points as depicted in <ref type="figure">Fig.2</ref>. The rectangle is then rotated to obtain a horizontal matrix containing Cartesian coordinates for the pixels.</p><p>Since the pixel space is limited by a fixed size, the points representing feature locations can have more than 1 feature per location due to the continuous nature of feature values. Therefore, during mapping features to their locations, averaging is required <ref type="figure">Fig. 2</ref>. Feature locations represented by blue points on a 2D plane, as described above. The green rectangle represents the smallest rectangle containing all points and is obtained using a Convex Hull algorithm. We can observe a size of [50x50] is obtained for the rectangle which is then rotated to obtain a horizontal image, ready for use in a Convolutional Neural Network.</p><p>to generate a discrete space of pixels. Each feature is then mapped to its location, averaging features which fall on the same point or location on the 2D plane. Respecting our hardware capacity and to avoid excessive overlapping of features on the same location, a size of [50x50] for our pixel space is chosen as a configurable parameter in t-SNE. <ref type="figure">Fig. 3</ref>. A Density Matrix, after rotating the Convex containing all the points representing locations projected on a 2D plane with respect to their Cartesian Coordinates obtained from t-SNE <ref type="bibr" target="#b22">[23]</ref>. Showing regions that are more dense, due to features being mapped to the same locations causing overlapping. We can observe that our new IMDB-Image Dataset has a certain geometrical distribution of features, with clear edges and blobs. <ref type="figure">Fig.3</ref> is an experimental result of applying t-SNE <ref type="bibr" target="#b22">[23]</ref> to our dataset D. The Density Matrix reflects the actual distribution of features in the new IMDB-Image Dataset. The next step is to map feature values to their Cartesian coordinates for each element in D, and averaging feature values that share the same coordinates.</p><p>3) Image Data Visualization: After running the method with a t-SNE <ref type="bibr" target="#b22">[23]</ref> backbone on our dataset D, we obtain gray-scale images with height=50, width=50 and 3 channels having the same values for each pixel, most probably caused by the unnatural data-source which is the pretrained BERT model. <ref type="figure">Fig.4</ref> demonstrates the Image-Embeddings for the first three IMDB input-samples. Text Samples for Images in <ref type="figure">Fig.4</ref>:</p><p>? "One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. The..." ? "A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming-very old-time-B..." ? "I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air con..." <ref type="figure">Fig. 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Domains and Transfer Learning</head><p>Our generated IMDB-Image dataset and the original dataset (ImageNet <ref type="bibr" target="#b3">[4]</ref>) on which the models used in this work are trained, are extremely different as depicted in <ref type="figure" target="#fig_1">Fig.5</ref>, which according to <ref type="bibr" target="#b2">[3]</ref> causes a distribution mismatch and domain shift problems to the classifiers. Generalization across domains is extremely affected by the nature of domains and the style of the data especially in Visual Understanding related tasks.</p><p>According to <ref type="bibr" target="#b31">[32]</ref> a domain D is composed of a feature space ? and a marginal distribution P (X) formulated as:</p><formula xml:id="formula_1">D = ?, P (X)<label>(1)</label></formula><p>where X is an instance set which is defined as:</p><formula xml:id="formula_2">X = x|xi ? ?, i = 1, ...., n</formula><p>The Task T is composed of a label space Y and a decision function f , meaning:</p><formula xml:id="formula_3">T = (Y, f )<label>(2)</label></formula><p>where f is learned explicitly via training data samples. 1) Data Domains: <ref type="figure" target="#fig_1">Fig.5</ref> shows obvious differences in data style in the source and target datasets, which according to <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b18">[19]</ref> and further discussed in <ref type="bibr" target="#b28">[29]</ref> causes a distribution mismatch and domain shifts, due to the differences between the two domains like channels, colors, background, lighting, etc. This is a major problem in Transfer Learning between image datasets. Survey <ref type="bibr" target="#b2">[3]</ref> gives a broad overview of the recent approaches and methods developed for Transfer learning to overcome the mentioned issues through Domain Adaptation. 2) Transfer Learning: For successful Transfer Learning to be achieved, an architecture should be able to adapt the Target Domain D T to Source Domain D S , which are IMDB-images and ImageNet <ref type="bibr" target="#b3">[4]</ref> respectively in our case. One special technique for domain adaptation so far proposes specific training for domain prediction <ref type="bibr" target="#b6">[7]</ref>. Limited by compute power and extreme domain shifts, in this work, another approach is taken. In <ref type="bibr" target="#b26">[27]</ref>, Jason Yosinski et al, stress on the different kinds of features different zones of layers learn in a neural network, stressing on the generality observed in lower layers and specificity in higher layers, meaning general features like curves, color blobs, and edges are extracted in the lower layers, and task specific features are handled by higher layers. Threatened by the large model sizes in our pretrained arsenal <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b19">[20]</ref> [9] <ref type="bibr" target="#b25">[26]</ref>, and the relatively small dataset with only 50000 samples, which could lead to extreme overfitting, the approach taken in this work is to focus on features that both datasets share instead of forcing the model to learn the domains themselves as targets. Since the color channels are clearly going to cause a major domain shift, the focus should be on geometrical features like edges, curves and blobs. In order to have more defined edges, normalization of the entire pixel space is applied, a normalization technique named Z ? N ormalization <ref type="bibr" target="#b7">[8]</ref>, adjusting image contrast after moving our input images to a clearer pixel space:</p><formula xml:id="formula_4">? = E x ? X [x], ? 2 = E x ? X [(x ? ?) 2 ], x i = x i ? ? ? + (3) X = [x 1 , x 2 , ..</formula><p>., x n ] is a set of input vectors, ?, ? are the mean and standard deviation of the entire image pixel space, is a small value to prevent dividing by zero or small denominators.</p><p>As <ref type="figure">Fig.6</ref> shows, the mentioned normalization technique prevails in creating more distinguishable feature zones in one image, due to the new mean which is close to 0.0 and standard deviation nearing 1.0, which imposes a standard normal distribution on the features, removing noisy regions and enhancing outlying pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Architectures used</head><p>Since our IMDB-Image dataset is very small compared to the source ImageNet dataset <ref type="bibr" target="#b3">[4]</ref>, the Convolutional Feature Extractors are sliced from their original pretrained models, and stacked to a Convolutional Auto-Encoder with randomly initialized parameters, followed by a Dense (Linear) Classifier. A common approach <ref type="figure">Fig. 6</ref>. The image to the left, represents a raw image from the generated IMDB-Images dataset. The image to the right, is the same as the one to its left after Z ? N ormalization, showing stronger edges and more defined geometrical shapes like blobs.</p><p>would be to freeze the pretrained feature extractors, which was followed in this paper to avoid any overfitting problems. We also note that the Convolutional Auto-Encoder was designed to keep the same activation flow based on the pretrained feature extractors, and thus keeps a similar choice of activation functions and initialization. As it is depicted in <ref type="figure" target="#fig_2">Fig.7</ref>, the most important part of the architecture is the pretrained feature extractor. In this paper, for the sake of comparison and confirmation, early layers from five pretrained models were used as feature extractors, followed by the exact same Conv-AE (Convolutional AutoEncoder) and Dense classifier to ensure fairness in comparison. The detailed architectures of the pretrained vision models are outside the scope of this paper.</p><p>1) pretrained Feature Extractors: <ref type="bibr" target="#b1">2</ref> The following list represents the number of layers sliced from the pretrained vision models. We note this choice has been immensely affected by the available hardware, and also by repetitive visual assessment of the feature-maps outputted by several combination of layers for each one.</p><p>? AlexNet: introduced in <ref type="bibr" target="#b13">[14]</ref>, Using the first two pretrained Convolutional Layers, outputs 192 feature maps for each input image from the IMDB-image dataset, with fairly distinguishable differences and focus. ? ResNet: A deep residual model from <ref type="bibr" target="#b8">[9]</ref>, known as wide-resnet50-2. We used the first downsampling Convolutional layer and the first residual layer. ? ResNext: <ref type="bibr" target="#b25">[26]</ref>, proposes an aggregated version of the previous ResNet. For the feature extractor we need in this experiment, the first Convolutional layer, and the first Residual layer are used as well.</p><p>2 All pretrained models can be found at: https://pytorch.org/vision/stable/ models.html ? ShuffleNet V2: From <ref type="bibr" target="#b14">[15]</ref> the first Convolutional layer followed by Batch normalization, and stage2 mentioned in the paper. ? VGG16: introduced in <ref type="bibr" target="#b19">[20]</ref>, We only use the first 12 layers, containing 4 Convolutional layers for the feature extractor in this experiment. <ref type="figure" target="#fig_3">Fig.8</ref> shows sample Feature maps from the feature extractors from: resNet <ref type="bibr" target="#b8">[9]</ref>,. ShuffleNet <ref type="bibr" target="#b14">[15]</ref>, Vgg16 <ref type="bibr" target="#b19">[20]</ref>.  <ref type="bibr" target="#b3">[4]</ref> dataset. We can observe the adjusted edges and blobs using Z ? N ormalization being successfully detected by all pretrained feature extractors, clearest in the depicted three.</p><p>In <ref type="figure" target="#fig_3">Fig.8</ref> we can observe that the pretrained models are in fact able to extract global features from the new dataset, edges and curves are distinguished by all the pretrained feature extractors, but clearest in the depicted three. Yielding that <ref type="bibr" target="#b26">[27]</ref> stated a concrete study of the transfer-ability of pretrained models to other datasets and tasks, requiring a considerate treatment of the nature of the features learned by different layers, and the different natures and domains of the source and target datasets.</p><p>V. E This section discusses the training procedure for the mentioned architecture containing five different pretrained feature extractors from the Vision Models <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b19">[20]</ref> [9] <ref type="bibr" target="#b25">[26]</ref>, pretrained on ImageNet <ref type="bibr" target="#b3">[4]</ref>, and the experimental results achieved by each one on the generated IMDB-Image dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training</head><p>As shown in <ref type="figure" target="#fig_2">Fig.7</ref>, our five models share a Convolutional AutoEncoder, and three Linear Layers, and differ in the pre-trained feature extractors obtained from 5 different pretrained vision models <ref type="bibr" target="#b13">[14]</ref> [15] <ref type="bibr" target="#b19">[20]</ref> [9] <ref type="bibr" target="#b25">[26]</ref>. Given the differences in data domains depicted in <ref type="figure" target="#fig_1">Fig.5</ref>, avoiding any possible dataset and covariate shifts is necessary. Using ReLU to keep the same activation patterns within layers of our architecture, given that all 5 pretrained feature extractors contain ReLU activations. Batch Normalization <ref type="bibr" target="#b9">[10]</ref> is applied after each convolutional layer in our Convolutional AutoEncoder block, since our pretrained feature extractors are trained with 1000 classes on the label side, and our classification task only has 2 classes, this causes an Internal Covariate Shift, representing the change in the distributions of internal nodes of our network as mentioned in <ref type="bibr" target="#b9">[10]</ref>. Adam Optimizer <ref type="bibr" target="#b12">[13]</ref>, is used for all 5 models, to ensure fairness in comparisons, and a fixed batch size of 32. Our IMDB-Image dataset contains 50000 images samples, we split the dataset into a 40000 train and a 10000 validation samples. Differential learning rates are used for our Adam <ref type="bibr" target="#b12">[13]</ref> optimizer, while keeping the pretrained feature extractors frozen during training, since our dataset is very small compared to the Source Dataset (imageNet <ref type="bibr" target="#b3">[4]</ref>). An NVIDIA GEFORCE GTX 1060 GPU was used for the entire procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Results</head><p>As mentioned in the previous subsection, the different models were trained, with different learning rates, but yet reached very close Validation Accuracies. Higher learning rates caused all models to stagnate and converge very fast to local optimal points, which prevented the models to further learn features necessary to distinguish between different variations contained in each image representing the text samples in the original IMDB-dataset <ref type="bibr" target="#b15">[16]</ref>.</p><p>The following table depicts the number of feature maps outputted by each Feature Extractor along with different Learning Rates used for each one, and the corresponding achieved Validation Accuracies. <ref type="figure">Fig.9</ref> shows the progress of the validation performance of the five models during training. TABLE 1 and <ref type="figure">Fig.9</ref> both emphasize the very close results obtained from training all five models on the same IMDB-Image dataset. Freezing the pretrained Feature Extractors allows for the complexity of the model as a whole for the five variations to drop, avoiding any chance of overfitting due to the gray scale nature of our IMDB-Image dataset and the RGB channel space of the Source Dataset (ImageNet <ref type="bibr" target="#b3">[4]</ref>). The experimental analysis AlexNet-based ResNet-based ResNext-based ShuffleNet-based ShuffleNet-based <ref type="figure">Fig. 9</ref>. Validation Accuracies VS. Epochs for the five models conducted in this paper, is explicitly dedicated to the results obtained by our method and architectures. Given that the pretrained Feature Extractors, followed by the exact same Convolutional AutoEncoder and Dense Classifier, are trained on the same generated IMDB-Image dataset, and observing the experimental results obtained in TABLE 1 and <ref type="figure">Fig.9</ref>, suggests the following:</p><p>? The normalized IMDB-Image Dataset, is still not fully avoiding domain shifts due to its raw gray-scale nature. ? The different number of features outputted by each feature extractor, and the almost identical Validation Accuracies, suggests that some feature maps are duplicated, also due to the gray scale nature of the IMDB-Image dataset. ? We can clearly see in <ref type="figure">Fig.9</ref> that the five models learned at different rates, yet reached close Validation results, this can imply that the models do indeed vary in complexity and generalization abilities, yet limited by dataset size. ? The general features learned from the early layers used as feature extractors are extremely similar as suggested in <ref type="bibr" target="#b26">[27]</ref>, yielding similar results even with different learning rates, since the feature maps play the role of fixed representations for the same dataset. ? Although the Validation Results accomplished do not rise to the State-Of-The-Art in Text Classification, they are still promising given the limited data size used..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. D C A</head><p>The code for:</p><p>? Used IMDB dataset can be found at IMDB. ? Code for generating IMDB-Image Dataset.</p><p>? BERT Embedding, Data Transformation and loading.</p><p>? Different architectures and training scripts using PyTorch.</p><p>? Reproducible paradigm with commented and explained steps. Are available and can be accessed at https://github.com/ EddCBen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. C</head><p>In this paper a new approach to Text Classification via Supervised Learning is suggested, using Transfer Learning of pretrained Vision Models on an Image Dataset (ImageNet <ref type="bibr" target="#b3">[4]</ref>), to a textual polarity Dataset (IMDB <ref type="bibr" target="#b15">[16]</ref>) embedded using a pretrained BERT <ref type="bibr" target="#b4">[5]</ref> model, where text embeddings are transformed into images using t-SNE <ref type="bibr" target="#b22">[23]</ref> feature similarity measuring (Inspired from the work done in DeepInsight <ref type="bibr" target="#b17">[18]</ref>), and pretrained Vision models, are used as feature extractors for a much smaller Neural Classifier to learn sentiment labels. The contributions of our work, are mainly the generation of a new dataset representing textual data from the IMDB <ref type="bibr" target="#b15">[16]</ref>, avoiding possible domain shifts with pixel normalization, and achieving text classification using pretrained layers for vision tasks. Future work aims to further normalize the discussed approach, as it gives promising results for a small dataset, opening a new challenge for the fusion of Language and Vision via Transfer Learning and Data transformation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>IMDB [CLS] Embeddings from the Last Six layers of BERT, where the input to the pretrained model is indexes representing each word in a sentence, outputting a fixed [768] sized vector from each layer. The outputs of [CLS] tokens from the last six layers are stacked into a [6X768] sized embedding for each text sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 .</head><label>5</label><figDesc>(a): An image sample from the generated IMDB-Images dataset, showing a gray scale distribution with weak edges and no obvious geometrical features or shapes, (b): An image of a Dog from the Source Dataset (ImageNet [4]), an RGB image with clear geometrical features and shapes. This figure depicts the differences between the Source and T arget datasets, and suggests inevitable domain shifts between the two.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 7 .</head><label>7</label><figDesc>Main Architecture Used, the pretrained block represents early layers from five pretrained Vision Models [14] [15] [20] [9] [26], outputting the input to a Convolutional Auto-encoder, stacked to a Dense Classifier (3 Linear fully connected layers)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 8 .</head><label>8</label><figDesc>Feature Maps from early layers of (a) : resNet, (b) : shuffleNet, (c) : VGG16. The pretrained feature extractors used in our common architecture do in fact output feature maps with defined geometrical features extracted due to the pre-training on the ImageNet</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">more information on the BERT model used is at: https://huggingface. co/transformers/pretrained_models.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The influence of pattern similarity and transfer learning upon the training of a base perceptron b2.(original in croatian)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bozinovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fulgosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium Informatica</title>
		<meeting>the Symposium Informatica</meeting>
		<imprint>
			<biblScope unit="page" from="3" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reminder of the first paper on transfer learning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stevo</forename><surname>Bozinovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Informatica</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2020</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Adaptive kernel principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1542" to="1553" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain-adversarial training of neural networks. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On similarity queries for time-series data: constraint specification and implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanellakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Principles and Practice of Constraint Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>corr abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">What does bert learn about the structure of language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djam?</forename><surname>Seddah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5882</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One weird trick for parallelizing convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepinsight: A methodology to transform a nonimage data to an image for convolution neural network architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edwin</forename><surname>Vans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daichi</forename><surname>Shigemizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuhiko</forename><surname>Boroevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsunoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How to finetune bert for text classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yige</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">China National Conference on Chinese Computational Linguistics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="194" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1521" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1792</idno>
		<title level="m">How transferable are features in deep neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey of modern deep learning based object detection models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Syed Sahil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asra</forename><surname>Samar Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><surname>Aslam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mamoona</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11892</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transfer adaptation learning: A decade survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.04687</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A sensitivity analysis of (and practitioners&apos; guide to) convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03820</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A comprehensive survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongbo</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="76" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
