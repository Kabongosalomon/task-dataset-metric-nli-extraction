<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNDERSTANDING OVER-SQUASHING AND BOTTLENECKS ON GRAPHS VIA CURVATURE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Topping</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">Di</forename><surname>Giovanni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">P</forename><surname>Chamberlain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Imperial College</orgName>
								<address>
									<settlement>London 3 Twitter</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNDERSTANDING OVER-SQUASHING AND BOTTLENECKS ON GRAPHS VIA CURVATURE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most graph neural networks (GNNs) use the message passing paradigm, in which node features are propagated on the input graph. Recent works pointed to the distortion of information flowing from distant nodes as a factor limiting the efficiency of message passing for tasks relying on long-distance interactions. This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of k-hop neighbors grows rapidly with k. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph. For this purpose, we introduce a new edge-based combinatorial curvature and prove that negatively curved edges are responsible for the over-squashing issue. We also propose and experimentally test a curvature-based graph rewiring method to alleviate the over-squashing. ? Equal contribution</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head> <ref type="figure">Figure 1</ref><p>: Top: evolution of curvature on a surface may reduce the bottleneck. Bottom: this paper shows how the same may be done on graphs to improve GNN performance. Blue/red shows negative/positive curvature.</p><p>In the past few years, deep learning on graphs and in particular graph neural networks (GNNs) <ref type="bibr" target="#b50">(Sperduti, 1994;</ref><ref type="bibr" target="#b16">Goller &amp; Kuchler, 1996;</ref><ref type="bibr" target="#b51">Sperduti &amp; Starita, 1997;</ref><ref type="bibr" target="#b13">Frasconi et al., 1998;</ref><ref type="bibr" target="#b17">Gori et al., 2005;</ref><ref type="bibr" target="#b46">Scarselli et al., 2008;</ref><ref type="bibr" target="#b5">Bruna et al., 2014;</ref><ref type="bibr" target="#b11">Defferrard et al., 2016;</ref><ref type="bibr" target="#b23">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b15">Gilmer et al., 2017)</ref> have become very popular in the machine learning community due to their ability to deal with broad classes of systems of relations and interactions, ranging from social networks to particle physics <ref type="bibr" target="#b49">(Shlomi et al., 2021)</ref>.</p><p>The vast majority of GNNs follow the message passing paradigm <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>, using learnable non-linear functions to diffuse information on the graph. Multiple popular GNN architectures such as <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref> and GAT <ref type="bibr" target="#b54">(Veli?kovi? et al., 2018)</ref> can be posed as particular flavors of this scheme and considered instances of a more general framework of geometric deep learning <ref type="bibr" target="#b4">(Bronstein et al., 2021)</ref>.</p><p>Some of the drawbacks of the message passing paradigm have now been identified and formalized, including the limits of expressive power <ref type="bibr">(Xu et al., 2019;</ref><ref type="bibr" target="#b30">Morris et al., 2019;</ref><ref type="bibr" target="#b27">Maron et al., 2019)</ref> and the problem of over-smoothing <ref type="bibr" target="#b36">(NT &amp; Maehara, 2019;</ref><ref type="bibr" target="#b39">Oono &amp; Suzuki, 2020)</ref>. On the other hand, much less is known about the phenomenon of over-squashing, consisting in the distortion of messages being propagated from distant nodes. <ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref> proposed rewiring the graph as a way of reducing the bottleneck, defined as those topological properties in the graph leading to over-squashing. This approach is in line with multiple other results e.g. using connectivity diffusion <ref type="bibr" target="#b24">(Klicpera et al., 2019)</ref> as a preprocessing step to facilitate graph learning. Yet, the exact understanding of the over-squashing and how it originates from the bottlenecks in the topology of the underlying Contributions and Outline. This paper, to our knowledge, is the first theoretical study of the bottleneck and over-squashing phenomena in message passing neural networks from a geometric perspective. In Section 2, we propose the Jacobian of node representations as a formal way of measuring the over-squashing and we show that the graph topology may compromise message propagation in graph neural networks by creating a bottleneck. In Section 3, we investigate how such a bottleneck is induced which leads to the over-squashing of information. To this aim, we introduce a new combinatorial edge-based curvature called Balanced Forman curvature that constitutes a sharp lower bound to the standard Ollivier curvature on graphs, and prove that negatively curved edges are responsible for the formation of bottlenecks (and hence for over-squashing). In Section 4, we present a new curvature-based method for graph rewiring called Stochastic Discrete Ricci Flow. According to the theoretical results in Section 3, this rewiring method is suited to address the graph bottleneck and hence alleviate the over-squashing by surgically targeting the edges responsible for the issue. By contrast, we rigorously show that a recently introduced diffusion-based rewiring scheme might generally fail to reduce the bottleneck. Finally, in Section 5, we compare different rewiring strategies experimentally on several standard graph learning datasets.</p><p>2 ANALYSIS OF THE OVER-SQUASHING PHENOMENON 2.1 PRELIMINARIES Let G = (V, E) be a simple, undirected, and connected graph, where (i, j) ? E iff i ? j. We focus on the unweighted case, although the theory extends to the weighted setting as well. We denote the adjacency matrix by A and let? = A + I be the adjacency matrix augmented with self-loops. Similarly we letD = D + I, with D the diagonal degree matrix, and let? =D ? 1 2?D ? 1 2 be the normalized augmented adjacency matrix (self-loops are commonly included in GNN architecture, and in Section 2 we formally explain why GNNs are expected to propagate information more reliably when self-loops are taken into account). Given i ? V , we denote its degree by d i and let</p><formula xml:id="formula_0">S r (i) := {j ? V : d G (i, j) = r}, B r (i) := {j ? V : d G (i, j) ? r},</formula><p>where d G is the standard shortest-path distance on the graph and r ? N. The set B r (i) represents the receptive field of an r-layer message passing neural network at node i.</p><p>Message passing neural networks <ref type="bibr">(MPNNs)</ref>. Assume that the graph G is equipped with node features X ? R n?p0 where x i ? R p0 is the feature vector at node i = 1, . . . , n = |V |. We denote by h ( ) i ? R p the representation of node i at layer ? 0, with h (0) i = x i . Given a family of message functions ? : R p ? R p ? R p and update functions ? : R p ? R p ? R p +1 , we can write the ( + 1)-st layer output of a generic MPNN as follows <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>:</p><formula xml:id="formula_1">h ( +1) i = ? ? ? h ( ) i , n j=1? ij ? (h ( ) i , h ( ) j ) ? ? .<label>(1)</label></formula><p>Here we have used the augmented normalized adjacency matrix to propagate messages from each node to its neighbors, which simply leads to a degree normalization of the message functions ? . To avoid heavy notations the node features and representations are assumed to be scalar from now on; these assumptions simplify the discussion and the vector case leads to analogous results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">THE OVER-SQUASHING PROBLEM</head><p>Multiple recent papers observed that MPNNs tend to perform poorly in situations when the learned task requires long-range dependencies and at the same time the structure of the graph results in exponentially many long-range neighboring nodes. We say that a graph learning problem has long-range dependencies when the output of a MPNN depends on representations of distant nodes interacting with each other. If long-range dependencies are present, messages coming from nonadjacent nodes need to be propagated across the network without being too distorted. In many cases however (e.g. in 'small-world' graphs such as social networks), the size of the receptive field B r (i) grows exponentially with r. If this occurs, representations of exponentially many neighboring nodes need to be compressed into fixed-size vectors to propagate messages to node i, causing a phenomenon referred to as over-squashing of information <ref type="bibr" target="#b0">(Alon &amp; Yahav, 2021)</ref>. In line with <ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref>, we refer to those structural properties of the graph that lead to over-squashing as a bottleneck 1 .</p><p>Sensitivity analysis. The hidden feature h</p><formula xml:id="formula_2">( ) i = h ( ) i (x 1 , . .</formula><p>. , x n ) computed by an MPNN with layers as in equation 1 is a differentiable function of the input node features {x 1 , . . . , x n } as long as the update and message functions ? and ? are differentiable. The over-squashing of information can then be understood in terms of one node representation h ( ) i failing to be affected by some input feature x s of node s at distance r from node i. Hence, we propose the Jacobian ?h (r) i /?x s as an explicit and formal way of assessing the over-squashing effect 2 . Lemma 1. Assume an MPNN as in equation</p><formula xml:id="formula_3">1. Let i, s ? V with s ? S r+1 (i). If |?? | ? ? and |?? | ? ? for 0 ? ? r, then ?h (r+1) i ?x s ? (??) r+1 (? r+1 ) is .<label>(2)</label></formula><p>Lemma 1 states that if ? and ? have bounded derivatives, then the propagation of messages is controlled by a suitable power of?. For example, if d G (i, s) = r + 1 and the sub-graph induced on B r+1 (i) is a binary tree, then (? r+1 ) is = 2 ?1 3 ?r , which gives an exponential decay of the node dependence on input features at distance r, as also heuristically argued by <ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref>.</p><p>The sensitivity analysis in Lemma 1 relates the over-squashing -as measured by the Jacobian of the node representations -to the graph topology via powers of the augmented normalized adjacency matrix. In the next section we explore this connection further by analyzing which local properties of the graph structure affect the right hand side in equation 2, hence causing the bottleneck. We will address this problem by introducing a new combinatorial notion of edge-based curvature and showing that negatively curved edges are those responsible for the over-squashing phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GRAPH CURVATURE AND BOTTLENECK</head><p>A natural object in Riemannian geometry is the Ricci curvature, a bilinear form determining the geodesic dispersion, i.e. whether geodesics starting at nearby points with 'same' velocity remain parallel (Euclidean space), converge (spherical space), or diverge (hyperbolic space). To motivate the introduction of a Ricci curvature for graphs, we focus on these three cases. Consider two nodes i ? j and two edges starting at i and j respectively. In a discrete spherical geometry <ref type="figure" target="#fig_0">(Figure 2a</ref>), the edges would meet at k to form a triangle (complete graph). In a discrete Euclidean geometry <ref type="figure" target="#fig_0">(Figure 2b</ref>), the edges would stay parallel and form a 4-cycle based at i ? j (orthogonal grid). Finally, in a discrete hyperbolic geometry <ref type="figure" target="#fig_0">(Figure 2c</ref>), the mutual distance of the edge endpoints would have grown compared to that of i and j (tree). Therefore, a Ricci curvature for graphs should provide us with more sophisticated tools than the degree to analyze the neighborhood of an edge. Curvatures on graphs. The main examples of edge-based curvature are the Forman curvature F (i, j) <ref type="bibr" target="#b12">(Forman, 2003)</ref> and the Ollivier curvature ?(i, j) in <ref type="bibr" target="#b37">Ollivier (2007;</ref><ref type="bibr" target="#b38">2009</ref>) (see Appendix). While F (i, j) is given in terms of combinatorial quantities <ref type="bibr" target="#b52">(Sreejith et al., 2016)</ref>, results are scarce and the definition is biased towards negative curvature. The theory on ?(i, j) instead is richer <ref type="bibr" target="#b25">(Lin et al., 2011;</ref><ref type="bibr" target="#b31">M?nch, 2019)</ref> but its formulation makes it hard to control local quantities.</p><p>Balanced Forman curvature. We propose a new curvature to address the shortcomings of the existing candidates. We use the following definitions to describe the neighborhood of an edge i ? j and we refer to the Appendix for a more complete discussion: (iii) ? max (i, j) is the maximal number of 4?cycles based at i ? j traversing a common node (see Definition 4).</p><formula xml:id="formula_4">(i) ? (i, j) := S 1 (i) ? S 1 (j) are the triangles based at i ? j. (ii) i (i, j) := {k ? S 1 (i) \ S 1 (j), k = j : ?w ? (S 1 (k) ? S 1 (j)) \ S 1 (i)}</formula><p>In line with the discussion about geodesic dispersion, one expects ? to be related to positive curvature (complete graph), i to zero curvature (grid), and the remaining outgoing edges to negative curvature (tree). Our new curvature formulation reflects such an intuition and recovers the expected results in the classical cases. In the example in <ref type="figure">Figure 3</ref> we have 0 (0, 1) = {2, 3} while 1 (0, 1) = {5}, both without 4,6 because of the triangle 1-6-0. The degeneracy factor ? max (0, 1) = 2, as there exist two 4-cycles passing through node 5.</p><p>Definition 1 (Balanced Forman curvature). For any edge i ? j in a simple, unweighted graph G, we let Ric(i, j) be zero if min{d i , d j } = 1 and otherwise</p><formula xml:id="formula_5">Ric(i, j) := 2 d i + 2 d j ? 2 + 2 | ? (i, j)| max{d i , d j } + | ? (i, j)| min{d i , d j } + (? max ) ?1 max{d i , d j } (| i | + | j |), (3)</formula><p>where the last term is set to be zero if | i | (and hence | j |) is zero. In particular Ric(i, j) &gt; ?2.</p><p>The curvature is negative when i ? j behaves as a bridge between S 1 (i) and S 1 (j), while it is positive when S 1 (i) and S 1 (j) stay connected after removing i ? j. We refer to Ric as Balanced Forman curvature. We can relate the Balanced Forman curvature to the Jacobian of hidden features, while also extending many results valid for the Ollivier curvature ?(i, j) thanks to our next theorem.</p><p>Theorem 2. Given an unweighted graph G, for any edge i ? j we have ?(i, j) ? Ric(i, j). We also note that the computational complexity for ? scales as O(|E|d 3 max ), while for our Ric we have O(|E|d 2 max ), with d max the maximal degree. From Theorem 2 and Paeng (2012), we find:</p><formula xml:id="formula_6">Graph G RicG Cycles C3 3 2 C4 1 C n?5 0 Complete Kn n n?1 Grid Gn 0 Tree Tr 4 r+1 ? 2</formula><p>Corollary 3. If Ric(i, j) ? k &gt; 0 for any edge i ? j, then there exists a polynomial P such that</p><formula xml:id="formula_7">|B r (i)| ? P (r), ?i ? V.</formula><p>Therefore, when the curvature is positive everywhere, the bottleneck effect should not play a crucial role as the receptive field of each node will be polynomial in the hop-distance. This limit case shows how the curvature determines whether a learning task on a graph would suffer from over-squashing.</p><p>Curvature and over-squashing. Thanks to our new combinatorial curvature and the sensitivity analysis in Lemma 1, we are able to relate local curvature properties to the Jacobian of the node representations. This leads to one of the main results of this paper: negatively curved edges are those causing the graph bottleneck and thus leading to the over-squashing phenomenon:</p><p>Theorem 4. Consider a MPNN as in equation 1. Let i ? j with d i ? d j and assume that:</p><formula xml:id="formula_8">(i) |?? | ? ? and |?? | ? ? for each 0 ? ? L ? 1, with L ? 2 the depth of the MPNN. (ii) There exists ? s.t. 0 &lt; ? &lt; (max{d i , d j }) ? 1 2 , ? &lt; ? ?1 max , and Ric(i, j) ? ?2 + ?.</formula><p>Then there exists Q j ? S 2 (i) satisfying |Q j | &gt; ? ?1 and for 0 ? 0 ? L ? 2 we have</p><formula xml:id="formula_9">1 |Q j | k?Qj ?h ( 0 +2) k ?h ( 0) i &lt; (??) 2 ? 1 4 .<label>(4)</label></formula><p>Condition (i) is always satisfied and allows us to control the message passing functions. The requirement (ii) instead means that the curvature of (i, j) is negative enough when compared to the degrees of i and j (recall that Ric(i, j) &gt; ?2). The further condition on ? max is to avoid pathological cases where we have a large number of degenerate 4-cycles passing through the same three nodes.</p><p>To understand the conclusions in Theorem 4, let us fix 0 = 0. The equation 4 shows that negatively curved edges are the ones causing bottlenecks, interpreted as how the graph topology prevents a representation h</p><p>(2) k to be affected by non-adjacent features h (0) i = x i . Theorem 4 implies that if we have a negatively curved edge as in (ii), then there exist a large number of nodes k such that GNNson average -struggle to propagate messages from i to k in two layers despite these nodes k being at distance 2 from i. In this case the over-squashing occurs as measured by the Jacobian in equation 4 and hence the propagation of information suffers. If the task at hand has long-range dependencies, then the over-squashing caused by the negatively curved edges may compromise the performance.</p><p>Bottleneck via Cheeger constant. We now relate the previous discussion about bottlenecks and curvature to spectral properties of the graph. In particular, since the spectral gap of a graph can be interpreted as a topological obstruction to the graph being partitioned into two communities, we argue below that this quantity is related to the graph bottleneck and should hence be controllable by the curvature. We start with an intuitive explanation: suppose we are given a graph G with two communities separated by few edges. In this case, we see that the graph can be easily disconnected. This property is encoded in the classical notion of the Cheeger constant <ref type="bibr" target="#b9">(Chung &amp; Graham, 1997)</ref> </p><formula xml:id="formula_10">h G := min S?V h S , h S := min S?V |?S| min{vol(S), vol(V \ S)} (5) where ?S = {(i, j) : i ? S, j ? V \ S} and vol(S) = i?S d i .</formula><p>The main result about the Cheeger constant is the Cheeger inequality <ref type="bibr" target="#b7">(Cheeger, 2015;</ref><ref type="bibr" target="#b9">Chung &amp; Graham, 1997)</ref>:</p><formula xml:id="formula_11">2h G ? ? 1 ? h 2 G 2<label>(6)</label></formula><p>where ? 1 is the first non-zero eigenvalue of the normalized graph Laplacian, often referred to as the spectral gap. A graph with two tightly connected communities (S and V \ S :=S) and few inter-community edges has a small Cheeger constant h G . For nodes in different communities to interact with each other, all messages need to go through the same few bridges hence leading to the over-squashing of information (a similar intuition was explored in <ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref>). Therefore, h G can be interpreted as a rough measure of graph 'bottleneckedness', in the sense that the smaller its value, the more likely the over-squashing is to occur across inter-community edges. Since Theorem 4 implies that negatively curved edges induce the bottleneck, we expect a relationship between h G and the curvature of the graph. The next proposition follows from Theorem 2 and <ref type="bibr" target="#b25">Lin et al. (2011)</ref>:</p><p>Proposition 5. If Ric(i, j) ? k &gt; 0 for all i ? j, then ? 1 /2 ? h G ? k 2 . Therefore, a positive lower bound on the curvature gives us a control on h G and hence on the spectral gap of the graph. In the next section, we show that diffusion-based graph-rewiring methods might fail to significantly alter h G and hence correct the graph bottleneck potentially induced by inter-community edges. This will lead us to propose an alternative curvature-based graph rewiring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CURVATURE-BASED REWIRING METHODS</head><p>The traditional paradigm of message passing graph neural networks assumes that messages are propagated on the input graph <ref type="bibr" target="#b15">(Gilmer et al., 2017)</ref>. More recently, there is a trend to decouple the input graph from the graph used for information propagation. This can take the form of graph subsampling or resampling to deal with scalability <ref type="bibr" target="#b19">(Hamilton et al., 2017)</ref> or topological noise <ref type="bibr">(Zhang et al., 2019)</ref>, using larger motif-based <ref type="bibr" target="#b29">(Monti et al., 2018)</ref> or multi-hop filters <ref type="bibr" target="#b43">(Rossi et al., 2020)</ref>, or changing the graph either as a preprocessing step <ref type="bibr" target="#b24">(Klicpera et al., 2019;</ref><ref type="bibr" target="#b0">Alon &amp; Yahav, 2021)</ref> or adaptively for the downstream task <ref type="bibr" target="#b55">(Wang et al., 2019;</ref><ref type="bibr" target="#b21">Kazi et al., 2020)</ref>. Such methods are often generically referred to as graph rewiring.</p><p>In the context of this paper, we assume that graph rewiring attempts to produce a new graph G = (V, E ) with a different edge structure that reduces the bottleneck and hence potentially alleviates the over-squashing of information. We propose a method that leverages the graph curvature to guide the rewiring steps in a surgical way by modifying the negatively-curved edges, so to decrease the bottleneck without significantly compromising the statistical properties of the input graph. We also rigorously show that a random-walk based rewiring method might generally fail to obtain an edge set E with a significant improvement in its bottleneckedness as measured by the Cheeger constant.</p><p>Curvature-based graph rewiring. Since according to Theorem 4 negatively curved edges induce a bottleneck and are hence responsible for over-squashing, a curvature-based rewiring method should attempt to alleviate a graph's strongly-negatively curved edges. To this end we implement a simple rewiring method called Stochastic Discrete Ricci Flow (SDRF), described in Algorithm 1.</p><formula xml:id="formula_12">Algorithm 1: Stochastic Discrete Ricci Flow (SDRF) Input: graph G, temperature ? &gt; 0, max number of iterations, optional Ric upper-bound C + Repeat 1) For edge i ? j with minimal Ricci curvature Ric(i, j): Calculate vector x where x kl = Ric kl (i, j) ? Ric(i, j), the improvement to Ric(i, j) from adding edge k ? l where k ? B 1 (i), l ? B 1 (j);</formula><p>Sample index k, l with probability softmax(? x) kl and add edge k ? l to G. 2) Remove edge i ? j with maximal Ricci curvature Ric(i, j) if Ric(i, j) &gt; C + . Until convergence, or max iterations reached;</p><p>At each iteration this preprocessing step adds an edge to 'support' the graph's most negatively curved edge, and then removes the most positively curved edge. The requirement on the added edge k ? l that k ? B 1 (i) and l ? B 1 (j) ensures that we're adding either an extra 3-or 4-cycle around the negative edge i ? j so that this is a local modification. The graph edit distance between the original and preprocessed graph is bounded above by 2 ? the max number of iterations. The temperature ? determines how stochastic the edge addition is, with ? = ? being fully deterministic (the best edge is always added). At each step we remove the edge with most positive curvature to balance the distributions of curvature and node degrees. We use Balanced Forman curvature as in equation 3 for Ric(i, j). C + can be chosen to stop the method skewing the curvature distribution negative, including C + = ? to not remove any edges. The method is inspired by the continuous (backwards) Ricci flow with the aim of homogenizing edge curvatures. This is different from more direct extensions of Ricci flow on graphs where it becomes increasingly expensive to propagate messages across negatively curved edges (as in other applications such as Ni et al. <ref type="formula" target="#formula_1">(2019)</ref>). An example alongside its continuous analogue can be seen in <ref type="figure">Figure 1</ref>.</p><p>Can random-walk based rewiring address bottlenecks? A good way of understanding the effectiveness of SDRF in reducing the graph bottleneck is through comparison with random-walk based rewiring strategies. Recall that, as argued in Section 3, the Cheeger constant h G of a graph constitutes a rough measure of its bottleneckedness as induced by the inter-community edges (a small h G is indicative of a bottleneck). Suppose we are given a graph G with a small h G and wish to rewire it into a graph G with a significantly improved Cheeger constant in order to reduce the inter-community bottleneck. A random-walk based rewiring method such as DIGL <ref type="bibr" target="#b24">(Klicpera et al., 2019)</ref> acts by smoothing out the graph adjacency and hence tends to promote connections among nodes at short diffusion distance <ref type="bibr" target="#b10">(Coifman &amp; Lafon, 2006)</ref>. Accordingly, such a rewiring method might fail to correct structural features like the bottleneck, which is instead more prominent for nodes that are at long diffusion distance. <ref type="bibr">3</ref> To emphasize this point, we consider a classic example: given ? ? (0, 1), the Personalized Page Rank (PPR) matrix is defined by <ref type="bibr" target="#b3">(Brin &amp; Page, 1998)</ref> as</p><formula xml:id="formula_13">R ? := ? k=0 ? P P R k (D ?1 A) k = ? ? k=0 (1 ? ?)(D ?1 A) k .</formula><p>Assume that we rewire the graph using R ? as in <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> with the PPR kernel, meaning that we replace the given adjacency A with R ? . Since R ? is stochastic, the new Cheeger constant of the rewired graph can be computed as</p><formula xml:id="formula_14">h S,? = |?S| ? vol ? (S) ? 1 |S| i?S j?S (R ? ) ij .</formula><p>By applying <ref type="bibr">(Chung, 2007, Lemma 5)</ref>, we show that we cannot improve the Cheeger constant (and hence the bottleneck) arbitrarily well (in contrast to a curvature-based approach). We refer to Proposition 17 and Remark 18 in Appendix E for results that are more tailored to the actual strategy adopted in <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> where we also take into account the effect of the sparsification.</p><formula xml:id="formula_15">Theorem 6. Let S ? V with vol(S) ? vol(G)/2. Then h S,? ? 1?? ? davg(S) dmin(S) h S ,</formula><p>where d avg (S) and d min (S) are the average and minimum degree on S, respectively.</p><p>The property that the new Cheeger constant is directly controlled by the old one stems from the fact that a random-walk approach like in <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> is meant to act more relevantly on intra-community edges rather than inter-community edges because it prioritizes short diffusion distance nodes. This is also why this method performs well on high-homophily datasets, as discussed below. In particular, for a fixed ? ? (0, 1), the bound in Theorem 6 can be very small. As a specific example, consider two complete graphs K n joined by one bridge. Then h G = (n(n ? 1) + 1) ?1 , which means that the bound on the right hand side is O(n ?2 ).</p><p>Theorem 6 implies that a diffusion approach such as DIGL might fail to yield a new edge set E with a sufficiently improved bottleneck. By contrast, from Theorem 4 and Proposition 5, we deduce that a curvature-based rewiring method such as SDRF properly addresses the edges that cause the bottleneck.</p><p>Graph structure preservation. Although a graph-rewiring approach aims at providing a new edge set E potentially more beneficial for the given learning task, it is still desirable to control how far E is from E. In this regard, we note that a curvature-based rewiring is surgical in nature and hence more likely to preserve the structure of the input graph better than a random-walk based approach. Consider, for example, that we are given ? &gt; 0 and wish to rewire the graph such that the new edge set E is within graph-edit distance ? from the original E. Theorem 4 tells us how to do the rewiring under such constraints in order to best address the over-squashing: the topological modifications need to be localized around the most negatively-curved edges. We can do this with SDRF, with the maximum number of iterations set to ?/2. Secondly, we also point out that Ric(i, j) &lt; ?2 + ? implies that min{d i , d j } &gt; 2/?. Therefore, if we mostly modify the edge set at those nodes i, j joined by an edge with large negative curvature, then we are perturbing nodes with high degree where such a change is relatively insignificant, and thus overall statistical properties of the rewired graph such as degree distribution are likely to be better preserved. Moreover, graph convolutional networks tend to be more stable to perturbations of high degree nodes <ref type="bibr">(Z?gner et al., 2020;</ref><ref type="bibr" target="#b22">Kenlay et al., 2021)</ref>, making curvature-based rewiring more suitable for the downstream learning tasks with popular GNN architectures.</p><p>Homophily and bottleneck. As a final remark, note that the graph rewiring techniques considered in this paper (both DIGL and SDRF) are based purely on the topological structure of the graph and completely agnostic to the node features and to whether the dataset is homophilic (adjacent nodes have same labels) or heterophilic. Nonetheless, the different nature of these rewiring methods allows us to draw a few broad conclusions about their suitability in each of these settings. A random-walk  approach such as DIGL tends to improve the connectivity among nodes that are at short diffusion distance; since for a high-homophily dataset these nodes often share the same label, a rewiring method like DIGL is likely to act as graph denoising and yield improved performance. On the other hand, for datasets with low homophily, nodes at short diffusion distance are more likely to belong to different label classes, meaning that a diffusion-based rewiring might inject noise and hence compromise performance as also noted in <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref>. Conversely, on a low-homophily dataset, a curvature-based approach as SDRF modifies the edge set mainly around the most negatively curved edges, meaning that it decreases the bottleneck without significantly increasing the connectivity among nodes with different labels. In fact, long-range dependencies are often more relevant in low-homophily settings, where nodes sharing the same labels are in general not neighbors. This observation is largely confirmed by experimental results reported in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTAL RESULTS</head><p>Experiment setup. To demonstrate the theoretical results in this paper we ran a suite of semisupervised node classification tasks comparing our curvature-based rewiring method SDRF to DIGL from <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> (GDC with the PPR kernel) and the +FA method from <ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref>, where the last layer of the GNN is made fully connected. We evaluate the methods on nine datasets: Cornell, Texas and Wisconsin from the WebKB dataset 4 ; Chameleon and Squirrel (Rozemberczki et al., 2021) along with Actor <ref type="bibr" target="#b53">(Tang et al., 2009);</ref><ref type="bibr">and Cora (McCallum et al., 2000)</ref>, Citeseer <ref type="bibr" target="#b47">(Sen et al., 2008)</ref> and Pubmed <ref type="bibr" target="#b32">(Namata et al., 2012)</ref>. Statistics for these datasets can be found in Appendix F.1. Our base model is a <ref type="bibr">GCN (Kipf &amp; Welling, 2017)</ref>. Following <ref type="bibr" target="#b48">Shchur et al. (2018)</ref> and <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> we optimized hyperparameters for all dataset-preprocessing combinations separately by random search over 100 data splits. Results are reported as average accuracies on a test set used once with 95% confidence intervals calculated by bootstrapping. We compared the performance on graphs with no preprocessing, making the graph undirected, +FA, DIGL, SDRF, and the given combinations. For DIGL + Undirected we symmetrized the diffusion matrix as in <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref>, and for SDRF + Undirected we made the graph undirected before applying SDRF. For more details on the experiments and datasets see Appendix F, and for the hyperparameters used for each model and preprocessing see Appendix F.4.</p><p>Node classification results. <ref type="table" target="#tab_2">Table 2</ref> shows the results of the experiments. As well as reporting results we give a measure of homophily H(G) proposed by <ref type="bibr" target="#b41">Pei et al. (2019)</ref> (restated in Appendix F, equation 21), by which we can see our experiment set is diverse with respect to homophily. We see that SDRF improves upon the baseline in all cases, and that the largest improvements are seen on the low-homophily datasets. We also see that SDRF matches or outperforms DIGL and +FA on most datasets, supporting our argument that curvature-based rewiring is a viable candidate for improving GNN performance.</p><p>Graph topology change. Furthermore, SDRF preserves the graph topology to a far greater extent than DIGL due to its surgical nature. <ref type="table" target="#tab_4">Table 3</ref> shows the number of edges added / removed by the two preprocessings on each dataset as a percentage of the original number of edges. We see that for optimal performance DIGL makes the graph much denser, which significantly affects the node degrees and may negatively impact the time and space complexity of the downstream GNN, which typically are O(E). In comparison, SDRF adds and removes a similar number of edges and approximately preserves the degree distribution. The effect on the full degree distribution for three  datasets is shown in <ref type="figure">Figure 4</ref>. We see that the degree distribution following SDRF is close to, or in some cases indistinguishable from, the original distribution, whereas DIGL has a more noticeable effect. We also compute the Wasserstein distance between the degree distributions, denoted by W 1 in the figure captions, to numerically confirm this observation. For this analysis extended to all nine datasets see Appendix F.2.</p><p>This difference is also visually evident from <ref type="figure">Figure 6</ref> in Appendix F.3, where we again observe that SDRF largely preserves the topology of the Cornell graph and that the curvature (encoded by the edge color) is homogenized across the graph. We also see that the entries of the trained network's Jacobian between a node's prediction and the features of its 2-hop neighbors (encoded as node colors) are increased over the graph, which we may attribute to both DIGL and SDRF's ability to alleviate the upper bound presented in Theorem 4 and thus reduce over-squashing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we studied the graph bottleneck and the over-squashing phenomena limiting the performance of message passing graph neural networks from a geometric perspective. We started with a Jacobian approach to determine how the over-squashing phenomenon is dictated by the graph topology as in equation 2. We then investigated further how the topology induces the bottleneck and hence causes over-squashing. We introduced a new notion of edge-based Ricci curvature called Balanced Forman curvature, relating it to the classical Ollivier curvature (Theorem 2). We then proved in Theorem 4 that negatively-curved edges are responsible for over-squashing, calling for a possibility of curvature-based rewiring of the graph in order to improve its bottleneckedness. We show one such possibility (Algorithm 1), inspired by the classical Ricci flow and comment on the advantages of surgical method such as this. We show both theoretically and experimentally that the proposed method can be advantageous compared to a diffusion-based rewiring approach, opening the door for curvature-based rewiring methods for improving GNN performance going forward.</p><p>Limitations and future directions. Our paper establishes a geometric perspective on the graph bottleneck and over-squashing, providing new tools to study and cope with these phenomena. One limitation of our work is that the theoretical results presented here do not currently extend to multigraphs. In addition, the current methodology is agnostic to information beyond the graph topology, such as node features. In future works, we will develop a notion of the curvature and the corresponding rewiring method that can take into account such information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head><p>The Appendix is structured as follows:</p><p>(i) In Appendix A we prove Lemma 1 and a side-result about the role of self-loops. We also summarize how to extend the analysis in Lemma 1 to message passing models with sum aggregations (not average), meaning those architectures where we do not normalize the adjacency matrix.</p><p>(ii) In Appendix B we introduce and describe different quantities we use to characterize the neighbourhood of a given edge i ? j. These objects are all essential to studying the new notion of balanced Forman curvature. The focus is on how we can distinguish 4-cycles in a computationally tractable way without losing too much accuracy.</p><p>(iii) In Appendix C we provide a brief literature review about existing curvature candidates. In particular, we report the definitions of (modified) Ollivier curvature and Forman curvature.</p><p>(iv) In Appendix D we prove the statements in Section 3, i.e. Theorem 2, Corollary 3, Theorem 4 and Proposition 5. We also comment on the role of the assumptions and compare the bound in Theorem 2 with the existing literature. Finally, we relate the classical notion of betweenness centrality to the over-squashing effect and the negatively curved edges in a graph.</p><p>(v) In Appendix E we prove the results in Section 4, namely Theorem 6 and an analogous result.</p><p>(vi) In Appendix F we describe more fully the experiments from Section 5, including a full analysis on degree distribution (Appendix F.2) and the hyperparameters used in our experiments (Appendix F.4).</p><p>(vii) In Appendix G we comment on hardware specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS OF RESULTS IN SECTION 2</head><p>Lemma 1. Assume an MPNN as in equation 1. Let i, s ? V with s ? S r+1 (i). If |?? | ? ? and |?? | ? ? for 0 ? ? r, then</p><formula xml:id="formula_16">?h (r+1) i ?x s ? (??) r+1 (? r+1 ) is .<label>(2)</label></formula><p>Proof. Let i ? V and s ? S r+1 (i). We recall that to ease the notations we assume that node features and hidden representations are scalar. The proof in the more general higher-dimensional case follows without any modification. We compute</p><formula xml:id="formula_17">?h (r+1) i ?x s = ? 1 ? r (. . .)? xs h (r) i + ? 2 ? r (. . .) n jr=1? ijr ? 1 ? r (h (r) i , h (r) jr )? xs h (r) i + ? 2 ? r (h (r) i , h (r) jr )? xs h (r) jr .</formula><p>We can iterate the computation above and see that the right hand side can be expanded as ?h (r+1) i ?x s = jr,...,j0 kr?{i,jr} ? ? ? k1?{i,jr,...,j1}?</p><formula xml:id="formula_18">ijr?krjr?1 . . .? k1j0 Z ijrkrjr?1...k1j0 (X)? xs h (0) j0 ,</formula><p>for some functions Z ijr...k1j0 of the input features obtained as products of r + 1 partial derivatives of the maps ? and r + 1 partial derivatives of the maps ? . Since H (0) = X, we have</p><formula xml:id="formula_19">? xs h (0) j0 = ? j0s</formula><p>meaning that the previous sum becomes ?h (r+1) i ?x s = jr,...,j1 kr?{i,jr} ? ? ? k1?{i,jr,...,j1}? ijr?krjr?1 . . .? k1s Z ijrkrjr?1...k1s (X).</p><p>Since d G (i, s) = r + 1, the only non-vanishing terms in the sum above are the minimal walks from i to s. In fact, if there existed a different choice of coefficients yielding a non-zero term then we would find a walk joining i to s of length lesser than r + 1, which is in contradiction with the definition of geodesic distance. Since Z i...s (X) is a product of r + 1-partial derivatives of the aggregation and update maps and by assumption their gradients are bounded by ? and ? respectively, we conclude that ?h</p><formula xml:id="formula_20">(r+1) i ?x s ? (??) r+1 jr,...,j1? ijr?jrjr?1 . . .? j1s = ? r+1 (? r+1 ) is</formula><p>which completes the proof of the Lemma.</p><p>As a byproduct of this analysis, we can also provide a rigorous motivation for the role of self-loops in GNNs (see Appendix for details):</p><formula xml:id="formula_21">Corollary 7. If h ( +1) i = j?i ? (h ( ) j ), then h ( +1) i</formula><p>only depends on nodes that can be reached via walks of length exactly + 1. By adding self-loops, the GNN also takes into account nodes that can be reached via walks of length r ? + 1.</p><formula xml:id="formula_22">Proof. If h ( +1) i = j?i ? (h ( ) j ) for each ? [0, L ? 1]</formula><p>, then we can argue as in the proof of Lemma 1 and find ?h</p><formula xml:id="formula_23">( +1) i ?x s = j ,...,j1 a ij . . . a j1s ? (h ( ) j ) ? ? ? ? 0 (x s ).</formula><p>The combinatorial coefficient a ij . . . a j1s is non-zero iff there exists a walk from i to s of length exactly +1, since we are not taking into account the contribution coming from self-loops. Conversely, if each term a ij was replaced by? ij then we would find that? ij . . .? j1s is non-zero iff there exists a walk from i to s of length at most r + 1, since the diagonal entries are now positive.</p><p>Remark 8. As a specific instance of Corollary 7, we note that if we do not include self-loops in the adjacency matrix, then the output of a 2-layer simplified graph neural network at node i is independent of the features of neighbours k that do not form a triangle with i. Once again here the dependence is precisely measured via the Jacobian of the hidden features with respect to the input features.</p><p>Remark 9. We note that the role of self-loops has also implicitly been noted in Xu et al. <ref type="formula" target="#formula_1">(2018)</ref> where the analysis of the Jacobian of node representations on the graph augmented with self-loops has been related to lazy random-walks.</p><p>GNNs with different aggregations We note that similar conclusions extend to message passing architectures where the aggregations are sums and not averages meaning that we take the augmented adjacency without normalizing by the degree matrices. Consistently with Lemma 1, we restrict to the setting where features and node representations at each layer are scalars to make the discussion simpler. In line with the Xu et al. <ref type="bibr">(2018)</ref> we consider a GNN-model of the form</p><formula xml:id="formula_24">h ( +1) i = ReLU ? ? j??i h ( ) j w ? ? .</formula><p>Note that the augmented neighbourhood? i is defined as N i ? {i}. Differently from the setting of Theorem 1 in <ref type="bibr">Xu et al. (2018)</ref>, the aggregation here is not an average but a simple sum. Let us now take nodes i and s such that s ? S r+1 (i) as in the statement of Lemma 1. In this case, instead of simply considering the quantity |?h </p><formula xml:id="formula_25">J r+1 (i, s) =? r+1 is k? r+1 ik ?? r+1 is Vol(B r+1 (i)) ,</formula><p>where? = A + I and vol(S) = j?S d j . In particular, we again find that if we have a tree structure, then the right hand side decays exponentially as 2 ?(r+1) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PRELIMINARY ANALYSIS OF AN EDGE-NEIGHBORHOOD</head><p>Given an edge i ? j, we introduce the sets below:</p><formula xml:id="formula_26">(i) ? (i, j) := S 1 (i) ? S 1 (j)</formula><p>, the number of triangles based at the edge i ? j. <ref type="figure">j)</ref>), simply the complement of the neighbours of i with respect to the sets introduced in (i) and (ii) once we also exclude j.</p><formula xml:id="formula_27">(ii) i (i, j) := {k ? S 1 (i) \ S 1 (j), k = j : ?w ? (S 1 (k) ? S 1 (j)) \ S 1 (i)}, the number of nodes k ? S 1 (i) forming a 4-cycle based at i ? j without diagonals inside. (iii) Q i (j) := S 1 (i) \ ({j} ? ? (i, j) ? i (i,</formula><p>In the following we simply write ? , i and Q i when the edge i ? j is clear from the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4-cycle contributions.</head><p>In general the sets i and j may differ. This may occur when there exists a node k belonging to i that admits multiple solutions w as in the definition of i . This feature needs to be taken into account when comparing Ollivier's Ricci curvature to the new notion we present below. We first introduce the following class to ease the notations. Definition 2. For any simple, undirected graph G = (V, E), if U ? V , then we set</p><formula xml:id="formula_28">D(U ) := {? : U ? V, |U | = |?(U )|, (z, ?(z)) ? E, ?z ? U } .</formula><p>We note that any ? ? D(U ) is injective.</p><p>We may now define a quantity which measures the maximal number of 1-1 pairings that can be performed from i to j . Definition 3. For any edge i ? j we let</p><formula xml:id="formula_29">m (i, j) := max |U | : U ? i , ?? : U ? j , ? ? D(U ) .</formula><p>We often simply write m . While the quantity m plays a role in the derivation of the Ollivier curvature of i ? j it is not computationally-friendly, as to determine m we need to identify and distinguish all possible 4-cycles based at i ? j and then choose a maximal pairing map. Accordingly, we consider a looser term which is easier to compute: Definition 4. For any pair of adjacent nodes i ? j we define</p><formula xml:id="formula_30">? max (i, j) := max max k? i {(A k ? (A j ? A i A j )) ? 1}, max w? j {(A w ? (A i ? A j A i )) ? 1} ,</formula><p>where A s denotes the s-th row of the adjacency matrix. We usually simply write ? max .</p><p>Remark 10. We note that given k ? S 1 (i) \ S 1 (j) the term (A k ? (A j ? A i A j )) ? 1 yields the number of nodes w forming a 4-cycle of the form i ? k ? w ? j ? i with no diagonals inside. The value ? max measures the maximal degeneracy of edges forming 4-cycles, meaning that it is equal to 1 iff for each k ? i there exists a unique node w ? j such that i ? k ? w ? j ? i is a 4-cycle.</p><p>We now end the discussion about 4-cycle contributions by proving the following inequality, which allows us to avoid to compute directly the term m up to giving up some accuracy. Lemma 11. For any edge i ? j we have</p><formula xml:id="formula_31">| m | ? max{| i |, | j |} ? max .</formula><p>Proof. The proof is based on a combinatorial argument. Let i = {k 1 , . . . , k r } and let m = {k 1 , . . . , k }, with &lt; r. By definition there exists ? : {k 1 , . . . , k } ? {w 1 , . . . , w }, with k i ? w i and w i ? j , for 1 ? i ? . Given k ? i \ {k 1 , . . . , k }, then there are no w ? j \ {w 1 , . . . , w } such that k ? w, otherwise we could extend ? by setting k ? ?(k) := w and we would then get</p><formula xml:id="formula_32">| m | = + 1. Accordingly, we have s=1 (A ws ? (A i ? A j A i )) ? 1) ? | i |, which implies ? max | m | ? ? max ? s=1 (A ws ? (A i ? A j A i )) ? 1) ? | i |.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EXISTING CURVATURE CANDIDATES</head><p>Ollivier Ricci curvature For i ? V and ? ? [0, 1) we define a probability measure on B 1 (i) by:</p><formula xml:id="formula_33">? ? i : j ? ? ? ? ? ? ?, j = i 1?? di , j ? S 1 (i), 0, otherwise.</formula><p>Before we introduce the Ollivier curvature, we recall that the transportation distance between two finitely supported probability measures as above can be computed as</p><formula xml:id="formula_34">W 1 (? ? i , ? ? j ) := inf M k?S1(i) w?S1(j) M kw d G (k, w),</formula><p>where d G (?, ?) is the geodesic distance on the graph and the infimum is taken over all matrices M satisfying the marginal constraints:</p><formula xml:id="formula_35">k?S1(i) M kw = ? ? j (w), w?S1(j) M kw = ? ? i (k).</formula><p>We are now ready to define the Ollivier Ricci curvature: the formulation below is due to <ref type="bibr" target="#b25">Lin et al. (2011)</ref>. Definition 5. Given i ? j we define the ?-Ollivier curvature by</p><formula xml:id="formula_36">? ? (i, j) := 1 ? W 1 (? ? i , ? ? j ).<label>(7)</label></formula><p>Since ? ? (1 ? ?) ?1 is increasing and bounded the quantity below is well-defined:</p><formula xml:id="formula_37">?(i, j) := lim ??1 1 ? W 1 (? ? i , ? ? j ) 1 ? ? .<label>(8)</label></formula><p>Forman Ricci curvature In the following we report a formula for the augmented Forman Ricci curvature on unweighted graphs <ref type="bibr" target="#b45">Samal et al. (2018)</ref>. We also note that Forman curvature on graphs has also been studied in <ref type="bibr" target="#b52">Sreejith et al. (2016)</ref>; <ref type="bibr" target="#b56">Weber et al. (2018)</ref>. Definition 6. For any edge i ? j the augmented Forman curvature is given by</p><formula xml:id="formula_38">F (i, j) := 4 ? d i ? d j + 3| ? |.</formula><p>We note that such formulation of curvature does not distinguish contributions coming from 4-cycles.</p><p>In fact, for the orthogonal grid with degree d ? 4, Forman Ricci curvature is equal to 2(2 ? d) &lt; 0. This does not reflect that the r-hop neighbourhood for such a graph grows polynomially in r.</p><p>We conclude this appendix by reporting a lower bound for the Ollivier Ricci curvature derived in <ref type="bibr" target="#b20">Jost &amp; Liu (2014)</ref>. We recall that ? ? , with ? ? [0, 1) was defined in equation 7. Theorem 12 <ref type="bibr" target="#b20">(Jost &amp; Liu (2014)</ref>). For any edge i ? j, with d i ? d j , the following bound is satisfied:</p><formula xml:id="formula_39">? 0 (i, j) ? ?(i, j) := ? 1 ? 1 d i ? 1 d j ? | ? | d j + ? 1 ? 1 d i ? 1 d j ? | ? | d i + + | ? | d j .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROOFS OF RESULTS IN SECTION 3</head><p>We first recall our definition of Balanced Forman: Definition 7. For any edge i ? j we let Ric(i, j) be zero if min{d i , d j } = 1, otherwise</p><formula xml:id="formula_40">Ric(i, j) := 2 d i + 2 d j ? 2 + 2 | ? | max{d i , d j } + | ? | min{d i , d j } + (? max ) ?1 max{d i , d j } (| i | + | j |)<label>(9)</label></formula><p>where the last term is set to be zero if | i | (and hence | j |) is zero.</p><p>We also extend the previous definition to the weighted case. In this setting we let G = (V, E, ?) be a simple, locally finite, undirected graph with normalized weights. We first report the formula for the augmented Forman in the weighted case <ref type="bibr" target="#b45">Samal et al. (2018)</ref>:</p><formula xml:id="formula_41">F (i, j) = ?(i) + ?(j) + k?S1(i)?S1(j) ? 2 ij ? ? ? k?S1(i)\S1(j) ?(i) ? ij ? ik ? k?S1(j)\S1(i) ?(j) ? ij ? jk ,</formula><p>where ? ? is taken to be the Heron formula for the area of a triangle while ?(?) denotes some weighting scheme for the nodes as well. We propose a similar definition for the weighted case, which reduces to the one discussed above in the combinatorial setting. We recall that W is the weighted adjacency matrix while A is the combinatorial one. Moreover, we write A i j = (A j ? A i ) + and similarly for W i j . Definition 8. For any pair of adjacent nodes i, j ? V we define Ric(i, j) to be 0 if min{|d i |, |d j |} = 1, otherwise we set</p><formula xml:id="formula_42">Ric(i, j) := 1 d i ? ? 1 ? k?Qi ? ij ? ik ? ? + 1 d j ? ? 1 ? k?Qj ? ij ? jk ? ? + 1 max{d i , d j } k?S1(i)?S1(j) ? 2 ij ? ? + k? i ? ij ? ? ij (? max ) ?1 max{d i , d j } ? ? ? ij ? d i + k? j ? ij ? ? ij (? max ) ?1 max{d i , d j } ? ? ? ij ? d j ,</formula><p>with</p><formula xml:id="formula_43">? ? := 1 3 (? 2 ij + ? 2 ik + ? 2 jk ), z ? S 1 (i) ? S 1 (j),</formula><p>and, for a given k ? i ,</p><formula xml:id="formula_44">? := W z ? A i j A k ? A i j , ? = 1 4 ? 2 ij + ? 2 ik + ? 2 + A k ? W i j A k ? A i j .</formula><p>Important convention. Without losing generality, in the following we always assume that 1 ? d i ? d j . In particular, we write d i . = d and d j = d + s, for some s ? 0, omitting to specify that both d and s are of course depending on i and j. Moreover, from now on we only focus on the unweighted case.</p><p>We can now prove our main comparison theorem.</p><p>Theorem 2. Given an unweighted graph G, for any edge i ? j we have ?(i, j) ? Ric(i, j).</p><p>Proof. We stick to the aforementioned convention: d i := d ? d j = d + s, s ? 0. The strategy of the proof amounts to finding a transportation plan providing an upper bound for W 1 (? ? i , ? ? j ) and hence a lower bound for the curvature ?(i, j). In particular, we consider plans moving the mass ? ? i from B 1 (i) to B 1 (j).</p><p>If d = 1, then the optimal transport plan consists of moving the mass ? from i to j and the remaining mass 1 ? ? on j to S 1 (j). This yields a unit Wasserstein distance between ? ? i and ? ? j and hence zero Ollivier curvature ?(i, j), which coincides with the value of balanced Forman Ric(i, j).</p><p>Assume now that d ? 2. A (possibly non-optimal) transport plan from ? ? i to ? ? j is given by:</p><p>(i) Move mass (1 ? ?)/(d + s) from each node k ? m ? i to its unique image in j under a bijection ? as per definition of m .</p><p>(ii) The remaining mass on each node k ? m will need to travel by at most distance 3 to S 1 (j).</p><p>(iii) The extra-mass (1 ? ?)(1/d ? 1/(d + s)) on each common neighbour k ? ? will need to travel by at most distance 2 to S 1 (j).</p><p>(iv) Move the mass (1 ? ?)/d from j to S 1 (j).</p><p>(v) Move the mass ? from i to j. This leaves left-over mass (1 ? ?)/(d + s) at i from the distribution ? ? j . This mass can be compensated from mass in S 1 (i) which is at distance one.</p><p>(vi) Finally, we move the mass (1 ? ?)/d of any untouched node in S 1 (i) to S 1 (j) along a path of length lesser or equal than three. Note that the remaining mass is equal to</p><formula xml:id="formula_45">((1 ? ?)/d)(d ? 1 ? | ? | ? | m |) ? (1 ? ?)(d + s),</formula><p>where the last terms comes from (v).</p><p>If we sum all the contributions we find</p><formula xml:id="formula_46">W 1 (? ? i , ? ? j ) ? (1 ? ?) | m | d + s + 3| m | 1 d ? 1 d + s + 2| ? | 1 d ? 1 d + s + 1 d + ? + 1 ? ? d + s + 3(1 ? ?) 1 d (d ? 1 ? | ? | ? | m |) ? 1 d + s = (1 ? ?) 1 d + s (?2| ? | ? 2| m | ? 2) + 1 d (?| ? | ? 2) + 2 + 1 = (1 ? ?) 1 d + s ?3| ? | ? s d | ? | ? 2| m | ? 4 ? 2 s d + 2(d + s) + 1.</formula><p>Therefore we have</p><formula xml:id="formula_47">?(i, j) = lim ??1 1 ? W (? ? i , ? ? j ) 1 ? ? ? 1 d + s 3| ? | + s d | ? | + 2| m | + 4 + 2 s d ? 2(d + s) .</formula><p>(i) |?? | ? ? and |?? | ? ? for each 0 ? ? L ? 1, with L ? 2 the depth of the MPNN.</p><p>(ii) There exists ? s.t. 0 &lt; ? &lt; (max{d i , d j }) ? 1 2 , ? &lt; ? ?1 max , and Ric(i, j) ? ?2 + ?.</p><p>Then there exists Q j ? S 2 (i) satisfying |Q j | &gt; ? ?1 and for 0 ? 0 ? L ? 2 we have</p><formula xml:id="formula_48">1 |Q j | k?Qj ?h ( 0 +2) k ?h ( 0) i &lt; (??) 2 ? 1 4 .<label>(4)</label></formula><p>Proof. As usual we let d i := d and d j := d + s, for some s ? 0. We first observe that from the requirement ? 2 (d + s) ? 1 in (ii), we derive Ric(i, j) ? ?2 + ? iff</p><formula xml:id="formula_49">4 + 2 s d + 3| ? | + s d | ? | + ? ?1 max (| i | + | j |) ? ?(d + s).</formula><p>Therefore we have</p><formula xml:id="formula_50">?| ? | 3 + s d ? ? 2 (d + s), meaning that ?| ? | ? 1.<label>(10)</label></formula><p>From now on we let Q j denote again the complement S 1 (j) \ (S 1 (i) ? j ? {i}). Without loss of generality we set 0 = 0 and hence h (0) i = x i ; the very same proof applies to any other choice of 0 . Given k ? Q j , since k ? S 2 (i), we can apply Lemma 1 and derive</p><formula xml:id="formula_51">?h (2) k ?x i ? (??) 2 (?) 2 ik .<label>(11)</label></formula><p>We may expand the power of the augmented normalized adjacency matrix as</p><formula xml:id="formula_52">(?) 2 ik = 1 (d k + 1)(d i + 1) w?S1(k)?S1(i) 1 d w + 1 .</formula><p>If we introduce the setQ j = {k ? Q j : ? ik &gt; 1}, we can then write</p><formula xml:id="formula_53">k?Qj (?) 2 ik = k?Qj 1 (d k + 1)(d i + 1) w?S1(k)?S1(i) 1 d w + 1 = = 1 ? d i + 1 ? ? k?Qj 1 ? d k + 1 1 d j + 1 + k?Qj 1 ? d k + 1 w?S1(k)?S1(i)?S1(j) 1 d w + 1 ? ?<label>(12)</label></formula><p>where in the last equality we have again used the fact that k ?Q j iff there is w ? S 1 (k)?S 1 (i)?S 1 (j).</p><p>To avoid heavy notations, we introduce V k := S 1 (k) ? S 1 (i) ? S 1 (j). Let us first focus on the first sum in equation 12. We have</p><formula xml:id="formula_54">1 ? d i + 1 k?Qj 1 ? d k + 1 1 d j + 1 ? 1 ? d i + 1 |Q j | 1 d j + 1 ? 1 ? d i + 1 ? 1.<label>(13)</label></formula><p>We now consider the second sum in equation 12. We assume | ? | ? 1, otherwiseQ j = ?. We let</p><formula xml:id="formula_55">? := w ? ? : d w &lt; 1 C |Q j | | ? | + 2 C</formula><p>for some C &gt; 0 to be chosen below. Then, the second sum in equation 12 can be split as</p><formula xml:id="formula_56">k?Qj 1 ? d k + 1 ? ? w?V k ?? 1 d w + 1 + w?V k \? 1 d w + 1 ? ? .<label>(14)</label></formula><p>Since any w ? V k has degree at least three, we can bound the first term in equation 14 as</p><formula xml:id="formula_57">k?Qj 1 ? d k + 1 w?V k ?? 1 d w + 1 ? k?Qj 1 ? d k + 1 |V k ? ?| 4 .</formula><p>We now observe that</p><formula xml:id="formula_58">k?Qj |V k ? ?| ? d k + 1 ? k?Qj |V k ? ?| = (k, w) ? E : k ?Q j , w ? V k ? ? ? max w?? d w |?|.</formula><p>Since d w ? (1/C)|Q j |/| ? | + 2/C for any w ? ? we see that the first term in equation 14 can be bounded by</p><formula xml:id="formula_59">k?Qj 1 ? d k + 1 w?V k ?? 1 d w + 1 ? k?Qj 1 ? d k + 1 |V k ? ?| 4 ? 1 C |Q j | | ? | + 2 C |?| 4 ? 1 C |Q j | | ? | + 2 C | ? | 4 .<label>(15)</label></formula><p>by definition of ?. We now bound the second term in equation 14 as</p><formula xml:id="formula_60">k?Qj 1 ? d k + 1 w?V k \? 1 d w + 1 ? k?Qj 1 ? d k + 1 C| ? | |Q j | |V k \ ?| where we have used that d ?1 w ? C| ? |/|Q j | if w ? V k \ ?. Since |V k \ ?| ? d k + 1 ? |V k | |S 1 (k)| ? |V k | |V k | ? |V k | ? | ? | we see that k?Qj 1 ? d k + 1 C| ? | |Q j | |V k \ ?| ? C| ? | |Q j | | ? ||Q j | = C| ? | 3 2 .<label>(16)</label></formula><p>We are now ready to complete the proof of the theorem. According to equation 11 it suffices to show that 1 |Q j | k?Qj (? 2 ) ik ? ? 1/4 . From equation 12 and equation 13 we derive that the left hand side of the equation above is bounded by</p><formula xml:id="formula_61">1 |Q j | k?Qj (? 2 ) ik ? 1 |Q j | + 1 |Q j | ? ? k?Qj 1 ? d k + 1 w?V k 1 d w + 1 ? ? ? ? + 1 |Q j | ? ? k?Qj 1 ? d k + 1 w?V k 1 d w + 1 ? ?</formula><p>where in the last inequality we have used Lemma 14 to bound |Q j | ?1 by ?. In particular we note that if ? = ? thenQ j = ?, and the bound would be simply controlled by ? as claimed. When | ? | &gt; 0, we can use equation 15 and equation 16 to estimate the second term from above by</p><formula xml:id="formula_62">1 |Q j | ? ? k?Qj 1 ? d k + 1 w?V k 1 d w + 1 ? ? ? 1 |Q j | 1 C |Q j | | ? | + 2 C | ? | 4 + 1 |Q j | C| ? | 3 2 ? 1 4 1 C + 2| ? | C|Q j | + C| ? | |Q j | | ? |.</formula><p>By applying Lemma 14 we get</p><formula xml:id="formula_63">1 4 1 C + 2| ? | C|Q j | + C| ? | |Q j | | ? | ? 1 4 1 C + 2 ? C + C? | ? |.</formula><p>We now choose C = ? ?1/4 , so that the previous quantity can be bounded by</p><formula xml:id="formula_64">1 4 1 C + 2 ? C + C? | ? | ? 1 4 ? 1 4 + 2? 5 4 + ? 1 4 ?| ? | ? 1 4 ? 1 4 + 2? 5 4 + ? 1 4</formula><p>where in the last inequality we have used equation 10. Therefore, we have shown that</p><formula xml:id="formula_65">1 |Q j | k?Qj (? 2 ) ik ? ? + 1 4 ? 1 4 + 2? 5 4 + ? 1 4 ? 3? 1 4</formula><p>where we have used that ? &lt; 1. This completes the proof (once we absorb the extra factor 3 in the constant ?? in equation 11).</p><p>Remark 15. The requirement ? max{d i , d j } &lt; 1 can be replaced by a more general bound ? max{d i , d j } &lt; r. The argument above extends to this case up to renaming the constant ?? in the statement so to include an extra factor r. We note that the condition ? max{d i , d j } &lt; r would be stronger than the one appearing in (ii) of Theorem 4. In this regard, we recall that for a d-tree the curvature satisfies Ric(i, j) = ?2 + 4 d .</p><p>We can also prove Proposition 5:</p><formula xml:id="formula_66">Proposition 5. If Ric(i, j) ? k &gt; 0 for all i ? j, then ? 1 /2 ? h G ? k 2 .</formula><p>Proof. This follows as an immediate Corollary of Theorem 2 and <ref type="bibr">(Lin et al., 2011, Theorem 4.2)</ref>.</p><p>Betweenness centrality to measure bottleneck. In equation 2 we have derived how the topology of the graph affects the dependence of the hidden node representation h (r+1) i on the input feature x s , for nodes i, s at distance r + 1. We note that in this case? r+1 is is exactly measuring the number of minimal paths from i to s. If the receptive field B r+1 (i) is a binary tree, then we have seen that the entry of the power matrix decays exponentially. The reason for such decay stems from the existence of exponentially many nodes in the receptive field combined with the lack of multiple minimal paths (shortcuts). When such conditions hold, most of the minimal paths go through the same nodes, which is exactly what happens for the tree where each node is in the minimal paths between different branches. Since the frequency in which a node appears in the minimal path of distinct pairs of nodes is measured by the betweenness centrality Freeman (1977), we propose a topological characterization of the 'bottleneckedness' of a graph as follows: Definition 9 (bottleneck). The bottleneck-value of G is b G := 1 n n i=1 b(i), where b(i) denotes the betweenness centrality on node i.</p><p>From a standard combinatorial argument it follows that if G is connected, then</p><formula xml:id="formula_67">b G = 1 n i,j (d G (i, j) ? 1) .<label>(17)</label></formula><p>We note that b G = 0 iff G is the complete graph K n . Therefore, b G determines how far the given topology is from K n , with the latter representing the limit case of a fully connected layer <ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref> where no bottleneck may occur as any pair of nodes would be neighbours. This further supports our intuition that the betweenness centrality is a good topological candidate for providing a global measurement of bottleneckedness in the graph.</p><p>It also follows from equation 17 that any update to the graph topology consisting of edge additions would decrease b G and thus reduce the bottleneck. The quantity b G is global in nature though and hence lacks robustness. As a pedagogical example, consider a barbell G(m, 2r + 1), with m the size of the two cliques joined by a path of length 2r + 1 and focus on the edge i ? j in the middle of such path. Nodes i and j are central to the graph, in the sense that most minimal paths go through them and indeed their betweenness centrality is b(i) = b(j) = (m + r) 2 + (m + r). If now we add a single edge joining the two cliques K m , the values b(i) and b(j) decrease dramatically by ?(m 2 + r).</p><p>Since the operation is non-local, we see that the representation h ( ) i of any MPNN is unaffected by the edge addition for any ? (0, r), and similarly for j. Eventually, if we keep adding edges, the receptive fields B s (i) will be affected for small values of s as well: the drawback of such approach is that the resulting adjacency may be significantly different. Conversely, the curvature provides a more precise, local and hence robust way of controlling the bottleneck and hence the over-squashing problem. Nonetheless, we relate the betweenness centrality to the Jacobian of the hidden features.</p><formula xml:id="formula_68">Theorem 16. Given i ? j, let ? j := S 1 (i) ? S 1 (j) ? {j}. If Ric(i, j) ? ?2 + ?, for 0 &lt; ? &lt; (1 + ? max ) ?1 , then 1 |? j | k??j b(k) ? ? ?1 .</formula><p>Proof. We rewrite the quantity in the statement as</p><formula xml:id="formula_69">1 | ? | + 1 ? ? k? ? b(k) + b(j) ? ? .</formula><p>By definition, given a node k ? V , the betweenness centrality of k is given by</p><formula xml:id="formula_70">b(k) := s,t?V :s =k,t =k ? st (k) ? st</formula><p>where ? st is the number of minimal paths between s and t while ? st (k) is the number of minimal paths from s to t passing through k. For convenience, we introduce the setQ j ? Q j defined b?</p><formula xml:id="formula_71">Q j := {w ? Q j : ? iw &gt; 1}.</formula><p>Equivalently,Q j consists of those nodes in S 1 (j) \ S 1 (i) which form a 4-cycle based at i ? j with a diagonal inside. Indeed, if w ? Q j and ? iw &gt; 1, then there exists more than one minimal path between i and w, in addition to the one passing through node j. For any such path there exists k ? S 1 (i) ? S 1 (w). Since w ? Q j and Q j ? j = ?, we derive that k ? S 1 (j) as well. We then get</p><formula xml:id="formula_72">k? ? b(k) = k? ? s,t?V :s =k,t =k ? st (k) ? st ? k? ? w?Qj ? iw (k) ? iw = w?Qj 1 ? iw k? ? ? iw (k).</formula><p>By summing ? iw (k) for all k ? ? we obtain all the 2-long minimal paths between i and w with the exception of the one passing through j:</p><formula xml:id="formula_73">k? ? b(k) ? w?Qj 1 ? iw (? iw ? 1).<label>(18)</label></formula><p>On the other hand we also have</p><formula xml:id="formula_74">b(j) = s,t?V :s =j,t =j ? st (j) ? st ? z?Qj ? iz (j) ? iz = z?Qj 1 ? iz .<label>(19)</label></formula><p>By combining equation 18 and equation 19 we finally get</p><formula xml:id="formula_75">1 | ? | + 1 ? ? k? ? b(k) + b(j) ? ? ? 1 | ? | + 1 ? ? w?Qj 1 ? iw (? iw ? 1) + z?Qj 1 ? iz ? ? = 1 | ? | + 1 ? ? |Q j | + z?Qj \Qj 1 ? iz ? ? = |Q j | | ? | + 1 ,</formula><p>where in the last equality we have used that by definition ? iz = 1 for all z ? Q j \Q j . By Lemma 14 the last quantity is larger than ? ?1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F EXPERIMENTS</head><p>Our experiments in this paper are semi-supervised node classification (semi-supervised in that the graph structure provides some unlabelled information) on nine common graph learning datasets. Cornell, Texas and Wisconsin are small heterophilic datasets based on webpage networks from the WebKB dataset. <ref type="bibr">Chameleon and Squirrel (Rozemberczki et al., 2021)</ref> are medium heterophilic datasets based on Wikipedia networks, along with Actor, the actor-only induced subgraph of the filmdirector-actor-writer network <ref type="bibr" target="#b53">(Tang et al., 2009)</ref>. Cora <ref type="bibr" target="#b28">(McCallum et al., 2000)</ref>, Citeseer <ref type="bibr" target="#b47">(Sen et al., 2008)</ref> and Pubmed <ref type="bibr" target="#b32">(Namata et al., 2012)</ref> are medium homophilic datasets based on citation networks. As in <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref>, for all experiments we consider the largest connected component of the graph.</p><p>When splitting the data into train/validation/test sets, we first separate the data into a development set and the test set. This is done once to ensure the test set is not used for any training or hyperparameter fitting before the final evaluation. For each of the 100 random splits the development set is divided randomly into a train set and a validation set, where we train models on the train set and evaluate on the validation set. We fit hyperparameters by random search, maximising the mean accuracy across the validation sets. The accuracy then reported in <ref type="table" target="#tab_2">Table 2</ref> is the mean accuracy on the test set from models trained on the train sets with the chosen hyperparameters, along with a 95% confidence interval calculated by bootstrapping the test set accuracies with 1000 samples. For Cora, Citeseer and Pubmed the development set contains 1500 nodes with the rest kept for the test set, and for each random split the train set is chosen to contain 20 nodes of each class while the rest form the validation set. As this is the same method as <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> and we use the same random seeds, we are using the same test set and expect to have comparable results. For the remaining datasets we perform a 60/20/20 split, with 20% of nodes set aside as the test set and then for each random split the remaining 80% is split into 60% training, 20% validation.   <ref type="figure">Figure 5</ref>: Comparing the degree distribution of the original graphs to the preprocessed version. The xaxis is node degree in log 2 scale, and the plots are a kernel density estimate of the degree distribution.</p><p>In the captions we see the Wasserstein distance W 1 between the original and preprocessed graphs. <ref type="figure">Figure 6</ref>: Rewiring of the Cornell graph. Left-to-right: original graph, DIGL, and SDRF rewiring. Edges are colored by curvature; nodes are colored by the maximum absolute entry of a trained 2-layer GCN's Jacobian between the GCN's prediction for that node and the features of the nodes 2 hops away in the original graph. SDRF homogenizes curvature and so lifts the upper bound on the Jacobian from Theorem 4. DIGL also does to an extent, though at the expense of preserving graph topology.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 VISUALIZING CURVATURE AND SENSITIVITY TO FEATURES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 HYPERPARAMETERS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Different regimes of curvatures on graphs analogous to spherical (a), planar (b), and hyperbolic (c) geometries in the continuous setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are the neighbors of i forming a 4-cycle based at the edge i ? j without diagonals inside.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>/</head><label></label><figDesc>?x s |, we normalize the Jacobian entries -obtaining what is referred to as influence score in Xu et al. (2018): J r+1 (i, s) This of course represents now a relative importance of feature x s on the representation of node i at layer r + 1. If -similarly to Theorem 1 in Xu et al. (2018) -we assume that all paths in the computational graph of the model are activated with the same probability, then we obtain that on average</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The homophily index H(G) proposed by<ref type="bibr" target="#b41">Pei et al. (2019)</ref> is defined asH(G) = 1 |V | v?VNumber of v's neighbors who have the same label as vNumber of v's neighbors .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Examples of the</cell></row><row><cell>Balanced Forman curvature.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>None 52.69 ? 0.21 61.19 ? 0.49 54.60 ? 0.86 41.80 ? 0.41 39.83 ? 0.14 28.70 ? 0.09 81.89 ? 0.79 72.31 ? 0.17 78.16 ? 0.23 Undirected 53.20 ? 0.53 63.38 ? 0.87 51.37 ? 1.15 42.63 ? 0.30 40.77 ? 0.16 28.10 ? 0.11 ---+FA 58.29 ? 0.49 64.82 ? 0.29 55.48 ? 0.62 42.33 ? 0.17 40.74 ? 0.13 28.68 ? 0.16 81.65 ? 0.18 70.47 ? 0.18 79.48 ? 0.12 DIGL (PPR) 58.26 ? 0.50 62.03 ? 0.43 49.53 ? 0.27 42.02 ? 0.13 34.38 ? 0.11 30.79 ? 0.10 83.21 ? 0.27 73.29 ? 0.17 78.84 ? 0.08 DIGL + Undirected 59.54 ? 0.64 63.54 ? 0.38 52.23 ? 0.54 42.68 ? 0.12 33.36 ? 0.21 29.71 ? 0.11 ---SDRF 54.60 ? 0.39 64.46 ? 0.38 55.51 ? 0.27 43.75 ? 0.31 40.97 ? 0.14 29.70 ? 0.13 82.76 ? 0.23 72.58 ? 0.20 79.10 ? 0.11 SDRF + Undirected 57.54 ? 0.34 70.35 ? 0.60 61.55 ? 0.86 44.46 ? 0.17 41.47 ? 0.21 29.85 ? 0.07</figDesc><table><row><cell></cell><cell>Cornell</cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Chameleon</cell><cell>Squirrel</cell><cell>Actor</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell></row><row><cell>H(G)</cell><cell>0.11</cell><cell>0.06</cell><cell>0.16</cell><cell>0.25</cell><cell>0.22</cell><cell>0.24</cell><cell>0.83</cell><cell>0.71</cell><cell>0.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on common node classification benchmarks. Top two in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Comparing the degree distribution of the original graphs to the preprocessed version. The xaxis is node degree in log 2 scale, and the plots are a kernel density estimate of the degree distribution. In the captions we see the Wasserstein distance W 1 between the original and preprocessed graphs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell><cell></cell><cell>Original DIGL SDRF</cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell cols="2">8 16 32 64 128 256 512 1024</cell><cell>0 1 2 4 8 16 32 64 128 256 512 1024 2048</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">(a) Wisconsin:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) Actor:</cell><cell></cell><cell>(c) Pubmed:</cell></row><row><cell></cell><cell cols="7">W1(Original, DIGL) = 11.83</cell><cell cols="5">W1(Original, DIGL) = 243.81</cell><cell></cell><cell>W1(Original, DIGL) = 247.01</cell></row><row><cell></cell><cell cols="7">W1(Original, SDRF) = 0.28</cell><cell cols="5">W1(Original, SDRF) = 1.03</cell><cell></cell><cell>W1(Original, SDRF) = 0.03</cell></row><row><cell cols="15">Figure 4: DIGL</cell><cell>SDRF</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cornell</cell><cell></cell><cell>351.1% / 0.0%</cell><cell>7.8% / 33.3%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Texas</cell><cell></cell><cell>483.3% / 0.0%</cell><cell>2.4% / 10.4%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Wisconsin</cell><cell></cell><cell>300.6% / 0.0%</cell><cell>1.4% / 7.5%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Chameleon 336.1% / 11.8%</cell><cell>6.4% / 6.4%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Squirrel</cell><cell></cell><cell>228.8% / 1.9%</cell><cell>0.7% / 0.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Actor</cell><cell cols="2">2444.0% / 2.3%</cell><cell>5.4% / 9.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cora</cell><cell cols="2">3038.0% / 0.5%</cell><cell>1.0% / 1.0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Citeseer</cell><cell cols="2">2568.3% / 0.0%</cell><cell>1.1% / 1.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pubmed</cell><cell cols="2">2747.1% / 0.1%</cell><cell>0.2% / 0.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>% edges added / removed by method.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>F.1 DATASETSFor datasets with disconnected graphs, the statistics shown here are for the largest connected component.</figDesc><table><row><cell cols="10">F.2 DEGREE DISTRIBUTIONS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Original DIGL SDRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell></cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64 128</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">(a) Cornell:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(b) Texas:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(c) Wisconsin:</cell></row><row><cell></cell><cell cols="9">W1(Original, DIGL) = 10.99</cell><cell cols="8">W1(Original, DIGL) = 17.98</cell><cell cols="8">W1(Original, DIGL) = 11.83</cell></row><row><cell></cell><cell cols="9">W1(Original, SDRF) = 1.01</cell><cell cols="8">W1(Original, SDRF) = 0.39</cell><cell cols="8">W1(Original, SDRF) = 0.28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Original DIGL SDRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell cols="5">16 32 64 128 256</cell><cell cols="8">0 1 2 4 8 16 32 64 128 256 512 1024 2048</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell cols="4">8 16 32 64 128 256 512 1024</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">(d) Chameleon:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(e) Squirrel:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">(f) Actor:</cell></row><row><cell></cell><cell cols="9">W1(Original, DIGL) = 96.30</cell><cell cols="8">W1(Original, DIGL) = 135.41</cell><cell cols="8">W1(Original, DIGL) = 243.81</cell></row><row><cell></cell><cell cols="9">W1(Original, SDRF) = 1.93</cell><cell cols="8">W1(Original, SDRF) = 0.50</cell><cell cols="8">W1(Original, SDRF) = 1.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Original DIGL SDRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Original DIGL SDRF</cell></row><row><cell>0</cell><cell>1</cell><cell>2</cell><cell>4</cell><cell cols="22">Cornell Texas Wisconsin Chameleon Squirrel Actor Cora Citeseer Pubmed 8 16 32 64 128 256 512 1024 0 1 2 4 8 16 32 64 128 256 512 1024 0 1 2 4 8 16 32 64 128 256 512 1024 2048</cell></row><row><cell cols="2">H(G) Nodes</cell><cell></cell><cell></cell><cell cols="3">0.11 140 (g) Cora:</cell><cell>0.06 135</cell><cell></cell><cell></cell><cell>0.16 184</cell><cell></cell><cell></cell><cell cols="4">0.25 832 (h) Citeseer:</cell><cell>0.22 2186</cell><cell>0.24 4388</cell><cell></cell><cell cols="5">0.83 2485 (i) Pubmed: 0.72 2120</cell><cell>0.79 19717</cell></row><row><cell cols="10">Edges W1(Original, DIGL) = 247.84 219 251</cell><cell cols="8">362 W1(Original, DIGL) = 178.28 12355 65224</cell><cell cols="8">21907 5069 W1(Original, DIGL) = 247.01 3679 44324</cell></row><row><cell cols="9">Features Classes W1(Original, SDRF) = 0.14 1703 1703 5 5</cell><cell cols="9">1703 5 W1(Original, SDRF) = 0.15 2323 2089 5 5</cell><cell cols="8">931 5 W1(Original, SDRF) = 0.03 1433 3703 500 7 6 3</cell></row><row><cell cols="3">Directed?</cell><cell></cell><cell cols="2">Yes</cell><cell></cell><cell>Yes</cell><cell></cell><cell></cell><cell>Yes</cell><cell></cell><cell></cell><cell></cell><cell>Yes</cell><cell></cell><cell></cell><cell>Yes</cell><cell>Yes</cell><cell></cell><cell cols="2">No</cell><cell></cell><cell>No</cell><cell></cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Hyperparameters for GCN with no preprocessing (None).</figDesc><table><row><cell>Dataset</cell><cell>Dropout</cell><cell>Hidden depth</cell><cell>Hidden dimension</cell><cell>Learning rate</cell><cell>Weight decay</cell></row><row><cell>Cornell</cell><cell>0.3060</cell><cell>1</cell><cell>128</cell><cell>0.0082</cell><cell>0.1570</cell></row><row><cell>Texas</cell><cell>0.2346</cell><cell>1</cell><cell>128</cell><cell>0.0072</cell><cell>0.0037</cell></row><row><cell>Wisconsin</cell><cell>0.2869</cell><cell>1</cell><cell>64</cell><cell>0.0281</cell><cell>0.0113</cell></row><row><cell>Chameleon</cell><cell>0.7304</cell><cell>3</cell><cell>128</cell><cell>0.0248</cell><cell>0.0936</cell></row><row><cell>Squirrel</cell><cell>0.5974</cell><cell>2</cell><cell>64</cell><cell>0.0136</cell><cell>0.1346</cell></row><row><cell>Actor</cell><cell>0.7605</cell><cell>1</cell><cell>64</cell><cell>0.0290</cell><cell>0.0619</cell></row><row><cell>Cora</cell><cell>0.4144</cell><cell>1</cell><cell>64</cell><cell>0.0097</cell><cell>0.0639</cell></row><row><cell>Citeseer</cell><cell>0.7477</cell><cell>1</cell><cell>128</cell><cell>0.0251</cell><cell>0.4577</cell></row><row><cell>Pubmed</cell><cell>0.4013</cell><cell>1</cell><cell>64</cell><cell>0.0095</cell><cell>0.0448</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters for GCN with the input graph made undirected (Undirected).</figDesc><table><row><cell>Dataset</cell><cell>Dropout</cell><cell>Hidden depth</cell><cell>Hidden dimension</cell><cell>Learning rate</cell><cell>Weight decay</cell></row><row><cell>Cornell</cell><cell>0.6910</cell><cell>1</cell><cell>64</cell><cell>0.0185</cell><cell>0.0285</cell></row><row><cell>Texas</cell><cell>0.2665</cell><cell>1</cell><cell>128</cell><cell>0.0069</cell><cell>0.0035</cell></row><row><cell>Wisconsin</cell><cell>0.2893</cell><cell>2</cell><cell>128</cell><cell>0.0142</cell><cell>0.0001</cell></row><row><cell>Chameleon</cell><cell>0.4657</cell><cell>3</cell><cell>64</cell><cell>0.0189</cell><cell>0.0423</cell></row><row><cell>Squirrel</cell><cell>0.5944</cell><cell>2</cell><cell>64</cell><cell>0.0081</cell><cell>0.0309</cell></row><row><cell>Actor</cell><cell>0.6626</cell><cell>2</cell><cell>64</cell><cell>0.0195</cell><cell>0.0219</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Hyperparameters for GCN with SDRF preprocessing (SDRF). Max iterations, ? and C + are the SDRF parameters described in Algorithm 1.</figDesc><table><row><cell>Dataset</cell><cell>Dropout</cell><cell>Hidden depth</cell><cell>Hidden dimension</cell><cell>Learning rate</cell><cell>Weight decay</cell><cell>Max iterations</cell><cell>?</cell><cell>C +</cell></row><row><cell>Cornell</cell><cell>0.2411</cell><cell>1</cell><cell>128</cell><cell>0.0172</cell><cell>0.0125</cell><cell>135</cell><cell>130</cell><cell>0.25</cell></row><row><cell>Texas</cell><cell>0.5954</cell><cell>1</cell><cell>128</cell><cell>0.0278</cell><cell>0.0623</cell><cell>47</cell><cell>172</cell><cell>2.25</cell></row><row><cell>Wisconsin</cell><cell>0.6033</cell><cell>1</cell><cell>128</cell><cell>0.0295</cell><cell>0.1920</cell><cell>27</cell><cell>32</cell><cell>0.5</cell></row><row><cell>Chameleon</cell><cell>0.7265</cell><cell>1</cell><cell>128</cell><cell>0.0180</cell><cell>0.2101</cell><cell>832</cell><cell>77</cell><cell>3.35</cell></row><row><cell>Squirrel</cell><cell>0.7401</cell><cell>2</cell><cell>16</cell><cell>0.0189</cell><cell>0.2255</cell><cell>6157</cell><cell>178</cell><cell>0.5</cell></row><row><cell>Actor</cell><cell>0.6886</cell><cell>1</cell><cell>128</cell><cell>0.0095</cell><cell>0.0727</cell><cell>1010</cell><cell>69</cell><cell>1.22</cell></row><row><cell>Cora</cell><cell>0.3396</cell><cell>1</cell><cell>128</cell><cell>0.0244</cell><cell>0.1076</cell><cell>100</cell><cell>163</cell><cell>0.95</cell></row><row><cell>Citeseer</cell><cell>0.4103</cell><cell>1</cell><cell>64</cell><cell>0.0199</cell><cell>0.4551</cell><cell>84</cell><cell>180</cell><cell>0.22</cell></row><row><cell>Pubmed</cell><cell>0.3749</cell><cell>3</cell><cell>128</cell><cell>0.0112</cell><cell>0.0138</cell><cell>166</cell><cell cols="2">115 14.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Hyperparameters for GCN with the input graph made undirected followed by SDRF preprocessing (SDRF + Undirected).</figDesc><table><row><cell>Dataset</cell><cell>Dropout</cell><cell>Hidden depth</cell><cell>Hidden dimension</cell><cell>Learning rate</cell><cell>Weight decay</cell><cell>Max iterations</cell><cell>?</cell><cell>C +</cell></row><row><cell>Cornell</cell><cell>0.2911</cell><cell>1</cell><cell>128</cell><cell>0.0056</cell><cell>0.0336</cell><cell>126</cell><cell>145</cell><cell>0.88</cell></row><row><cell>Texas</cell><cell>0.2160</cell><cell>1</cell><cell>64</cell><cell>0.0229</cell><cell>0.0137</cell><cell>89</cell><cell>22</cell><cell>1.64</cell></row><row><cell>Wisconsin</cell><cell>0.2452</cell><cell>1</cell><cell>64</cell><cell>0.0113</cell><cell>0.1559</cell><cell>136</cell><cell>12</cell><cell>7.95</cell></row><row><cell>Chameleon</cell><cell>0.4886</cell><cell>1</cell><cell>32</cell><cell>0.0268</cell><cell>0.4056</cell><cell>2441</cell><cell>252</cell><cell>2.84</cell></row><row><cell>Squirrel</cell><cell>0.4249</cell><cell>1</cell><cell>64</cell><cell>0.0295</cell><cell>0.1397</cell><cell>787</cell><cell>43</cell><cell>17.19</cell></row><row><cell>Actor</cell><cell>0.6705</cell><cell>1</cell><cell>128</cell><cell>0.0115</cell><cell>0.0447</cell><cell>1141</cell><cell>44</cell><cell>11.17</cell></row><row><cell cols="4">G HARDWARE SPECIFICATIONS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Our experiments were performed on a server with the following specifications:</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Component</cell><cell></cell><cell cols="2">Specification</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Architecture</cell><cell></cell><cell>x86_64</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>CPU</cell><cell cols="5">40x Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz</cell><cell></cell><cell></cell></row><row><cell></cell><cell>GPU</cell><cell></cell><cell cols="4">4x GeForce RTX 3090 (24268MiB/GPU)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>RAM</cell><cell></cell><cell></cell><cell>126GB</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>OS</cell><cell></cell><cell cols="3">Ubuntu 20.04.2 LTS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We note that the over-squashing issue is different from the problem of under-reaching; the latter simply amounts to a MPNN failing to fully explore a graph when the depth is smaller than the diameter<ref type="bibr" target="#b1">(Barcel? et al., 2019)</ref>. The over-squashing phenomenon instead may occur even in deep GNNs with the number of layers larger than the graph diameter, as tested experimentally in<ref type="bibr" target="#b0">Alon &amp; Yahav (2021)</ref>.2  The Jacobian of a GNN-output was also used byXu et al. (2018)  to set a similarity score among nodes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We refer to the right hand side of equation 2 where the power of the normalized augmented adjacency is measuring the number of walks of distance r from i to s.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-11/www/wwkb/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research was supported in part by the EPSRC CDT in Modern Statistics and Statistical Machine Learning (EP/S023151/1) and the ERC Consolidator Grant No. 724228 (LEMAN). X.D. gratefully acknowledges support from the Oxford-Man Institute of Quantitative Finance and the EPSRC (EP/T023333/1).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Published as a conference paper at ICLR 2022 By using Lemma 11, we can bound the right hand side as</p><p>which completes the proof.</p><p>Remark 13. By inspection Ric(i, j) ? ?(i, j), with ?(i, j) as in Theorem 12. We have three cases:</p><p>(i) ?(i, j) = 2/d + 2/(d + s) ? 2 + 2| ? |/(d + s) + | ? |/d ? Ric(i, j), because Ric(i, j) takes into account the positive contribution of 4-cycles as well.</p><p>From the previous inequalities we derive</p><p>In this case we have</p><p>If Ric(i, j) ? k &gt; 0 for any edge i ? j, then there exists a polynomial P such that</p><p>Proof. This follows immediately from Theorem 2 and a Bishop-Gromov type of result for discrete Ollivier curvature on graphs Paeng (2012).</p><p>To address the proof of Theorem 4, we first need the Lemma below.</p><p>Proof. According to our convention we let d i = d and d j = d + s, for s ? 0. We also recall that Q j = S 1 (j)\( ? ? j ?{i}). If we multiply equation 3 by d j = d+s, we see that</p><p>Therefore, we conclude</p><p>Theorem 4. Consider a MPNN as in equation 1. Let i ? j with d i ? d j and assume that:</p><p>Published as a conference paper at ICLR 2022</p><p>where d avg (S) and d min (S) are the average and minimum degree on S, respectively.</p><p>Proof. Given a signal f : V ? R on the vertex set and U ? V , analogously to <ref type="bibr" target="#b8">Chung (2007)</ref>, we introduce the notation</p><p>Let us rewrite the new Cheeger constant h S,? as follows:</p><p>with ? S the characteristic function of the subset S, i.e. ? S (i) = 1 iff i ? S. Since the graph G is connected, we can bound h S,? from above as</p><p>.</p><p>It was proven in <ref type="bibr">(Chung, 2007, Lemma 5)</ref> that</p><p>By applying equation equation 20 to the bound for the Cheeger constant h S,? we finally see that</p><p>We also report an equivalent result, again relying on <ref type="bibr">(Chung, 2007, Lemma 5)</ref>.</p><p>Proof. Let k ? N. By modifying slightly the argument in <ref type="bibr">(Chung, 2007, Lemma 5)</ref>, we derive that</p><p>Therefore, we obtain vol(S k,? ) ? 1 k vol(S) 2 .</p><p>We then conclude that the complement of S k,? has volume greater or equal than vol(S)(1 ? (2k) ?1 ), which completes the proof.</p><p>Remark 18. The previous proposition shows that after sparsifying the personalized page rank operator R ? as suggested in <ref type="bibr" target="#b24">Klicpera et al. (2019)</ref> by setting entries below some threshold equal to zero, there will still be only few edges connecting different communities, once again highlighting that random-walk based methods are generally not suited to tackle the graph bottleneck.   </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the bottleneck of graph neural networks and its practical implications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=i80OPhOCVH2" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The logical expressiveness of graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Barcel?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Egor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Kostylev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Reutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Network geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marian</forename><surname>Boguna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Bonamassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manlio</forename><surname>De Domenico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Havlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitri</forename><surname>Krioukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M ?ngeles</forename><surname>Serrano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Physics</title>
		<imprint>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The anatomy of a large-scale hypertextual web search engine. Computer networks and ISDN systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13478</idno>
		<title level="m">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperbolic graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>R?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A lower bound for the smallest eigenvalue of the laplacian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Cheeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Problems in analysis</title>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Four proofs for the cheeger inequality and graph partition algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCM</title>
		<meeting>ICCM</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">378</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><forename type="middle">Chung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral graph theory. Number 92</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Diffusion maps. Applied and computational harmonic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="5" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Discrete and computational geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Forman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A general framework for adaptive processing of data structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="786" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A set of measures of centrality based on betweenness. Sociometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
			<biblScope unit="page" from="35" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning task-dependent distributed representations by backpropagation through structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Goller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kuchler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICNN</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. 2005 IEEE International Joint Conference on Neural Networks</title>
		<meeting>2005 IEEE International Joint Conference on Neural Networks</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The ricci flow on surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics and general relativity, Proceedings of the AMS-IMS-SIAM Joint Summer Research Conference in the Mathematical Sciences on Mathematics in General Relativity</title>
		<meeting><address><addrLine>Santa Cruz, California</addrLine></address></meeting>
		<imprint>
			<publisher>Amer. Math. Soc</publisher>
			<date type="published" when="1986" />
			<biblScope unit="page" from="237" to="262" />
		</imprint>
		<respStmt>
			<orgName>Univ. of California</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ollivier&apos;s ricci curvature, local clustering and curvature-dimension inequalities on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete &amp; Computational Geometry</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="300" to="322" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Differentiable graph module (dgm) graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anees</forename><surname>Kazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Cosmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04999</idno>
		<ptr target="https://arxiv.org/pdf/2002.04999.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Interpretable stability bounds for spectral graph filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Kenlay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorina</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09587</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations, ICLR &apos;17</title>
		<meeting>the 5th International Conference on Learning Representations, ICLR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Diffusion improves graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ricci curvature of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shing-Tung</forename><surname>Yau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tohoku Mathematical Journal, Second Series</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="605" to="627" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hyperbolic graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Kachites</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamal</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristie</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Motifnet: A motif-based graph convolutional network for directed graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Otness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Data Science Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentin</forename><surname>M?nch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13514</idno>
		<title level="m">Non-negative ollivier curvature on graphs, reverse poincar\&apos;e inequality, buser inequality, liouville property, harnack inequality and eigenvalue estimates</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umd</forename><surname>Edu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Network alignment by discrete ollivier-ricci flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Yao</forename><surname>Chien-Chun Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Graph Drawing and Network Visualization</title>
		<imprint>
			<biblScope unit="page" from="447" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1809.00320" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Yao</forename><surname>Chien-Chun Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Community detection on networks with ricci flow. Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Revisiting graph neural networks: All we have is low-pass filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takanori</forename><surname>Maehara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ricci curvature of metric spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathematique</title>
		<imprint>
			<biblScope unit="volume">345</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="643" to="646" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ricci curvature of markov chains on metric spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ollivier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Functional Analysis</title>
		<imprint>
			<biblScope unit="volume">256</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="810" to="864" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiji</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Volume and diameter of a graph and ollivier&apos;s ricci curvature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seong-Hun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Combinatorics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1808" to="1819" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grisha</forename><surname>Perelman</surname></persName>
		</author>
		<title level="m">Finite extinction time for the solutions to the ricci flow on certain three-manifolds. arXiv preprint math/0307245</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Sign: Scalable inception graph neural networks. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.11198" />
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multi-scale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Benedek Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Complex Networks</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Comparative analysis of two discretizations of ricci curvature for complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Areejit</forename><surname>Samal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Sreejith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiping</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Saucan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Graph neural networks in particle physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shlomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Roch</forename><surname>Vlimant</surname></persName>
		</author>
		<idno type="DOI">10.1088/2632-2153/abbf9a</idno>
		<ptr target="https://doi.org/10.1088/2632-2153/abbf9a" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning: Science and Technology</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">21001</biblScope>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Encoding labeled graphs by labeling RAAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Supervised neural networks for the classification of structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonina</forename><surname>Starita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="714" to="735" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Forman curvature for complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R P</forename><surname>Sreejith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthikeyan</forename><surname>Mohanraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Jost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Saucan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Areejit</forename><surname>Samal</surname></persName>
		</author>
		<idno type="DOI">10.1088/1742-5468/2016/06/063206</idno>
		<ptr target="http://dx.doi.org/10.1088/1742-5468/2016/06/063206" />
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Mechanics: Theory and Experiment</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">63206</biblScope>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/1557019.1557108</idno>
		<ptr target="https://doi.org/10.1145/1557019.1557108" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09</title>
		<meeting>the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;09<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJXMpikCZ" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dynamic graph CNN for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Coarse geometry of evolving networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Saucan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Jost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of complex networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="706" to="732" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
