<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MATE: Multi-view Attention for Table Transformer Efficiency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">Martin</forename><surname>Eisenschlos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maharshi</forename><surname>Gor</surname></persName>
							<email>mgor@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
							<email>thomas.mueller@symanto.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Symanto</forename><surname>Research</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valencia</forename><forename type="middle">,</forename><surname>Spain</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MATE: Multi-view Attention for Table Transformer Efficiency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work presents a sparse-attention Transformer architecture for modeling documents that contain large tables. Tables are ubiquitous on the web, and are rich in information. However, more than 20% of relational tables on the web have 20 or more rows <ref type="bibr" target="#b3">(Cafarella et al., 2008)</ref>, and these large tables present a challenge for current Transformer models, which are typically limited to 512 tokens. Here we propose MATE, a novel Transformer architecture designed to model the structure of web tables. MATE uses sparse attention in a way that allows heads to efficiently attend to either rows or columns in a table. This architecture scales linearly with respect to speed and memory, and can handle documents containing more than 8000 tokens with current accelerators. MATE also has a more appropriate inductive bias for tabular data, and sets a new state-of-the-art for three table reasoning datasets. For HY-BRIDQA (Chen et al., 2020b), a dataset that involves large documents containing tables, we improve the best prior result by 19 points.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Transformer architecture <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> is expensive to train and run at scale, especially for long sequences, due to the quadratic asymptotic complexity of self-attention. Although some work addresses this limitation <ref type="bibr" target="#b18">Kitaev et al., 2020;</ref><ref type="bibr" target="#b34">Zaheer et al., 2020)</ref>, there has been little prior work on scalable Transformer architectures for semi-structured text. 1 However, although some of the more widely used benchmark tasks involving semi-structured data have been restricted to moderate size tables, many semi-structured documents are large: more than 20% of relational tables on the web have 20 or more rows (Cafarella et al., <ref type="figure">Figure 1</ref>: Sparse self-attention heads on tables in MATE are of two classes: Row heads attend to tokens inside cells in the same row, as well as the query. Column heads attend to tokens in the same column and in the query. Query tokens attend to all other tokens. 2008), and would pose a problem for typical Transformer models.</p><p>Here we study how efficient implementations for transformers can be tailored to semi-structured data. <ref type="figure">Figure 1</ref> highlights our main motivation through an example: to obtain a contextual representation of a cell in a table, it is unlikely that the information in a completely different row and column is needed.</p><p>We propose the MATE architecture 2 (Section 3), which allows each attention head to reorder the input so as to traverse the data by multiple points of view, namely column or row-wise ( <ref type="figure" target="#fig_0">Figure 2</ref>). This allows each head to have its own data-dependent notion of locality, which enables the use of sparse attention in an efficient and context-aware way.</p><p>This work focuses on question answering (QA) and entailment tasks on tables. While we apply our model to several such tasks (see section 6), HYBRIDQA <ref type="bibr" target="#b7">(Chen et al., 2020b)</ref> is particularly interesting, as it requires processing tables jointly with long passages associated with entities mentioned in the table, yielding large documents that may not fit in standard Transformer models. Each attention head reorders the tokens by either column or row index and then applies a windowed attention mechanism. This figure omits the global section that attends to and from all other tokens. Since column/row order can be pre-computed, the method is linear for a constant block size.</p><p>Overall, our contributions are the following: i) We show that table transformers naturally focus attention according to rows and columns, and that constraining attention to enforce this improves accuracy on three table reasoning tasks, yielding new state-of-the-art results in SQA and TABFACT.</p><p>ii) We introduce MATE, a novel transformer architecture that exploits table structure to allow running training and inference in longer sequences. Unlike traditional self-attention, MATE scales linearly in the sequence length.</p><p>iii) We propose POINTR (Section 4), a novel two-phase framework that exploits MATE to tackle large-scale QA tasks, like HYBRIDQA, that require multi-hop reasoning over tabular and textual data. We improve the state-of-the-art by 19 points.</p><p>All the code is available as open source. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Transformers for tabular data Traditionally, tasks involving tables were tackled by searching for logical forms in a semantic parsing setting. More recently Transformers <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> have been used to train end-to-end models on tabular data as well <ref type="bibr" target="#b6">(Chen et al., 2020a)</ref>. For example, TAPAS <ref type="bibr" target="#b14">(Herzig et al., 2020)</ref> relies on Transformerbased masked language model pre-training and special row and column embeddings to encode the table structure. <ref type="bibr" target="#b5">Chen et al. (2021)</ref> use a variant of ETC  on an open-domain version of HYBRIDQA to read and choose an answer span from multiple candidate passages and cells, but the proposed model does not jointly process the 3 github.com/google-research/tapas full table with passages. In order to overcome the limitations on sequence length  propose heuristic column selection techniques, and they also propose pre-training on synthetic data.  propose a model based cell selection technique that is differentiable and trained end-to-end together with the main task model. Our approach is orthogonal to these methods, and can be usefully combined with them, as shown in <ref type="table" target="#tab_2">Table 4</ref>.</p><p>Recently, <ref type="bibr" target="#b35">Zhang et al. (2020)</ref> proposed SAT, which uses an attention mask to restrict attention to tokens in the same row and same column. SAT also computes an additional histogram row appended at the bottom of the table and encodes the table content as text only (unlike TAPAS). The proposed method is not head dependent as ours is, which prevents it from being implemented efficiently to allow scaling to larger sequence lengths. Controlling for model size and pre-training for a fair comparison, we show that our model is both faster <ref type="table" target="#tab_2">(Table 4</ref>) and more accurate <ref type="table" target="#tab_5">(Table 6)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>than SAT.</head><p>Efficient Transformers There is prior work that tries to improve the asymptotic complexity of the self-attention mechanism in transformers. <ref type="bibr" target="#b26">Tay et al. (2020)</ref> review the different methods and cluster them based on the nature of the approach. We cover some of the techniques below and show a theoretical complexity comparison in <ref type="table">Table 1</ref>.</p><p>The LINFORMER model  uses learned projections to reduce the sequence length axis of the keys and value vectors to a fixed length. The projections are then anchored to a specific input length which makes adapting the sequence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Complexity</head><p>Class</p><formula xml:id="formula_0">Transformer-XL O(n 2 ) RC REFORMER O(n log n) LP LINFORMER O(n) LR BIGBIRD O(ng + nb) FP+RP+M ETC O(ng + nb) FP+M MATE (Ours)</formula><p>O(ng + nb) FP <ref type="table">Table 1</ref>: Comparison of transformer models following <ref type="bibr" target="#b26">Tay et al. (2020)</ref>. Class abbreviations include: FP = Fixed Patterns, RP = Random Patterns, M = Memory, LP = Learnable Pattern, LR = Low Rank and RC = Recurrence. The block size for local attention is denoted as b and g the size of the global memory. For our MATE model, a n log n sorting step can be pre-computed before training or inference for known tables so it is omitted.</p><p>length during pre-training and fine-tuning challenging, and makes the model more sensitive to position offsets in sequences of input tokens. REFORMER <ref type="bibr" target="#b18">(Kitaev et al., 2020)</ref> uses locality sensitive hashing to reorder the input tokens at every layer in such a way that similar contextual embeddings have a higher chance of being clustered together. We instead rely on the input data structure to define ways to cluster the tokens. Although the limitation can be circumvented by adapting the proposed architecture, REFORMER was originally defined for auto-regressive training.  introduce ETC, a framework for global memory and local sparse attention, and use the mechanism of relative positional attention <ref type="bibr" target="#b8">(Dai et al., 2019)</ref> to encode hierarchy. ETC was applied to large document tasks such as Natural Questions <ref type="bibr" target="#b20">(Kwiatkowski et al., 2019)</ref>. The method does not allow different dynamic or static data re-ordering. In practice, we have observed that the use of relative positional attention introduces a large overhead during training. BIGBIRD <ref type="bibr" target="#b34">(Zaheer et al., 2020)</ref> presents a similar approach with the addition of attention to random tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The MATE model</head><p>Following TAPAS <ref type="bibr" target="#b14">(Herzig et al., 2020)</ref>, the transformer input in MATE, for each table-QA example, is the query and the table, tokenized and flattened, separated by a [SEP] token, and prefixed by a <ref type="bibr">[CLS]</ref>. Generally the table comprises most of the the input. We use the same row, column and rank embeddings as TAPAS.</p><p>To restrict attention between the tokens in the table, we propose having some attention heads limited to attending between tokens in the same row (plus the non-table tokens), and likewise for columns. We call these row heads and column heads respectively. In both cases, we allow attention to and from all the non-table tokens. Formally, if X ? R d?n is the input tensor for a Transformer layer with sequence length n, the k-th position of the output of the i-th attention head is:</p><formula xml:id="formula_1">Head i k (X) = W i V X A i k ? W i K X A i k W i Q X k where W i Q , W i K , W i V ? R m?d</formula><p>are query, key and value projections respectively, ? is the softmax operator, and A i k ? {1, ? ? ? , n} represents the set of tokens that position k can attend to, also known as the attention pattern. Here X A i k denotes gathering from X only the indexes in A i k . When A i k contains all positions (except padding) for all heads i and token index k then we are in the standard dense transformer case. For a token position k, we define r k , c k ? N 0 the row and column number, which is set to 0 if k belongs to the query set Q: the set of token positions in the query text including [CLS] and [SEP] tokens.</p><p>In MATE, we use two types of attention patterns. The first h r ? 0 heads are row heads and the remaining h c are column heads:</p><formula xml:id="formula_2">A i k = ? ? ? ? ? {1, ? ? ? , n} if k ? Q, else Q ? {j : r j = r k } if 1 ? i ? h r Q ? {j : c j = c k } otherwise.</formula><p>One possible implementation of this is an attention mask that selectively sets elements in the attention matrix to zero. (Similar masks are used for padding tokens, or auto-regressive text generation.) The ratio of row and column heads is a hyperparameter but empirically we found a 1 : 1 ratio to work well. In Section 6 we show that attention masking improves accuracy on four table-related tasks. We attribute these improvements to better inductive bias, and support this in Section 7 showing that full attention models learn to approximate this behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Efficient implementation</head><p>Although row-and column-related attention masking improves accuracy, it does not improve Transformer efficiency-despite the restricted attention, the Transformer still uses quadratic memory and time. We thus also present an approximation of row and column heads that can be implemented more efficiently. Inspired by , the idea is to divide the input into a global part of length G that attends to and from everything, and a local (typically longer) part that attends only to the global section and some radius R around each token in the sequence. ETC does this based on a fixed token order. However, the key insight used in MATE is that the notion of locality can be configured differently for each head: one does not need to choose a specific traversal order for tokens ahead of time, but instead tokens can be ordered in a datadependent (but deterministic) way. In particular, row heads can order the input according to a row order traversal of the table, and column heads can use a column order traversal. The architecture is shown in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>After each head has ordered its input we split off the first G tokens and group the rest in evenly sized buckets of length R. By reshaping the input matrix in the self-attention layer to have R as the last dimension, one can compute attention scores from each bucket to itself, or similarly from each bucket to an adjacent one. Attention is further restricted with a mask to ensure row heads and column heads don't attend across rows and columns respectively. See model implementation details in Appendix D. When G is large enough to contain the question part of the input and R is large enough to fit an entire column or row, then the efficient implementation matches the mask-based one.</p><p>As observed in , asymptotic complexity improvements often do not materialize for small sequence lengths, given the overhead of tensor reshaping and reordering. The exact breakeven point will depend on several factors, including accelerator type and size as well as batch size. In the experiments below the best of the two functionally equivalent implementations of MATE is chosen for each use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Compatibility with BERT weights</head><p>The sparse attention mechanism of MATE adds no additional parameters. As a consequence, a MATE checkpoint is compatible with any BERT or TAPAS pre-trained checkpoint. Following <ref type="bibr" target="#b14">Herzig et al. (2020)</ref> we obtained best results running the same masked language model pre-training used in TAPAS with the same data but using the sparse attention mask of MATE.</p><p>For sequence lengths longer than 512 tokens, we reset the index of the positional embeddings at <ref type="table" target="#tab_2">Split  Train  Dev  Test  Total   In-Passage 35,215 2,025 20,45 39,285  In-Table  26,803 1,349 1,346 29,498  Missing  664  92  72  828  Total  62,682 3,466 3,463 69,611   Table 2</ref>: Statistics for HYBRIDQA. In- <ref type="table">Table and</ref> In-Passage groups mark the location of the answer. Missing denotes answers that do not match any span and may require complex computations.</p><p>the beginning of each cell. This method removes the need to learn positional embeddings for larger indexes as the maximum sequence length grows while avoiding the large computational cost of relative positional embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Universal approximators</head><p>Yun et al. <ref type="formula">(2020a)</ref> showed that Transformers are universal approximators for any continuous sequence-to-sequence function, given sufficient layers. This result was further extended by <ref type="bibr" target="#b33">Yun et al. (2020b)</ref>; <ref type="bibr" target="#b34">Zaheer et al. (2020)</ref> to some Sparse Transformers under reasonable assumptions. However, prior work limits itself to the case of a single attention pattern per layer, whereas MATE uses different attention patterns depending on the head. We will show that MATE is also a universal approximator for sequence to sequence functions.</p><p>Formally, let F be the class of continuous functions f : D ? R d?n ? R d?n with D compact, with the p-norm || ? || p . Let T MATE be any family of transformer models with a fixed set of hyperparameters (number of heads, hidden dimension, etc.) but with an arbitrary number of layers. Then we have the following result. Theorem 1. If the number of heads is at least 3 and the hidden size of the feed forward layer is at least 4, then for any f ? F and ? R + there exist?</p><formula xml:id="formula_3">f ? T MATE such that ||f ? f || p &lt; .</formula><p>See the Appendix C for a detailed proof, which relies on the fact that 3 heads will guarantee at least two heads of the same type. The problem can then be reduced to the results of <ref type="bibr" target="#b33">Yun et al. (2020b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The POINTR architecture</head><p>Many standard table QA datasets <ref type="bibr" target="#b23">(Pasupat and Liang, 2015;</ref><ref type="bibr" target="#b6">Chen et al., 2020a;</ref><ref type="bibr" target="#b15">Iyyer et al., 2017)</ref>, perhaps by design, use tables that can be limited to 512 tokens. Recently, more datasets <ref type="bibr" target="#b17">(Kardas et al., 2020;</ref><ref type="bibr" target="#b25">Talmor et al., 2021)</ref> requiring parsing larger semi-structured documents have been released.  <ref type="figure">Figure 3</ref>: An example from the HYBRIDQA dataset processed by POINTR. The first paragraph in the Wikipedia page for each underlined entity was available to the dataset authors. We expand the text in the cells with this descriptions for the top-k most relevant sentences, as shown in the second table, and train a model to find the cell containing or linking to the answer (marked here with a ). The goal is to provide the model with all the context needed to locate the answer. A second model extracts a span from the selected cell content and linked text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Input</head><p>Among them, we focus on HYBRIDQA <ref type="bibr" target="#b7">(Chen et al., 2020b)</ref>. It uses Wikipedia tables with entity links, with answers taken from either a cell or a hyperlinked paragraph. Dataset statistics are shown in <ref type="table">Table 2</ref>. Each question contains a table with on average 70 cells and 44 linked entities. Each entity is represented by the first 12 sentences of the Wikipedia description, averaging 100 tokens. The answer is often a span extracted from the table or paragraphs but the dataset has no ground truth annotations on how the span was obtained, leaving around 50% of ambiguous examples where more than one answer-sources are possible. The total required number of word pieces accounting for the table, question and entity descriptions grows to more than 11, 000 if one intends to cover more than 90% of the examples, going well beyond the limit of traditional transformers.</p><p>To apply sparse transformers to the HYBRIDQA task, we propose POINTR, a two stage framework in a somewhat similar fashion to open domain QA pipelines <ref type="bibr" target="#b4">(Chen et al., 2017;</ref>. We expand the cell content by appending the descriptions of its linked entities. The two stages of POINTR correspond to (Point)ing to the correct expanded cell and then (R)eading a span from it. See <ref type="figure">Figure 3</ref> for an example. Full set-up details are discussed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">POINTR: Cell Selection Stage</head><p>In the first stage we train a cell selection model using MATE whose objective is to select the expanded cell that contains the answer. MATE accepts the full table as input; therefore, expanding all the cells with their respective passages is impractical. Instead, we consider the top-k sentences in the entity descriptions for expansion, using a TF-IDF metric against the query. Using k = 5, we can fit 97% of the examples in 2048 tokens; for the remaining examples, we truncate the longest cells uniformly until they fit in the budget.</p><p>The logit score S for each cell c is obtained by mean-pooling the logits for each of the tokens t inside it, which are in turn the result of applying a single linear layer to the contextual representation of each token when applying MATE to the query q and the expanded table e.</p><formula xml:id="formula_4">S(t) = MLP(MATE(q, e)[t]) S(c) = avg t?c S(t) P (c) = exp(S(c)) c ?e exp(S(c ))</formula><p>We use cross entropy loss for training the model to select expanded cells that contain the answer span. Even though the correct span may appear in multiple cells or passages, in practice many of these do so only by chance and do not correspond to a reasoning path consistent with the question asked. In <ref type="figure">Figure 3</ref> for instance, there could be other British divers but we are only interested in selecting the cell marked with a star symbol ( ). In order to handle these cases we rely on Maximum Marginal Likelihood (MML) <ref type="bibr" target="#b2">Berant et al., 2013)</ref>. As shown by <ref type="bibr" target="#b11">Guu et al. (2017)</ref> MML can be interpreted as using the online model predictions (without gradients) to compute a soft label distribution over candidates. For an input query x, and a set C of candidate cells, the loss is:</p><formula xml:id="formula_5">L(?, x, C) = z?C ?q(z) log p ? (z|x)</formula><p>with q(z) = p ? (z|x, z ? C) the probability distribution given by the model restricted to candidate cells containing the answer span, taken here as a constant with zero gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">POINTR: Passage Reading Stage</head><p>In the second stage we develop a span selection model that reads the answer from a single expanded cell selected by the POINTR Cell Selector. In order to construct the expanded cell for each example, we concatenate the cell content with all the sentences of the linked entities and keep the first 512 tokens. Following various recent neural machine reading works <ref type="bibr" target="#b4">(Chen et al., 2017;</ref><ref type="bibr" target="#b13">Herzig et al., 2021)</ref>, we fine-tune a pre-trained BERT-uncased-large model  that attempts to predict a text span from the text in a given table cell c (and its linked paragraphs) and the input query q. We compute a span representation as the concatenation of the contextual embeddings of the first and last token in a span s <ref type="table" target="#tab_5">TABFACT WIKITQ  SQA   Examples  118,275  22,033 17,553  Tables  16,573  2,108  982   Table 3</ref>: Statistics for SQA, WIKITQ and TABFACT. and score it using a multi-layer perceptron:</p><formula xml:id="formula_6">h start = BERT r (q, c)[START(s)] h end = BERT r (q, c)[END(s)] S read (q, c) = MLP([h start , h end ])</formula><p>A softmax is computed over valid spans in the input and the model is trained with cross entropy loss. If the span-text appears multiple times in a cell we consider only the first appearance. To compute EM and F1 scores during inference, we evaluate the trained reader on the highest ranked cell output predictions of the POINTR Cell Selector using the official evaluation script.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>We begin by comparing the performance of MATE on HYBRIDQA to other existing systems. We focus on prior efficient transformers to compare the benefits of the table-specific sparsity. We follow <ref type="bibr" target="#b14">Herzig et al. (2020)</ref>;  in reporting error bars with the interquartile range.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>The first baselines for HYBRIDQA are <ref type="table" target="#tab_3">Table-</ref>Only and Passage-Only, as defined in <ref type="bibr" target="#b7">Chen et al. (2020b)</ref>. Each uses only the part of the input indicated in the name but not both at the same time. Next, the HYBRIDER model from the same authors, consists of four stages: entity linking, cell ranking, cell hopping and finally a reading comprehension stage, equivalent to our final stage. The first three stages are equivalent to our single cell selection stage; hence, we use their reported error rates to estimate the retrieval rate. The simpler approach enabled by MATE avoids error propagation and yields improved results.</p><p>We also consider two recent efficient transformer architectures as alternatives for the POINTR Cell Selector, one based on LINFORMER  and one based on ETC . In both cases we preserve the row, column and rank embeddings introduced by <ref type="bibr" target="#b14">Herzig et al. (2020)</ref>. LINFORMER learns a projection matrix that reduces the sequence length dimension of the keys and values tensor to a fixed length of 256 (which performed better than 128 and 512 in our tests.) ETC is a general architecture which requires some choices to be made about how to allocate global memory and local attention <ref type="bibr" target="#b8">(Dai et al., 2019)</ref> Here we use a 256-sized global memory to summarize the content of each cell, by assigning each token in the first half of the global memory to a row, and each token in the second half to a column. Tokens in the input use a special relative positional value to mark when they are interacting with their corresponding global row or column memory position. We will refer to this model as TABLEETC.</p><p>Finally we consider two non-efficient models: A simple TAPAS model without any sparse mask, and an SAT <ref type="bibr" target="#b35">(Zhang et al., 2020)</ref> model pretrained on the same MLM task as TAPAS for a fair comparison. For the cell selection task TAPAS obtains similar results to MATE, but both TAPAS and SAT lack the efficiency improvements of MATE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Other datasets</head><p>We also apply MATE to three other datasets involving tables to demonstrate that the sparse attention bias yields stronger table reasoning models. SQA <ref type="bibr" target="#b15">(Iyyer et al., 2017</ref>) is a sequential QA task, WIK-ITQ <ref type="bibr" target="#b23">(Pasupat and Liang, 2015)</ref> is a QA task that sometimes also requires aggregation of table cells, and TABFACT <ref type="bibr" target="#b6">(Chen et al., 2020a</ref>) is a binary entailment task. See <ref type="table">Table 3</ref> for dataset statistics. We evaluate with and without using the intermediate pre-training tasks (CS) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In <ref type="figure" target="#fig_1">Figure 4</ref> we compare inference speed of different models as we increase the sequence length. Similar results showing number of FLOPS and memory usage are in Appendix A. The linear scaling of LINFORMER and the linear-time version MATE can be seen clearly. Although LINFORMER has a slightly smaller linear constant, the pre-training is 6 times slower, as unlike the other models, LIN-FORMER pretraining must be done at the final sequence length of 2048. <ref type="table">Table 5</ref> shows the end-to-end results of our system using POINTR with MATE on HYBRIDQA, compared to the previous state-of-the-art as well as the other efficient transformer baselines from Section 5. MATE outperforms the previous SOTA HYBRIDER by over 19 points, and LINFORMER, the next best efficient-transformer system, by over   2.5 points, for both exact-match accuracy and F1. We also applied MATE to three tasks involving table reasoning over shorter sequences. In Table 4 we see that MATE provides improvements in accuracy, which we attribute to a better inductive bias for tabular data. When combining MATE with Counterfactual + Synthetic intermediate pretraining (CS)  we often get even better results. For TABFACT and SQA we improve over the previous state-of-the-art. For WIKITQ we close the gap with the best published system TABERT <ref type="bibr" target="#b31">(Yin et al., 2020</ref>) (51.8 mean test accuracy), which relies on traditional semantic parsing, instead of an end-to-end approach. Dev results show a similar trend and can be found in Appendix B. No special tuning was done on these models-we used the same hyper-parameters as the open source release of TAPAS. Test In- <ref type="table" target="#tab_3">Table  In-Passage  Total  In-Table  In-Passage  Total  EM  F1  EM  F1  EM  F1  EM  F1  EM  F1</ref> EM F1  <ref type="table">Table 5</ref>: Results of different large transformer models on HYBRIDQA. In- <ref type="table">Table and</ref> In-Passage subsets refer to the location of the answer. For dev, we report errors over 5 runs using half the interquartile range. Since the test set is hidden and hosted online, we report the results corresponding to the model with the median total EM score on dev.</p><p>homa athlete drafted in? (G)old answer: "second", (P)redicted: "second round"). While around 30% of such misses involved numerical answers (eg: "1" vs "one"), the predictions for the rest of them prominently (58% of the near misses) either had redundant or were missing auxiliary words (e.g., Q: What climate is the northern part of the home country of Tommy Douglas? G: "Arctic" P: "Arctic climate"). The inconsistency in the gold-answer format and unavailability of multiple gold answers are potential causes here.</p><p>Among the non near-misses, the majority predictions were either numerically incorrect, or were referencing an incorrect entity but still in an relevant context-especially the questions involving more than 2 hops. (e.g. Q: In which sport has an award been given every three years since the first tournament held in 1948-1949? G: "Badminton", P: "Thomas Cup"). Reassuringly, for a huge majority (&gt; 80%), the entity type of the predicted answer (person, date, place, etc.) matches the type of the gold answer. The observed errors suggest potential gains by improving the entity <ref type="bibr" target="#b30">(Xiong et al., 2020)</ref> and numerical <ref type="bibr" target="#b1">(Andor et al., 2019)</ref> reasoning skills.</p><p>Ablation Study In <ref type="table" target="#tab_5">Table 6</ref> we compare architectures for cell selection on HYBRIDQA. Hits@k corresponds to whether a cell containing an answer span was among the top-k retrieved candidates. As an ablation, we remove the sparse pre-training and try using only row/column heads. We observe a drop also when we discard the ambiguous examples from training instead of having MML to deal with them. Unlike the other datasets, TAPAS shows comparable results to MATE, but without any of the theoretical and practical improvements.  Observed Attention Sparsity Since we are interested to motivate our choices on how to sparsify the attention matrix, we can inspect the magnitude of attention connections in a trained dense TAPAS model for table question answering. It is important to note that in this context we are not measuring attention as an explanation method <ref type="bibr" target="#b16">(Jain and Wallace, 2019;</ref><ref type="bibr" target="#b29">Wiegreffe and Pinter, 2019)</ref>. Instead we are treating the attention matrix in the fashion of magnitude based pruning techniques <ref type="bibr" target="#b12">(Han et al., 2015;</ref><ref type="bibr" target="#b24">See et al., 2016)</ref>, and simply consider between which pairs of tokens the scores are concentrated. Given a token in the input we can aggregate the attention weights flowing from it depending on the position of the target token in the input <ref type="bibr">(CLS token, question, header, or table)</ref> and whether the source and target tokens are in the same column or row, whenever it makes sense. We average scores across all tokens, heads, layers and examples in the development set. As a baseline, we also compare against the output of the same process when using  a uniform attention matrix, discarding padding. In <ref type="table" target="#tab_7">Table 7</ref>, we show the obtained statistics considering only table tokens as a source. We use the WIKITQ development set as a reference. While we see that 23% of the attention weights are looking at tokens in different columns and rows, this is only about one third of the baseline number one would obtain with a uniform attention matrix. This effect corroborates the approach taken in MATE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduce MATE, a novel method for efficiently restricting the attention flow in Transformers applied to Tabular data. We show in both theory and practice that the method improves inductive bias and allows scaling training to larger sequence lengths as a result of linear complexity. We improve the state-of-the-art on TABFACT, SQA and HYBRIDQA, the last one by 19 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>Although one outcome of this research is more efficient Transformers for table data, it remains true that large Transformer models can be expensive to train from scratch, so experiments of this sort can incur high monetary cost and carbon emissions. This cost was reduced by conducting some experiments at relatively smaller scale, e.g. the results of <ref type="figure" target="#fig_1">Figure 4</ref>. To further attenuate the impact of this work, we plan release all the models that we trained so that other researchers can reproduce and extend our work without re-training.</p><p>All human annotations required for the error analysis (Section 7) are provided by authors, and hence a concern of fair compensation for annotators did not arise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experimental setup</head><p>A.1 Pre-training Pre-training for MATE was performed with constrained attention with a masked language modeling objective applied to the corpus of tables and text extracted by <ref type="bibr" target="#b14">Herzig et al. (2020)</ref>. With a sequence length of 128 and batch size of 512, the total training of 1 million steps took 2 days.</p><p>In contrast, for LINFORMER the pre-training was done with a sequence length of 2048 and a batch size of 128, and the total training took 12 days for 2 million steps. For TABLEETC we also pre-trained for 2 million steps but the batch size had to be lowered to 32. In all cases the hardware used was a 32 core Cloud TPUs V3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Fine-tuning</head><p>For all experiments we use Large models over 5 random seeds and report the median results. Errors are estimated with half the interquartile range. For TABFACT, SQA and WIKITQ we keep the original hyper-parameters used in TAPAS and provided in the open source release. In <ref type="figure" target="#fig_3">Figure 5</ref> we show the floating point operation count of the different Transformer models as we increase the sequence length, as extracted from the execution graph. We also measure the memory doing CPU inference in <ref type="figure" target="#fig_4">figure 6</ref>. The linear scaling of LINFORMER and MATE can be observed. No additional tuning or sweep was done to obtain the published results. We set the global size G to 116 and the radius R for local attention to 42. We use an Adam optimizer with weight decay with the same configuration as BERT. The number of parameters for MATE is the same as for BERT: 340M for Large models and 110M for Base Models.</p><p>In the HYBRIDQA cell selection stage, we use a batch size of 128 and train for 80, 000 steps and a sequence length of 2048. Training requires 1 day. We clip the gradients to 10 and use a learning rate of 1 ? 10 ?5 under a 5% warm-up schedule. For the  reader stage use a learning rate of 5 ? 10 ?5 under a 1% warm-up schedule, a batch size of 512 and train for 25, 000 steps, which takes around 6 hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Development set results for SQA, WIKITQ and TABFACT</head><p>We show in <ref type="table" target="#tab_9">Table 8</ref> the dev set results for all datasets we attempted, which show consistent results with the test set reported in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Theorem 1</head><p>In this section we discuss the proof that MATE are universal approximators of sequence functions.</p><p>Theorem. If the number of heads is at least 3 and  the hidden size of the feed forward layer is at least 4, then for any f ? F and ? R + there exist? f ? T MATE such that ||f ? f || p &lt; Proof. When the number of heads is at least 3, there are at least 2 heads of the same type. Fixing those two heads, we may restrict the value of the projection weights W V to be 0 for the rest of the heads. This is equivalent to having only those two heads with the same attention pattern to begin with. This restriction only makes the family of functions modelled by MATE smaller. In a similar way, we can assume that the hidden size of the feed-forward layer is exactly 4 and that the head size is 1.</p><p>Note that the attention pattern of the two heads, regardless of its type contains a token (the first one) which attends to and from every other token. We also have that every token attends to itself. Then Assumption 1 of <ref type="bibr" target="#b33">Yun et al. (2020b)</ref> is satisfied. Hence we rely on Theorem 1 of <ref type="bibr" target="#b33">Yun et al. (2020b)</ref>, which asserts that sparse transformers with 2 heads, hidden size 4 and head size 1 are universal approximators, which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D TensorFlow Implementation</head><p>In <ref type="figure">figure 7</ref> we provide an approximate implementation of MATE in the TensorFlow library. For the sake of simplicity we omit how attention is masked between neighbor buckets for tokens in difference columns or rows. We also omit the tensor manipulation steps to reorder and reshape the sequence into equally sized buckets to compute attention across consecutive buckets. The full implementation will be part of the open source release. <ref type="figure">Figure 7</ref>: Implementation of MATE in TensorFlow. The creation of MultiViewEmbedding is ommited and relies on tf.gather for ordering the input. We also omit the use of the input mask and column and row index to further mask the sparse attention matrix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Efficient implementation for MATE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of inference speed on a cloud VM with 64GB. At a sequence length of 2048, MATE is nearly twice as fast as TAPAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We randomly sample 100 incorrectly answered examples from the development set. 55% of the examples have lexical near-misses-predictions have the correct information, but have slightly different formatting (e.g. (Q)uestion: In what round was the Okla-Model Dev</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Comparison of inference FLOPS obtained from execution graph. While TABLEETC is linear, relative attention adds a high computation cost so keep it out of range in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of the peak memory usage during CPU inference shows the linear asymptotic curve of the memory footprint of MATE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table :</head><label>:</label><figDesc>Entity Descriptions (some are omitted for space):? Rubens Barrichello is a Brazilian racing driver who competed in Formula One between 1993 and 2011.? Jenson Alexander Lyons Button MBE is a British racing driver. He won the 2009 F1 World Championship.? Scuderia Ferrari S.p.A. is the racing division of luxury Italian auto manufacturer Ferrari. Ferrari supplied cars complete with V8 engines for the A1 Grand Prix series from the 2004 season.? Honda Motor Company, Ltd is a Japanese public multinational conglomerate manufacturer of automobiles, motorcycles, and power equipment, headquartered in Minato, Tokyo, Japan. . . . The driver who finished in position 4 in the 2004 Grand Prix was of what nationality? British ExpandedTable with k Description Sentences Most Similar to the Question:</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Pos No Driver</cell><cell></cell><cell>Constructor Time</cell><cell>Gap</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell cols="2">Rubens Barrichello</cell><cell>Ferrari</cell><cell>1:10.223 -</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>1</cell><cell cols="3">Michael Schumacher Ferrari</cell><cell>1:10.400</cell><cell>+0.177</cell></row><row><cell></cell><cell></cell><cell>3</cell><cell>10</cell><cell>Takuma Sato</cell><cell></cell><cell>Honda</cell><cell>1:10.601</cell><cell>+0.378</cell></row><row><cell></cell><cell></cell><cell>4</cell><cell>9</cell><cell>Jenson Button</cell><cell></cell><cell>Honda</cell><cell>1:10.820</cell><cell>+0.597</cell></row><row><cell cols="2">Question:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Pos No Driver</cell><cell></cell><cell></cell><cell cols="2">Constructor</cell><cell>Time</cell><cell>Gap</cell></row><row><cell>1</cell><cell>2</cell><cell cols="2">Rubens Barrichello</cell><cell></cell><cell cols="2">Ferrari (Ferrari supplied cars complete with V8 engines</cell><cell>1:10.223 -</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">for the A1 Grand Prix series from the 2004 season.)</cell></row><row><cell>2</cell><cell>1</cell><cell cols="2">Michael Schumacher</cell><cell></cell><cell cols="2">Ferrari (Ferrari supplied cars complete with V8 engines</cell><cell>1:10.400</cell><cell>+0.177</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">for the A1 Grand Prix series from the 2004 season.)</cell></row><row><cell>3</cell><cell>10</cell><cell>Takuma Sato</cell><cell></cell><cell></cell><cell>Honda</cell><cell>1:10.601</cell><cell>+0.378</cell></row><row><cell>4</cell><cell>9</cell><cell cols="3">Jenson Button (Jenson Alexander Lyons Button</cell><cell>Honda</cell><cell>1:10.820</cell><cell>+0.597</cell></row><row><cell></cell><cell></cell><cell cols="3">MBE is a British racing driver.)</cell><cell></cell></row><row><cell cols="4">POINTR inference pipeline:</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">Expand table cells content</cell><cell cols="2">Cell selection model picks a</cell><cell>Keep entity descriptions and</cell><cell>Reader model extracts</cell></row><row><cell></cell><cell cols="3">with entity descriptions</cell><cell cols="2">cell that contains answer</cell><cell>content of the selected cell</cell><cell>a span with the answer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Test results of using MATE on other table parsing datasets show improvements due to the sparse attention mechanism. Using Counterfactual + Synthethic pretraining (CS) in combination with MATE achieves state-of-the-art in SQA and TABFACT. Errors are estimated with half the interquartile range over 5 runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table -</head><label>-</label><figDesc>?0.33 71.8 ?0.28 60.3 ?0.11 69.2 ?0.04 61.2 ?0.29 68.7 ?0.31 64.6 70.1 59.6 68.5 60.1 67.4 POINTR + TAPAS 68.1 ?0.33 73.9 ?0.37 62.9 ?0.25 72.0 ?0.21 63.3 ?0.25 70.8 ?0.12 67.?1.26 42.4 ?1.13 37.8 ?1.19 45.3 ?1.53 36.1 ?1.30 42.9 ?1.36 35.8 40.7 38.8 45.7 36.6 42.6 POINTR + LINFORMER 65.5 ?0.78 71.1 ?0.55 59.4 ?0.59 69.0 ?0.68 60.8 ?0.68 68.4 ?0.63 66.1 71.7 58.9 67.8 60.2 67.6 POINTR + MATE 68.6 ?0.37 74.2 ?0.26 62.8 ?0.25 71.9 ?0.20 63.4 ?0.16 71.0 ?0.17 66.9 72.3 62.8 71.9 62.8 70.2</figDesc><table><row><cell>Only</cell><cell>14.7</cell><cell>19.1</cell><cell>2.4</cell><cell>4.5</cell><cell>8.4</cell><cell>12.1</cell><cell cols="2">14.2 18.8</cell><cell>2.6</cell><cell>4.7</cell><cell>8.3</cell><cell>11.7</cell></row><row><cell>Passage-Only</cell><cell>9.2</cell><cell>13.5</cell><cell>26.1</cell><cell>32.4</cell><cell>19.5</cell><cell>25.1</cell><cell>8.9</cell><cell cols="5">13.8 25.5 32.0 19.1 25.0</cell></row><row><cell>HYBRIDER (? =0.8)</cell><cell>54.3</cell><cell>61.4</cell><cell>39.1</cell><cell>45.7</cell><cell>44.0</cell><cell>50.7</cell><cell cols="6">56.2 63.3 37.5 44.4 43.8 50.6</cell></row><row><cell>POINTR + SAT</cell><cell cols="12">66.5 8 73.2 62.0 70.9 62.7 70.0</cell></row><row><cell cols="2">POINTR +TABLEETC 36.0 Human</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">88.2 93.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Retrieval results over HYBRIDQA (dev set) for models used in POINTR Cell Selection stage. Efficient transformer models are grouped together. HY-BRIDER results are obtained from<ref type="bibr" target="#b7">Chen et al. (2020b)</ref> by composing the errors for the first components.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Average attention flow from a token in the table to other token types. We compare to an uniform attention matrix as a baseline. Attention to tokens in different rows and columns is the relative smallest with one third of the baseline. Computed on WIKITQ Dev.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Dev results of using MATE on other table parsing datasets. Errors are estimated with half the interquartile range over 5 runs.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The term "semi-structured text" refers to text that has structure that does not reflect a known data schema. Typically semi-structured text is organized as an HTML tree or variable length lists and tables.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank Yasemin Altun, Ankur Parikh, Jordan Boyd-Graber, Xavier Garcia, Syrine Krichene, Slav Petrov, and the anonymous reviewers for their time, constructive feedback, useful comments and suggestions about this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide all details on our experimental setup to reproduce the results in Section A. In Section B we show the development set results for our experiments. The proof for Theorem 1 is described in Section C and in Section D we include the main code blocks for implementing MATE efficiently on a deep learning framework.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ETC: Encoding long and structured inputs in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Giving BERT a calculator: Finding operations and arguments with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5947" to="5952" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncovering the relational web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebDB</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1171</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Open question answering over tables and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eva</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tabfact: A large-scale dataset for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">HybridQA: A dataset of multi-hop question answering over tabular and textual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1026" to="1036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1285</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2978" to="2988" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding tables with intermediate pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syrine</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="281" to="296" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">From language to programs: Bridging reinforcement learning and maximum marginal likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Guu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1097</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1051" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Open domain question answering over tables via dense retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syrine</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.43</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="512" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.398</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Search-based neural structured learning for sequential question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1821" to="1831" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention is not Explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1357</idno>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3543" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">AxCell: Automatic extraction of results from machine learning papers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Kardas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Czapla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Stojnic</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.692</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8580" to="8594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DoT: An efficient double transformer for NLP tasks with tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syrine</forename><surname>Krichene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.289</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3273" to="3283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. Transactions of the Association of Computational Linguistics</title>
		<editor>Kenton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Latent retrieval for weakly supervised open domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1612</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6086" to="6096" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning dependency-based compositional semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="DOI">10.1162/COLI_a_00127</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="389" to="446" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1142</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compression of neural machine translation models via pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1029</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="291" to="301" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mul-timodal{qa}: complex question answering over text, tables and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ori</forename><surname>Yoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amnon</forename><surname>Catav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Lahav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient transformers: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>with linear complexity</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is not not explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wiegreffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Pinter</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">TaBERT: Pretraining for joint understanding of textual and tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.745</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Association for Computational Linguistics</title>
		<meeting>the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8413" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Are transformers universal approximators of sequence-to-sequence functions?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">O(n) connections are expressive enough: Universal approximability of sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sashank</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Table fact verification with structure-aware transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Empirical Methods in Natural Language Processing</title>
		<meeting>Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1624" to="1629" />
		</imprint>
	</monogr>
	<note>Results of sorting and reshaping an embedding tensor</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Different views of the tensor created to facilitate attention across tokens from global/long parts. First two dimensions are &apos;batch_size&apos; and &apos;num_heads&apos;: Attributes: full: &lt;float32&gt;</title>
		<imprint/>
	</monogr>
	<note>seq_length, embedding_size</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Original tensor without any bucketing</title>
		<imprint/>
	</monogr>
	<note>global: &lt;float32&gt;[..., global_length, embedding_size</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Same as &apos;long&apos; but also a rotation to the left and right, in order to achieve attention to the previous and next bucket</title>
	</analytic>
	<monogr>
		<title level="m">full: tf.Tensor global: tf.Tensor long: tf.Tensor window: tf.Tensor def multi_view_attention( Q: MultiViewEmbedding, K: MultiViewEmbedding, V: MultiViewEmbedding, embedding_size: int, global_length: int, long_length: int, num_heads: int, ): # &lt;float32&gt;</title>
		<imprint>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>long_length/radius, 3 * radius. batch_size, num_heads, global_length, sequence_length] attention_prob_from_global = tf.nn.softmax(tf.einsum( &apos;BHFE,BHTE-&gt;BHFT&apos;, Q.global, K.full) / sqrt(embedding_size)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">:global_length] attention_prob_to_window = attention_prob_from_long</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><forename type="middle">.</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">global_length:] # &lt;float32&gt;</title>
		<imprint/>
	</monogr>
	<note>batch_size, num_heads, global_length, embedding_size] context_layer_from_global = tf.einsum(&apos;BHFT,BHTE-&gt;BHFE&apos;, attention_prob_from_global, V.full</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<title level="m">context_layer_from_long = tf.reshape( context_layer_to_first + context_layer_to_window</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>1, num_heads, long_length, embedding_size]) return tf.concat( [context_layer_from_global, context_layer_from_long</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
