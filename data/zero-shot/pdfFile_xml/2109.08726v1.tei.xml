<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ChipQA: No-Reference Video Quality Prediction via Space-Time Chips</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">P</forename><surname>Ebenezer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Zaixi</forename><surname>Shang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Sethuraman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
						</author>
						<title level="a" type="main">ChipQA: No-Reference Video Quality Prediction via Space-Time Chips</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video quality assessment</term>
					<term>natural video statis- tics</term>
					<term>human visual system</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new model for no-reference video quality assessment (VQA). Our approach uses a new idea of highly-localized space-time (ST) slices called Space-Time Chips (ST Chips). ST Chips are localized cuts of video data along directions that implicitly capture motion. We use perceptuallymotivated bandpass and normalization models to first process the video data, and then select oriented ST Chips based on how closely they fit parametric models of natural video statistics. We show that the parameters that describe these statistics can be used to reliably predict the quality of videos, without the need for a reference video. The proposed method implicitly models ST video naturalness, and deviations from naturalness. We train and test our model on several large VQA databases, and show that our model achieves state-of-the-art performance at reduced cost, without requiring motion computation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO content continues to proliferate, already accounting for more than 70% of internet traffic, and projected to exceed 82% of internet traffic by 2021. Live internet video will account for 13 percent of Internet video traffic by 2021, and is predicted to grow 15-fold from 2016 to 2021 <ref type="bibr" target="#b0">[1]</ref>. Distortions can affect videos as they are captured, transmitted, and received. The task of assessing the quality of a video in the presence of distortions is thus an increasingly important open problem. In most instances in this process there is no reference against which to measure their eventual perceived quality. Nevertheless, it is of vital importance to providers of video content to be able to monitor and predict the perceptual quality of their videos, since this directly impacts customer satisfaction. Video quality tools can also help make wellinformed design choices while creating systems for capturing, processing, transmitting, and displaying videos. Video quality assessment algorithms also have applications in video denoising, designing loss functions for deep learning, video compression, and many other high-impact areas.</p><p>Collecting a large number of human opinion scores on the quality of a video is the most reliable way to measure its quality. However, collecting subjective opinions of video quality is a cumbersome and expensive task. It is also time-consuming and cannot be deployed prior to or during transmission of a video, when they are being live-streamed or have other J.P. <ref type="bibr">Ebenezer</ref>  latency constraints. Subjective opinions are nevertheless useful as a gold standard when designing objective video quality assessment (VQA) algorithms. Objective VQA algorithms are designed to correlate well with these subjective human judgments, and deployed effectively and cheaply in video processing systems. VQA algorithms are typically evaluated on the basis of data gathered from studies on human judgments of video quality. Subjective judgments of video quality are first collected from a statistically significant number of human observers and normalized with respect to each observer's scores to form an opinion score for each observer and for each video. These opinion scores are then averaged across the observers yielding single mean opinion score (MOS) for each video. These mean opinion scores are the ground truth against which objective VQA algorithms are trained and tested. Objective VQA algorithms fall into three categories: fullreference (FR), reduced-reference (RR), and no-reference (NR). FR VQA algorithms require a reference video against which the distorted video is compared. RR VQA algorithms require only some information from the reference video, but not all, to predict the quality of a distorted video. NR VQA algorithms do not make use of a reference video, and the models we present here fall in this category. NR VQA algorithms rely on distortion-specific features or models of arXiv:2109.08726v1 [eess.IV] 17 Sep 2021 natural video statistics to predict video quality, and are of great interest because of their potentially broader applications.</p><p>In this work, we propose a NR VQA algorithm based on the natural video statistics of space-time (ST) chips. ST Chips are a new feature space that are defined as localized and oriented cuts of a video volume, and an illustration of the concept is shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. We show that when a pristine video is processed using models derived from the human visual system, ST Chips extracted from the processed video that are along the direction of motion follow certain regular statistics, which is a breakthrough in our understanding of natural video statistics. We first proposed the idea of using ST Chips in <ref type="bibr" target="#b1">[2]</ref>, where we extracted ST Chips using optical flow in a prototype algorithm. In this work, we develop that idea further, introducing temporal processing of the video data based on models of the human visual system, and doing away with optical flow by using a simpler and more elegant approach based on regularities revealed by analysis of video statistics. Directions of motions are found in an implicit manner by using well-known models of natural image statistics and the smoothness of motion fields. The statistics of ST Chips extracted along these directions of motion can be modelled with parametrized distributions, and we show that these parameters can be used to reliably predict the quality of videos. We call our model ChipQA, which we designed to be able to handle different kinds of videos. We show that ChipQA achieves state-of-the-art (SOTA) performance on a large new high-motion VQA database. We also test ChipQA on several other VQA databases of professional and user generated content and show that it achieves highcorrelations with human judgments of video quality, while also being very computationally efficient.</p><p>The paper is organized as follows. In the following section, we briefly review previous work in the area of NR VQA. In section III, we describe our algorithm and its perceptual underpinnings. In section IV we report and analyze results on several large VQA database, and we conclude in section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PREVIOUS WORK</head><p>V-BLIINDS <ref type="bibr" target="#b2">[3]</ref> is a no-reference video quality algorithm that models the natural video statistics of the discrete cosine transform (DCT) of frame differences. V-BLIINDS also makes use of features that capture global and local motion coherency. VIIDEO <ref type="bibr" target="#b3">[4]</ref> is a "completely blind" NR VQA algorithm, in that it is not trained on a database at all and can be deployed as-is. VIIDEO makes use of the high inter-subband correlations of statistical features that have been observed in pristine videos but not in distorted videos. VIIDEO predicts the quality of videos based on this observation and without any training. Manasa and Channappayya <ref type="bibr" target="#b4">[5]</ref> proposed an NR VQA algorithm based on the statistics of optical flow. The coefficient of variation of the standard deviation of optical flow at different spatial locations is used to quantify irregularities in motion. Dendi and Channappayya <ref type="bibr" target="#b5">[6]</ref> proposed a statistical model for the distribution of spatio-temporal bandpass coefficients. The statistics of these coefficients are modelled as following an assymmetric generalized Gaussian distribution. The parameters from the statistical fits are used to predict quality. ChipQA-0 <ref type="bibr" target="#b1">[2]</ref> introduced the idea of localized cuts in space-time, ST Chips, which may be viewed as highly localized variations of space-time slices, which are defined over the global range of spatial and temporal video coordinates, instead of locally. The ST Chips in ChipQA-0 were extracted using optical flow, making the algorithm expensive and impractical for use when low-latency is a requirement. The statistics of ST Chips are modelled based on the general observation that natural videos follow regular statistics, and that the regularity of these statistics is disturbed in the presence of distortions. Quantifying these deviations from natural statistics can thus be used to quantify the degree of distortion and the perceptual quality of the video, by learning mappings between these statistics and perception. Finding and describing these statistics is a challenge but many clues about these patterns can be gleaned from the human visual system. The human visual system has adapted to the regular statistics of videos, using them to reduce redundancies in visual signals. Mimicking the front-end visual processes involved in encoding the visual signal, it is possible to reveal departures from these regularities and use them to quantify video quality. TLVQM <ref type="bibr" target="#b6">[7]</ref> is a recent NR VQA algorithm that defines a number of distortion-specific and motion-related features that are relevant to video quality. It does this in two stages. In the first stage, a number of low-complexity features are computed on every frame. These features capture the intensity and spread of motion vectors and also include specific features responsive to blockiness, blur, and interlacing. In the second stage, high-complexity features are computed on one frame each second. These include features that are tailored to capture underexposure, overexposure, noise, blur, blockiness, low contrast, interlacing, low sharpness, low brightness, and low colorfulness. TLVQM has many distortion-specific features and can hence be used as a general purpose NR VQA algorithm in many settings, but it also has many parameters that must be tuned. It represents a different paradigm from natural video statistics-based models, since it does not attempt to model naturalness but instead explicitly models specific distortions. CNN-TLVQM <ref type="bibr" target="#b7">[8]</ref> is a variant of this method where deep features are added to TLVQM features to obtain better performance.</p><p>MMSP-VQA <ref type="bibr" target="#b8">[9]</ref> is a deep learning based approach for NR VQA. It was trained on a very large-scale dataset called FlickrVid-150k. Features are extracted on each frame from multiple layers of an Inception-Resnet-v2 <ref type="bibr" target="#b9">[10]</ref> network pretrained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. The features were then averaged and trained with a deep neural network. FlickrVid-150k and the source code for MMSP-VQA have not yet been released.</p><p>NR Image Quality Assessment (IQA) algorithms have been found to be quite competitive with NR VQA algorithms on user-generated content (UGC). This is because UGC is dominated by spatial distortions and does not usually present much temporal variation on quality. NR IQA algorithms such as FRIQUEE <ref type="bibr" target="#b11">[12]</ref> and HIGRADE <ref type="bibr" target="#b12">[13]</ref> have been found to outperform NR VQA algorithms on datasets such as LIVE VQC <ref type="bibr" target="#b13">[14]</ref>, Konvid-1k <ref type="bibr" target="#b14">[15]</ref>, and YouTube-UGC <ref type="bibr" target="#b14">[15]</ref>. FRIQUEE uses a bag of perceptually motivated statistical features from different spaces, including luminance, color, and (a) k(t) for t ? (0, 10) for a = 0.5 (b) Discrete samples of k[n] for n from 0 to 4 and for a = 0.5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2.</head><p>Continuous and discrete versions of the temporal filter. We use 5 discrete coefficients for the filter in our implementation.</p><p>gradients. HIGRADE models the statistics of the log-derivative of gradients, and was designed for HDR content, but has been found to work well on UGC content as well. BRISQUE <ref type="bibr" target="#b16">[16]</ref> is an earlier NR IQA algorithm that utilizes a spatial model of the statistics of distorted pictures. BRISQUE models the statistics of bandpass, divisively normalized coefficients of images, based on the observation that bandpass, divisively normalized pristine images follow a first order Gaussian distribution. Distorted images change these statistics, and the parameters of fits to the statistics of an image can be used to reliably predict the quality of an image. The statistical models of distorted pictures discovered in BRISQUE underpin subsequent advances in NSS-based IQA research. NIQE <ref type="bibr" target="#b17">[17]</ref> also models spatially bandpassed coefficients, but does not require training. NIQE quantifies the deviation of the statistics of an image via a statistical fit to a small corpus of high-quality natural images. CORNIA <ref type="bibr" target="#b18">[18]</ref> is an NR IQA algorithm that does not attempt to model the statistics of natural images, but instead uses a dictionary to effectively represent images for quality assessment. Li et al. <ref type="bibr" target="#b19">[19]</ref> proposed a CNN based method for UGC quality assessment and trained it on a combination of three major UGC databases. Space-time slices are cuts of a video through space and time along fixed, pre-determined directions and spanning an entire video. Space-time slices have been effectively used for FR VQA, but are only applicable to stored videos that are available to an algorithm in their entirety <ref type="bibr" target="#b20">[20]</ref>- <ref type="bibr" target="#b23">[23]</ref> Space-time chips significantly modify this concept, since they are highly localized in space and time, are sensitive to local motion, and can be used in real-time applications. Space-time chips were first introduced in ChipQA-0 <ref type="bibr" target="#b1">[2]</ref>, but were found using optical flow in that method. Optical flow is generally expensive to compute, and their requirement can make algorithms computationally impractical, although motion is relevant to any study of video quality. Motion has also been used in several FR VQA models <ref type="bibr" target="#b24">[24]</ref>- <ref type="bibr" target="#b26">[26]</ref> and NR VQA models <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In our work, we use implicit motion to define the ST Chips used in quality prediction, based on a simple regularity maximizing concept and without using optical flow. Our new model is able to obtain better performance with much lower computational complexity than the original prototype ChipQA-0. ChipQA-0 also performed poorly on UGC databases, while the full model, ChipQA, incorporates temporal filtering, and utilizes chroma and gradient features, yielding a holistic algorithm that performs well on both professional and user generated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VIDEO QUALITY ASSESSMENT USING SPACE-TIME CHIPS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Space-Time Perception</head><p>When a video signal is incident on the retina, it is subjected to bandpass spatial filtering expressed at the outputs of the retinal ganglion cells. In a simple model of this process, local spatial averages of the signal are subtracted from the signal, and a form of adaptive gain control is applied on the difference <ref type="bibr" target="#b27">[27]</ref>. The resultant signal has a greatly reduced entropy and is carried by the optical nerve at a reduced bandwidth to further stages along the visual pathway. This "contrast signal" is subsequently subjected to temporal entropy reduction filtering ( [28]- <ref type="bibr" target="#b31">[31]</ref>) which can also be modelled in a simple way as a temporal bandpass filter operation, with filter kernel given by</p><formula xml:id="formula_0">k(t) = t(1 ? at) exp(?2at)u(t),<label>(1)</label></formula><p>where t denotes time, a is a constant parameter, and u(t) is the unit step function. The function is plotted against t in <ref type="figure">Fig. 2a</ref>. These processes serve to spatially and temporally decorrelate the visual signal. These initial stages of the human visual system motivate the use of spatial and temporal decorrelating functions on videos before analyzing their statistics. When the visual signal arrives at area V1 (the primary visual cortex) it is decomposed into orientation and scaletuned spatial and temporal channels. Neurons in area V1 are also sensitive to specific local orientations of motion. From here, the visual signal is passed to area middle temporal (MT) in extrastriate cortex, where further motion processing occurs <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b28">[28]</ref>. MT contains neurons sensitive to motion over larger spatial fields, and the neural representation of the space-time visual signal at this point makes efficient use of space-time regularity. Similarly, ST Chips are sensitive to local orientations of motions aggregated over large spatial fields, which we use to build spatiotemporal representations of video data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Defining Space-Time Chips</head><p>The first step in our algorithm is to compute spatial meansubtracted and contrast-normalized (MSCN) coefficients of each frame in a given video. Given a luminance image I[i, j, n] at frame index (time) n, the MSCN coefficients?[i, j, n] are:</p><formula xml:id="formula_1">I[i, j, n] = I[i, j, n] ? ?[i, j, n] ?[i, j, n] + C<label>(2)</label></formula><p>where i ? 1, 2..M , j = 1, 2..N are the spatial indices, M and N are the height and width of the image respectively, C is a constant for numerical stability, and</p><formula xml:id="formula_2">?[i, j, n] = k=K k=?K k=K k=?K w[k, l]I[i + k, j + l, n]<label>(3)</label></formula><formula xml:id="formula_3">?[i, j, n] = k=K k=?K k=K k=?K w[k, l](I[i + k, j + l, n] ? ?[i, j, n]) 2</formula><p>(4) are the local spatial mean and standard deviation of luminance, respectively. w = {w[k, l], k ? ?K, .., K, l ? ?K, .., K} is a 2D circularly-symmetric Gaussian weighting function sampled out to 3 standard deviations and rescaled to unit volume. We use K = 3 in our implementation. Research on natural image statistics <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[16]</ref> has shown that, in the absence of distortion, the coefficients?[i, j, n] can be expected to reliably follow a first-order generalized Gaussian distribution (GGD). This is the basis of state-of-the-art NR IQA algorithms such as FRIQUEE, HIGRADE, and BRISQUE, and is used in VQA algorithms as well to model spatial statistics, e.g., in V-BLIINDS. Moreover, the MSCN operation supplies a reasonable approximation to bandpass processing and adaptive gain control (relevant to contrast masking) that occurs in the retina.</p><p>Following spatial MSCN processing, we apply the causal temporal filter (Eq.1) to groups of T consecutive frames, with no overlap between adjacent groups of frames. This is cheaper than using overlapping blocks and we found that this does not affect performance. The discrete coefficients of the filter, k[n], are shown in <ref type="figure">Fig. 2b</ref> for a = 0.5 and the length of the filter P = 5. We experimented with different values of a and discuss how they affect performance in the results section. We denote the result of this temporal operation as D.</p><formula xml:id="formula_4">D[i, j, n] =?[i, j, n] * k[n]<label>(5)</label></formula><p>We use reflective padding in the temporal dimension at the boundaries of each block of T frames such that the output also has T frames. We fix P = T to minimize the effect of boundary artifacts, as increasing P to be greater than T would result in a greater use of padded points. For ease of representation, denote the processed frame D[i, j, n] for i ? 1, 2..M , j = 1, 2..N at time instance n as D n . We are interested in finding important directions at different spatial locations along which ST Chips can be extracted. In our experimental model, we fixed T = R for simplicity, so that each ST chip is extracted from an R?R?R volume. At a particular time instance T , consider the output of the previous operation D n over indices n = T ?R+1 to n = T , which is a single block of R frames. Divide D T into spatial windows of size R ? R. For each R ?R window, we define chips that pass through the block of frames from D T backwards in time to D T ?R+1 , and that are constrained to pass through the center of the R ? R window such that the normal vector to any chip lies on the xy plane. Some examples of chips are shown in <ref type="figure" target="#fig_3">Fig. 5</ref> (in blue, with the R ? R windows in red) and chips for a single R ? R ? R volume are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. These chips can be oriented at diverse angles. Among these angles, one is assumed to best capture the local motion, and a chip that is oriented perpendicular to the motion vector at this location will capture objects in motion along the motion vector. This is illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Under these constraints on the chip, we are assuming that the motion is along a vector on the xy plane, which relies implicitly on the assumption that motion is linear and translational in small spatiotemporal volumes. This is a reasonable assumption that forms the basis of most modern motion estimation algorithms <ref type="bibr" target="#b38">[32]</ref>, <ref type="bibr" target="#b39">[33]</ref>.</p><p>In our earlier work <ref type="bibr" target="#b1">[2]</ref>, we found the directions of motion explicitly using optical flow. This is expensive and depends on the accuracy of the optical flow algorithm. Assuming that motion is smooth for a pristine video, we expect the chips that are perpendicular to the directions of motion to follow similar statistics as natural images, since they contain projections of natural scenes as they move. The MSCNs of natural images are known to reliably obey a Gaussian law <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b40">[34]</ref>. Assuming the veracity of these natural image statistics models, and smoothness and linearity Chips are extracted from a R ? R ? R video volume along 6 angles that are equally spaced from 0 to ?. The angles are shown next to their corresponding chips. The chip which has the minimum excess kurtosis is selected as the chip that best captures motion. This criterion is based on the Gaussianity of natural images and the smoothness of motion.</p><p>of motion in local regions of pristine videos, we find the directions of motions implicitly by selecting a chip having a sample set that is closest to being Gaussian amongst all of the potential chips. This is done in a simple and direct way by computing the sample kurtosis <ref type="bibr" target="#b41">[35]</ref> of each chip along Q different equally spaced angles, from 0 to ?, and selecting the chip that has the kurtosis closest to 3, which is the kurtosis of a Gaussian random variable, as is illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>. There are many possible tests of the Gaussianity of a set of chip samples. These include the Shapiro-Wilk test <ref type="bibr" target="#b42">[36]</ref>, the Anderson-Darling test <ref type="bibr" target="#b43">[37]</ref>, the Martinez-Iglewicz test <ref type="bibr" target="#b44">[38]</ref>, and the D'agostino kurtosis test <ref type="bibr" target="#b45">[39]</ref>, which is similar to the kurtosis method that we apply. There are a number of reasons we use the simple sample kurtosis. First, we are not actually testing for Gaussianity, which these frequentist tests are designed for. Rather, we are instead ranking the chips by kurtosis and selecting which among them is most Gaussian in that sense. It is possible that all, none, or a subset of the chips may pass a given Gaussianity test, e.g., if there is little or no motion present, all may present as Gaussian. Ranking procedures on test statistics like those in <ref type="bibr" target="#b42">[36]</ref>- <ref type="bibr" target="#b45">[39]</ref> have not been shown to measure relative Gaussianity. Further, by using the sample kurtosis we are aligning with the powerful a priori and well-founded assumption that the bandpass chips will reliably obey a zero-mean generalized Gaussian law. The members of this distribution class may be viewed as differing only in kurtosis, hence we may view our use of kurtosis as a conditional measure. Given the small sample size of 25, this is a powerful constraint. Lastly, the computational efficiency of the kurtosis lends it to fast implementations. We chose Q = 6 in our implementation, and found that increasing Q improves performance, although computational cost increases as well. Variation in performance as Q is varied is discussed in section IV-E.</p><p>Having selected the chip that is most Gaussian at a window, we then aggregate them across windows. We do not collect chips from all R ? R windows, but skip D = 4 windows in each of the x and y directions. We study how performance varies with D in Section IV-D. The centers of the windows from which ST Chips are extracted are thus separated from each other by a distance of 4R pixels in each dimension. This is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. We discuss how this spatial downsampling affects performance in Section IV-E . The aggregated chips form a single "frame" S for every group of T frames. We discretize coordinates while searching for the best chip such that each chip is of dimension</p><formula xml:id="formula_5">R ? R. The dimension of the aggregated frame S of ST Chips is M ? N , where M = R 4 M R and N = R 4 N R . We chose R = T = 5 in our implementation. Variation in performance as R is varied is discussed in section IV.</formula><p>We repeated the process described above for the spatial gradient magnitude field of the video as well. Gradients contain important information descriptive of edges and contrast variations and have been found to be useful for image and video quality assessment. Gradient-based features find a place in most SOTA algorithms <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b46">[40]</ref>. ChipQA computes the gradient components in the vertical and horizontal directions using a Sobel kernel of size 3 ? 3. The Sobel filter eliminates low frequency information and has highpass characteristics that detects edges. The statistics of these edges are useful for quality assessment since they are often heavily affected by distortions. We then find the MSCNs of the gradient magnitude, apply the temporal filter k[n], and extract ST Chips along the directions with kurtosis closest to 3 at windows that are separated by a distance of 4R in each dimension. We refer to these as "ST Gradient Chips".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Statistics of ST Chips</head><p>MSCNs of spatial frames and of gradient fields are known to follow regular statistics, and this is true of ST Chips of spatial frames and of gradient fields as well because of the smoothness of motion. ST Chips and ST Gradient Chips are found to follow a generalized Gaussian distribution (GGD) of the form:</p><formula xml:id="formula_6">f (x; ?; ?) = ? 2??( 1 ? ) exp(?( |x| ? ) ? )<label>(6)</label></formula><p>where ?(.) is the gamma function:</p><formula xml:id="formula_7">?(?) = ? 0 t ??1 exp(?t)dt.<label>(7)</label></formula><p>The shape parameter ? of the GGD and the variance of the distribution are estimated using the moment-matching method described in <ref type="bibr" target="#b47">[41]</ref>. Examples of the first-order distribution of ST Chips and ST Gradient Chips are shown in <ref type="figure" target="#fig_3">Fig. 5</ref> and <ref type="figure">Fig. 6</ref> respectively. Though the ST chip is chosen to be as Gaussian as possible, previous research has shown that Gaussianity breaks in the presence of distortions <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[16]</ref>, and for ST chips the statistics could also deviate from Gaussianity for large motion fields. These deviations from Gaussianity are useful for quantifying losses in quality, and we find that the GGD is able to model these deviations in the statistics well.  We also model the second-order statistics of ST Chips. Define the collection of ST Chips aggregated at each time instance T as S, and define the pairwise products</p><formula xml:id="formula_8">H[i, j, T ] = S[i, j, T ]S[i, j + 1, T ] V [i, j, T ] = S[i, j, T ]S[i + 1, j, T ] D 1 [i, j, T ] = S[i, j, T ]S[i + 1, j + 1, T ] D 2 [i, j, T ] = S[i, j, T ]S[i + 1, j ? 1, T ]<label>(8)</label></formula><p>H, V , D 1 , and D 2 are found to follow a asymmetric generalized Gaussian distribution (AGGD), which is given by:</p><formula xml:id="formula_9">f (x; ?, ? 2 l , ? 2 r ) = ? (? l +?r)?( 1 ? ) exp(?(? x ? l ) ? ) x &lt; 0 ? (? l +?r)?( 1 ? ) exp(?( x ?r ) ? ) x &gt; 0 (9) where ? l = ? l ?( 1 ? ) ?( 3 ? ) and ? r = ? r ?( 1 ? ) ?( 3 ? )<label>(10)</label></formula><p>where ? controls the shape of the distribution and ? l and ? r control the spread on each side of the mode. The parameters (?, ?, ? 2 l , ? 2 r ) are extracted from the best AGGD fit to each pairwise product, where</p><formula xml:id="formula_10">? = (? r ? ? l ) ?( 2 ? ) ?( 1 ? ) .<label>(11)</label></formula><p>Empirical histograms of the paired products of ST Chips and ST Gradient Chips are shown in <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 8</ref>, respectively.</p><p>Images and videos are inherently multiscale, and distortions can manifest themselves differently at different scales. We compute ST Chips at two scales, and compute the above features at each scale. We first apply low-pass filtering to the video and then downsample it to half the original size. ST Chips are computed as described previously, and statistical features are extracted from the lower scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Spatial Features</head><p>Spatial features are important for video quality assessment. Since user-generated content is often dominated by spatial distortions, NR IQA algorithms such as FRIQUEE and HI-GRADE achieve very good performance on UGC datasets. Compression, aliasing, and interlacing are common spatial distortions that affect professional-grade content as well. In VIDEVAL <ref type="bibr" target="#b46">[40]</ref>, top-performing features were selected from a number of state-of-the-art algorithms. It was found that luma, color, and gradient information were vital to building a competitive VQA algorithm. Likewise, we incorporate features that describe the statistics of luma, color, and gradient magnitude in our algorithm.</p><p>1) Luma:</p><p>? NIQE naturalness: We compute luminance features and a naturalness score based on them every T frames using the image naturalness index NIQE. They consist of parameters of GGD and AGGD fits to spatial MSCNs of selected patches of luma in each frame, and a naturalness score that measures the distance of the parameters from a statistical fit to a corpus of natural images. These features are averaged over the entire video over all nonoverlapping groups of T frames. ? ? map: We also model the statistics of the bandpass standard deviation ? in (4). The ? map is calculated using equation 4. The MSCNs of the ? map also follows a GGD, as shown in <ref type="bibr" target="#b40">[34]</ref>, we extract the shape, variance, skewness, and kurtosis of the distribution. These features are averaged over the entire video clip. Previous research <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b46">[40]</ref>, <ref type="bibr" target="#b48">[42]</ref> has also shown that standard deviation pooling of features over time is useful for video quality assessment. We therefore compute the standard deviation of these features over every non-overlapping five-frame interval, and average the standard deviation values over the entire video. Again, T = 5 frames are used for ST chip computation, hence the pooling is copacetic with the computation of ST Chips. 2) Color: We also model the statistics of the chrominance of videos. We use the CIELAB <ref type="bibr" target="#b49">[43]</ref> color space, which is designed to model human perception of color. CIELAB has a luminance channel (L * ) and two chrominance channels (a * and b * ), where a * denotes the position of the color along the red-green axis, and b * denotes the position of the color on the yellow-blue axis. Chroma (C) captures the intensity of a color, and is defined as</p><formula xml:id="formula_11">C = a * 2 + b * 2<label>(12)</label></formula><p>We compute the chroma map for each frame in the video, and find the MSCNs of the chroma map. The MSCNs are known to follow a first order GGD <ref type="bibr" target="#b11">[12]</ref>, and the shape, variance, skewness, and kurtosis are extracted from the empirical distribution of each frame.</p><p>We also compute the ? map of the chroma, using (4). We extract the shape, variance, skewness, and kurtosis of the distribution. These features are averaged over the entire video clip. We also find the average standard deviation of features over T = 5 frame intervals, as described for the ? calculations in luma space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Gradients</head><p>Gradients are known to capture important information about edges, and since some distortions modify (reduce or increase) gradients, they have been effectively used to predict video quality in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b46">[40]</ref>, <ref type="bibr" target="#b50">[44]</ref>. We find the gradients of the luminance in the vertical and horizontal directions using a Sobel kernel of size 3. We then find the gradient magnitude at each pixel and compute the MSCNs of the gradient magnitude field. The second order statistics of the MSCNs of the gradient magnitude are particularly useful for predicting video quality, when combined with the features defined previously. The paired products of the MSCNs of the gradient magnitude are computed using <ref type="bibr" target="#b7">(8)</ref>, and modelled using AGGDs <ref type="bibr" target="#b8">(9)</ref>. Four parameters are extracted on each paired product of the gradient magnitude at two scales. Just as for chroma, we average these features across time and also find the average standard deviation over groups of 5 frames. <ref type="table" target="#tab_1">Table I</ref> gives a summary of all the features used in ChipQA. A total of 221 features are extracted on each video, starting from the T = 5 th frame. These are trained with a support vector regressor, as described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Quality Assessment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Validating ST Chips</head><p>We conducted a series of experiments to examine the effectiveness of our method of generating ST chips. First, we studied how well our method is able to predict motion directions. The goal of our design is not to conduct very accurate motion estimation, since it is not necessary for the end goal of perceptual video quality measurement; the perception of absolute motion by the human visual system is not particularly accurate. Instead, we are interested in obtaining motion data that is consistent with statistical information available to human perception, rather than from complex search or optimization processes. Nevertheless, it is of interest to evaluate the efficacy of our simple kurtosis-based motion orientation selector. However, several challenges arise when attempting to quantitatively evaluate ChipQA's implicit motion orientation estimation process:</p><p>? Many popular optical flow databases <ref type="bibr" target="#b51">[45]</ref>- <ref type="bibr" target="#b53">[47]</ref> only have optical flow on pairs of images, while ChipQA requires at least 5 frames to select motion directions. ? ChipQA only finds the directions of motions, not the magnitudes. Therefore some standard metrics such as endpoint error (EPE) cannot be used to evaluate ChipQA. Average across all frames of GGD shape, GGD scale, skewness, and kurtosis at two scales.</p><formula xml:id="formula_12">f 1 ? f 8 Chroma ? map</formula><p>Average across all frames of GGD shape, GGD scale, skewness, and kurtosis at two scales.</p><formula xml:id="formula_13">f 9 ? f 16 Gradient</formula><p>Average across all frames of four parameters from AGGD fitted to pairwise products at two scales.</p><formula xml:id="formula_14">f 17 ? f 48</formula><p>Luma ? map Average across all frames of GGD shape, GGD scale, skewness, and kurtosis at two scales.</p><formula xml:id="formula_15">f 49 ? f 56 Chroma</formula><p>Standard deviation across 5 frames of GGD shape, GGD scale, skewness, and kurtosis at two scales, averaged across all non-overlapping groups of 5 frames.</p><formula xml:id="formula_16">f 57 ? f 64</formula><p>Chroma ? map Standard deviation across 5 frames of GGD shape, GGD scale, skewness, and kurtosis at two scales, averaged across all non-overlapping groups of 5 frames.</p><p>f 65 ? f 72 Gradient Standard deviation across 5 frames of four parameters from AGGD fitted to pairwise products at two scales, averaged across all non-overlapping groups of 5 frames.</p><p>f 73 ? f 104</p><p>Luma ? map Standard deviation across 5 frames of GGD shape, GGD scale, skewness, and kurtosis at two scales, averaged across all non-overlapping groups of 5 frames.</p><p>f 105 ? f 112</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NIQE naturalness</head><p>Average over every 5 frames of features and scores of spatial naturalness index NIQE.</p><formula xml:id="formula_17">f 113 ? f 149 ST-Chip</formula><p>Average over all chips of shape and scale parameters from GGD fits and four parameters from AGGD fits to pairwise products at two scales.</p><p>f 150 ? f 185</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ST Gradient Chips</head><p>Average over all chips of shape and scale parameters from GGD fits and four parameters from AGGD fits to pairwise products at two scales. ? ChipQA computes the motion directions of patches, while most optical flow algorithms and ground truth data compute motion at the pixel level. ? ChipQA computes motion directions in quantized steps of ?/6 from 0 to ?, while the ground truth data in optical flow databases are generally more fine-grained. ? ChipQA does not differentiate between the angles ? and ? + ? of any motion direction ?, since the chips along both directions are the same.</p><p>The Sintel optical flow database <ref type="bibr" target="#b54">[48]</ref> is a viable way to examine the implicit motion estimation of ChipQA, since it has optical flow ground truth on entire videos. Sintel is not a database of natural videos but of 3D animated films, so the videos from this database do not necessarily obey natural video statistics models. However, since animations are often made to be reasonably naturalistic, we have found that natural video statistics models appear to hold well on this database. As discussed earlier, absolute motion metrics are not appropriate for evaluating ChipQA. We therefore used the mean absolute angular deviation (MAAD) between the ground truth angular directions of motion ? (in radians) and the angles of motion ? predicted by ChipQA:</p><formula xml:id="formula_18">MAAD(?, ? ) = |? ? ? |.<label>(13)</label></formula><p>We compared ChipQA's performance against the classical New Three</p><p>Step Search algorithm (NTSS) <ref type="bibr" target="#b55">[49]</ref>, which is a block motion algorithm, and the Farneb?ck dense optical flow predictor <ref type="bibr" target="#b56">[50]</ref>. NTSS is used in VBLIINDS and some implementations of the MPEG and H.26x codec families. Farneb?ck is used in ChipQA-0 and is the optical flow algorithm in the popular OpenCV library. Since ChipQA computes motion over patches, in order to compare the ground truth motion angles with those of ChipQA, the ground truth motion vectors were first averaged across 5x5 patches and across 5 frames in time.</p><p>We then found those angles along which these vectors are oriented and mapped them to [0, ?) by replacing all angles ? &gt; ? with ? ? ?. We did the same for the motion vectors predicted by NTSS and Farneb?ck, so that they could be compared to ground truth in the same way as ChipQA. The results are shown in <ref type="table" target="#tab_1">Table II</ref>, which shows that ChipQA was competitive with respect to MAAD, outperforming NTSS but was not as accurate than Farneb?ck. The effectiveness of ChipQA is quite remarkable, given its extremely simple design. Indeed, we view these experiments as suggestive of the type of information that might be used by the visual brain to compute motion. We also conducted a separate experiment whereby we studied the variation of kurtosis of ST chips on the Sintel database, as the angular difference between the chip's orientation and the ground truth was varied. We computed the kurtosis of all the chips on the Sintel database (on all videos), for fixed differences between the true motion direction and each chip's orientation, and plotted the results as the differences were increased by steps of ?/6, in <ref type="figure" target="#fig_0">Fig. 10</ref>. When the angular difference was 0, i.e., when all the chips across all videos were exactly aligned with the ground truth motion vectors, the average kurtosis was closest to 3, the kurtosis of a Gaussian. As the angular difference was increased by steps of ?/6, the average kurtosis diverged from 3 up to angular difference ?/2, after which it again converged towards 3 as the angular cycle completed. This provides further validation of the simple angular motion estimator used in ChipQA, while providing insights into the relationship between natural video statistics and motion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Databases</head><p>We evaluated our algorithm on four large databases, which are described below:  consists of 1500 20 second clips sampled from millions of user-generated YouTube videos. Spatial and temporal features were used to ensure the sampled videos were diverse and representative of videos on YouTube. Each video was rated by more than 100 subjects. These videos are of different resolutions and include content from categories such as animation and VR. 4) Konvid-1k <ref type="bibr" target="#b14">[15]</ref> -This database consists of 1200 videos of authentically distorted user-generated content. All videos are of resolution 960x540. Videos in this database are known to not have significant temporal variation in quality <ref type="bibr" target="#b46">[40]</ref>, <ref type="bibr" target="#b60">[54]</ref>. Each video has 114 subject ratings on average. 5) LIVE Video Quality Challenge (VQC) <ref type="bibr" target="#b13">[14]</ref> -LIVE VQC contains 585 videos of authentically distorted unique user-generated content. Each video was labeled by 40 human subjects on average. All videos are of usergenerated content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Details</head><p>We used a support vector regressor (SVR) with a radial basis function as the learning engine for all our experiments. 20% of the data was randomly selected for testing and 80% was used for training and validation. 5-fold cross-validation was performed on the training set to find the best set of hyperparameters for the SVR. Content separation is performed for all experiments on the LIVE Livestream database and the LIVE ETRI database, hence videos containing the same content are always in the same fold of either training, validation, or testing. This prevents scores from being artificially boosted. We repeated 1000 random splits on LIVE Livestream and LIVE ETRI, which are databases containing synthetic distortions and reference videos. We conducted 100 splits for the Konvid-1K, VQC, and YT-UGC databases, which are UGC databases. Databases containing synthetic distortions generally result in larger standard deviations of the results, which is why we performed more train-validation-test splits on them. Grid search was performed over values of the kernel coefficient ?, and the regularization parameter C that controls the squared L2 penalty. ? was geometrically increased by 10 from 10 ?8 to 10. C was doubled from 2 to 1024. We report Spearman's Rank Order Correlation Coefficient (SROCC), Pearson's Linear Correlation Coefficient (LCC), and the root mean square error (RMSE) between the predicted scores and the mean opinion scores for each algorithm. SROCC measures the monotonicity of the relationship between the two quantities, while the LCC measures the linear correlation. Since the relationship between the predicted scores and the MOS may not necessarily be linear, the predicted scores s were first passed through a logistic non-linearity <ref type="bibr" target="#b61">[55]</ref> f (s) = ? 1 (</p><formula xml:id="formula_19">1 2 ? 1 (1 + exp(? 2 (s ? ? 3 )) ) + ? 4 x + ? 5 ,<label>(14)</label></formula><p>before computing the LCC. The parameters are found by fitting f (s) to the MOS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>We used the luma definition from ITU Recommendation BT.709 <ref type="bibr" target="#b62">[56]</ref>. We use a frame-buffer of length T = 5 frames to compute ST Chips, keeping in mind potential applications of our algorithm in scenarios requiring low-latency, where using a larger buffer size might cause delays in the overall system. With a = 0.5, the discrete 5-tap causal filter k[n] is representative of the continuous filter k(t). After t = 8, there was a slight increase in the response which we rectified to 0 by choosing P (length of filter) = T (length of frame buffer) = 5, since in any case the filter converges to 0 as t tends to infinity. Using a temporal filter of length greater than the frame buffer could cause serious boundary artifacts, hence we fixed the length of the filter (P ) to be the length of the frame buffer (T ), which was fixed at 5. We also fixed R = T , so that each ST chip is square and is chosen from a 5 ? 5 ? 5 volume. The SROCC values obtained on the LIVE Livestream VQA database for different values of T and a are shown in <ref type="table" target="#tab_1">Table IV</ref>. <ref type="table">The Table shows</ref> that taking a = 0.5 and R = 5 not only provides a discrete filter that is representative of k(t), but also provides the best predictions. We used a window size of 2K + 1 ? 2K + 1, where K = 3, for MSCN computation. We also studied how performance varied as this parameter was changed. Consistent with previous studies <ref type="bibr" target="#b16">[16]</ref> of spatial MSCNs, we found that the size of the window did not significantly affect performance. We report the median SROCC over 1000 splits of the LIVE Livestream VQA database for different values of K, in <ref type="table" target="#tab_1">Table III</ref>. Since increasing K increases the computational cost, we used K = 3 in our final implementation.</p><p>A look-up table is used to implement the search for the best ST-chip direction. Coordinates along different directions from ? = 0 to ? = ?/Q are pre-computed and rounded to the nearest integer, since pixel coordinates are integer values. The look-up table coordinates are computed using the polar form, and are indexed by the value of ? and r, where r varies from ?(R + 1)/2 ? 1 to (R + 1)/2, where R ? R is the dimension of each ST chip. Values are read from the look-up table during the search for the best direction. We fixed Q = 6 in our implementation, and thus search 6 directions at each R ? R window to find the direction that best captures motion at that location. Results for different values of Q are shown in <ref type="table" target="#tab_1">Table VI</ref> for ChipQA for R = 5. Increasing Q improves performance, but beyond Q = 6 performance seems to drop off.</p><p>We also studied the effects of the chip downsampling factor D. The performance of the algorithm on the LIVE Livestream database for D = 1, D = 4 and D = 8 are presented in <ref type="table" target="#tab_4">Table V</ref>. Increasing D greatly reduces the cost of ChipQA (see <ref type="table" target="#tab_1">Table XIX</ref>), and using all of the chips is not necessary to conduct effective video quality assessment. We used D = 4 in our final implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Video Quality Assessment Results</head><p>Results on the LIVE Livestream VQA database, the LIVE ETRI database, Konvid-1k, LIVE VQC database and the YouTube UGC are shown in Tables VII,VIII, X, IX, and XI respectively. The scores of each top-performing algorithm are boldfaced in each table. We also give box-plots of the SROCCs over the 1000 splits on the LIVE Livestream database in <ref type="figure" target="#fig_0">Figure 11</ref>. ChipQA was able to achieve state-of-the-art performance on all databases. In particular, ChipQA significantly outperformed other compared algorithms on the LIVE Livestream database <ref type="table" target="#tab_1">(Table VII</ref>) and the LIVE ETRI database <ref type="table" target="#tab_1">(Table VIII)</ref>, both of which contain videos captured by professional videographers subjected to distortions commonly encountered at different stages of professional video capturing pipelines. These distortions include judder, frame-drop, spacetime subsampling, etc. ChipQA produced the least variation in SROCC across the splits of the LIVE Livestream database as can be seen from the box plot and the standard deviations of the SROCCs reported in <ref type="table" target="#tab_1">Table VII</ref>. ChipQA also performed very competitively on the UGC databases, which are known to be dominated by spatial distortions <ref type="bibr" target="#b46">[40]</ref>, <ref type="bibr" target="#b60">[54]</ref>, hence image quality algorithms were among the top performing algorithms for UGC databases.</p><p>We also evaluated two deep networks (VGG-19 <ref type="bibr" target="#b63">[57]</ref> and ResNet-50 <ref type="bibr" target="#b9">[10]</ref>) on the LIVE Livestream database. Both networks were pre-trained on ImageNet and used to extract features from 25 random 227x227 crops from one frame per second on the LIVE Livestream database. The 4096 outputs of the fully connected layer of VGG-19, and the 2048 outputs of the average-pooled layer of ResNet-50 for each patch were averaged across all patches, forming single 4096 and 2048 dimensional vectors, respectively, for each video. These videolevel vectors were then used as features to train an SVR to predict video quality, using the same cross validation method as described earlier. These feature-extractors have been shown to perform quite well on full-reference tasks <ref type="bibr" target="#b64">[58]</ref> as well as on UGC databases <ref type="bibr" target="#b46">[40]</ref>. However, they did not perform as well as the other methods on the LIVE Livestream database, which presents a significant variety and amount of temporal distortions. We ensured that videos of the same content did not appear in the same fold of training, validation, or testing. Reference videos and all of their distorted versions always appeared in the same fold. This ensured that the algorithm would not rely on learning content quality, and to prevent overfitting. We also made use of the regularization parameter C, chosen by crossvalidation to prevent overfitting by the SVR. We also measured the difference between the training and test SROCC to verify that ChipQA was not overfitting to the database, and found that the median difference between the SROCC on the training set and the SROCC on the test set was 0.1598 (16%)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance by Distortion</head><p>We also evaluated each algorithm on each single distortion in the LIVE Livestream database. The reference video and the synthetically distorted video for each content were used to evaluate the performance of each algorithm on each distortion separately. The results of this experiment are shown in <ref type="table" target="#tab_1">Table XII and Table XIII</ref>. ChipQA performed very competitively on each distortion as well. The LIVE Livestream database has a number of scenes with high motion and ChipQA appears to be able to predict video quality well even for complex, high-motion scenes. TLVQM uses "jerkiness" and "jerkiness consistency" features which measure the similarity of motion vectors in the forward and backward directions, and which are hand-crafted to capture frame drops, which is probably why it does so well on the frame drop category of distortions. CORNIA constructs codewords from image patches, which can capture simple patterns such as salt and pepper noise, blockiness, and interlacing. Interlacing has a very specific pattern of even and odd fields which lends itself to codeword construction, which is probably why CORNIA does so well on that category of distortions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablation Studies and Feature Ranking</head><p>We performed ablation studies of the studied feature spaces on the LIVE Livestream VQA database and the Konvid-1k database. The Konvid-1k database was chosen to be representative of UGC datasets. The results are shown in <ref type="table" target="#tab_1">Table XIV  and Table XV</ref>. Each feature space contains important qualityaware information about the video, but removing some of the spatial features (chroma and gradient features) appears to improve performance on LIVE Livestream. On the other hand, removing chroma and gradient features severely affects performance on Konvid-1k, showing that spatial features are vital for UGC but can only bring about the curse of dimensionality for datasets that have a significant amount of temporal distortion. We also find that removing ST chip features and  <ref type="bibr" target="#b63">[57]</ref> 0.5887 (0.0853) 0.6600 (0.0687) 9.3951 (0.6587) ResNet-50 <ref type="bibr" target="#b9">[10]</ref> 0.6395 (0.0793) 0.7011 (0.0666) 8.9941 (0.6361) BRISQUE <ref type="bibr" target="#b16">[16]</ref> 0.6381 (0.1035) 0.6841 (0.0835) 9.1330 (0.9514) HIGRADE <ref type="bibr" target="#b12">[13]</ref> 0.7088 (0.0805) 0.7247 (0.0742) 9.0958 (1.1249) CORNIA <ref type="bibr" target="#b18">[18]</ref> 0.6482 (0.0977) 0.6826 (0.0819) 9.6472 <ref type="bibr">(</ref>     information is not as important in current UGC databases, validating earlier work that came to the same conclusion <ref type="bibr" target="#b46">[40]</ref>, <ref type="bibr" target="#b60">[54]</ref>. We also studied performance when just the ST Chips, ST Gradient Chips, and the NIQE features were used. This set of features corresponds to what was used in ChipQA-0, but with our different and novel method of finding the chips, and removing all the other feature spaces that are in ChipQA but not in ChipQA-0. The median SROCC for just this set of features is 0.7901, greater than the 0.7513 SROCC for ChipQA-0, showing that ChipQA is conclusively better than ChipQA-0, and that our novel method of finding chips contributes significantly to the boost in performance.</p><p>We also ranked the importance of individual features in the database using sequential forward selection. We started with an empty set and then added a single feature to the feature set that maximized the median SROCC over 100 contentseparated splits of the LIVE Livestream database. Repeating this process, we obtained an ordering of the features. The top 10 features and their descriptions are presented in <ref type="table" target="#tab_1">Table XVI</ref>. We also report the median SROCC obtained when each feature is added to the set of features ranked higher than it. The NIQE score is, not expectedly, one of the top 10 features. The GGD shape of the ST chips at both scales are also among the top 10 features. Gradients and chroma information also supply features that lie among the top 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Cross Database Performance</head><p>We also evaluated the generalizability of ChipQA, TLVQM, and VBLIINDS across the studied databases. LIVE Livestream and LIVE ETRI are databases of professionally captured content and are thus treated separately from Konvid, VQC, and YT-UGC, which are UGC databases. Results of the crossdatabase performance comparisons on LIVE Livestream and LIVE ETRI are shown in <ref type="table" target="#tab_1">Table XVII</ref>, while results on the UGC databases are shown in <ref type="table" target="#tab_1">Table XVIII</ref>. An SVR was trained on the features from the database in the row entry and the SROCC when the trained SVR was used to predict scores using features from the database on the column entry are reported. All of the compared methods yielded poor crossdatabase performance, primarily because these databases have very different characteristics in terms of both distortion type and content type. On UGC videos, cross database performance between Konvid and UGC was much better than on other databases among all the compared methods, probably because their contents are similar. YT-UGC spans a very wide range of content, including animations, gaming, and VR, hence these videos do not share many properties with the videos of natural scenes found in VQC and Konvid, which is probably why the cross-database performances were so poor on YT-UGC. The distortions in the LIVE Livestream and LIVE ETRI databases are very different, involving high motions and temporal subsampling, respectively, though they both contain professionally-captured content. LIVE Livestream presents distortions such as interlacing, judder, flicker etc, while LIVE ETRI contains compression and temporal subsampling artifacts, which may explain the poor generalizability of all three VQA methods on these two databases.In summary, NR VQA algorithms trained on one type of database sometimes perform quite poorly on others. While this might suggest weaknesses of the predictive models, more likely it is because the databases contain very different contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Computational Complexity</head><p>We found the computational cost of each algorithm by computing the time each required to generate features on a single 4K video containing 210 frames. We also provide a crude estimate of the O(n) computational complexity and the number of giga floating point operations (GFLOPS) for each algorithm. The results are shown in <ref type="table" target="#tab_1">Table XIX</ref>. ChipQA was the most efficient NR VQA algorithm, much more efficient than applying SOTA NR IQA algorithms on each frame. The algorithms are not implemented in the same language and may not be optimized, hence the computational times and FLOPS cannot be compared directly, but since most practitioners use these algorithms off-the-shelf, this provides a rough estimate of relative complexity. All the algorithms run in linear time but differ in the coefficient of linearity. The compute times for the IQA algorithms were obtained assuming they would be applied on all frames of the video. The times taken for FRIQUEE and HIGRADE are rough estimates obtained by finding the times taken on a single frame and multiplying by the number of frames. All the algorithms were run on an Intel Xeon E5-2620 CPU with a maximum frequency of 3 GHz. ChipQA is much faster than ChipQA-0, because it does not involve the use of optical flow, and also because it performs spatial and temporal downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We presented a new algorithm for no-reference video quality assessment that is highly competitive with the state-of-theart and is computationally very efficient. ST Chips are a novel feature space and the fact that they follow regular statistics represents a significant advance in our understanding of natural videos. We showed how these statistics can be parametrized, and how the parameters of the statistical fits can be used to predict video quality without the use of any distortion-specific features. ChipQA achieves high correlations with human judgments of video quality, especially of high motion videos, and is also computationally very efficient. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Space-Time Chips capture elements in motion. On the left are frames 1 and 15 of a video of a basketball game. The ST-chip is marked in blue. The player near the edges of the chip on the xy plane is moving to his right as time progresses. The chip is a localized cut of all the frames between 1 and 15, perpendicular to the direction of his motion, which captures the player, as shown on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Extracting ST Chips from a video volume of spatially and temporally decorrelated frames from D T ?R+1 to D T . One portion of the video is shown for illustration. ST Chips are extracted by cutting through the volume over R ? R windows (that are spaced apart by 4R pixels) R frames back in time. ST Chips are the angled squares in blue, and the windows are shown in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Finding the best ST-Chip over a particular R ? R spatial window.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Aliased and pristine (b) Compressed and pristine (c) Flicker and pristine (d) Interlacing and pristine Empirical distributions of ST Chips. Pristine (original) distributions are in black and distorted distributions are in red. (a) Flicker and pristine (b) Frame Drop and pristine (c) Interlacing and pristine (d) Judder and pristine.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .Fig. 7 .Fig. 8 .</head><label>678</label><figDesc>Empirical distributions of ST Gradient Chips. Pristine (original) distributions are in black and distorted distributions are in red. (a) Compressed and pristine (b) Flicker and pristine (c) Interlacing and pristine (d) Judder and pristine Empirical distributions of paired products of ST-Chip. Pristine (original) distributions are in black and distorted distributions are in red. (a) Judder and pristine (b) Aliased and pristine (c) Compressed and pristine (d) Flicker and pristine Empirical distributions of paired products of ST Gradient Chips. Pristine (original) distributions are in black and distorted distributions are in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>fFig. 9 .</head><label>9</label><figDesc>186 ? f 221 (a) GGD fitted to ST Chip distribution (b) GGD fitted to ST Gradient Chip distribution (c) AGGD fitted to ST Chip diagonal paired product distribution (d) AGGD fitted to ST Gradient Chip horizontal paired product distribution Fits to empirical distributions. Fits are in red and histograms are in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Mean kurtosis across all chips in Sintel vs. the angular deviation from the true motion direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, Z. Shang, and A.C. Bovik are with the Laboratory for Image and Video Engineering, The University of Texas at Austin, Austin, TX, 78712, USA e-mail: joshuaebenezer@utexas.edu.</figDesc><table /><note>Y. Wu, H. Wei, and S. Sethuraman are with Amazon Prime Video. J.P. Ebenezer and Z. Shang contributed equally to this work</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I DESCRIPTIONS</head><label>I</label><figDesc>OF FEATURES IN CHIPQA.</figDesc><table><row><cell>Domain</cell><cell>Description</cell><cell>Feature index</cell></row><row><cell>Chroma</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II MEAN</head><label>II</label><figDesc></figDesc><table><row><cell cols="4">ABSOLUTE ANGULAR DIFFERENCE (MAAD) BETWEEN PREDICTED</cell></row><row><cell cols="4">AND ACTUAL DIRECTION OF MOTION ? FOR SINTEL DATABASE</cell></row><row><cell>SEQUENCE</cell><cell>ChipQA</cell><cell>NTSS</cell><cell>FARNEB?CK</cell></row><row><cell>alley 1</cell><cell>0.9133 (0.5975)</cell><cell>1.2202 (0.5934)</cell><cell>0.4295 (0.5410 )</cell></row><row><cell>alley 2</cell><cell>1.0526 (0.7281)</cell><cell>1.1773 (0.7723)</cell><cell>0.7439 (0.8335 )</cell></row><row><cell>ambush 2</cell><cell>1.0453 (0.7261)</cell><cell>0.9914 (0.6943)</cell><cell>0.8535 (0.6754 )</cell></row><row><cell>ambush 4</cell><cell>1.0316 (0.7273)</cell><cell>0.9871 (0.6842)</cell><cell>0.9603 (0.7493 )</cell></row><row><cell>ambush 5</cell><cell>0.9759 (0.6902)</cell><cell>1.0201 (0.7237)</cell><cell>0.9439 (0.7267 )</cell></row><row><cell>ambush 6</cell><cell>1.0354 (0.7381)</cell><cell>0.8732 (0.5989)</cell><cell>0.7790 (0.5633 )</cell></row><row><cell>ambush 7</cell><cell>1.0891 (0.7740)</cell><cell>1.0856 (0.7536)</cell><cell>0.9086 (0.8656 )</cell></row><row><cell>bamboo 1</cell><cell>0.8173 (0.4868)</cell><cell>0.9146 (0.4904)</cell><cell>0.4114 (0.3262 )</cell></row><row><cell>bamboo 2</cell><cell>1.3628 (0.8921)</cell><cell>1.3069 (0.5563)</cell><cell>1.5411 (1.3005 )</cell></row><row><cell>bandage 1</cell><cell>0.8503 (0.6008)</cell><cell>0.7377 (0.5720)</cell><cell>0.6774 (0.5623 )</cell></row><row><cell>bandage 2</cell><cell>0.9607 (0.6911)</cell><cell>0.8218 (0.6342)</cell><cell>0.8610 (0.7053 )</cell></row><row><cell>cave 2</cell><cell>0.8766 (0.5860)</cell><cell>0.8615 (0.5611)</cell><cell>0.5518 (0.5526 )</cell></row><row><cell>cave 4</cell><cell>0.9357 (0.6451)</cell><cell>0.9018 (0.6236)</cell><cell>0.6130 (0.6235 )</cell></row><row><cell>market 2</cell><cell>0.8616 (0.5444)</cell><cell>0.8612 (0.6297)</cell><cell>0.3914 (0.4283 )</cell></row><row><cell>market 5</cell><cell>1.0810 (0.7546)</cell><cell>1.0584 (0.7651)</cell><cell>0.8849 (0.7895 )</cell></row><row><cell>market 6</cell><cell>1.1404 (0.7934)</cell><cell>1.1665 (0.7833)</cell><cell>1.0946 (0.8966 )</cell></row><row><cell cols="2">mountain 1 1.0020 (0.6989)</cell><cell>1.0628 (0.7388)</cell><cell>0.9432 (0.6799 )</cell></row><row><cell>shaman 2</cell><cell>0.8969 (0.6403)</cell><cell>0.8540 (0.6044)</cell><cell>0.6092 (0.6132 )</cell></row><row><cell>shaman 3</cell><cell>1.1720 (0.8081)</cell><cell>1.2471 (0.8956)</cell><cell>1.0625 (0.7017 )</cell></row><row><cell>sleeping 1</cell><cell>1.1975 (0.8345)</cell><cell>1.2405 (0.8591)</cell><cell>1.0812 (0.7469 )</cell></row><row><cell>sleeping 2</cell><cell>1.0476 (0.7201)</cell><cell>1.0677 (0.7126)</cell><cell>0.8090 (0.8862 )</cell></row><row><cell>temple 2</cell><cell>0.8724 (0.5483)</cell><cell>0.8519 (0.5546)</cell><cell>0.5240 (0.5110 )</cell></row><row><cell>temple 3</cell><cell>0.8945 (0.6144)</cell><cell>0.9933 (0.6217)</cell><cell>0.7312 (0.6574 )</cell></row><row><cell>ALL</cell><cell>1.0048 (0.1294)</cell><cell>1.0131 (0.1549)</cell><cell>0.8002 (0.2607)</cell></row><row><cell cols="4">1) LIVE Livestream VQA database [51] -This is a new</cell></row><row><cell cols="4">database containing 315 professional-grade videos with</cell></row><row><cell cols="4">synthetic distortions. 45 videos are of pristine quality,</cell></row><row><cell cols="4">and 7 different distortions were synthetically applied to</cell></row><row><cell cols="4">each pristine video to create 315 synthetically distorted</cell></row><row><cell cols="4">videos. The distortions that were applied are compres-</cell></row><row><cell cols="4">sion, aliasing, interlacing, judder, flicker, and frame-</cell></row><row><cell cols="4">drop. All videos are of resolution 3840x2160, and the</cell></row><row><cell cols="4">study was conducted on a 4K TV. 40 people participated</cell></row><row><cell cols="2">in the study.</cell><cell></cell><cell></cell></row><row><cell cols="4">2) LIVE ETRI database [52] -The LIVE ETRI database</cell></row><row><cell cols="4">contains 437 videos generated by applying various levels</cell></row></table><note>of combined space-time subsampling and video com- pression on 15 diverse video contents. The bitrates in this database vary from around 500 kbps to around 50 Mbps, and the database has 30 fps, 60 fps, and 120 fps videos. A total of 34 subjects took part in the study. 3) YouTube UGC [53] -The YouTube UGC database</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III MEDIAN</head><label>III</label><figDesc>SROCC FOR 1000 SPLITS ON THE LIVE LIVESTREAM VQA DATABASE FOR DIFFERENT VALUES OF K, WHERE WINDOW SIZE IS 2K + 1 ? 2K + 1</figDesc><table><row><cell></cell><cell>K</cell><cell cols="2">SROCC</cell></row><row><cell></cell><cell>3</cell><cell>0.7967</cell></row><row><cell></cell><cell>5</cell><cell>0.7882</cell></row><row><cell></cell><cell>7</cell><cell>0.7963</cell></row><row><cell></cell><cell cols="3">TABLE IV</cell></row><row><cell cols="4">MEDIAN SROCC FOR 1000 SPLITS ON THE LIVE LIVESTREAM VQA</cell></row><row><cell cols="4">DATABASE FOR DIFFERENT VALUES OF a AND R = T</cell></row><row><cell>a</cell><cell cols="2">R = T</cell><cell>SROCC</cell></row><row><cell>0.25</cell><cell>5</cell><cell></cell><cell>0.7847</cell></row><row><cell>0.25</cell><cell>9</cell><cell></cell><cell>0.7274</cell></row><row><cell>0.5</cell><cell>5</cell><cell></cell><cell>0.7967</cell></row><row><cell>0.75</cell><cell>5</cell><cell></cell><cell>0.7869</cell></row><row><cell>0.75</cell><cell>9</cell><cell></cell><cell>0.7864</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V MEDIAN</head><label>V</label><figDesc>SROCC FOR 1000 SPLITS ON THE LIVE LIVESTREAM VQA DATABASE FOR DIFFERENT VALUES OF CHIP DOWNSAMPLING FACTOR D</figDesc><table><row><cell>D</cell><cell>SROCC</cell></row><row><cell>1</cell><cell>0.7877</cell></row><row><cell>4</cell><cell>0.7967</cell></row><row><cell>8</cell><cell>0.7920</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI MEDIAN</head><label>VI</label><figDesc>SROCC FOR 1000 SPLITS ON THE LIVE LIVESTREAM VQA DATABASE FOR DIFFERENT VALUES OF Q</figDesc><table><row><cell cols="2">Q SROCC</cell></row><row><cell>3</cell><cell>0.7852</cell></row><row><cell>4</cell><cell>0.7658</cell></row><row><cell>6</cell><cell>0.7967</cell></row><row><cell>9</cell><cell>0.7626</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VII MEDIAN</head><label>VII</label><figDesc>SROCC, LCC, AND RMSE FOR LIVE LIVESTREAM DATABASE. STANDARD DEVIATIONS ARE IN PARENTHESES. BEST PERFORMING ALGORITHM IS BOLD-FACED.</figDesc><table><row><cell>METHOD</cell><cell>SROCC?</cell><cell>LCC?</cell><cell>RMSE?</cell></row><row><cell>NIQE [17]</cell><cell>0.3104 (0.0963)</cell><cell>0.48674 (0.3323)</cell><cell>11.2799 (1.0362)</cell></row><row><cell>VGG-19</cell><cell></cell><cell></cell><cell></cell></row><row><cell>IMAGE QUALITY METRICS</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII MEDIAN</head><label>VIII</label><figDesc>SROCC, LCC, AND RMSE FOR LIVE ETRI. STANDARD DEVIATIONS ARE IN PARENTHESES. BEST PERFORMING ALGORITHM IS BOLD-FACED.</figDesc><table><row><cell>METHOD</cell><cell></cell><cell>SROCC?</cell><cell>LCC?</cell><cell>RMSE?</cell></row><row><cell>IMAGE QUALITY METRICS</cell><cell>NIQE [17] BRISQUE [16]</cell><cell>0.3966 (0.1983) 0.2656 (0.3042)</cell><cell>0.4435 (0.3049) 0.4315 (0.2461)</cell><cell>12.2663 (1.6530) 12.3339 (1.5116)</cell></row><row><cell></cell><cell>TLVQM [7]</cell><cell>0.2343 (0.1934)</cell><cell>0.3018 (0.1957)</cell><cell>13.0663 (1.2398)</cell></row><row><cell>VIDEO QUALITY METRICS</cell><cell>V-BLIINDS [3] ChipQA-0 [2]</cell><cell>0.4798 (0.1411) 0.4012 (0.1591)</cell><cell>0.5344 (0.1194) 0.4634 (0.1356)</cell><cell>11.7581 (1.4058) 12.1476 (1.3464)</cell></row><row><cell></cell><cell>ChipQA</cell><cell>0.6323 (0.1474)</cell><cell>0.6822 (0.1182)</cell><cell>10.0769 (1.4701)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX MEDIAN</head><label>IX</label><figDesc>SROCC, LCC, AND RMSE FOR KONVID 1K DATABASE. STANDARD DEVIATIONS ARE IN PARENTHESES. BEST PERFORMING ALGORITHM IS BOLD-FACED. LCC, AND RMSE FOR LIVE VQC. STANDARD DEVIATIONS ARE IN PARENTHESES. BEST PERFORMING ALGORITHM IS BOLD-FACED. LCC, AND RMSE FOR YOUTUBE UGC. STANDARD DEVIATIONS ARE IN PARENTHESES. BEST PERFORMING ALGORITHM IS BOLD-FACED. Box-plot of distribution of SROCCs for different algorithms for 1000 splits of the LIVE Livestream database.</figDesc><table><row><cell>Fig. 11.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">METHOD</cell><cell>SROCC?</cell><cell>LCC?</cell><cell>RMSE?</cell></row><row><cell></cell><cell>NIQE [17]</cell><cell>0.3559 (0.0544)</cell><cell>0.2533 (0.3338)</cell><cell>0.6318 (0.0299)</cell></row><row><cell>IMAGE QUALITY METRICS</cell><cell>BRISQUE [16] (1 fps) HIGRADE [13]</cell><cell>0.5876 (0.0422) 0.7310 (0.0311)</cell><cell>0.5989 (0.0407) 0.7390 (0.0296)</cell><cell>0.5118 (0.0244) 0.4306 (0.0219)</cell></row><row><cell></cell><cell>FRIQUEE [12] (1 fps)</cell><cell>0.7452 (0.0274)</cell><cell>0.7506 (0.0260)</cell><cell>0.4212 (0.0208)</cell></row><row><cell></cell><cell>CORNIA [18] (1 fps)</cell><cell>0.7685 (0.0253)</cell><cell>0.7671 (0.0240)</cell><cell>0.4093 (0.0196)</cell></row><row><cell></cell><cell>VIIDEO [4]</cell><cell>0.3107 (0.0492)</cell><cell>0.3261 (0.0500)</cell><cell>0.6043 (0.0243)</cell></row><row><cell>VIDEO QUALITY METHODS</cell><cell>TLVQM [7] V-BLIINDS [3]</cell><cell>0.7750 (0.0242) 0.7127 (0.0355)</cell><cell>0.7715(0.0226) 0.7085 (0.0330)</cell><cell>0.4058 (0.0178) 0.4523 (0.0210)</cell></row><row><cell></cell><cell>ChipQA-0 [2]</cell><cell>0.6973 (0.0311)</cell><cell>0.6943 (0.0311)</cell><cell>0.4600 (0.0221)</cell></row><row><cell></cell><cell>ChipQA</cell><cell>0.7629 (0.0260)</cell><cell>0.7625 (0.0256)</cell><cell>0.4105 (0.0206)</cell></row><row><cell></cell><cell cols="2">TABLE X</cell><cell></cell><cell></cell></row><row><cell cols="2">MEDIAN SROCC, METHOD</cell><cell>SROCC?</cell><cell>LCC?</cell><cell>RMSE?</cell></row><row><cell></cell><cell>NIQE [17]</cell><cell>0.4603 (0.0735)</cell><cell>0.3782 (0.4259)</cell><cell>16.7678 (1.3756)</cell></row><row><cell>IMAGE QUALITY METRICS</cell><cell>BRISQUE [16] (1 fps) HIGRADE [13]</cell><cell>0.6192 (0.0529) 0.6103 (0.068)</cell><cell>0.6519 (0.0470) 0.6332 (0.065)</cell><cell>12.7489 (0.7664) 13.027 (0.904)</cell></row><row><cell></cell><cell>FRIQUEE [12] (1 fps)</cell><cell>0.6579 (0.053)</cell><cell>0.7000 (0.058)</cell><cell>12.198 (0.914)</cell></row><row><cell></cell><cell>CORNIA [18] (1 fps)</cell><cell>0.6719 (0.047)</cell><cell>0.7183 (0.042)</cell><cell>11.832 (0.700)</cell></row><row><cell></cell><cell>TLVQM [7]</cell><cell>0.8026 (0.0359)</cell><cell>0.7996 (0.0363)</cell><cell>10.1033 (0.7920)</cell></row><row><cell>VIDEO QUALITY METRICS</cell><cell>VIIDEO [4] V-BLIINDS [3]</cell><cell>-0.0336 (0.0770) 0.7005 (0.0457)</cell><cell>-0.0064 (0.0954) 7251 (0.0466)</cell><cell>16.8163 (0.9094) 11.4744 (0.7483)</cell></row><row><cell></cell><cell>ChipQA-0 [2]</cell><cell>0.6692 (0.0532)</cell><cell>6965 (0.0477)</cell><cell>12.1574 (0.8065)</cell></row><row><cell></cell><cell>ChipQA</cell><cell>0.7192 (0.0550)</cell><cell>0.7299 (0.0479)</cell><cell>11.6014 (0.8370)</cell></row><row><cell></cell><cell cols="2">TABLE XI</cell><cell></cell><cell></cell></row><row><cell cols="2">MEDIAN SROCC, METHOD</cell><cell>SROCC?</cell><cell>LCC?</cell><cell>RMSE?</cell></row><row><cell></cell><cell>NIQE [17]</cell><cell>0.1788 (0.050)</cell><cell>0.1788 (0.094)</cell><cell>0.6390 (0.026)</cell></row><row><cell>IMAGE QUALITY METRICS</cell><cell>BRISQUE [16] (1 fps) HIGRADE [13]</cell><cell>0.3820 (0.051) 0.7376 (0.033)</cell><cell>0.3952 (0.048) 0.7216 (0.033)</cell><cell>0.5919 (0.021) 0.4471 (0.024)</cell></row><row><cell></cell><cell>FRIQUEE [12] (1 fps)</cell><cell>0.7652 (0.03)</cell><cell>0.7571 (0.032)</cell><cell>0.4169 (0.023)</cell></row><row><cell></cell><cell>CORNIA [18] (1 fps)</cell><cell>0.5972 (0.041)</cell><cell>0.6057 (0.039)</cell><cell>0.5136 (0.024)</cell></row><row><cell></cell><cell>TLVQM [7]</cell><cell>0.6693 (0.03)</cell><cell>0.6590 (0.03)</cell><cell>0.4849 (0.022)</cell></row><row><cell>VIDEO QUALITY METRICS</cell><cell>V-BLIINDS [3] ChipQA-0 [2]</cell><cell>0.5590 (0.049) 0.5595(0.038)</cell><cell>0.5551 (0.046) 0.5572 (0.040)</cell><cell>0.5356 (0.022) 0.5386 (0.024)</cell></row><row><cell></cell><cell>ChipQA</cell><cell>0.7014 (0.031)</cell><cell>0.6911 (0.033)</cell><cell>0.4729 (0.025)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE XII SROCC</head><label>XII</label><figDesc>OF THE COMPARED NR VQA MODELS ON DIFFERENT SUBSETS OF THE LIVE LIVESTREAM DATABASE. THE SCORES OF THE TOP PERFORMING ALGORITHM IN EACH CATEGORY ARE BOLDFACED THE COMPARED NR VQA MODELS ON THE LIVE LIVESTREAM DATABASE. THE SCORES OF THE THREE TOP PERFORMING ALGORITHMS ARE BOLDFACED.</figDesc><table><row><cell></cell><cell>METHOD</cell><cell>OVERALL</cell><cell cols="2">COMPRESSION</cell><cell>ALIASING</cell><cell>JUDDER</cell><cell cols="2">FLICKER</cell><cell>FRAME DROP</cell><cell>INTERLACING</cell></row><row><cell cols="2">NIQE(1 fps)</cell><cell>0.3232</cell><cell></cell><cell>0.2775</cell><cell>0.2860</cell><cell>0.2863</cell><cell cols="2">0.2832</cell><cell>0.2842</cell><cell>0.2780</cell></row><row><cell cols="2">BRISQUE(1 fps)</cell><cell>0.6381</cell><cell></cell><cell>0.5748</cell><cell>0.7564</cell><cell>0.8235</cell><cell cols="2">0.6574</cell><cell>0.3849</cell><cell>0.8689</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.0044</cell><cell></cell><cell>0.0053</cell><cell>0.0073</cell><cell>0.0055</cell><cell cols="2">0.0013</cell><cell>0.0024</cell><cell>0.0064</cell></row><row><cell cols="2">CORNIA(1 fps)</cell><cell>0.6778</cell><cell></cell><cell>0.6894</cell><cell>0.7853</cell><cell>0.7657</cell><cell cols="2">0.7049</cell><cell>0.2776</cell><cell>0.8824</cell></row><row><cell cols="2">HIGRADE(1 fps)</cell><cell>0.6916</cell><cell></cell><cell>0.6244</cell><cell>0.7141</cell><cell>0.7441</cell><cell cols="2">0.6440</cell><cell>0.6130</cell><cell>0.8287</cell></row><row><cell cols="2">V-BLIINDS</cell><cell>0.7330</cell><cell></cell><cell>0.6450</cell><cell>0.7606</cell><cell>0.8679</cell><cell cols="2">0.6182</cell><cell>0.7131</cell><cell>0.8060</cell></row><row><cell></cell><cell>TLVQM</cell><cell>0.7503</cell><cell></cell><cell>0.5614</cell><cell>0.7420</cell><cell>0.8328</cell><cell cols="2">0.6202</cell><cell>0.8555</cell><cell>0.8173</cell></row><row><cell></cell><cell>ChipQA-0</cell><cell>0.7513</cell><cell></cell><cell>0.6594</cell><cell>0.7791</cell><cell>0.8513</cell><cell cols="2">0.6491</cell><cell>0.6780</cell><cell>0.8534</cell></row><row><cell></cell><cell>ChipQA</cell><cell>0.7575</cell><cell></cell><cell>0.7028</cell><cell>0.7792</cell><cell>0.8740</cell><cell cols="2">0.7647</cell><cell>0.7647</cell><cell>0.8513</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE XIII</cell><cell></cell></row><row><cell cols="2">PLCC OF METHOD</cell><cell>OVERALL</cell><cell cols="2">COMPRESSION</cell><cell>ALIASING</cell><cell>JUDDER</cell><cell cols="2">FLICKER</cell><cell>FRAME DROP</cell><cell>INTERLACING</cell></row><row><cell cols="2">NIQE(1 fps)</cell><cell>0.4962</cell><cell></cell><cell>0.2805</cell><cell>0.2865</cell><cell>0.2860</cell><cell cols="2">0.2848</cell><cell>0.2849</cell><cell>0.2850</cell></row><row><cell cols="2">BRISQUE(1 fps)</cell><cell>0.6698</cell><cell></cell><cell>0.7345</cell><cell>0.9321</cell><cell>0.8726</cell><cell cols="2">0.7268</cell><cell>0.3902</cell><cell>0.9118</cell></row><row><cell></cell><cell>VIIDEO</cell><cell>0.2155</cell><cell></cell><cell>0.1222</cell><cell>0.1222</cell><cell>0.1235</cell><cell cols="2">0.1247</cell><cell>0.1256</cell><cell>0.1259</cell></row><row><cell cols="2">CORNIA(1 fps)</cell><cell>0.7257</cell><cell></cell><cell>0.8115</cell><cell>0.9508</cell><cell>0.7875</cell><cell cols="2">0.8063</cell><cell>0.2138</cell><cell>0.9142</cell></row><row><cell cols="2">HIGRADE(1 fps)</cell><cell>0.6990</cell><cell></cell><cell>0.7595</cell><cell>0.9353</cell><cell>0.7614</cell><cell cols="2">0.6889</cell><cell>0.6062</cell><cell>0.8800</cell></row><row><cell cols="2">V-BLIINDS</cell><cell>0.7477</cell><cell></cell><cell>0.8055</cell><cell>0.9202</cell><cell>0.9200</cell><cell cols="2">0.7086</cell><cell>0.7443</cell><cell>0.8873</cell></row><row><cell></cell><cell>TLVQM</cell><cell>0.7513</cell><cell></cell><cell>0.6788</cell><cell>0.9273</cell><cell>0.8914</cell><cell cols="2">0.7724</cell><cell>0.8738</cell><cell>0.8358</cell></row><row><cell></cell><cell>ChipQA-0</cell><cell>0.7565</cell><cell></cell><cell>0.7783</cell><cell>0.9490</cell><cell>0.9071</cell><cell cols="2">0.6609</cell><cell>0.6945</cell><cell>0.9075</cell></row><row><cell></cell><cell>ChipQA</cell><cell>0.7705</cell><cell></cell><cell>0.8719</cell><cell>0.9735</cell><cell>0.9472</cell><cell cols="2">0.8989</cell><cell>0.8730</cell><cell>0.9133</cell></row><row><cell></cell><cell cols="2">TABLE XIV</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE XV</cell></row><row><cell cols="6">ABLATION STUDY WITH MEDIAN SROCC FOR 1000 SPLITS ON THE LIVE</cell><cell cols="3">ABLATION STUDY WITH MEDIAN SROCC FOR 100 SPLITS ON THE</cell></row><row><cell cols="6">LIVESTREAM VQA DATABASE AS DIFFERENT FEATURE SPACES ARE</cell><cell cols="3">KONVID-1K DATABASE AS DIFFERENT FEATURE SPACES ARE REMOVED</cell></row><row><cell></cell><cell cols="2">REMOVED</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RANK</cell><cell>FEATURE SPACES REMOVED</cell><cell>SROCC</cell></row><row><cell>RANK</cell><cell cols="3">FEATURE SPACES REMOVED</cell><cell>SROCC</cell><cell></cell><cell></cell><cell>1</cell><cell>Chroma+Gradient</cell><cell>0.7308</cell></row><row><cell>1</cell><cell cols="2">ST chip+ST Gradient chip</cell><cell></cell><cell>0.7376</cell><cell></cell><cell></cell><cell>2</cell><cell>Luma</cell><cell>0.7448</cell></row><row><cell>2</cell><cell></cell><cell>Luma</cell><cell></cell><cell>0.7805</cell><cell></cell><cell></cell><cell>3</cell><cell>Gradient</cell><cell>0.7460</cell></row><row><cell>3</cell><cell cols="2">ST Gradient chip</cell><cell></cell><cell>0.7865</cell><cell></cell><cell></cell><cell>4</cell><cell>ST chip + ST Gradient chip</cell><cell>0.7552</cell></row><row><cell>4</cell><cell cols="2">Gradient</cell><cell></cell><cell>0.7876</cell><cell></cell><cell></cell><cell>5</cell><cell>Chroma</cell><cell>0.7589</cell></row><row><cell>5</cell><cell cols="2">ST chip</cell><cell></cell><cell>0.7963</cell><cell></cell><cell></cell><cell>6</cell><cell>ST Gradient chip</cell><cell>0.7641</cell></row><row><cell>6</cell><cell cols="2">Chroma</cell><cell></cell><cell>0.7967</cell><cell></cell><cell></cell><cell>7</cell><cell>ST chip</cell><cell>0.7669</cell></row><row><cell>7</cell><cell cols="2">Chroma+Gradient</cell><cell></cell><cell>0.8064</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ST gradient chip features from the algorithm severely affects</cell><cell></cell><cell></cell></row></table><note>performance on LIVE Livestream, but does not significantly affect performance on Konvid-1k. This suggests that temporal</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE XVI FEATURE</head><label>XVI</label><figDesc>RANKING FOR THE LIVE LIVESTREAM VQA DATABASE BASED ON SEQUENTIAL FORWARD SELECTION This is the median SROCC obtained when the feature in the row entry is added to the set of the features ranked above it.</figDesc><table><row><cell>RANK</cell><cell>FEATURE</cell><cell>FEATURE DESCRIPTION</cell><cell>SROCC*</cell></row><row><cell></cell><cell>INDEX</cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>130</cell><cell>Variance of right half of MSCN distribution of luma</cell><cell>0.6515</cell></row><row><cell>2</cell><cell>204</cell><cell>GGD shape of ST chips at half-scale</cell><cell>0.7133</cell></row><row><cell>3</cell><cell>149</cell><cell>NIQE score</cell><cell>0.7411</cell></row><row><cell>4</cell><cell>9</cell><cell>GGD shape of MSCNs of Chroma ? map</cell><cell>0.7696</cell></row><row><cell>5</cell><cell>25</cell><cell>AGGD shape of pairwise products (along the main diagonal) of MSCNs of the gradient</cell><cell>0.8066</cell></row><row><cell></cell><cell></cell><cell>magnitude</cell><cell></cell></row><row><cell>6</cell><cell>150</cell><cell>GGD shape of ST chips at full-scale</cell><cell>0.8258</cell></row><row><cell>7</cell><cell>161</cell><cell>AGGD shape of pairwise products (along the main diagonal) of MSCNs of ST chips</cell><cell>0.8217</cell></row><row><cell>8</cell><cell>123</cell><cell>AGGD shape of pairwise products (along the main diagonal) of MSCNs of luma</cell><cell>0.8043</cell></row><row><cell>9</cell><cell>29</cell><cell>AGGD shape of pairwise products (along the horizontal) of the gradient magnitude</cell><cell>0.8062</cell></row><row><cell>10</cell><cell>98</cell><cell>Variance of left half of MSCN distribution of pairwise product (along the main diagonal) of</cell><cell>0.8134</cell></row><row><cell></cell><cell></cell><cell>MSCNs of the gradient magnitude at half-scale</cell><cell></cell></row><row><cell>*Note:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XVII CROSSTABLE XVIII CROSS</head><label>XVIIXVIII</label><figDesc>DATABASE SROCC FOR METHODS TRAINED ON ROW ENTRY AND EVALUATED ON COLUMN ENTRY FOR DATABASES WITH PROFESSIONALLY-CAPTURED CONTENT. DATABASE SROCC FOR METHODS TRAINED ON ROW ENTRY AND EVALUATED ON COLUMN ENTRY FOR UGC DATABASES.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell cols="3">Livestream ETRI</cell></row><row><cell>CHIPQA</cell><cell cols="2">Livestream ETRI</cell><cell cols="2">-0.2201</cell><cell>0.1286 -</cell></row><row><cell>TLVQM</cell><cell cols="2">Livestream ETRI</cell><cell cols="2">-0.2568</cell><cell>0.1478 -</cell></row><row><cell>VBLIINDS</cell><cell cols="2">Livestream ETRI</cell><cell cols="2">-0.4331</cell><cell>0.3067 -</cell></row><row><cell>Method</cell><cell></cell><cell>VQC</cell><cell></cell><cell>Konvid</cell><cell>YT-UGC</cell></row><row><cell></cell><cell>VQC</cell><cell>-</cell><cell></cell><cell>0.5569</cell><cell>0.0757</cell></row><row><cell>CHIPQA</cell><cell>Konvid</cell><cell cols="2">0.5478</cell><cell>-</cell><cell>0.0817</cell></row><row><cell></cell><cell>YT-UGC</cell><cell cols="2">0.5436</cell><cell>0.4030</cell><cell>-</cell></row><row><cell></cell><cell>VQC</cell><cell>-</cell><cell></cell><cell>0.6026</cell><cell>0.1768</cell></row><row><cell>TLVQM</cell><cell>Konvid</cell><cell cols="2">0.6983</cell><cell>-</cell><cell>0.3241</cell></row><row><cell></cell><cell>YT-UGC</cell><cell cols="2">0.3279</cell><cell>0.5588</cell><cell>-</cell></row><row><cell></cell><cell>VQC</cell><cell>-</cell><cell></cell><cell>0.6323</cell><cell>0.0969</cell></row><row><cell>VBLIINDS</cell><cell>Konvid</cell><cell cols="2">0.6320</cell><cell>-</cell><cell>0.1677</cell></row><row><cell></cell><cell>YT-UGC</cell><cell cols="2">0.1026</cell><cell>0.2072</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIX COMPUTATIONAL</head><label>XIX</label><figDesc>ANALYSIS OF ALGORITHMS ON A SINGLE 3840X2160 VIDEO WITH 210 FRAMES FROM THE LIVE LIVESTREAM VQA DATABASED 2 + Q RD 2 + Q log Q R 3 D 2 )N T ) d:MSCN window size, D: downsampling factor Q: chip search's quantization factor, R: size of each N +k 2 K)T 1 +h 2 2 N T 2 ) h 1 , h 2 : filter size, k: motion estimation block size, K: number of key points</figDesc><table><row><cell>METHOD</cell><cell>Number of fea-</cell><cell>Language</cell><cell>Computational Complexity</cell><cell cols="2">GFLOPS Time</cell></row><row><cell></cell><cell>tures</cell><cell></cell><cell></cell><cell></cell><cell>(s)</cell></row><row><cell>BRISQUE [16]</cell><cell>36</cell><cell>MATLAB</cell><cell>O(d 2 N T ) d: MSCN window size</cell><cell>352</cell><cell>301</cell></row><row><cell>ChipQA</cell><cell>221</cell><cell>Python</cell><cell>O(( d 2</cell><cell>700</cell><cell>814</cell></row><row><cell></cell><cell></cell><cell></cell><cell>dimension of a chip</cell><cell></cell><cell></cell></row><row><cell>TLVQM [7]</cell><cell>75</cell><cell>MATLAB</cell><cell>O((h 2 1</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors thank the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported in this paper. URL: http://www.tacc.utexas.edu.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cisco</surname></persName>
		</author>
		<ptr target="https://www.cisco.com/c/en/us/solutions/executive-perspectives/annual-internet-report/" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Annual Internet Report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Noreference video quality assessment using space-time chips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ebenezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.00031</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind prediction of natural video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1352" to="1365" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A completely blind video integrity oracle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An optical flow-based no-reference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2400" to="2404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No-reference video quality assessment using natural spatiotemporal scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V R</forename><surname>Dendi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5612" to="5624" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Two-level approach for no-reference consumer video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5923" to="5938" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blind natural video quality prediction via statistical temporal features and deep spatial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Korhonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Int. Conf. Multimed., 2020</title>
		<imprint>
			<biblScope unit="page" from="3311" to="3319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">No-reference video quality assessment using multi-level spatially pooled features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>G?tz-Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07966</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inception-v4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artif. Intell</title>
		<meeting>AAAI Conf. Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual quality prediction on authentically distorted images using a bag of features approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="32" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">No-reference quality assessment of tone-mapped HDR pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2957" to="2971" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale study of perceptual video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="612" to="627" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The Konstanz natural video database (Konvid-1k)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hosu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jenadeleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Szir?nyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Saupe</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>in Int</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<title level="m">Conf. Quality of Multimedia Experience</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unified quality assessment of in-the-wild videos with mixed datasets training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1238" to="1257" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video quality assessment using space-time slice mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">115749</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ViS3: an algorithm for video quality assessment via analysis of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Video quality assessment via gradient magnitude similarity deviation of spatial and spatiotemporal slices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mobile Devices and Multimedia: Enabling Technologies</title>
		<imprint>
			<publisher>SPIE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9411</biblScope>
			<biblScope unit="page" from="182" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video quality assessment based on motion structure partition similarity of spatiotemporal slice images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Electron. Imag</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Motion tuned spatio-temporal quality assessment of natural videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="350" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A structural similarity metric for video based on motion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Acoust., Speech, Signal Process</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="869" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An optical flow-based full reference video quality assessment algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Manasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Channappayya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2480" to="2492" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Natural image statistics and divisive normalization: Modeling nonlinearity and adaptation in cortical neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Probabilistic models of the brain</title>
		<editor>R. Rao, B. Olshausen, and M. Lewicki</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="203" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A model of neuronal responses in visual area MT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Res</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal decorrelation: a theory of lagged and nonlagged responses in the lateral geniculate nucleus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Atick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Netw.: Comput. Neural Syst</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="178" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatiotemporal energy models for the perception of motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="284" to="299" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Model of human visual-motion sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ahumada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Opt. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1002</biblScope>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matlab O</surname></persName>
		</author>
		<title level="m">KN T ), d: window size, K: codebook size</title>
		<imprint>
			<biblScope unit="volume">4480</biblScope>
			<biblScope unit="page">2056</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<idno>Python O(d 2 +1.5(1?0.5</idno>
		<title level="m">P +1 )Iw 2 )N T , d: MSCN window size, P : number of pyramids for optical flow, I: number of iterations per pyramid, w: window size for optical flow 2605</title>
		<imprint>
			<biblScope unit="page">2655</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">log(k)N + k 2 w 3 )T ), d: MSCN window size, k: DCT block size, w: motion vector tensor size</title>
		<editor>Python O((d N +</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">+ k)N T ), d: MSCN window size, k: gradient kernel size 9604 16240 FRIQUEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matlab O</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">560</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">dN + 4N (log(N ) + m 2 ))T ), h 1 , h 2 : filter size, f : number of color spaces, k: motion estimation block size, K: number of key points 470</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matlab O</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">505</biblScope>
			<biblScope unit="page">93240</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">N -number of pixels per frame. T -number of frames in video. T1 -total number of frames divided by 2. T2 -number of frames sampled at 1 frame per second</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Note</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Determining Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Techniques and Applications of Image Understanding</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">281</biblScope>
			<biblScope unit="page" from="319" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Robust dynamic motion estimation over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Comp. Soc. Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Comp. Soc. Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="296" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The statistics of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Ruderman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Network: Comp. Neural Sys</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="517" to="548" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kokoska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zwillinger</surname></persName>
		</author>
		<title level="m">CRC standard probability and statistics tables and formulae</title>
		<imprint>
			<publisher>Crc Press</publisher>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">An approximate analysis of variance test for normality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Francia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">337</biblScope>
			<biblScope unit="page" from="215" to="216" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Evaluating the anderson-darling distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marsaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marsaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical software</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A test for departure from normality based on a biweight estimator of scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Iglewicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="331" to="333" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A suggestion for using powerful and informative tests of normality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>D&amp;apos;agostino</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="316" to="321" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Ugcvqa: Benchmarking blind video quality assessment for user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14354</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Estimation of shape parameter for generalized gaussian distributions in subband decompositions of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon-Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="56" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Colorimetry: understanding the CIE system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schanda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Rapique: Rapid and accurate video quality prediction of user generated content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10955</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comp. Vis</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haz?rba?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comp. Vis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV), ser. Part IV</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012-10" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A new three-step search algorithm for block motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Liou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Circuits Syst. Video Tech</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="442" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Very high accuracy velocity estimation using orientation tensors, parametric motion, and simultaneous segmentation of the motion field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farneback</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="171" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Assessment of subjective and objective quality of live streaming sports videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ebenezer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sethuraman</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">A subjective and objective study of space-time subsampled video quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Homan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00088</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Youtube ugc dataset for video compression research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Inguva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Workshop Multimed. Signal Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A comparative evaluation of temporal pooling methods for blind video quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Birkbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Adsumilli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10651</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A statistical evaluation of recent full reference image quality assessment algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Sabir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3440" to="3451" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">709 : Parameter values for the HDTV standards for production and international programme exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Itu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-05-15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comp. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
