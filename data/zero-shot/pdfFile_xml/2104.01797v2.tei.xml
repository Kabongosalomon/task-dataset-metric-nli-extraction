<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Game AI Research Center</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Tencent Game AI Research Center</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
							<email>robby.tan@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Yale-NUS College</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In monocular video 3D multi-person pose estimation, inter-person occlusion and close interactions can cause human detection to be erroneous and human-joints grouping to be unreliable. Existing top-down methods rely on human detection and thus suffer from these problems. Existing bottom-up methods do not use human detection, but they process all persons at once at the same scale, causing them to be sensitive to multiple-persons scale variations. To address these challenges, we propose the integration of top-down and bottom-up approaches to exploit their strengths. Our top-down network estimates human joints from all persons instead of one in an image patch, making it robust to possible erroneous bounding boxes. Our bottomup network incorporates human-detection based normalized heatmaps, allowing the network to be more robust in handling scale variations. Finally, the estimated 3D poses from the top-down and bottom-up networks are fed into our integration network for final 3D poses. Besides the integration of top-down and bottom-up networks, unlike existing pose discriminators that are designed solely for a single person, and consequently cannot assess natural interperson interactions, we propose a two-person pose discriminator that enforces natural two-person interactions. Lastly, we also apply a semi-supervised method to overcome the 3D ground-truth data scarcity. Quantitative and qualitative evaluations show the effectiveness of the proposed method. Our code is available publicly. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D multi-person poses from a monocular video has drawn increasing attention due to its importance for real-world applications (e.g., <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8]</ref>). Unfortunately, it is generally still challenging and an open prob- The top-down method is RootNet <ref type="bibr" target="#b36">[35]</ref>, the bottom-up method is SMAP <ref type="bibr" target="#b57">[56]</ref>. The input images are from MuPoTS-3D dataset <ref type="bibr" target="#b32">[32]</ref>. The top-down method suffers from inter-person occlusion and the bottom-up method is sensitive to scale variations (i.e., the 3D poses of the two persons in the back are inaccurately estimated). Our method substantially outperforms the state-of-the-art.</p><p>lem, particularly when multiple persons are present in the scene. Multiple persons can generate inter-person occlusion, which causes human detection to be erroneous. Moreover, multiple persons in a scene are likely in close contact with each other and interact, which makes human-joints grouping unreliable.</p><p>Although existing 3D human pose estimation methods (e.g., <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b56">55,</ref><ref type="bibr" target="#b39">38,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b7">8]</ref>) show promising results on single-person datasets like Human3.6M <ref type="bibr" target="#b18">[19]</ref> and Hu-manEva <ref type="bibr" target="#b44">[43]</ref>, these methods do not perform well in 3D multi-person scenarios. Generally, we can divide existing methods into two approaches: top-down and bottomup. Existing top-down 3D pose estimation methods rely considerably on human detection to localize each person, prior to estimating the joints within the detected bounding boxes, e.g., <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b36">35]</ref>. These methods show promising performance for single-person 3D-pose estimation <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b8">9]</ref>, yet since they treat each person individually, they have no awareness of non-target persons and the possible interactions. When multiple persons occlude each other, human detection also become unreliable. Moreover, when target persons are closely interacting with each other, the pose estimator may be misled by the nearby persons, e.g., predicted joints may come from the nearby non-target persons.</p><p>Recent bottom-up methods (e.g., <ref type="bibr" target="#b57">[56,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref>) do not use any human detection and thus can produce results with higher accuracy when multiple persons interact with each other. These methods consider multiple persons simultaneously and, in many cases, better distinguish the joints of different persons. Unfortunately, without using detection, bottom-up methods suffer from the scale variations, and the pose estimation accuracy is compromised, rendering inferior performance compared with top-down approaches <ref type="bibr" target="#b5">[6]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, neither top-down nor bottom-up approach alone can handle all the challenges at once, particularly the challenges of: inter-person occlusion, close interactions, and human-scale variations. Therefore, in this paper, our goal is to integrate the top-down and bottom-up approaches to achieve more accurate and robust 3D multiperson pose estimation from a monocular video.</p><p>To achieve this goal, we introduce a top-down network to estimate human joints inside each detected bounding box. Unlike existing top-down methods that only estimate one human pose given a bounding box, our top-down network predicts 3D poses for all persons inside the bounding box. The joint heatmaps from our top-down network is feed to our bottom-up network, so that our bottom network can be more robust in handling the scale variations. Finally, we feed the estimated 3D poses from both top-down and bottom-up networks into our integration network to obtain the final estimated 3D poses given an image sequence.</p><p>Moreover, unlike existing methods' pose discriminators, which are designed solely for single person, and consequently cannot enforce natural inter-person interactions, we propose a two-person pose discriminator that enforces twoperson natural interactions. Lastly, semi-supervised learning is used to mitigate the data scarcity problem where 3D ground-truth data is limited.</p><p>In summary, our contributions are listed as follows.</p><p>? We introduce a novel two-branch framework, where the top-down branch detects multiple persons and the bottom-up branch incorporates the normalized image patches in its process. Our framework gains benefits from the two branches, and at the same time, overcomes their shortcomings.</p><p>? We employ multi-person pose estimation for our topdown network, which can effectively handle the interperson occlusion and interactions caused by detection errors.</p><p>? We incorporate human detection information into our bottom-up branch so that it can better handle the scale variation, which addresses the problem in existing bottom-up methods.</p><p>? Unlike the existing discriminators that focus on single person pose, we introduce a novel discriminator that enforces the validity of human poses of close pairwise interactions in the camera-centric coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Top-Down Monocular 3D Human Pose Estimation Existing top-down 3D human pose estimation methods commonly use human detection as an essential part of their methods to estimate person-centric 3D human poses <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8]</ref>. They demonstrate promising performance on single-person evaluation datasets <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">43]</ref>, unfortunately the performance decreases in multi-person scenarios, due to inter-person occlusion or close interactions <ref type="bibr" target="#b35">[34,</ref><ref type="bibr" target="#b8">9]</ref>. Moreover, the produced person-centric 3D poses cannot be used for multi-person scenarios, where cameracentric 3D-pose estimation is needed. Top-down methods process each person independently, leading to inadequate awareness of the existence of other persons nearby. As a result, they perform poorly on multi-person videos where inter-person occlusion and close interactions are commonly present. Rogez et al. <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b43">42]</ref> develop a pose proposal network to generate bounding boxes and then perform pose estimation individually for each person. Recently, unlike previous methods that perform person-centric pose estimation, Moon et al. <ref type="bibr" target="#b36">[35]</ref> propose a top-down 3D multi-person poseestimation method that can estimate the poses for all persons in an image in the camera-centric coordinates. However, the method still relies on detection and process each person independently; hence it is likely to suffer from interperson occlusion and close interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-Up Monocular 3D Human Pose Estimation</head><p>A few bottom-up methods have been proposed <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27]</ref>. Fabbri et al. <ref type="bibr" target="#b12">[13]</ref> introduce an encoder-decoder framework to compress a heatmap first, and then decompress it back to the original representations in the test time for fast  HD image processing. Mehta et al. <ref type="bibr" target="#b31">[31]</ref> propose to identify individual joints, compose full-body joints, and enforce temporal and kinematic constraints in three stages for realtime 3D motion capture. Li et al. <ref type="bibr" target="#b24">[25]</ref> develop an integrated method with lower computation complexity for human detection, person-centric pose estimation, and human depth estimation from an input image. Lin et al. <ref type="bibr" target="#b26">[27]</ref> formulate the human depth regression as a bin index estimation problem for multi-person localization in the camera coordinate system. Zhen et al. <ref type="bibr" target="#b57">[56]</ref> estimate the 2.5D representation of body parts first and then reconstruct camera-centric multiperson 3D poses. These methods benefit from the nature of the bottom-up approach, which can process multiple persons simultaneously without relying on human detection. However, since all persons are processed at the same scale, these methods are inevitably sensitive to human scale variations, which limits their applicability on wild videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrated results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-person Discriminator</head><p>Top-Down and Bottom-Up Combination Earlier nondeep learning methods exploring the combination of topdown and bottom-up approaches for human pose estimation are in the forms of data-driven belief propagation, different classifiers for joint location and skeleton, or probabilistic Gaussian mixture modelling <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b52">51,</ref><ref type="bibr" target="#b23">24]</ref>. Recent deep learning based methods that attempt to make use of both top-down and bottom-up information are mainly on estimating 2D poses <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref>. Hu and Ramanan <ref type="bibr" target="#b16">[17]</ref> propose a hierarchical rectified Gaussian model to incor-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Top-Down Network</head><p>Given a human detection bounding box, existing topdown methods estimate full-body joints of one person. Consequently, if there are multiple persons inside the box or partially out-of-bounding box body parts, the full-body joint estimation are likely to be erroneous. <ref type="figure" target="#fig_4">Figure 3</ref> shows such failure examples of existing methods. In contrast, our method produces the heatmaps for all joints inside the bounding box (i.e., enlarged to accommodate inaccurate detection), and estimate the ID for each joint to group them into corresponding persons, similar to <ref type="bibr" target="#b37">[36]</ref>.</p><p>Given an input video, for every frame we apply a human detector <ref type="bibr" target="#b14">[15]</ref>, and crop the image patches based on the detected bounding boxes. A 2D pose detector <ref type="bibr" target="#b5">[6]</ref> is applied to each patch to generate heatmaps for all human joints, such as shoulder, pelvis, ankle, and etc. Specifically, our topdown loss of 2D pose heatmap is an L2 loss between the predicted and ground-truth heatmaps, formulated as:</p><formula xml:id="formula_0">L T D hmap = |H ?H| 2 2 ,<label>(1)</label></formula><p>where H andH are the predicted and ground-truth heatmaps, respectively. Having obtained the 2D pose heatmaps, a directed GCN network is used to refine the potentially incomplete poses caused by occlusions or partially out-of-bounding box body parts, and two TCNs are used to estimate both personcentric 3D pose and camera-centric root depth based on a given sequence of 2D poses similar to <ref type="bibr" target="#b6">[7]</ref>. As the TCN requires the input sequence of the same instance, a pose tracker <ref type="bibr" target="#b49">[48]</ref> is used to track each instance in the input video. We also apply data augmentation in training our TCN so that it can handle occlusions <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bottom-Up Network</head><p>Top-down methods perform estimation inside the bounding boxes, and thus are lack of global awareness of other persons, leading to difficulties to estimate poses in the camera-centric coordinates. To address this problem, we further propose a bottom-up network that processes multiple persons simultaneously. Since the bottom-up pose estimation suffers from human scale variations, we concatenate the heatmaps from our top-down network with the original input frame as the input of our bottom-up network. With the guidance of the top-down heatmaps, which are the results of the object detector and pose estimation based on the normalized boxes, the estimation of the bottom-up network will be more robust to scale variations. Our bottom-up network outputs four heatmaps : a 2D pose heatmap, ID-tag map, relative depth map, and root depth map. The 2D pose heatmap and ID-tag map are defined in the same way as in the previous section (3.1). The relative depth map refers to the depth map of each joint with respect to its root (pelvis) joint. The root depth map represents the depth map of the root joint.</p><p>In particular, the loss functions L BU hmap and L BU id for the heatmap and ID-tag map are similar to <ref type="bibr" target="#b37">[36]</ref>. In addition, we apply the depth loss to the estimations of both the relative depth map h rel and the root depth h root . Please see supplementary material for example of the four estimated heatmaps from the bottom-up network. For N persons and K joints, the loss can be formulated as:</p><formula xml:id="formula_1">L depth = 1 N K n k |h k (x nk , y nk ) ? d nk | 2 ,<label>(2)</label></formula><p>where h is the depth map and d is the ground-truth depth value. Note that, for pelvis (i.e., the root joint), the depth is a camera-centric depth. For other joints, the depth is relative with respect to the corresponding root joint. We group the heatmaps into instances (i.e., persons), and retrieve the joint locations using the same procedure as in the top-down network. Moreover, the values of the cameracentric depth of the root joint z root and the relative depth for the other joints z rel k are obtained by retrieving from the corresponding depth maps where the joints (i.e., root or others) are located. Specifically:</p><formula xml:id="formula_2">z root i = h root (x root i , y root i ) (3) z rel i,k = h rel k (x i,k , y i,k )<label>(4)</label></formula><p>where i, k refer to the i th instance and k th joint, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Integration with Interaction-Aware Discriminator</head><p>Having obtained the results from the top-down and bottom-up networks, we first need to find the corresponding poses between the results from the two networks, i.e., the top-down pose P T D i and bottom-up pose P BU j belong to the same person. Note that P stands for camera-centric 3D pose throughout this paper.</p><p>Given two pose sets from bottom-up branch P BU and top-down branch P T D , we match the poses from both sets, in order to form pose pairs. The similarity of two poses is defined as:</p><formula xml:id="formula_3">Sim i,j = K k=0 min(c BU i,k , c T D j,k )OKS(P BU i,k , P T D j,k ),<label>(5)</label></formula><p>where:</p><formula xml:id="formula_4">OKS(x, y) = exp(? d(x, y) 2 2s 2 ? 2 ),<label>(6)</label></formula><p>OKS stands for object keypoint similarity <ref type="bibr" target="#b53">[52]</ref>, which measures the joint similarity of a given joint pair. d(x, y) is the Euclidean distance between two joints. s and ? are two controlling parameters. Sim i,j measures the similarity between the i th 3D pose P BU i from the bottom-up network and the j th 3D pose P T D j from the top-down network over K joints. Note that both poses from top-down P T D and bottom-up P BU are camera-centric; thus, the similarity is measured based on the camera coordinate system. The c BU i,k and c T D j,k are the confidence values of joint k for 3D poses P BU i and P T D j , respectively. Having computed the similarity matrix between the two sets of poses P T D and P BU according to the Sim i,j definition, the Hungarian algorithm <ref type="bibr" target="#b22">[23]</ref> is used to obtain the matching results.</p><p>Once the matched pairs are obtained, we feed each pair of the 3D poses and the confidence score of each joint to our integration network. Our integration network consists of 3 fully connected layers, which outputs the final estimation.</p><p>Integration Network Training To train the integration network, we take some samples from the ground-truth 3D poses. We apply data augmentation: 1) random masking the joints with a binary mask M kpt to simulate occlusions; 2) random shifting the joints to simulate the inaccurate pose detection; and 3) random zeroing one from a pose pair to simulate unpaired poses. The loss of the integration network is an L2 loss between the predicted 3D pose and its ground-truth:</p><formula xml:id="formula_5">L int = 1 K k |P k ?P k | 2 ,<label>(7)</label></formula><p>where K is the number of the estimated joints. P andP are the estimated and ground-truth 3D poses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-Person Discriminator</head><p>For training the integration network, we propose a novel inter-person discriminator. Unlike most existing discriminators for human pose estimation (e.g. <ref type="bibr" target="#b51">[50,</ref><ref type="bibr" target="#b7">8]</ref>), where they can only discriminate the plausible 3D poses of one person, we propose an interaction-aware discriminator to enforce the interaction of a pose pair is natural and reasonable, which not only includes the existing single-person discriminator, but also generalize to interacting persons. Specifically, our discriminator contains two sub-networks: D 1 , which is dedicated for one person-centric 3D poses; and, D 2 , which is dedicated for a pair of camera-centric 3D poses from two persons. We apply the following loss to train the network, which is formulated as:</p><formula xml:id="formula_6">L dis = log(C) + log(1 ? C)<label>(8)</label></formula><p>where:</p><formula xml:id="formula_7">C = 0.25(D 1 (P a ) + D 1 (P b )) + 0.5D 2 (P a , P b ) C = 0.25(D 1 (P a ) + D 1 (P b )) + 0.5D 2 (P a ,P b )<label>(9)</label></formula><p>where P a , P b are the estimated poses of person a and person b, respectively.P are the estimated and ground-truth 3D poses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Semi-Supervised Training</head><p>Semi-supervised learning is an effective technique to improve the network performance, particularly when the data with ground-truths are limited. A few works also explore to make use of the unlabeled data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b55">54]</ref>. In our method, we apply a noisy student training strategy <ref type="bibr" target="#b54">[53]</ref>. We first train a teacher network with the 3D ground-truth dataset only, and then use the teacher network to generate their pseudo-labels of unlabelled data, which are used to train a student network.</p><p>The pseudo-labels cannot be directly used because some of them are likely incorrect. Unlike in the noisy student training strategy <ref type="bibr" target="#b54">[53]</ref>, where data with ground-truth labels and pseudo-labels are mixed to train the student network by adding various types of noise (i. e., augmentations, dropout, etc), we propose two-consistency loss terms to assess the quality of the pseudo-labels, including the reprojection error and multi-perspective error <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">39]</ref>.</p><p>The reprojection error measures the deviation between the projection of generated 3D poses and the detected 2D poses. Since there are more abundant data variations in 2D pose dataset compared to 3D pose dataset (e.g., COCO is much larger compared to H36M), the 2D estimator is expected to be more reliable than its 3D counterpart. Therefore, minimizing a reprojection error is helpful to improve the accuracy of 3D pose estimation.</p><p>The multi-perspective error, E mp , measures the consistency of the predicted 3D poses from different viewing angles. This error indicates the reliability of the predicted 3D poses. Based on the two terms, our semi-supervised loss, L SSL , is formulated as,</p><formula xml:id="formula_8">L SSL = w(E rep + E mp ) + L dis ,<label>(10)</label></formula><p>where w is a weighting factor to balance the contribution of the reprojection and multi-perspective errors. In the training stage, w first focuses on easy samples and gradually includes the hard samples. The weight, w, is formulated as:</p><formula xml:id="formula_9">w = softmax( E rep r ) + softmax( E mp r ),<label>(11)</label></formula><p>where r is the number of training epochs. More details regarding to the reprojection and multi-perspective errors and the self-training process are discussed in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>Datasets We use MuPoTS-3D <ref type="bibr" target="#b32">[32]</ref> and JTA <ref type="bibr" target="#b13">[14]</ref> datasets to evaluate the camera-centric 3D multi-person pose estimation performance by following the existing methods <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b12">13]</ref>  and their training protocols (i.e., train, test split). In addition, we use 3DPW <ref type="bibr" target="#b50">[49]</ref> to evaluate person-centric 3D multi-person pose estimation performance following <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">45]</ref>. We also perform evaluation on the widely used Hu-man3.6M dataset <ref type="bibr" target="#b18">[19]</ref> for person-centric 3D human pose estimation following <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b51">50]</ref>. Details of the datasets information are in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We use HRNet-w32 <ref type="bibr" target="#b45">[44]</ref> as the backbone network for both multi-person pose estimator in the top-down and bottom-up networks. The top-down network is trained for 100 epochs on the COCO dataset <ref type="bibr" target="#b27">[28]</ref> with the Adam optimizer and learning rate 0.001. The bottom-up network is trained for 50 epochs with the Adam optimizer and learning rate 0.001 on a combined dataset of MuCO <ref type="bibr" target="#b33">[33]</ref> and COCO <ref type="bibr" target="#b27">[28]</ref>. More details are in the supplementary material.</p><p>Evaluation Metrics Since the majority of 3D human pose estimation methods produce person-centric 3D poses, to be able to compare, we perform person-centric 3D human pose estimation. We use Mean Per Joint Position Error (MPJPE), Procrustes analysis MPJPE (PA-MPJPE), Percentage of Correct 3D Keypoints (PCK), and area under PCK curve from various thresholds (AU C rel ) following the literature <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b7">8]</ref>. Since we focus on 3D multi-person camera-centric pose estimation, we also use the metrics designed for evaluating performance in the camera coordinate system, including average precision of 3D human root location (AP root 25 ) and PCK abs , which is PCK without root alignment to evaluate the absolute camera-centric coordinates from <ref type="bibr" target="#b36">[35]</ref>, and F1 value following <ref type="bibr" target="#b12">[13]</ref>.</p><p>Ablation Studies Ablation studies are performed to validate the effectiveness of each sub-module of our framework. We validate our top-down network by using an existing top-down pose estimator (i.e., detection of one fullbody joints) as a baseline, abbreviated as TD (w/o MP) to compare to our top-down network denoted as TD (w MP). We also validate our bottom-up network by using existing bottom-up heatmap estimation (i.e., estimate all person at  Other than validating our top-down and bottom-up networks, we also perform ablation analysis on our semisupervised learning. We show the result of using reprojection loss, multi-perspective loss, reprojection loss with our discriminator, and reprojection &amp; multi-perspective loss with discriminator in <ref type="table">Table 2</ref>. We can see that the reprojection loss is more useful than the multi-perspective loss because it leverages the information from the 2D pose estimator, which is trained with 2D datasets with a large number of poses and environment variations. More importantly, we observe that our proposed interaction-aware discriminator makes the largest performance improvement compared with the other modules, demonstrating the importance of enforcing the validity of the interaction between persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation</head><p>To evaluate the performance for 3D multi-person camera-centric pose estimation in both indoor and outdoor scenarios, we perform evaluations on MuPoTS-3D as summarized in <ref type="table">Table 3</ref>. The results show that our camera-centric multi-person 3D pose estimation outperforms the SOTA <ref type="bibr" target="#b24">[25]</ref> on P CK abs by 2.3%. We also perform person-centric 3D pose estimation evaluation using P CK where we outperform the SOTA method <ref type="bibr" target="#b26">[27]</ref> by 2.1%. The evaluation on MuPoTS-3D shows that our method outperforms the state-of-the-art methods in both camera-centric and person-centric 3D multi-person pose estimation as our framework overcomes the weaknesses of both bottom-up and top-down branches and at the same time benefits from their strengths.</p><p>Following recent work <ref type="bibr" target="#b12">[13]</ref>, we also perform evaluations on JTA, which is a synthetic dataset acquired from computer game, to further validate the effectiveness of our method for camera-centric 3D multi-person pose estimation. As shown in <ref type="table" target="#tab_4">Table 4</ref>, our method is superior over the SOTA method <ref type="bibr" target="#b12">[13]</ref> (e.g., our result shows 12.6% improvement on F1 value, t = 0.4m) on this challenging dataset where both inter-person occlusion and large person scale variation present, which again illustrate that our proposed method can handle these challenges in 3D multi-person pose estimation.</p><p>Human3.6M is widely used for evaluating 3D singleperson pose estimation. As our method is focused on dealing with inter-person occlusion and scale variation, we do not expect our method performs significantly better than the SOTA methods. <ref type="table" target="#tab_5">Table 5</ref> summarizes the quantitative evaluation on Human3.6M where our method is comparable with the SOTA methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref> on person-centric 3D human pose evaluation metrics (i.e., MPJPE and PA-MPJPE).</p><p>3DPW is an outdoor multi-person 3D human shape reconstruction dataset. It is unfair to compare the errors between skeleton-based method with ground-truth defined on SMPL model <ref type="bibr" target="#b29">[29]</ref> due to the different definitions of joints <ref type="bibr" target="#b48">[47]</ref>. We run human detection on all frames and create an occlusion subset where the frames with the large overlay between persons are selected. The performance drop between the full testing test of 3DPW and the occlusion subset can effectively tell if a method can handle interperson occlusion, which is shown in <ref type="table" target="#tab_6">Table 6</ref>. We observe that our method shows the least performance drop from the testing set to the subset, which demonstrates our method is indeed more robust to inter-person occlusion.</p><p>Qualitative Evaluation <ref type="figure">Fig. 4</ref> shows the comparison among a SOTA bottom-up method SMAP <ref type="bibr" target="#b57">[56]</ref>, our bottomup branch, top-down branch, and full model. We observe that SMAP suffers from person scale variation where the person who is far from the camera is missing in frame 280 as well as inter-occlusion (e.g., frame 365 and 340). Our bottom-up branch is robust to scale variance, but fragile to the out-of-image poses as our discriminator is not used here (e.g., frame 365 and 330). Moreover, our top-down branch produces reasonable relative poses with the aid of GCN and TCNs. However, there exists error of camera-centric root depth in our top-down branch, because our top-down branch    estimates root depth based on individual 2D poses and lacks global awareness (e.g., frame 280). Finally, our full model benefits from both branches and produces the best 3D pose estimations among these baselines.</p><p>We also provide results of the estimated 3D poses in novel viewpoints and the estimated 2D poses overlaid on input images as in <ref type="figure">Fig. 5</ref> where our estimated camera-centric 3D poses visualized from different angles further validate the effectiveness of our method. Two failure cases are shown in <ref type="figure" target="#fig_7">Fig. 6</ref> where the samples are taken from MPII dataset. The common failure cases are constant heavy occlusion (left) and unusual poses (right). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel method for monocular-video 3D multi-person pose estimation, which addresses the problems of inter-person occlusion and close interactions. We introduced the integration of top-down and bottom-up approaches to exploit their strengths. Our quantitative and qualitative evaluations show the effectiveness of our method compared to the state-of-the-art baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>1. Network Structure GCN Structure Unlike existing GCN methods which use an undirected graph <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b3">4]</ref>, we use a directed graph. The advantage of using directed graph is that more reliable joints with higher confidence are capable to influence the unreliable ones with low confidence with non-symmetric adjacency matrix. We adopt the GCN method following <ref type="bibr" target="#b6">[7]</ref>.</p><p>The features are propagated according to an adjacent matrix in GCNs, implying the edge values in the propagation graph. Given the heatmap H from the 2D pose estimator, we choose the location of the highest value in the map as a vertex in the graph for each joint, and the adjacency matrix is formed by the following equation:</p><formula xml:id="formula_10">A i,j = max(H i ) exp(?order(i, j)) (i = j) max(H i ) (i = j) ,<label>(12)</label></formula><p>where the A i,j is the outward weight from vertex i to vertex j. max(H i ) stands for the confidence of the i th joint. order(i, j) is the minimal number of hops that is required to reach vertex j from vertex i. This formation of adjacency imposes more weight for close vertices and less for far ones. More details please refer to <ref type="bibr" target="#b6">[7]</ref>.</p><p>TCN Structure Our GCN can complete the pose under occlusion or missing information, yet produces jittering results because of its lack of temporal smoothness. Previous works on the Temporal Convolutional Network (TCN) show the effectiveness of a TCN to constrain the temporal smoothness of predicted 3D poses <ref type="bibr" target="#b40">[39,</ref><ref type="bibr" target="#b8">9]</ref>. We adopt the TCN structure <ref type="bibr" target="#b40">[39]</ref>. As shown in <ref type="figure" target="#fig_8">Fig. 7</ref>, we utilize two TCNs to estimate the person-centric 3D poses (i.e., joints) and the camera-centric root joint depths, respectively. We named the two TCNs as: Joint-TCN and Root-TCN. The Joint-TCN takes the 3D pose sequence produced by our GCN as input, and outputs the refined person-centric 3D poses by considering the temporal information. The loss is L2 between the estimated pose P T CN and its ground-truth P , formulated as:</p><formula xml:id="formula_11">L JT CN = 1 K K k=0 |P T CN k ?P k | 2 ,<label>(13)</label></formula><p>where K is the number of the joints. The Root-TCN takes the 3D pose sequence generated by the GCN and the 2D pose sequence produced by the pose estimator as input, and outputs the estimated camera-centric root depths. Instead of directly estimating the cameracentric depth Z, we estimate the normalized root depth, which is R T CN = Z f based on focal length f to avoid the  influence of the camera intrinsic parameters. The loss function is L2 between the estimated R T CN and its ground truth R:</p><formula xml:id="formula_12">L RT CN = 1 K K k=0 |R T CN k ?R k | 2<label>(14)</label></formula><p>where K is the number of the joints. Based on the personcentric 3D pose from Eq. (13) and the root-joint depth from Eq. <ref type="formula" target="#formula_0">(14)</ref>, the camera-centric 3D pose is obtained.</p><p>Illustration of the heatmaps estimated from the bottomup network <ref type="figure" target="#fig_9">Fig. 8</ref> illustrates an example output of the four heatmaps estimated by our bottom-up network. Top left is an input image. Top middle is a joint map, which shows the heatmap of joints where all channels are merged together for better visualization of all joints. Top right is the estimated 3D poses. Bottom left shows the ID tag distribution. Bottom middle is the root depth map where the red represents a person is farther to camera than others. Bottom right is an example of relative depth map with respect to pelvis joint, where left arm depth is used as an example. The arm of left person is farther from the camera (red) compared to his pelvis while the right person's is closer to camera (blue) with respect to his pelvis.</p><p>Details of Semi-Supervised Learning Our Semisupervised Learning (SSL) pipeline is shown in <ref type="figure">Fig. 9</ref>. First, we use the trained model to generate the pseudo-label of the unlabelled data, which is the COCO dataset in our experiment. Note that, we use only the images, and not the 2D ground-truths of the joints to mimic the unlabelled data scenario. Unfortunately, the pseudo-labels cannot be directly used because some of them are incorrect. Therefore, we use two consistency terms to measure the quality of all the pseudo-labels: the reprojection error and multi-perspective error as mentioned in the main paper.</p><p>As the pose variations of 2D datasets are more abundant than those of 3D datasets, e.g. COCO compared to H36M, the estimated 2D poses are more robust than the estimated 3D poses in terms of different environments and poses. Existing reprojection error <ref type="bibr" target="#b51">[50]</ref> measures the deviation between generated 3D poses and detected 2D poses. Unlike this, we make use of the confidence of the joints from the 2D pose heatmap as weight in computating the reprojection error to adjust adaptively how much we should enforce the reprojected 3D poses to match the estimated 2D poses based on the confidence of the joints. Thus, the reprojection error is formulated as:</p><formula xml:id="formula_13">E rep = 1 K K k=1 C k |rep(X 3D,k ) ? X 2D,k | 2<label>(15)</label></formula><p>where the X 3D is the predicted 3D pose from the network, and X 2D stands for the 2D estimations from our multiperson 2D pose estimator. rep(?) is the reprojection function from 3D to 2D. K stands for the number of joints in total. Moreover, the error is a weighted sum of each joint's confidence score C k , which is explained in Eq. (12). We follow <ref type="bibr" target="#b4">[5]</ref> to use a multi-perspective error as an additional measure to enforce the consistency of the predicted 3D poses from different viewing angles. Given a pseudolabel 3D pose P pse 3D , we randomly rotate the pose along y axis (i.e., y-axis is perpendicular to the ground plane) to obtain P pse 3D , and re-project it to the 2D coordinates P pse 2D . Finally, we predict the P pse 3D based on the re-projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Implementation Details</head><p>Multi-Person Pose Estimator Our multi-person pose estimator uses HRNet-w32 <ref type="bibr" target="#b45">[44]</ref> as the backbone and is trained on the combination of the MuCO and COCO dataset. We duplicate the COCO dataset twice to balance the training data between two datasets. The network is trained with the Adam optimizer with learning rate starts at 0.001 and decreases to 1 10 at epoch 30 and 40. The network is trained for 50 epochs and it takes 35 hours to train on 8x RTX Quadro 8000 GPUs.</p><p>GCN and TCNs Our GCN and TCNs are trained based on the pre-extracted heatmaps from our multi-person pose estimator. We train the networks with the Adam optimizer with learning rate starts at 0.001 and decrease to <ref type="bibr" target="#b0">1</ref> 10 every 40 epochs. The networks are trained with 100 epochs and takes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Rotation</head><p>Input image 3D Pseudo-label Estimated 2D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Re-estimation</head><p>Re-projected 2D</p><p>Multi-perspective error Reprojection error <ref type="figure">Figure 9</ref>. The illustration of our SSL pipeline. The SSL aims to keep two consistency: reprojection and multi-perspective.</p><p>25 hours on single RTX 2080Ti GPU. We use the augmentation mentioned in <ref type="bibr" target="#b8">[9]</ref> to train the network to better handle the occlusion. Integration Network Our integration network contains 5 fully connected layers with layer size 512. The network is trained with the Adam optimizer with learning rate 0.001 in beginning, and decreased to <ref type="bibr" target="#b0">1</ref> 10 every 50 epochs. The network is trained for 150 epochs and takes 3.5 hours on single RTX 2080Ti GPU. The data augmentation procedure is discussed in the main paper. We briefly explain here for clarity: 1) We use random masking to simulate the occlusion, where the occluded joints are masked to (0, 0). 2) We apply a random shifting of joints based on a Gaussian random to simulate the inaccurate pose estimation. 3) We randomly make one of the poses in the pair to be zero, to simulate the unpaired poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-Up Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets Description</head><p>MuPoTS-3D <ref type="bibr" target="#b32">[32]</ref> is a 3D multi-person testing set that consists of &gt;8000 frames of 5 indoor and 15 outdoor scenes, and its corresponding training set is augmented from 3DHP, called MuCo-3DHP. The ground-truth 3D pose of each person in a video is obtained from multi-view markerless motion capture system, which is suitable for evaluat-ing 3D multi-person pose estimation performance in both person-centric and camera-centric coordinates. Following <ref type="bibr" target="#b36">[35]</ref>, the training set (MuCo-3DHP) is used for training our bottom-up network, and MuPoTS-3D is used only for performance evaluation.</p><p>JTA <ref type="bibr" target="#b13">[14]</ref> is a synthesized dataset from Grand Theft Auto V (GTA-V) game scene including various of illumination, viewpoints, and occlusion. It is a multi-person dataset with at most 32 persons appear in one frame. In addition, the images also demonstrate large person size variation as the crowd spread from close to camara and far from camera in various scenes. Because of these reasons, even it is a synthetic dataset, we'd like to perform evaluation on it. The dataset contains 512 videos, in which there are 256, 128, 128 for training, validation and testing, respectively. We follow the work <ref type="bibr" target="#b12">[13]</ref> to estimate the F1 score under different distance threshold as a performance evaluation metric.</p><p>Human3.6M <ref type="bibr" target="#b18">[19]</ref> is widely used for 3D human pose estimation. The dataset contains 3.6 million single-person images where an actor performs different activities in mocap studio at each video clip, so it is suitable for evaluation of 3D single-person pose estimation. Human3.6M is used for evaluating person-centric pose estimation performance. Following previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b51">50]</ref>, the subject 1,5,6,7,8 are used for training, and 9 and 11 for testing.</p><p>3DPW <ref type="bibr" target="#b50">[49]</ref> is an outdoor multi-person video dataset for 3D human pose reconstruction. In each video, one target person wearing inertial measurement units (IMUs) performs daily activities outdoor, so 3D ground-truth is available for the target person only. Following previous methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">45]</ref>, we use 3DPW for testing without any finetuning. The ground-truth of 3DPW is SMPL 3D mesh model <ref type="bibr" target="#b29">[29]</ref>, where the definition of joints differs from what is used in 3D human pose estimation (skeleton-based) like Human3.6M, so 3DPW is rarely used in the evaluation of skeleton-based methods <ref type="bibr" target="#b48">[47]</ref>.</p><p>Evaluation errors on 3DPW cannot objectively reflect the performance of the skeleton-based methods, due to different definitions of joints. We select the top 3000 frames with the largest IoU between the target person (i.e., the person with 3D ground-truth label) and other persons based on detection out of 3DPW test set to create an inter-person occlusion subset, and then perform evaluation on it. The IoU statistics of the 3DPW test set is shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, and the threshold at 3000 th frame is 0.26. Some samples of different occlusion level is shown in <ref type="figure" target="#fig_0">Fig. 11</ref>.</p><p>In fact, the error on this subset is still not a good performance indicator, the performance change of a method between the full testing set and this subset can measure how well the method can handle the inter-person occlusion problem. As shown in <ref type="table" target="#tab_6">Table 6</ref> in the main paper, our method  shows the smallest error increase among all the existing methods, which demonstrates that our method is indeed capable of handling inter-person occlusion more effectively.</p><p>Training Datasets Both the 2D datasets and 3D datasets are used to train our networks. In the following, we explain the details of the used datasets in the training processes of our pose estimator, top-down and bottom-up networks, pose discriminator, and semi-supervised learning.</p><p>? 2D datasets for pose estimator training: We use both COCO and MuCO for training the multi-person pose estimator. Because the MuCO dataset is a synthesized dataset, solely training on the MuCO dataset will result in overfitting problem and produces unstable predictions on natural or wild images. Therefore, COCO is included for enhance the generalization ability of the network.</p><p>? 3D dataset for top-down network training: We use MuCO and its original 3DHP dataset to train the GCN and TCNs in the top-down network. MuCO and 3DHP are used for the GCN on single frame pose refinement, while the 3DHP is used to train the TCN that incorporates the temporal information. Since the network works on the x, y, z coordinates, no overfitting problem was observed from the trained models.</p><p>? 3D dataset for bottom-up network training: We use both MuCO and COCO to train the bottom-up network. We additionally include COCO, which is used only for training joint heatmaps and ID tag maps.</p><p>? 3D dataset for pose discriminator training: MuCO is used for training the integration net and pose discriminator. In addition, we do random translation and rotation of the poses to generate more synthesized interaction pairs.</p><p>? Additional 2D data for semi-supervised learning training: We use COCO for the unlabeled image dataset in training our semi-supervised learning.</p><p>Evaluation Protocols While we include the discussion of the datasets for the tables in the main paper, here we provide the details for the sake of clarity. Our model is trained with the datasets explained in the previous section (i.e., Training Datasets Used) for ablation study in <ref type="table" target="#tab_1">Table 1</ref> and 2, evaluations in <ref type="table">Table 3</ref> (MuPoTS-3D), <ref type="table" target="#tab_5">Table 5</ref> (Human3.6M), and <ref type="table" target="#tab_6">Table 6</ref> (3DPW). The JTA dataset is captured from computer game, which has a domain gap to the real-world images. To perform the evaluation on the JTA dataset in <ref type="table" target="#tab_4">Table 4</ref> (JTA), we use the JTA training set to re-train the whole pipeline and perform the evaluation on the JTA test set.</p><p>As mentioned in the 3DPW dataset explanation, we follow the literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b46">45]</ref> and only perform testing on 3DPW. Note that, the SOTA methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21]</ref> both use additional 2D and 3D datasets in training their networks. We do not use the 3DPW dataset to train our network, but used it to train the joint adaptation network <ref type="bibr" target="#b48">[47]</ref>, which transfers </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Detailed Experimental Results</head><p>As our method focuses on the 3D multi-person scenarios, our network is trained on the 3D multi-person datasets as discussed in section 3. To have a fair comparison against existing methods that are trained only with the single-person Human3.6M dataset, we re-trained the whole pipeline from scratch on H3.6M dataset following the training protocols <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b40">39]</ref>. The evaluation result on the person-centric 3D human pose estimation is shown in <ref type="table" target="#tab_7">Table 7</ref>. Similar to <ref type="table" target="#tab_5">Table 5</ref> in the main paper, our method achieves comparable performance against the SOTA top-down or bottom-up 3D multi-person pose estimation methods <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b57">56,</ref><ref type="bibr" target="#b24">25]</ref> on this single-person dataset.</p><p>Following <ref type="bibr" target="#b36">[35]</ref>, we also calculate our method's accuracy using the MPRE metrics, which measures the cameracentric 3D human pose estimation performance. In particular, <ref type="bibr" target="#b36">[35]</ref> is 120.0, ours is 86.5, which shows 27.9% error reduction. HDNet <ref type="bibr" target="#b26">[27]</ref> reports a better value on MPRE as 77.6, however, their method can only handle single-person cases, and performs poorly on multi-person cases where their value of PCK abs is 35.2, but ours is 48.0, which is a 36.4% improvement. As camera-centric 3D pose estimation is for multi-person scenario, showing good result on single-person dataset but poor performance on multi-person dataset is not applicable to the real problem in 3D multiperson pose estimation.</p><p>To have a better understanding on how our method compare with existing methods for each test sequence in MuPoTS-3D dataset, extended version of <ref type="table">Table 3</ref> in the main paper for each test sequence is summarized in <ref type="table">Table  8</ref> and 9 for the camera-centric and person-centric evaluations using P CK abs and P CK metrics. We observe that our method consistently outperforms other methods in both the camera-centric and person-centric 3D multi-person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">More Qualitative Results</head><p>In this section, we provide additional results compared with the SOTA 3D multi-person pose estimation methods. In the main paper, we already provided a qualitative comparison on 3DPW test set in <ref type="figure">Fig. 5</ref>, where the results of SMAP <ref type="bibr" target="#b57">[56]</ref> is used as they released their code and we can <ref type="table">Table 9</ref>. P CK on MuPoTS-3D dataset for all poses. Best in bold.</p><p>perform testing with it.</p><p>Additional Comparison on MuPoTS-3D To compare with more methods, we provide additional results on MuPoTS-3D as RootNet <ref type="bibr" target="#b36">[35]</ref> released their pretrained model on this dataset, so we can perform testing on MuPoTS-3D using their released model. Together with SMAP <ref type="bibr" target="#b57">[56]</ref>, we show the qualitative results of our method compared with that of the two SOTA methods RootNet (topdown) and SMAP (bottom-up) in <ref type="figure" target="#fig_0">Fig 12.</ref> Additional Comparison on Wild Videos To further demonstrate the performance of our method compared with the SOTA 3D multi-person pose estimation method. We provide the qualitative results of our method compared with that of the SOTA bottom-up method SMAP <ref type="bibr" target="#b57">[56]</ref> in <ref type="figure" target="#fig_0">Fig 13.</ref> The video clips are selected from MPII <ref type="bibr" target="#b0">[1]</ref> dataset which is neither used for training or evaluation for both methods.</p><p>Additional Comparison on JTA As we reported our quantitative performance on JTA dataset in <ref type="table" target="#tab_4">Table 4</ref> in the main paper, we also provide the qualitative results of our method compared with that of the SOTA method reported and released their trained model on the JTA dataset <ref type="bibr" target="#b12">[13]</ref> in <ref type="figure" target="#fig_0">Fig 14.</ref> The two video clips in <ref type="figure" target="#fig_0">Fig 14 show</ref> both interperson occlusions and large multi-person scale variation where we observe our method can handle both challenges well and produce accurate camera-centric 3D multi-person pose estimation compared with LoCO <ref type="bibr" target="#b12">[13]</ref>. For each video clip, the first row is the frames from the video clip; the second row is the result of SMAP; the third row is the result of RootNet; the fourth row is the result of our method. It is observed from these results that the SOTA methods suffer from inter-person occlusions while our method can handle these challenges and produce accurate camera-centric 3D multi-person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SMAP</head><p>Ours SMAP Ours <ref type="figure" target="#fig_0">Figure 13</ref>. Results of our method compared with that of SMAP <ref type="bibr" target="#b57">[56]</ref> (i.e., the SOTA bottom-up method) on wild videos. Results from eight video clips are included (i.e., one frame for each video). Four results are at top part of the figure, the other four are at the bottom, separated by the dashed line. For either part, the first row is the frames from the video clip; the second row is the results of SMAP; the third row is the results of our method. These results again show that the SOTA method cannot handle inter-person occlusions. In contrast, our method produces accurate camera-centric 3D multi-person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LoCO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>LoCO Ours <ref type="figure" target="#fig_0">Figure 14</ref>. Result of our method compared with that of LoCO <ref type="bibr" target="#b12">[13]</ref> (i.e., a SOTA method released trained model on JTA) on JTA dataset. Results from two video clips are included: top and bottom separated by the dashed line. For each video clip, the first row is the frames from the video clip; the second row is the result of LoCO; the third row is the result of our method. These results show that on this synthetic datasets, our method is able to produce more accurate and robust 3D multi-person pose estimation compared with other methods. We use red circle to indicate the wrong results of LoCO and green circle to point out the corresponding correct results of our method. In the first row of the top video clip, due the four persons are far from the camera which are small, we use four red arrows to indicate each of them.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1Figure 1 .</head><label>1</label><figDesc>https://github.com/3dpose/3D-Multi-Person-Pose Incorrect 3D multi-person pose estimation from existing top-down (2nd row) and bottom-up (3rd row) methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The overview of our framework. Our proposed method comprises three components: 1) A top-down branch to estimate finegrained instance-wise 3D pose. 2) A bottom-up branch to generate global-aware camera-centric 3D pose. 3) An integration network to generate final estimation based on paired poses from top-down and bottom-up to take benefits from both branches. Note that the semisupervised learning part is a training strategy so it is not included in this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>porate top-down feedback with bottom-up CNNs. Tang et al. [46] develop a framework with bottom-up inference followed by top-down refinement based on a compositional model of the human body. Cai et al. [4] introduce a spatialtemporal graph convolutional network (GCN) that uses both bottom-up and top-down features. These methods explore to benefit from top-down and bottom-up information. However, they are not suitable for 3D multi-person pose estimation because the fundamental weaknesses in both topdown and bottom-up methods are not addressed completely, which include inter-person occlusion caused detection and joints grouping errors, and the scale variation issue. Li et al. [26] adopt LSTM and combine bottom-up heatmaps with human detection for 2D multi-person pose estimation. They address occlusion and detection shift problems. Unfortunately, they use a bottom-up network and only add the detection bounding box as the top-down information to group the joints. Hence, their method is essentially still bottom-up and thus still vulnerable to human scale variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2</head><label>2</label><figDesc>shows our pipeline, which consists of three major parts to accomplish the multi-person camera-centric 3D human pose estimation: a top-down network for fine-grained instance-wise pose estimation, a bottom-up network for global-aware pose estimation, and an integration network to integrate the estimations of top-down and bottom-up branches with inter-person pose discriminator. Moreover, a semi-supervised training process is proposed to enhance the 3D pose estimation based on reprojection consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Examples of estimated heatmaps of human joints. The left image shows the input frame overlaid with inaccurate detection bounding box (i.e., only one person detected). The middle image shows the estimated heatmap of existing top-down methods. The right image shows the heatmap of our top-down branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the same scale) as a baseline, named BU (w/o CH) to compare to our bottom-up network, called BU (w CH). To evaluate our integration network, we use three baselines. The first is a straightforward integration by combining existing TD and BU networks. The second is hard integration, abbre-viated TD + BU (hard), where the top-down person-centric pose is always used, plus the root depth from the bottom-up network. The third is linear integration, abbreviated TD + BU (linear), where the person-centric 3D pose from topdown is combined with its corresponding bottom-up one based on the confidence values of the estimated heatmap. As shown in Table 1, we observe that our top-down network, bottom-up network, and integration network clearly outperform their corresponding baselines. Our top-down network tends to have better person-centric 3D pose estimations compared with our bottom-up network, because the top-down network benefits from not only multi-person pose estimator, also GCN and TCN that help to deal with interoccluded poses. On the contrary, our bottom-up network achieves better performance for the root joint estimation, because it estimates the root depth based on a full image; while the root depth of top-down network is estimated based on an individual skeleton. Finally, our integration network demonstrates superior performance compared to hard or linear combining the poses from the top-down and bottom-up networks, which validates its effectiveness.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .igure 5 .</head><label>45</label><figDesc>Examples of results from our whole framework compared with different baseline results. First row shows the images from two video clips; second row shows the results from SMAP [56]; third row shows the result of of our bottom-up (BU) branch; fourth row shows the results of our top-down (TD) branch; last row shows the results of our full model. Wrong estimations are labeled with red circles. Frame 0?90?45?F Qualitative results of the estimated 2D poses overlaying on input images and the estimated 3D poses visualized in novel viewpoints (virtual camera rotated by 0, 45, 90 degrees clockwise). Different colors are used for different persons in both 2D and 3D human poses for better visualization purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Two representative failure cases of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Pipeline of our TCNs. Our TCNs include one Joint TCN for relative pose estimation and one Root TCN for camera-centric root depth estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Visualization of estimated heatmaps from the bottom-up branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Our bottom-up network is trained based on the combination of the MuCO and COCO dataset. To balance the number of training samples, we duplicate the COCO dataset twice and combine with the MuCO dataset. The bottom-up network is trained with the Adam optimizer with learning rate starts at 0.001 and decrease to 1 10 at the 30 th and 40 th epoch. The network is trained for 50 epochs and it takes 65 hours on 8x RTX Quadro 8000 GPUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 . 3 Figure 11 .</head><label>10311</label><figDesc>Interaction IoUs of 3DPW test set. Some sample images of different IoU level that are selected for inter-person occlusion subset. IoU values are added below each image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation study on MuPoTS-3D dataset. TD, BU, MP, CH, IN, and PM stand for top-down, bottom-up, multi-person pose estimator, combined heatmap, integration network, and pose matching, respectively. Best in bold, second best underlined.</figDesc><table><row><cell>Method</cell><cell>AP root 25</cell><cell>AU C rel</cell><cell cols="2">PCK PCK abs</cell></row><row><cell>TD (w/o MP)</cell><cell>43.7</cell><cell>41.0</cell><cell>81.6</cell><cell>42.8</cell></row><row><cell>TD (w MP)</cell><cell>45.2</cell><cell>48.9</cell><cell>87.5</cell><cell>45.7</cell></row><row><cell>BU (w/o CH)</cell><cell>44.2</cell><cell>34.5</cell><cell>76.6</cell><cell>40.2</cell></row><row><cell>BU (w CH)</cell><cell>46.1</cell><cell>35.1</cell><cell>78.0</cell><cell>41.5</cell></row><row><cell>TD + BU (w/o MP,CH)</cell><cell>44.9</cell><cell>42.6</cell><cell>82.8</cell><cell>43.1</cell></row><row><cell>TD + BU (hard)</cell><cell>46.1</cell><cell>48.9</cell><cell>87.5</cell><cell>46.2</cell></row><row><cell>TD + BU (linear)</cell><cell>46.1</cell><cell>49.2</cell><cell>88.0</cell><cell>46.7</cell></row><row><cell>TD + BU (w/o PM)</cell><cell>46.0</cell><cell>48.6</cell><cell>85.5</cell><cell>45.3</cell></row><row><cell>TD + BU (IN)</cell><cell>46.3</cell><cell>49.6</cell><cell>88.9</cell><cell>47.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 2. Ablation study on MuPoTS-3D dataset. Rep, MP, and dis stand for reprojection, multi-perspective, and discriminator. Best in bold, second best underlined.</figDesc><table><row><cell>Method</cell><cell>AP root 25</cell><cell>AU C rel</cell><cell>PCK</cell><cell>PCK abs</cell></row><row><cell>Rep</cell><cell>46.3</cell><cell>43.4</cell><cell>77.2</cell><cell>40.7</cell></row><row><cell>MP</cell><cell>46.3</cell><cell>32.2</cell><cell>72.8</cell><cell>29.5</cell></row><row><cell>Rep+dis</cell><cell>46.3</cell><cell>49.9</cell><cell>89.1</cell><cell>46.8</cell></row><row><cell>Rep+MP+dis</cell><cell>46.3</cell><cell>50.6</cell><cell>89.6</cell><cell>48.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table 3. Quantitative evaluation on multi-person 3D dataset, MuPoTS-3D. Best in bold, second best underlined.</figDesc><table><row><cell>Group</cell><cell></cell><cell>Method</cell><cell>PCK</cell><cell>PCK abs</cell></row><row><cell></cell><cell cols="2">Mehta et al. [32]</cell><cell>65.0</cell><cell>n/a</cell></row><row><cell>Person-</cell><cell cols="2">Rogez et al., [42]</cell><cell>70.6</cell><cell>n/a</cell></row><row><cell>centric</cell><cell cols="2">Cheng et al. [9]</cell><cell>74.6</cell><cell>n/a</cell></row><row><cell></cell><cell cols="2">Cheng et al. [8]</cell><cell>80.5</cell><cell>n/a</cell></row><row><cell></cell><cell cols="2">Moon et al. [35]</cell><cell>82.5</cell><cell>31.8</cell></row><row><cell>Camera-</cell><cell></cell><cell>Lin et al. [27]</cell><cell>83.7</cell><cell>35.2</cell></row><row><cell>centric</cell><cell cols="2">Zhen et al. [56]</cell><cell>80.5</cell><cell>38.7</cell></row><row><cell></cell><cell></cell><cell>Li et al. [25]</cell><cell>82.0</cell><cell>43.8</cell></row><row><cell></cell><cell cols="2">Cheng et al. [7]</cell><cell>87.5</cell><cell>45.7</cell></row><row><cell></cell><cell></cell><cell>Our method</cell><cell>89.6</cell><cell>48.0</cell></row><row><cell>Method</cell><cell></cell><cell>t = 0.4m</cell><cell cols="2">t = 0.8m t = 1.2m</cell></row><row><cell cols="2">[40] + [30] + [42]</cell><cell>39.14</cell><cell>47.38</cell><cell>49.03</cell></row><row><cell>LoCO [13]</cell><cell></cell><cell>50.82</cell><cell>64.76</cell><cell>70.44</cell></row><row><cell>Ours</cell><cell></cell><cell>57.22</cell><cell>68.51</cell><cell>72.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Group</cell><cell>Method</cell><cell cols="2">MPJPE PA-MPJPE</cell></row><row><cell></cell><cell>Hossain et al., [16]</cell><cell>51.9</cell><cell>42.0</cell></row><row><cell></cell><cell>Wandt et al., [50]*</cell><cell>50.9</cell><cell>38.2</cell></row><row><cell>Person-</cell><cell>Pavllo et al., [39]</cell><cell>46.8</cell><cell>36.5</cell></row><row><cell>centric</cell><cell>Cheng et al., [9]</cell><cell>42.9</cell><cell>32.8</cell></row><row><cell></cell><cell>Kocabas et al., [21]</cell><cell>65.6</cell><cell>41.4</cell></row><row><cell></cell><cell>Kolotouros et al. [22]</cell><cell>n/a</cell><cell>41.1</cell></row><row><cell></cell><cell>Moon et al., [35]</cell><cell>54.4</cell><cell>35.2</cell></row><row><cell>Camera-</cell><cell>Zhen et al., [56]</cell><cell>54.1</cell><cell>n/a</cell></row><row><cell>centric</cell><cell>Li et al., [25]</cell><cell>48.6</cell><cell>30.5</cell></row><row><cell></cell><cell>Ours</cell><cell>40.7</cell><cell>30.4</cell></row></table><note>Quantitative results on JTA dataset. F1 values are reported based on different threshold t when the point is considered "true positive" when the distance from corresponding distance is less than t. Best in bold, second best underlined.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Quantitative evaluation on Human3.6M for normalized and camera-centric 3D human pose estimation. * denotes groundtruth 2D labels are used. Best in bold, second best underlined.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>PA-MPJPE</cell><cell>?</cell></row><row><cell></cell><cell>Doersch et al. [12]</cell><cell>74.7</cell><cell>n/a</cell></row><row><cell></cell><cell>Kanazawa et al. [20]</cell><cell>72.6</cell><cell>n/a</cell></row><row><cell>Original</cell><cell>Arnab et al. [2]</cell><cell>72.2</cell><cell>n/a</cell></row><row><cell></cell><cell>Cheng et al. [8]</cell><cell>71.8</cell><cell>n/a</cell></row><row><cell></cell><cell>Sun et al. [45]</cell><cell>69.5</cell><cell>n/a</cell></row><row><cell></cell><cell>Kolotouros et al. [22]*</cell><cell>59.2</cell><cell>n/a</cell></row><row><cell></cell><cell>Kocabas et al., [21]*</cell><cell>51.9</cell><cell>n/a</cell></row><row><cell></cell><cell>Our method</cell><cell>62.9</cell><cell>n/a</cell></row><row><cell></cell><cell>Cheng et al. [8]</cell><cell>92.3</cell><cell>+20.5</cell></row><row><cell></cell><cell>Sun et al. [45]</cell><cell>84.4</cell><cell>+14.9</cell></row><row><cell>Subset</cell><cell>Kolotouros et al. [22]*</cell><cell>79.1</cell><cell>+19.9</cell></row><row><cell></cell><cell>Kocabas et al., [21]*</cell><cell>72.2</cell><cell>+20.3</cell></row><row><cell></cell><cell>Our method</cell><cell>75.6</cell><cell>+12.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Quantitative evaluation using PA-MPJPE on original 3DPW test set and its occlusion subset. * denotes extra 3D datasets were used in training. Best in bold, second best underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Quantitative evaluation on Human3.6M for person-centric 3D human pose estimation. Best in bold, second best underlined.our predicted 3D poses of MuCO joint's definition to that of 3DPW defined on the SMPL model<ref type="bibr" target="#b29">[29]</ref>.</figDesc><table><row><cell>Method</cell><cell>MPJPE</cell><cell>PA-MPJPE</cell></row><row><cell>Moon et al., [35]</cell><cell>54.4</cell><cell>35.2</cell></row><row><cell>Zhen et al., [56]</cell><cell>54.1</cell><cell>n/a</cell></row><row><cell>Li et al., [25]</cell><cell>48.6</cell><cell>30.5</cell></row><row><cell>Ours</cell><cell>42.1</cell><cell>31.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Results of our method compared with that of SMAP [56] (i.e., the SOTA bottom-up method) and RootNet [35] (i.e., the SOTA top-down method) on MuPoTS dataset. Results from four video clips are included: top-left, top-right, bottom-left, and bottom-right.</figDesc><table><row><cell>Frame 60</cell><cell>Frame 80</cell><cell>Frame 100</cell><cell>Frame 120</cell><cell>Frame 340</cell><cell>Frame 360</cell><cell>Frame 380</cell><cell>Frame 400</cell></row><row><cell>SMAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RootNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame 240</cell><cell>Frame 280</cell><cell>Frame 300</cell><cell>Frame 365</cell><cell>Frame 305</cell><cell>Frame 315</cell><cell>Frame 330</cell><cell>Frame 340</cell></row><row><cell>SMAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RootNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 12.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported by the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monoloco: Monocular 3d pedestrian localization and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Hang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph and temporal convolutional networks for 3d multi-person pose estimation in monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">man pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>3d hu</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning 3d human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uday</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Safeer</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compressed volumetric heatmaps for multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to detect and track visible and occluded body joints in a virtual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imtiaz</forename><surname>Mir Rayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5600" to="5609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning to estimate human pose with data driven belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="747" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06-06" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Integration of bottom-up/top-down approaches for 2d pose estimation using probabilistic gaussian modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Christophe</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hmor: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation using bounding box constraint and lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaopeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinguo</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2653" to="2663" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hdnet: Human depth estimation for multi-person camera-space localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d motion capture with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3d multiperson pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Bruce Xiaohan Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3467" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="190" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Posenet3d: Unsupervised 3d human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhant</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Agrawal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03473</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Self-supervised keypoint correspondences for multiperson pose estimation and tracking in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafi</forename><surname>Umer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Bodo Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Combined top-down/bottom-up human articulated pose estimation using adaboost learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayoshi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">3d human shape and pose from a single low-resolution image with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>L?szl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De La Torre</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13666</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Smap: Single-shot multiperson absolute 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<title level="m">Method S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 -Moon</title>
		<imprint/>
	</monogr>
	<note>et al. [35] 59.5 44.7 51.4 46.0 52.2 27.4 23.7 26.4 39.1 23.6</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhen</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>56] 41.6 33.4 45.6 16.2 48.8 25.8 46.5 13.4 36.7 73.5 Ours 69.2 57.1 49.3 68.9 55.1 36.1 49.4 33.0 43.5 52.8</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<title level="m">Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg Moon</title>
		<imprint/>
	</monogr>
	<note>et al. [35] 18.3 14.9 38.2 26.5 36.8 23.4 14.4 19.7 18.8 25.1 31.5</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhen</surname></persName>
		</author>
		<idno>56] 43.6 22.7 21.9 26.7 47.1 32.5 31.4 18.0 33.8 47.8 35.4 Ours 48.8 36.5 51.2 37.1 47.3 52.0 20.3 43.7 57.5 50.4 48.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<title level="m">Table 8. P CK abs on MuPoTS-3D dataset for all poses. Best in bold. Method S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 -Rogez</title>
		<imprint/>
	</monogr>
	<note>et al. [41] 67.7 49.8 53.4 59.1 67.5 22.8 43.7 49.9 31.1 78.1</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogez</surname></persName>
		</author>
		<idno>42] 87.3 61.9 67.9 74.6 78.8 48.9 58.3 59.7 78.1 89.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dabral</surname></persName>
		</author>
		<idno>11] 85.1 67.9 73.5 76.2 74.9 52.5 65.7 63.6 56.3 77.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta</surname></persName>
		</author>
		<idno>32] 81.0 59.9 64.4 62.8 68.0 30.3 65.0 59.2 64.1 83.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta</surname></persName>
		</author>
		<idno>34] 88.4 65.1 68.2 72.5 76.2 46.2 65.8 64.1 75.1 82.4 Zhen et al. [56] 88.8 71.2 77.4 77.7 80.6 49.9 86.6 51.3 70.3 89.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
		<idno>35] 94.4 77.5 79.0 81.9 85.3 72.8 81.9</idno>
		<imprint/>
	</monogr>
	<note>2 90.4 Ours 93.4 91.3 84.7 83.3 89.1 85.2 95.4 92.1 89.5 93.1</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<title level="m">Method S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg Rogez</title>
		<imprint/>
	</monogr>
	<note>et al. [41] 50.2 51.0 51.6 49.3 56.2 66.5 65.2 62.9 66.1 59.1 53.8</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogez</surname></persName>
		</author>
		<idno>42] 69.2 73.8 66.2 56.0 74.1 82.1 78.1 72.6 73.1 61.0 70.6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dabral</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">76</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta</surname></persName>
		</author>
		<idno>32] 67.2 68.3 60.6 56.5 59.9 79.4 79.6 66.1 66.3 63.5 65.0</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehta</surname></persName>
		</author>
		<idno>34] 74.1 72.4 64.4 58.8 73.7 80.4 84.3 67.2 74.3 67.8 70.4 Zhen et al. [56] 72.3 81.7 63.6 44.8 79.7 86.9 81.0 75.2 73.6 67.2 73.5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moon</surname></persName>
		</author>
		<idno>35] 79.2 79.9</idno>
		<imprint/>
	</monogr>
	<note>1 72.7 81.1 89.9 89.6 81.8 81.7 76.2 81.8 Ours 85.4 85.7 89.9 90.1 88.8 93.7 92.2 87.9 89.7 91.9 89.6</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
