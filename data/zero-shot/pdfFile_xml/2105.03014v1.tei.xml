<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BasisNet: Two-stage Model Synthesis for Efficient Inference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Zhang</surname></persName>
							<email>mzhang@cs.pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Te</forename><surname>Chu</surname></persName>
							<email>ctchu@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
							<email>azhmogin@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
							<email>howarda@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
							<email>bjou@google.com</email>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Hwa</surname></persName>
							<email>hwa@cs.pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Kovashka</surname></persName>
							<email>kovashka@cs.pitt.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Pittsburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BasisNet: Two-stage Model Synthesis for Efficient Inference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present BasisNet which combines recent advancements in efficient neural network architectures, conditional computation, and early termination in a simple new form. Our approach incorporates a lightweight model to preview the input and generate input-dependent combination coefficients, which later controls the synthesis of a more accurate specialist model to make final prediction. The two-stage model synthesis strategy can be applied to any network architectures and both stages are jointly trained. We also show that proper training recipes are critical for increasing generalizability for such high capacity neural networks. On ImageNet classification benchmark, our BasisNet with MobileNets as backbone demonstrated clear advantage on accuracy-efficiency trade-off over several strong baselines. Specifically, BasisNet-MobileNetV3 obtained 80.3% top-1 accuracy with only 290M Multiply-Add operations, halving the computational cost of previous state-of-the-art without sacrificing accuracy. With early termination, the average cost can be further reduced to 198M MAdds while maintaining accuracy of 80.0% on ImageNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>High-accuracy yet low-latency convolutional neural networks enable opportunities for on-device machine learning, and are playing increasingly important roles in various mobile applications, including but not limited to intelligent personal assistants, AR/VR, and real-time speech translations. Therefore designing efficient convolutional neural networks especially for edge devices has received significant research attention. Prior research attempted to tackle this challenge from different perspectives, such as novel network architectures <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b12">13]</ref>, better incorporation with hardware accelerators <ref type="bibr" target="#b15">[16]</ref>, or conditional computation and adaptive inference algorithms <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b32">33]</ref>. However, focusing on one * This work was done when Mingda Zhang was an intern at Google.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MAdds (FLOPs)</head><p>Top-1 Acc. (%) MobileNetV2 1.0x <ref type="bibr" target="#b21">[22]</ref> 300M 72.0 CondConv-MobileNetV2 1.0x <ref type="bibr" target="#b36">[36]</ref>? 329M 74.6 DY-MobileNetV2 1.0x <ref type="bibr" target="#b4">[5]</ref>? 313M 75.2 MobileNetV3-Large <ref type="bibr" target="#b12">[13]</ref> 219M 75.2 Dy-MobileNetV3-Large <ref type="bibr" target="#b40">[40]</ref> 228M 77.1 ShuffleNetV2 1.5x <ref type="bibr" target="#b16">[17]</ref> 299M 72.6 EfficientNet-B0 <ref type="bibr" target="#b24">[25]</ref> 390M 77.1 EfficientNet-B0 (Noisy Stdt.) <ref type="bibr">[</ref> perspective in isolation may have side effects. For example, novel network architectures may introduce custom operators that are not well-supported by hardware accelerators, thus a promising new model may have limited practical improvements on real devices due to a lack of hardware support. We believe that these perspectives should be better integrated to form a more holistic general approach for broader applicability.</p><p>In this paper, we present BasisNet, which takes advantage of progress in all these perspectives and combines several key ideas in a simple new form. The core idea behind BasisNet is dynamic model synthesis, which aims at efficiently generating input-dependent specialist model from a <ref type="figure" target="#fig_7">Figure 1</ref>. An overview of the BasisNet and more details can be found in Sec. 3.2. For easy images (e.g. distinguishing cats from dogs), lightweight model can give sufficiently accurate predictions thus the second stage could be skipped. For more difficult images (e.g., distinguishing different breeds of dogs), a specialist model is synthesized following guidance from lightweight model, which is good at recognizing subtle differences to make more accurate predictions about the given images. collection of bases, so the resultant model is specialized at handling the given input and can give more accurate predictions. This concept is flexible and can be applied to any novel network architectures. On the hardware side, the twostage model synthesis strategy allows the execution of the lightweight and synthesized specialist model on different processing units (e.g., CPU, mobile GPUs, dedicated accelerators, etc.) in parallel to better handle streaming data. The BasisNet design is naturally compatible with early termination, and can easily balance between computation budget and accuracy. With extensive experiments, we also show that a proper training recipe is critical to mitigate overfitting and improve generalizability.</p><p>An overview of the BasisNet is shown in <ref type="figure" target="#fig_7">Fig. 1</ref>. Using image classification as an example, our BasisNet has two stages: the first stage relies on a lightweight model to preview the input image and produce both an initial prediction and a group of combination coefficients. In the second stage, the coefficients are used to combine a set of models, which we call basis models, into a single one to process the image and generate the final prediction. The second stage could be skipped if the initial prediction is sufficiently confident. The basis models share the same architecture but differ in some weight parameters, while other weights are shared to avoid overfitting and reduce the total model size.</p><p>We validated BasisNet with different generations and sizes of MobileNets and observed significant improvements in inference efficiency. In <ref type="table" target="#tab_0">Table 1</ref> we show comparisons with selected efficient networks on ImageNet classification benchmark. Notably, without using early termination, our BasisNet with 16 basis models of MobileNetV3-large only requires 290M Multiply-Adds (MAdds) to achieve 80.3% top-1 accuracy, halving the computation cost of previous state-of-the-art <ref type="bibr" target="#b2">[3]</ref> without sacrificing accuracy. If we enable early termination, the average cost can be further reduced to 198M MAdds with the top-1 accuracy remaining 80.0% on ImageNet. <ref type="bibr" target="#b0">1</ref> In summary, our main contribution is two-fold: ? We propose a two-stage model synthesis strategy that combines efficient neural networks, conditional computation, and early termination in a simple new form. Our BasisNet achieves state-of-the-art performance of accuracy with respect to computation budget on Ima-geNet even without early termination; if enabling early termination, the average computation cost can be further reduced with only marginal accuracy drop. ? We propose an accompanying training recipe for the new BasisNet, which is critical to improve generalizability for high capacity dynamic neural networks, and can also improve the performance of other models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Efficient neural networks. Different approaches for building efficient networks have been studied. Early effort includes knowledge distillation <ref type="bibr" target="#b11">[12]</ref>, post-training pruning <ref type="bibr" target="#b10">[11]</ref> and quantization <ref type="bibr" target="#b14">[15]</ref>. Later work distinguishes model complexity (size) and run-time latency (speed), and optimizes for them either with human expertise <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">39]</ref> and/or neural architecture search <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">25]</ref>. All these approaches aim at producing a static model that is generally efficient but agnostic to inputs. On the contrary, our Basis-Net is built on efficient network architectures, and is dynamically adaptive based on inputs. In this work we optimize for inference speed rather than model size.</p><p>Conditional computation. Several prior work have explored accelerating inference by skipping part of computation graph based on input-dependent signals. For example, <ref type="bibr" target="#b8">[9]</ref> propose a ResNet extension that dynamically adjusts the number of executed layers based on image regions. <ref type="bibr" target="#b26">[27]</ref> propose HydraNet which creates multiple parallel branches across the network, and adopts a soft gating module to selectively activate few branches to reduce inference cost. <ref type="bibr" target="#b22">[23]</ref> use mixture of experts with a gating network to choose from thousands of candidates. Recently, <ref type="bibr" target="#b36">[36]</ref> propose conditionally parameterized convolution (CondConv), which applies weighted combinations of convolution kernels. This idea is adopted by several later work <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b4">5]</ref>, because it has equivalent expressive power as linear mixture of experts, but requires much fewer computations than combining feature maps. However, one common characteristic of these approaches is that their conditioning modules are inserted before each configurable component (e.g., layer or branch), thus these dynamic adjustments only rely on local information. This concept is defined by <ref type="bibr" target="#b5">[6]</ref>, and according to them lacking global knowledge may be less than optimal because shallower layers cannot benefit from semantic knowledge which is only available from deeper layers. Some other work also identified similar issues and have attempted to leverage global knowledge in dynamic modulation. For example, in SkipNet <ref type="bibr" target="#b32">[33]</ref> a gating network is built to conditionally skip certain layers in the backbone, and the authors report that the best performance comes from a RNN-based gating network because it can access feature maps across multiple layers. <ref type="bibr" target="#b5">[6]</ref> introduce GaterNet where a dedicated deep neural network is used to analyze the inputs before generating input-dependent masks for the filters in backbone network. BasisNet use a lightweight but fully-fledged model to process the inputs and produce combination coefficients, thus the model synthesis is relying on semantic-aware global knowledge. Different from SkipNet and GaterNet, our lightweight model can synthesize new kernels that do not exist beforehand via linear combination. Another distinction is that by separating conditioning model from backbone, our BasisNet is more flexible and easier to adapt to different architectures and hardware constraints.</p><p>Cascading networks and early exiting. Since input samples are naturally of varying difficulty, using a single model to equally process all inputs with a fixed computation budget is wasteful. This observation has been leveraged by prior work, e.g., the famous Viola-Jones face detector <ref type="bibr" target="#b29">[30]</ref> built a cascade of increasingly more complex classifiers to achieve real-time execution. Similar ideas were also used in deep learning, e.g., reducing unnecessary inference com-putations for easy cases in a cascaded system <ref type="bibr" target="#b28">[29]</ref>, attaching multiple classification heads on different layers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14]</ref>, or cascading multiple models <ref type="bibr" target="#b1">[2]</ref>. One common limitation in previous work is that only the exit point adapts to the samples but the underlying models remain static. Instead, our BasisNet dynamically adjusts the convolution kernel weights based on the guidance from lightweight model, thus the synthesized specialist can better handle the more difficult cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In general, our BasisNet has two stages: the first stage lightweight model, and the second stage model synthesis from a set of basis models. Given a specific input, the lightweight model generates two outputs, an initial prediction and a group of basis combination coefficients. If the initial prediction is of high confidence, the input is presumably easy and BasisNet could directly return the initial prediction and terminate early. But if the initial prediction is less confident (implying the input is difficult, e.g. identifying dogs by breed), the coefficients will be used to guide the synthesis of a specialist model in the second stage. The synthesized specialist will handle final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Lightweight model</head><p>The lightweight model is a fully-fledged network handling two tasks: generating initial category prediction and generating combination coefficients for second stage model synthesis. The first is a standard classification task thus we only elaborate on the second below. Assuming there are N basis models and each has K layers, the lightweight model will predict combination coefficients ? ? R</p><formula xml:id="formula_0">K?N ? = ?(LM(f (x)))<label>(1)</label></formula><p>where LM stands for lightweight model and ? represents a non-linear activation function. We use softmax by default because it enforces convexity, which promotes sparsity and can lead to more efficient executions. f (x) represents a transformation of the input image, and we typically use</p><formula xml:id="formula_1">f (x) = x or f (x) = DownSampling(x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Basis model synthesis</head><p>Our basis models are a collection of model candidates, which share the same architecture but differ in model parameters. By combining basis models with different weights, a specialist network can be synthesized. Various strategies can be used for building basis models, such as mixture of experts <ref type="bibr" target="#b22">[23]</ref> or using multiple parameter-efficient patches <ref type="bibr" target="#b18">[19]</ref>. We explored a few options and found that the recently proposed CondConv <ref type="bibr" target="#b36">[36]</ref> best fits our needs for building a low-latency but high-capacity model. Specifically, consider a regular deep network with image input x. Assume the output of the k-th convolutional layer is O k (x), which could be obtained by</p><formula xml:id="formula_2">O k (x) = ?(W 0 * x), if k = 0 ?(W k * O k?1 (x)), if k &gt; 0 (2)</formula><p>where W k represents the convolution kernel at the k-th layer and * represents a convolution operation. For simplicity some operations like batch normalization and squeezeand-excitation are omitted from the notation. In Basis-Net, different inputs will be processed by different, inputdependent kernelW k at k-th layer, which is obtained by linearly combining the kernels from N basis models at k-th layer, denoted by {W n k } n=1,...,N :</p><formula xml:id="formula_3">W k =? 1 k ? W 1 k + ? +? N k ? W N k<label>(3)</label></formula><p>where? n k represents the weight for the k-th layer of the n-th basis. We useW and? to emphasize their dependency on x. This design allows us to increase model capacity effectively but retain the same number of convolution operations. Besides, since the number of parameters is much less than number of MAdds in a single basis architecture, the combination only marginally increase the computation cost.</p><p>Besides, using sparse convex coefficients further reduces the overhead. Thus we generally consider convex coefficients, but also studied two special cases:</p><p>? ? k is the same for all layers. In this case, the combination is per-model instead of per-layer. ? ? k as an N -dimension vector is one-hot encoded. In this case, model synthesis becomes model selection.</p><p>Key difference from CondConv. Our model synthesis mechanism is inspired by CondConv <ref type="bibr" target="#b36">[36]</ref> but there exists many distinctions. In CondConv the combination coefficients for k-th layer are computed following</p><formula xml:id="formula_4">? k = ?(FC(GAP(O k?1 (x))))<label>(4)</label></formula><p>where FC stands for fully connected layer and GAP stands for global average pooling. This formulation shows the dynamic kernels in CondConv can only be synthesized layer by layer, because the combination coefficients for next layer depend on output of previous layer. This complicates scheduling of computation thus is not hardware friendly <ref type="bibr" target="#b38">[38]</ref>. In BasisNet, the issue is addressed by the lightweight model, which generates the combination coefficients for all layers simultaneously as shown in Equation 1.</p><p>Therefore the entire specialist model can be synthesized all at once. Separating kernel combination from execution also enables BasisNet to be easily deployed to (or even across) different hardware accelerators on edge devices if needed. Besides, early termination is naturally supported by Basis-Net, but is much harder to be incorporated for CondConv.</p><p>Arguably one can try attaching additional prediction heads like <ref type="bibr" target="#b25">[26]</ref> to enable "layer-level early termination" for Cond-Conv. However, this change requires non-trivial efforts for designing the proper exit points in CondConv, let alone introducing extra computational cost. More specifically, since the backbone efficient network is already highly compact (e.g. MobileNets), it is unlikely that early layers can offer signals sufficient for prediction which is required for early termination. For BasisNet, the signal comes from fully-fledged lightweight model, which generates the prediction as a side product thus offers early termination for free. Lastly, BasisNet is complementary to CondConv, as we find (in Sec. 4.5) that combining CondConv and Basis-Net can further boost prediction accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training BasisNet properly</head><p>BasisNet significantly increases model capacity, but the risk of overfitting also increases. We found the standard training procedures used to train MobileNets lead to severe overfitting on BasisNet. Here we describe a few regularization techniques that are crucial for training BasisNet successfully. This is also a key contribution of our work, as previously there is no good practice on how to effectively train such high capacity dynamic neural networks.</p><p>? Basis model dropout (BMD) Inspired by <ref type="bibr" target="#b9">[10]</ref>, we experimented with randomly shutting down certain basis model candidates during training. It is similar to applying Drop-Connect <ref type="bibr" target="#b31">[32]</ref> on the predicted coefficient matrix from the lightweight model. We find this approach is extremely effective against "experts degeneration" <ref type="bibr" target="#b22">[23]</ref> where the controlling model always picks the same few candidates and never activates the rest. ? AutoAugment (AA) AutoAugment <ref type="bibr" target="#b6">[7]</ref> is a search-based procedure for finding specific data augmentation policy towards a target dataset. We find that replacing the original data augmentation in MobileNets <ref type="bibr" target="#b21">[22]</ref> with the Ima-geNet policy in AutoAugment can significantly improve the model generalizability. ? Knowledge distillation <ref type="bibr" target="#b11">[12]</ref> showed that using soft targets from a well-trained teacher network can effectively prevent a student model from overfitting. We observe that knowledge distillation is also effective on training Basis-Net, and find EfficientNet-B2 with noisy student training <ref type="bibr" target="#b34">[35]</ref> can be a good teacher. In addition to stronger regularization, we applied a few other tricks in order to properly train BasisNet. Since the lightweight model directly controls how the specialist model is synthesized, any slight changes in the combination coefficients will propagate to the parameter of the synthesized model and finally affect the final prediction. Since we train the two stages from scratch, this is especially troublesome at the early phase when the lightweight model is still ill-trained. To deal with the unstable training, we introduced ? [0, 1] to balance between a uniform combination and a predicted combination coefficients from the lightweight model,</p><formula xml:id="formula_5">? ? = ? 1 N ? 1 K?N + (1 ? ) ? ?<label>(5)</label></formula><p>When = 1 all bases are combined equally while when = 0 the synthesis is following the combination coefficients. In practice linearly decays from 1 to 0 in the early phase of training then remains at 0, thus the lightweight model can gradually take over the control of model synthesis. This approach effectively stabilizes training and accelerates convergence. A recent work <ref type="bibr" target="#b4">[5]</ref> proposed temperaturecontrolled softmax to achieve similar goal. All models in both stages are trained together in an end-to-end manner via back-propagation. In other words, all basis models are trained from scratch by gradients from the synthesized model. The total loss includes two cross-entropy losses for the synthesized model and the lightweight model, respectively, and L2 regularization,</p><formula xml:id="formula_6">L = ? log P (y|x;W ) + ?(? log P (y|f (x); W LM )) + ?({W n } n=1,...,N , W LM )<label>(6)</label></formula><p>where ? is the weight for cross-entropy loss from lightweight model (? = 1 in our experiments), and ?(?) is a L2 regularizer applied to all model parameters. The lightweight model receives all gradients, while basis models are only updated by the first term and regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and model architecture setup</head><p>We demonstrate the effectiveness of BasisNet on both MobileNetV2 and MobileNetV3 architectures, and evaluate on the ImageNet ILSVRC 2012 classification dataset <ref type="bibr" target="#b19">[20]</ref> consisting of 1.28M images for training and 50K for validation. We did not explicitly use extra data, but one teacher model we used for knowledge distillation, i.e., EfficientNet-b2 with noisy student training <ref type="bibr" target="#b34">[35]</ref>, is obtained with extra data. For BasisNet-MV2, the basis models follow the architecture described in <ref type="table" target="#tab_1">Table 2</ref> of <ref type="bibr" target="#b21">[22]</ref>. For simplicity in notation, we sequentially number all the layers starting from L0, e.g. the first conv2d layer is L0 and the avgpool 7x7 layer is L19. For BasisNet-MV3, the basis models follow the MobileNetV3-large architecture described in <ref type="table" target="#tab_0">Table 1</ref> of <ref type="bibr" target="#b12">[13]</ref>. We also sequentially number all the layers, e.g. the pool,7x7 layer is L17.</p><p>For fair comparison, we retrained all models including BasisNet and all the baselines using the same training recipe, and reported the performance without early termination except for Sec. 4.6. Note that the lightweight model introduces computation overhead for BasisNet, but our reported MAdds statistics for BasisNet always include the lightweight model. More details about our model as well as training recipes can be found in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with MobileNets</head><p>For both BasisNet-MV2 and BasisNet-MV3, we compute the accuracy-MAdds curves by varying the input image resolution to the synthesized model from <ref type="bibr">{128, 160, 192, 224}.</ref> We compute the curves for the MobileNets in the same way. As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, even with the computation overhead of the lightweight model, our BasisNets consistently outperform the MobileNets with large margins.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The effect of regularization for proper training</head><p>In <ref type="figure" target="#fig_1">Fig. 3</ref> we show the performance improvements when different regularizations (basis model dropout, AutoAugment, and knowledge distillation) discussed in Sec. 3.3 are individually applied to BasisNet-MV2 training, as well as combined altogether. Each regularization helps generalization, and the most effective single regularization is the knowledge distillation. By combining all strategies the validation accuracy increases the most. In fact, we observed that the proposed training recipe also helps improving performance of other models like original MobileNets, as shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. However, applying the regularization is more crucial for BasisNet training, as the top-1 accuracy of BasisNet-MV2 (1.0x224) improves by +3.4 percentage points (74.7% ? 78.1%), while for MobileNetV2 the improvement is +2.0 percentage points (72.9% ? 74.9%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Number of bases in basis models</head><p>We varied the number of bases to investigate their effect on the model size, inference cost and final accuracy. Intuitively, the more bases in the candidate pool, the more diverse domains the final synthesized model can adapt to. We chose a fix-sized MV3-small (1.0x224) as our lightweight model, and use different numbers of MV3-large (1.0x224) for basis. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the top-1 accuracy improves monotonically with increased number of bases. With 16 bases, our BasisNet-MV3 achieved 80.3% accuracy with 290M MAdds. The shaded area represents the relative model size (#Params). Note that we explicitly trained a regular MobileNetV3-large with large multiplier and low image resolution (2.5x128), so it has similar model size with BasisNet. We show that BasisNet requires only 2/3 of computations (290M vs 435M) to achieve the comparable accuracy with the MobileNetV3 counterpart (80.3% vs 80.4%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with CondConv</head><p>We re-implemented CondConv 2 to directly compare with our BasisNet. We choose MobileNetV3 as backbone, <ref type="bibr" target="#b1">2</ref> Our re-implementation of CondConv-MV2 achieved 76.2% accuracy, better than the reported 74.6% from <ref type="bibr" target="#b36">[36]</ref>.     and selected N = 16 for both BasisNet and CondConv from layers 11 to 15. We chose MV3-small as the lightweight model for BasisNet, and disabled early termination for fair comparison. All models including CondConv baselines are re-trained using the same recipe as in Sec. 3.3.</p><p>The top-1 accuracy for CondConv-MV3 and BasisNet-MV3 is 79.9% and 80.3% respectively, although Basis-Net has relatively larger overhead due to the lightweight model. However, we find that BasisNet is more flexible than CondConv. CondConv reports that simultaneously activating multiple routes is essential for any single input, therefore sigmoid activation has to be used. For BasisNet, we find both sigmoid and softmax work fine (80.0% and 80.3% accuracy respectively). In fact, using softmax can lead to sparse and even one-hot combination coefficients (see Sec. 4.9), which may help reducing latency from model loading I/O perspective. We also experiment to combine CondConv with BasisNet, and the accuracy can be further boosted to 80.5%, showing the performance gain from Ba-sisNet is complementary to CondConv.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Early stop to reduce average inference cost</head><p>The two stage design of BasisNet naturally supports early termination, as the lightweight model can make an initial prediction. We chose the maximum value of softmax probability <ref type="bibr" target="#b13">[14]</ref> of initial prediction as the criterion to mea-sure the confidence. Specifically, for each input image if the initial prediction confidence is higher than a predefined threshold then second stage could be skipped, otherwise the second stage specialist needs be synthesized to make the final prediction.</p><p>We verified the early termination strategy on Ima-geNet validation set with a well-trained BasisNet-MV3 (1.0x224,16 basis) model. In <ref type="figure" target="#fig_4">Fig. 6</ref>, we alter thresholds of initial prediction confidence and plot the average cost and accuracy of BasisNet. For fair comparison, we cascade two well-trained MobileNets of the same size as the lightweight model and basis model respectively. In general the figure shows that BasisNet achieves better results for the same cost, except when the computation budget is very limited. Particularly for BasisNet, with a threshold of 0.7, 39.3% of images will skip the second stage thus the average computation cost reduces to 198M MAdds while the overall accuracy remains 80.0% on ImageNet validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Convex combination: special cases</head><p>Per-model model synthesis. When lightweight model predicts a single vector of combination coefficients for all layers, i.e. ? 1 = ? 2 = ? ? ? = ? K ? R N , it can be seen as a per-model synthesis. Note that per-model synthesis of Ba-sisNet is still different from HydraNets <ref type="bibr" target="#b26">[27]</ref>, as the branches in HydraNets span across multiple layers and do not fuse in the middle; instead, in BasisNet the convolution kernels are obtained from linear combination for each layer. We use BasisNet-MV3 with 8 bases and a lightweight model of MV3-small (1.0x224), and share all layers in basis models except for L11-15. Interestingly both per-model Ba-sisNet and per-layer BasisNet have the same performance, 79.6% top-1 accuracy on ImageNet validation set, implying the combination coefficients across layers may have high correlations for BasisNet-MV3. We also experiment with BasisNet-MV2 in a similar setting, but it turns out training per-model BasisNet-MV2 is more challenging because the model easily collapses after roughly 30K steps in our multiple attempts. We suspect that training per-model model synthesis is generally more difficult as it has stronger constraints on the basis models, and it may depend on the base architectures (MobileNetV2 or MobileNetV3).</p><p>Model selection instead of model synthesis. When the predicted combination coefficients are one-hot encoded, the model synthesis can be simplified as model selection, as only one base will be selected for a particular layer. We experimented with BasisNet-MV3 with 8 bases, and the lightweight model is MV3-small (1.0x128). Basis models share all layers except for L8-15, and the original BasisNet-MV3 has an accuracy of 79.8% under this setting. After training for 100K steps we froze the lightweight model and transformed the predicted combination coefficients into one-hot embedding, then continued training the basis mod- els. The resulting BasisNet finally achieved 78.5% accuracy. This is +0.7% better than post-processing a welltrained BasisNet (77.8%) implying the potential for training model selection end-to-end. We leave more careful finetuning for the model selection as future work, but emphasize that model selection has potential to further reduce latency in practice from a model loading I/O perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">On-device latency measurements</head><p>To validate the practical applicability, we measured the latency of the proposed BasisNet and other baselines on physical mobile device. We choose Google Pixel 3XL and run floating-point models on the big core of the phone's CPU. In <ref type="table" target="#tab_2">Table 3</ref> we show that BasisNet can run efficiently on existing mobile device. Our efficiency conclusion drawn from MAdds also applies to real latency. Specifically, Mo-bileNetV3 with 1.25x and 1.5x multipliers have similar accuracy as BasisNet-MV3 with 8 and 16 routes, while the BasisNet has lower latency. We also measured the latency for CondConv. Primarily because of the first stage lightweight model, BasisNet without early termination has higher latency than CondConv (62.9ms vs 53.1ms). However, we emphasize that the first stage lightweight model generates better combination coefficients thus improves the top-1 accuracy (80.3% vs 79.9%). Besides, the lightweight model generates initial prediction to enable early termination. When early termination is enabled, the average latency for BasisNet reduced significantly to 43.6ms <ref type="bibr" target="#b2">3</ref> , which is much lower than CondConv (53.1ms) while retaining slightly superior accuracy (80.0% vs 79.9%). As we described in Sec. 3.2, deploying early termination for Cond-Conv is much more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Understanding the learned BasisNet models</head><p>Visualizing the specialization of basis models. We visualized the combination coefficient vectors on ImageNet validation set to better understand the effectiveness of model synthesis. In <ref type="figure" target="#fig_5">Fig. 7</ref> we show visually similar and distinct categories, as well as the combination coefficients of L15. From (B) top, we can see that the lightweight model chooses the same specialist for distinguishing dif-  <ref type="table">Table 4</ref>. Apply various disturbance to combination coefficients. ferent dogs, in order to better handle the subtleties between dog breeds. But for visually distinct categories, the synthesized models are very different evidenced by the nonoverlapping curves in (B) bottom. In <ref type="figure" target="#fig_5">Fig. 7 (D)</ref> we show the coefficients for all images using t-SNE <ref type="bibr" target="#b27">[28]</ref>. The dog categories form a single cluster while the others reside in very different clusters. More interestingly, we find that even fine-grained visual patterns can be distinguished as different base models are activated, e.g., fluffy dogs mainly activate 2nd base but short-haired dogs use 14th base. More qualitative results are provided in supplementary materials.</p><p>The importance of optimal basis model synthesis. To verify the importance of model synthesis, we apply disturbances to the predicted combination coefficients. The specialist should be most effective for the corresponding image, and a disturbed synthesis signal is expected to hurt performance. We train BasisNet-MV2 (Accuracy 78.2%) and BasisNet-MV3 (Accuracy 79.8%), and share only the first 7 layers in the basis, then disturb the coefficients ? as follows: (1) preserving the highest probable basis model only (TOP-1), (2) uniformly combining all basis models (UNIFORM), (3) using mean weights over entire validation set (MEAN), or (4) randomly shuffling the coefficients within each layer (SHUFFLED). As shown in <ref type="table">Table 4</ref>, all disturbances lead to inferior performance validating that basis models have varied expertise. SHUFFLED leads to a totally mismatched specialist thus performance drops over 20 percentage points. Reference 78.2 79.8 <ref type="table">Table 5</ref>. Top-1 accuracy drops when SHUFFLED disturbance was applied at different layer. The last row shows the reference model that uses undisturbed predicted coefficients. sensitivity within the model. As shown in <ref type="table">Table 5</ref>, we find the layers closer to the final classification layer have more impacts, as the accuracy drop is more significant. Interestingly, the regular convolutional layer right after the residual bottleneck layers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13]</ref> (e.g. L18 of MobileNetV2 and L16 of MobileNetV3) seems less sensitive towards inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of model synthesis at different layers We also apply disturbances on each individual layer to investigate the</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present BasisNet, which combines the recent advancements in multiple perspectives such as efficient model design and dynamic inference. With a standalone lightweight model, the unnecessary computation on easy examples can be saved and the information extracted by the lightweight model help synthesizing a specialist network for better prediction. With extensive experiments on ImageNet we show the proposed BasisNet is particularly effective on efficient inference, and BasisNet-MV3 achieves 80.3% top-1 accuracy with only 290M MAdds even without early termination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed model architecture of BasisNet</head><p>Here we describe the details about the proposed Basis-Net, including the lightweight model and basis models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Lightweight model</head><p>For BasisNet-MV2, the lightweight model follows the architecture described in <ref type="table" target="#tab_1">Table 2</ref> of <ref type="bibr" target="#b21">[22]</ref>, and we use multiplier of 0.5 and input image resolution of 128. The lightweight model has a computation overhead of 30.3M MAdds and a model size of 1.2M parameters.</p><p>For BasisNet-MV3, we use MobileNetV3-small for our lightweight model as described in <ref type="table" target="#tab_1">Table 2</ref> of <ref type="bibr" target="#b12">[13]</ref>, and we use multiplier of 1.0 and input resolution of 128 or 224 for different experiments. The model size for the lightweight model is 2.5M parameters regardless of input image resolutions. With 128 ? 128 image, the lightweight model has 19.9M MAdds computation overhead, and with 224 ? 224 image the computation overhead is 56.5M MAdds.</p><p>As described in Sec. 3.1, the lightweight model has two tasks, one for initial classification prediction and the other for combination coefficients prediction. The first task is similar with any regular classification task, and can be formally described as:?</p><formula xml:id="formula_7">= LM(f (x); W LM )<label>(7)</label></formula><p>Note that the two tasks share all but the final classification head, thus the extra computation for predicting the combination coefficients is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Detailed architectures for different experiments</head><p>Here we describe the detail about models in different experiments. Unless stated otherwise, we use the following settings as default for BasisNet-MV2 and BasisNet-MV3:</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementations and training recipe</head><p>Our project is implemented with TensorFlow <ref type="bibr" target="#b0">[1]</ref>. Following <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b12">[13]</ref>, we train all models using synchronous training setup on 8x8 TPU Pod, and we use standard RM-SProp optimizer with both decay and momentum set to 0.9. The initial learning rate is set to 0.006 and linearly warms up within the first 20 epochs. The learning rate decays every 6 epochs for BasisNet-MV2 (4.5 epochs for BasisNet-MV3) by a factor of 0.99. The total batch size is 16384 (i.e. 128 images per chip). For stabilizing the training, as described in Section 3.3 we keep = 1 for the first 10K training steps then linearly decays to 0 in the next 40K steps. We also used gradients clipping with clip norm of 0.1 for BasisNet-MV3. In general, all BasisNet and reference baseline models are trained for 400K steps. We set the L2 weight decay to 1e-5, and used the data augmentation policy for ImageNet from AutoAugment <ref type="bibr" target="#b6">[7]</ref>. We choose the checkpoint from <ref type="bibr" target="#b34">[35]</ref> as our EfficientNet-b2 teacher model for distillation, and for BasisNet-MV3 both lightweight model and all basis models are trained with teacher supervision.</p><p>For BasisNet-MV2, we only distill the basis models but use the groundtruths labels without label smoothing for training the lightweight model. For basis models dropout, we use dropout rate of 1/8 for all BasisNets with no more than 8 bases, and use 1/16 for the rest which has 16 or more bases. Following <ref type="bibr" target="#b12">[13]</ref>, we also use exponential moving average with decay 0.9999 and set the dropout keep probability to 0.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with other efficient networks</head><p>In <ref type="table" target="#tab_0">Table 1</ref> of our main paper, we show a comparison table with recent efficient neural networks on Ima-geNet classification benchmark. For baselines we directly use the statistics from the corresponding original papers, even though the training procedures could be very different. Some common tricks in literature include knowledge distillation?, training with extra data?, applying custom data augmentation?, or using AutoML-based learned training recipes (hyperparameters)?. Different models may choose subsets of these tricks in their training procedure. For example, <ref type="bibr" target="#b34">[35]</ref> use 3.5B weakly labeled images as extra data and use knowledge distillation to iteratively train better student models. CondConv <ref type="bibr" target="#b36">[36]</ref> use AutoAugment <ref type="bibr" target="#b6">[7]</ref> and mixup <ref type="bibr" target="#b37">[37]</ref> as custom data augmentation. <ref type="bibr" target="#b33">[34]</ref> reported in a concurrent work that combining AutoAugment and knowledge distillation can have even stronger performance boost, because soft-labels from knowledge distillation helps alleviating label misalignment during aggressive data augmentation. In FBNetV3 <ref type="bibr" target="#b7">[8]</ref> the training hyperparameters are treated as components in the search space and are obtained from AutoML-based joint architecture-recipe search. OFA <ref type="bibr" target="#b2">[3]</ref> use the largest model as teacher to perform knowledge distillation to improve the smaller models. Notably, in our main paper, unless stated otherwise, we always reported the statistics from our re-implementations, thus the comparison in our ablation studies are fair, but some results might be inconsistent with this table. It is also worth mentioning that even though we did not explicitly use extra data for training BasisNet, the teacher model checkpoint that we used for knowledge distillation is from noisy student training <ref type="bibr" target="#b34">[35]</ref>, thus our model may indirectly benefit from the extra data (thus noted by ?). However, we also experimented with 1.4x MobileNet-V2 as teacher model (which is not exposed to extra data) to train BasisNet-MV2, and verified that the main conclusion still holds. In <ref type="table">Table 6</ref>, we show original data of <ref type="figure" target="#fig_1">Fig. 3</ref> of the main paper, so readers can get the exact accuracy numbers more easily. Specifically, we show the model performance with different regularizations at 4 different image resolutions <ref type="bibr">{128, 160, 192, 224}</ref> in the last four columns. We compare the data augmentation (Preprocess, regular represents the Inception preprocess as in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13]</ref>, and AA represents AutoAugment from <ref type="bibr" target="#b6">[7]</ref>), distillation with different teachers (MV2 1.4x represents MobileNetV2 with 1.4x multiplier, EfN-b2 represents EfficientNet-b2 model from <ref type="bibr" target="#b34">[35]</ref>), and basis model dropout.</p><p>We experimented with different teacher network to distill the BasisNet. Note that the MobileNetV2 1.4x teacher we used is from <ref type="bibr" target="#b21">[22]</ref> and has accuracy of 74.9%, and our BasisNet achieves even higher accuracy of 75.4% than the teacher. We also experimented different variations of Effi-cientNet (b0, b2, b4, b7) and find that models trained with EfficientNet-b2 has the best performance, and using even better teacher network does not bring performance gain to the BasisNet. We suspect this is related to the gap between teacher and student network as reported in <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Detailed experiments for number of bases in basis models (Sec. 4.4)</head><p>In <ref type="table" target="#tab_4">Table 7</ref>, we present the original data for <ref type="figure" target="#fig_2">Fig. 4</ref> of the main paper, so readers can get the exact accuracy numbers more easily. Notably, we find that BasisNet-MV3 with 16 bases is a good balance between model accuracy and computation budget, achieving 80.3% top-1 accuracy with 290M Madds. This table also shows that BasisNet technique optimizes MAdds at the expense of model size. We studied the performance of BasisNet with lightweight model of different size. Here the size is measured by the Multiply-adds (MAdds) as we pay more attention to the inference cost. We experimented with a BasisNet-MV3 of MV3-large (1.0x224) with 8 bases. The lightweight model is MV3-small, and we experimented with two hyperparameters, i.e. the input image resolution As shown in <ref type="figure" target="#fig_8">Figure 8</ref>, even an extremely efficient lightweight model (MV3-small (0.35x128), computation overhead of 13.8M Madds) can lead to a performance boost from 77.7% to 78.9% (+1.2%). This experiment shows that resolution and multiplier can have an equivalent effect as reported in <ref type="bibr" target="#b20">[21]</ref> and a lightweight model with a smaller computation overhead can bring most of the performance gain. Thus it might be more beneficial to scale the model multiplier and resolution coordinately <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Model synthesis with varying sized lightweight model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Model synthesis with early termination</head><p>To better understand the capability of the lightweight model and the synthesized specialist, we split the 50K validation images into multiple buckets according to the sorted highest probability, and show the accuracy of different models for each bucket in <ref type="figure">Figure 9</ref>. Specifically, we show the accuracy within each bucket by the lightweight model, synthesized specialist model and a reference MobileNetV3 <ref type="figure">Figure 9</ref>. Prediction accuracy is comparable for more confident predictions (e.g. top 40%), and the synthesized specialist consistently outperforms regular MobileNet in all buckets.</p><p>baseline. We observe that for at least one third of images where lightweight model has high prediction confidence, the accuracy gaps between these three models are negligible (&lt; 1%). The BasisNet has clear advantage over MobileNet in all buckets, especially for more difficult (low confidence) cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More qualitative visualizations E.1. Categories handled by different basis models</head><p>In <ref type="figure" target="#fig_7">Figure 10</ref> we show several most strongly activated categories for four different basis models on ImageNet validation set. Specifically we trained BasisNet-MV3 with 16 bases, and checked the mean weights at the last non-sharing layer (L15) and show the categories that have the highest mean weights. It is clear that the lightweight model captures the fine-grained visual similarity, for example the base 2 seems to handle the fluffy dogs while the base 14 is more about short-haired dogs. Another example is for base 13 that a clear grid pattern can be found in the images, but semantically these categories are loosely related.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Combination coefficients for visually similar categories</head><p>In <ref type="figure" target="#fig_7">Figure 11</ref> we show 10 categories regarding different types of cars and the mean predicted combination coefficients for these categories in all layers. Obviously the lightweight model assigns similar coefficients for various cars, implying the effectiveness of the lightweight model. For example, we see that in Layer 14 almost all cars are re-lying on base 8, and in L15 all cars use a combination of base 3 and base 6. Quantitatively BasisNet over these 10 categories have an accuracy of 76.6%, but a corresponding regular MobileNetV3 has only 73.2%.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Accuracy-MAdds trade-off comparison of the proposed BasisNet and Mo-bileNet on ImageNet validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Performance boost with various regularizations on BasisNet-MV2. All combined gives the largest improvement.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Prediction accuracy monotonically increases when more bases are added to the basis models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>MobileNet and BasisNet training using different regularizations. BasisNet uses MV2-0.5x as its lightweight model and 8 MV2-1.0x for basis models. Input image resolutions vary from {128, 160, 192, 224}. Note that basis model dropout (BMD) is not applicable to MobileNet because it has only one model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Simulated accuracy comparison of BasisNet-MV3 and cascaded MobileNets with early exiting under varying thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>(A,C) Sample images from visually similar or distinct categories. (B) Mean coefficient weights at L15 layer for selected categories. (D) t-SNE visualization of combination coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>D. More quantitative experiments D. 1 .</head><label>1</label><figDesc>Detailed comparison with MobileNets (Sec. 4.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>BasisNet-MV3 with lightweight model of different sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>Categories with highest mean coefficients for different basis models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .</head><label>11</label><figDesc>Visualization of predicted combination coefficients for similar categories over all layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with other efficient networks on ImageNet. Statistics on referenced baselines are cited from original papers. Different training strategies are applied, e.g., ? knowledge distillation; ? training with extra data; ? custom data augmentation; ? AutoML-based learned training recipes.</figDesc><table><row><cell>35]??</cell><cell>390M</cell><cell>78.1</cell></row><row><cell>EfficientNet-B0 (AA + KD) [34]??</cell><cell>390M</cell><cell>78.0</cell></row><row><cell>CondConv-EfficientNet-B0 [36]?</cell><cell>413M</cell><cell>78.3</cell></row><row><cell>ProxylessNas [4]</cell><cell>320M</cell><cell>74.6</cell></row><row><cell>FBNetV2-L1 [31]</cell><cell>325M</cell><cell>77.2</cell></row><row><cell>FBNetV3-A [8]?</cell><cell>343M</cell><cell>78.0</cell></row><row><cell>MnasNet-A1 [24]</cell><cell>312M</cell><cell>75.2</cell></row><row><cell>CondConv-MnasNet-A1 [36]?</cell><cell>325M</cell><cell>76.2</cell></row><row><cell>EfficientNet-B2 [25]</cell><cell>1.0B</cell><cell>80.1</cell></row><row><cell>EfficientNet-B1 (Noisy Stdt.) [35]??</cell><cell>700M</cell><cell>80.2</cell></row><row><cell>FBNetV3-E [8]?</cell><cell>752M</cell><cell>80.4</cell></row><row><cell>OFA [3]?</cell><cell>595M</cell><cell>80.0</cell></row><row><cell>BasisNet-MV3 (Ours)???</cell><cell>290M</cell><cell>80.3</cell></row><row><cell cols="3">+ Early Termination (Ours)???~198M~80.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of BasisNet with CondConv.</figDesc><table><row><cell>CondConv-MV3</cell><cell>Sigmoid</cell><cell>253M</cell><cell>79.9%</cell></row><row><cell>BasisNet-MV3</cell><cell>Softmax</cell><cell>290M</cell><cell>80.3%</cell></row><row><cell>BasisNet-MV3</cell><cell>Sigmoid</cell><cell>290M</cell><cell>80.0%</cell></row><row><cell>(BasisNet+CC)-MV3</cell><cell>Softmax</cell><cell>290M</cell><cell>80.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Latency measurements on Google Pixel 3XL.</figDesc><table><row><cell>Models</cell><cell cols="3">Top-1 Acc. MAdds (M) Latency (ms)</cell></row><row><cell>BasisNet-MV3 8-routes</cell><cell>79.6%</cell><cell>281</cell><cell>60.6</cell></row><row><cell>BasisNet-MV3 16-routes</cell><cell>80.3%</cell><cell>290</cell><cell>62.9</cell></row><row><cell>-With early termination</cell><cell>80.0%</cell><cell>198 (avg.)</cell><cell>43.6 (avg.)</cell></row><row><cell>MobileNetV3 (1.25x224)</cell><cell>79.7%</cell><cell>356</cell><cell>66.3</cell></row><row><cell>MobileNetV3 (1.5x224)</cell><cell>80.6%</cell><cell>489</cell><cell>86.2</cell></row><row><cell>CondConv-MobileNetV3</cell><cell>79.9%</cell><cell>253</cell><cell>53.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Comparison with MobileNets (Sec. 4.2) We use BasisNet-MV2 with 8 bases and the lightweight model is MV2 (0.5x128). Each basis model is a MV2 (1.0x224) and they only differ in parameters from L11-17. The basis models dropout rate is 1/8. All basis models share parameters except for in layers L8-15. The basis model dropout rate is 1/16. We use the same BasisNet-MV3 model as in Sec. 4.5.</figDesc><table><row><cell>For BasisNet-MV3, we use 16 bases each of a MV3-</cell></row><row><cell>large (1.0x224), and the lightweight model is MV3-small</cell></row><row><cell>(1.0x128). The effect of regularization for proper training (Sec. 4.3)</cell></row><row><cell>We use the same model architectures for BasisNet-MV2 and</cell></row><row><cell>BasisNet-MV3 with Sec. 4.2.</cell></row><row><cell>Number of bases in basis models (Sec. 4.4) We use</cell></row><row><cell>BasisNet-MV3 with different number of basis models, but</cell></row><row><cell>each is a MV3-large (1.0x224). The lightweight model is</cell></row><row><cell>MV3-small (1.0x224) and all basis models share parame-</cell></row><row><cell>ters except for layers L11-15. For BasisNet with no more</cell></row><row><cell>than 8 bases we use basis model dropout rate of 1/8 and for</cell></row><row><cell>all others (16 to 128 bases) we use a basis model dropout</cell></row><row><cell>rate of 1/16.</cell></row><row><cell>Comparison with CondConv (Sec. 4.5) For BasisNet-</cell></row><row><cell>MV3, we use 16 basis models each of a MV3-large</cell></row><row><cell>(1.0x224), and the lightweight model is MV3-small</cell></row><row><cell>(1.0x224). All basis models share parameters except for</cell></row><row><cell>layers L11-15. The basis model dropout rate is 1/16.</cell></row><row><cell>Early stop to reduce average inference cost (Sec. 4.6)</cell></row><row><cell>For MobileNetV2 experiments, the first-stage</cell></row><row><cell>lightweight model is MobileNetV2 with 0.5x mul-</cell></row><row><cell>tiplier and input image resolution of 128 (MV2,</cell></row><row><cell>0.5x128) and the second stage has 8 basis models</cell></row><row><cell>of MobileNetV2 1.0x with image resolution of 224</cell></row><row><cell>(MV2, 1.0x224). Basis models share parameters in</cell></row><row><cell>layers from L1 to L10 and final classification layer,</cell></row><row><cell>and differ in parameters in L11 to L17.</cell></row><row><cell>? For MobileNetV3, the lightweight model is</cell></row><row><cell>MobileNetV3-small with 1.0x multiplier and input</cell></row><row><cell>image resolution of 128 (MV3-small, 1.0x128). The</cell></row><row><cell>second stage has 16 basis models of MobileNetV3-</cell></row><row><cell>large with 1.0x multiplier and resolution of 224</cell></row><row><cell>(MV3-large, 1.0x224), and they share parameters in</cell></row><row><cell>first 7 and last 2 layers, and differ in parameters in L8</cell></row><row><cell>to L15.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>Comparison of BasisNets with different number of bases. to lightweight model ({128, 160, 192, 224}) and the multiplier ({0.35, 0.5, 0.75, 1.0}).</figDesc><table><row><cell cols="2">Model</cell><cell cols="4">Preprocess Distillation # Bases (BMD) 128 160 192 224</cell></row><row><cell cols="2">MobileNetV2</cell><cell>regular</cell><cell>None</cell><cell>N/A</cell><cell>66.6 69.5 71.5 72.9</cell></row><row><cell cols="2">MobileNetV2</cell><cell>AA</cell><cell>None</cell><cell>N/A</cell><cell>67.8 70.7 72.7 73.7</cell></row><row><cell cols="2">MobileNetV2</cell><cell>AA</cell><cell>MV2 1.4x</cell><cell>N/A</cell><cell>68.8 71.4 72.4 73.1</cell></row><row><cell cols="2">MobileNetV2</cell><cell>AA</cell><cell>EfN-b2</cell><cell>N/A</cell><cell>69.8 72.6 73.8 74.9</cell></row><row><cell cols="2">BasisNet-MV2</cell><cell>regular</cell><cell>None</cell><cell>8 (0)</cell><cell>68.6 71.4 73.3 74.7</cell></row><row><cell cols="2">BasisNet-MV2</cell><cell>AA</cell><cell>None</cell><cell>8 (0)</cell><cell>70.4 72.8 74.6 75.6</cell></row><row><cell cols="2">BasisNet-MV2</cell><cell>regular</cell><cell>EfN-b2</cell><cell>8 (0)</cell><cell>71.8 74.8 76.2 77.2</cell></row><row><cell cols="2">BasisNet-MV2</cell><cell>regular</cell><cell>None</cell><cell>8 (1/8)</cell><cell>69.1 71.9 73.7 75.0</cell></row><row><cell cols="2">BasisNet-MV2</cell><cell>AA</cell><cell>None</cell><cell>8 (1/8)</cell><cell>70.9 73.2 75.1 75.9</cell></row><row><cell cols="2">BasisNet-MV2</cell><cell>AA</cell><cell>MV2 1.4x</cell><cell>8 (1/8)</cell><cell>72.3 73.8 74.7 75.4</cell></row><row><cell cols="2">BasisNet-MV2</cell><cell>AA</cell><cell>EfN-b2</cell><cell>8 (1/8)</cell><cell>73.5 75.9 77.0 78.1</cell></row><row><cell></cell><cell></cell><cell cols="4">Table 6. Detailed comparison of BasisNet-MV2 with MobileNetV2.</cell></row><row><cell>Model</cell><cell cols="3">#MAdds(M) #Params(M) Acc.(%)</cell><cell></cell></row><row><cell>MV3 (1.0x224)</cell><cell>217</cell><cell>5.45</cell><cell>77.7</cell><cell></cell></row><row><cell>MV3 (1.25x224)</cell><cell>356</cell><cell>8.22</cell><cell>79.7</cell><cell></cell></row><row><cell>MV3 (1.5x224)</cell><cell>489</cell><cell>11.3</cell><cell>80.6</cell><cell></cell></row><row><cell>MV3 (2.0x128)</cell><cell>276</cell><cell>19.1</cell><cell>79.2</cell><cell></cell></row><row><cell>MV3 (2.5x128)</cell><cell>435</cell><cell>29.0</cell><cell>80.4</cell><cell></cell></row><row><cell>#Bases=1</cell><cell>273</cell><cell>8.07</cell><cell>77.7</cell><cell></cell></row><row><cell>#Bases=2</cell><cell>274</cell><cell>9.19</cell><cell>78.0</cell><cell></cell></row><row><cell>#Bases=4</cell><cell>277</cell><cell>11.4</cell><cell>78.8</cell><cell></cell></row><row><cell>#Bases=8</cell><cell>281</cell><cell>15.9</cell><cell>79.6</cell><cell></cell></row><row><cell>#Bases=16</cell><cell>290</cell><cell>24.9</cell><cell>80.3</cell><cell></cell></row><row><cell>#Bases=32</cell><cell>308</cell><cell>42.8</cell><cell>80.5</cell><cell></cell></row><row><cell>#Bases=64</cell><cell>344</cell><cell>78.6</cell><cell>80.7</cell><cell></cell></row><row><cell>#Bases=128</cell><cell>416</cell><cell>150.3</cell><cell>80.9</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Average cost is reduced since easy inputs are only handled by lightweight model; max remains 290M MAdds.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">With threshold of 0.7 on ImageNet, 39.3% of images can skip second stage thus the estimated average latency is reduced to 0.393 ? 13.7ms +(1 ? 0.393) ? 62.9ms = 43.6ms.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive neural networks for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Once-for-all: Train one network and specialize it for efficient deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR), 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic convolution: Attention over convolution kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You look twice: Gaternet for dynamic filter selection in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02049</idno>
		<title level="m">Joint architecture-recipe search using neural acquisition function</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-scale dense networks for resource efficient image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Binarized neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On-device neural net inference with mobile gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Chirkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Ignasheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yury</forename><surname>Pisarchyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mogan</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Sarokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Kulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Grundmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Efficient Deep Learning for Computer Vision CVPR 2019 (ECV2019)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved knowledge distillation via teacher assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Seyed-Iman Mirzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghasemzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Pramod Kaushik Mudrakarta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Howard</surname></persName>
		</author>
		<title level="m">Parameter-efficient multi-task and transfer learning. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-discriminative data or weak model? on the relative importance of data and model resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Baccash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Outrageously large neural networks: The sparsely-gated mixtureof-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Branchynet: Fast inference via early exiting from deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surat</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Tsung</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition (ICPR)</title>
		<meeting>the International Conference on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hydranets: Specialized dynamic architectures for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravi Teja Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayvon</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scalable-effort classifiers for energyefficient machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagath</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anand</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Shoaib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Design Automation Conference (DAC)</title>
		<meeting>the Annual Design Automation Conference (DAC)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fb-netv2: Differentiable neural architecture search for spatial and channel dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Circumventing outliers of autoaugment with knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.13237</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Domain-aware dynamic networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dynet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10694</idno>
		<title level="m">Dynamic convolution for accelerating convolutional neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
