<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AlignMixup: Improving Representations By Interpolating Aligned Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>IRISA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ewa</forename><surname>Kijak</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>IRISA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Amsaleg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<region>IRISA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Athena</settlement>
									<region>RC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AlignMixup: Improving Representations By Interpolating Aligned Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Mixup is a powerful data augmentation method that interpolates between two or more examples in the input or feature space and between the corresponding target labels. However, how to best interpolate images is not well defined. Recent mixup methods overlay or cut-and-paste two or more objects into one image, which needs care in selecting regions. Mixup has also been connected to autoencoders, because often autoencoders generate an image that continuously deforms into another. However, such images are typically of low quality.</p><p>In this work, we revisit mixup from the deformation perspective and introduce AlignMixup, where we geometrically align two images in the feature space. The correspondences allow us to interpolate between two sets of features, while keeping the locations of one set. Interestingly, this retains mostly the geometry or pose of one image and the appearance or texture of the other. We also show that an autoencoder can still improve representation learning under mixup, without the classifier ever seeing decoded images. AlignMixup outperforms state-of-theart mixup methods on five different benchmarks. Code available at https://github.com/shashankvkt/ AlignMixup_CVPR22.git</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Data augmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref> is a powerful regularization method that increases the amount and diversity of data, be it labeled or unlabeled <ref type="bibr" target="#b15">[16]</ref>. It improves the generalization performance and helps learning invariance <ref type="bibr" target="#b48">[49]</ref> at almost no cost, because the same example can be transformed in different ways over epochs. However, by operating on one image at a time and limiting to label-preserving transformations, it has limited chances of exploring beyond the image manifold. Hence, it is of little help in combating memorization of training data <ref type="bibr" target="#b66">[67]</ref> and sensitivity to adversarial examples <ref type="bibr" target="#b52">[53]</ref>.</p><p>Mixup operates on two or more examples at a time, interpolating between them in the input space <ref type="bibr" target="#b68">[69]</ref> or feature space <ref type="bibr" target="#b57">[58]</ref>, while also interpolating between target la- <ref type="bibr">Image 1</ref> Input mixup <ref type="bibr" target="#b68">[69]</ref> CutMix <ref type="bibr" target="#b64">[65]</ref> Image 2 Manifold mixup <ref type="bibr" target="#b57">[58]</ref> AlignMixup (Ours) <ref type="figure">Figure 1</ref>. Different mixup methods. AlignMixup retains the pose of image 2 and the texture of image 1. This different from overlay (Input and Manifold mixup) or combination of two objects (Cut-Mix). Manifold mixup and AlignMixup visualized by a decoder (subsection 3.3) that is not used at training. bels for image classification. This flattens class representations <ref type="bibr" target="#b57">[58]</ref>, reduces overly confident incorrect predictions, and smoothens decision boundaries far away from training data. However, input mixup images are overlays and tend to be unnatural <ref type="bibr" target="#b64">[65]</ref>. Interestingly, recent mixup methods focus of combining two <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b64">65]</ref> or more <ref type="bibr" target="#b30">[31]</ref> objects from different images into one in the input space, making efficient use of training pixels. However, randomness in the patch selection and thereby label mixing may mislead the classifier to learn uninformative features <ref type="bibr" target="#b56">[57]</ref>, which raises the question: what is a good interpolation of images?</p><p>Bengio et al. <ref type="bibr" target="#b2">[3]</ref> show that traversing along the manifold of representations obtained from deeper layers of the network more likely results in finding realistic examples. This is because the interpolated points smoothly traverse the underlying manifold of the data, capturing salient characteristics of the two images. Furthermore, <ref type="bibr" target="#b3">[4]</ref> show the ability of autoencoders to capture semantic correspondences obtained by decoding mixed latent codes. This is because the autoencoder may disentangle the underlying factors of variation. Efforts have followed on mixing latent representations of autoencoders to generate realistic images for data aug-mentation. However, these approaches are more expensive, requiring three networks (encoder, decoder, classifier) <ref type="bibr" target="#b3">[4]</ref> and more complex, often also requiring an adversarial discriminator <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>. More importantly, they perform poorly compared to standard input mixup on large datasets <ref type="bibr" target="#b38">[39]</ref>, due to the low quality of generated images.</p><p>In this work, we are motivated by the idea of deformation as a natural way of interpolating images, where one image may deform into another, in a continuous way. Contrary to previous efforts, we do not interpolate directly in the input space, we do not limit to vectors as latent codes and we do not decode. We rather investigate geometric alignment for mixup, based on explicit semantic correspondences in the feature space. In particular, we explicitly align the feature tensors of two images, resulting in soft correspondences. The tensors can be seen as sets of features with coordinates. Hence, each feature in one set can be interpolated with few features in the other.</p><p>By choosing to keep the coordinates of one set or the other, we define an asymmetric operation. What we obtain is one object continuously morphing, rather than two objects in one image. Interestingly, observing this asymmetric morphing reveals that we retain the geometry or pose of the image where we keep the coordinates and the appearance or texture of the other. <ref type="figure">Figure 1</ref> illustrates that our method, AlignMixup, retains the pose of image 2 and the texture of image 1, which is different from existing mixup methods. Note that, as in manifold mixup, we do not decode, hence we are not concerned about the quality of generated images.</p><p>We make the following contributions:</p><p>1. We introduce a novel mixup operation, called Align-Mixup, advocating interpolation of local structure in the feature space (subsection 3.2). Feature tensors are ideal for alignment, giving rise to semantic correspondences and being of low resolution. Alignment is efficient by using Sinkhorn distance <ref type="bibr" target="#b10">[11]</ref>. 2. We also show that a vanilla autoencoder can further improve representation learning under mixup training, without the classifier seeing decoded clean or mixed images (section 4). 3. We set a new state-of-the-art on image classification, robustness to adversarial attacks, calibration, weaklysupervised localization and out-of-distribution detection against more sophisticated mixup operations on several networks and datasets (section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Mixup <ref type="bibr" target="#b68">[69]</ref>, concurrently with similar methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56]</ref>, introduce mixup, augmenting data by linear interpolation between two examples. While <ref type="bibr" target="#b68">[69]</ref> apply mixup on intermediate representations, it is <ref type="bibr" target="#b57">[58]</ref> who make this work, introducing manifold mixup. Without alignment, the result is an overlay of either images <ref type="bibr" target="#b68">[69]</ref> or features <ref type="bibr" target="#b57">[58]</ref>. <ref type="bibr" target="#b22">[23]</ref> eliminate "manifold intrusion"-mixed data conflicting with true data. Unlike manifold mixup, AlignMixup interpolates feature tensors from deeper layers after aligning them. Nonlinear mixing over random image regions is an alternative, e.g. from masking square regions <ref type="bibr" target="#b13">[14]</ref> to cutting a rectangular region from one image and pasting it onto another <ref type="bibr" target="#b64">[65]</ref>, as well as several variants using arbitrary regions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b53">54]</ref>. Instead of choosing regions at random, saliency can be used to locate objects from different images and fit them in one <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b56">57]</ref>. Exploiting the knowledge of a teacher network to mix images based on saliency has been proposed in <ref type="bibr" target="#b11">[12]</ref>. Instead of combining more than one objects in an image, AlignMixup attempts to deform one object into another.</p><p>Another alternative is Automix <ref type="bibr" target="#b71">[72]</ref>, which employs a U-Net rather than an autoencoder, mixing at several layers. It is limited to small datasets and provides little improvement over manifold mixup <ref type="bibr" target="#b57">[58]</ref>. StyleMix and StyleCut-Mix <ref type="bibr" target="#b27">[28]</ref> interpolate content and style between two images, using AdaIN <ref type="bibr" target="#b28">[29]</ref>, a style transfer autoencoder network. By contrast, AlignMixup aligns feature tensors and interpolates matching features directly, without using any additional network.</p><p>Alignment Local correspondences from intra-class alignment of feature tensors have been used in image registration <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b39">40]</ref>, optical flow <ref type="bibr" target="#b60">[61]</ref>, semantic alignment <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b45">46]</ref> and image retrieval <ref type="bibr" target="#b49">[50]</ref>. Here, we mostly use inter-class alignment. In few-shot learning, local correspondences between query and support images are important in finding attention maps, used e.g. by CrossTransformers <ref type="bibr" target="#b14">[15]</ref> and DeepEMD <ref type="bibr" target="#b67">[68]</ref>. The earth mover's distance (EMD) <ref type="bibr" target="#b46">[47]</ref>, or Wasserstein metric, is an instance of optimal transport <ref type="bibr" target="#b58">[59]</ref>, addressed by linear programming. To accelerate, <ref type="bibr" target="#b10">[11]</ref> computes optimal matching by Sinkhorn distance with entropic regularization. This distance is widely applied between distributions in generative models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42]</ref>.</p><p>EMD has been used for mixup in the input space, for instance point mixup for 3D point clouds <ref type="bibr" target="#b5">[6]</ref> and OptTrans-Mix for images <ref type="bibr" target="#b71">[72]</ref>, which is the closest to our work. However, aligning coordinates only applies to images with clean background. We rather align tensors in the feature space, which is generic. We do so using the Sinkhorn distance, which is orders of magnitude faster than EMD <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AlignMixup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>Problem formulation Let (x, y) be an image x ? X with its one-hot encoded class label y ? Y , where X is the input image space, Y = [0, 1] k and k is the number of classes. An encoder network F : X ? R c?w?h maps x to feature tensor A = F (x), where c is the number of channels and w ? h is the spatial resolution. A classifier g : R c?w?h ? R k then maps A to the vector p = g(A) of probabilities over classes.</p><p>Mixup We follow <ref type="bibr" target="#b57">[58]</ref> in mixing the representations from different layers of the network, focusing on the deepest layers near the classifier. We are given two labeled images (x, y), (x ? , y ? ) ? X ? Y . We draw an interpolation factor ? ? [0, 1] from Beta(?, ?) <ref type="bibr" target="#b68">[69]</ref> and then we interpolate labels y, y ? linearly by the standard mixup operator</p><formula xml:id="formula_0">mix ? (y, y ? ) := ?y + (1 ? ?)y ?<label>(1)</label></formula><p>and inputs x, x ? by the generic formula</p><formula xml:id="formula_1">Mix f1,f2 ? (x, x ? ) := f 2 (Mix ? (f 1 (x), f 1 (x ? )),<label>(2)</label></formula><p>where Mix ? is a mixup operator to be defined. This generic formula allows interpolation of the input or feature as f 2 ?f 1 according to</p><formula xml:id="formula_2">input (x) : f 1 := id, f 2 := F (3) feature (A) : f 1 := F, f 2 := id,<label>(4)</label></formula><p>where id is the identity mapping. For (3), we define Mix ? in (2) as standard mixup mix ? (1), like <ref type="bibr" target="#b68">[69]</ref>; while for (4), we define Mix ? as discussed in subsection 3.2. By default, we train the encoder network and the classifier by using a classification loss L c on the output of the classifier g for mixed examples along with the corresponding mixed labels:</p><formula xml:id="formula_3">L c (g(Mix f1,f2 ? (x, x ? )), mix ? (y, y ? )),<label>(5)</label></formula><p>where L c (p, y) := ? k i=1 y i log p i is the standard crossentropy loss. More options using an autoencoder architecture are investigated in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Interpolation of aligned feature tensors</head><p>Alignment Alignment refers to finding a geometric correspondence between image elements before interpolation. The feature tensor is ideal for this purpose, because its spatial resolution is low, reducing the optimization cost, and allows for semantic correspondence, because features close to the classifier are small. Importantly, we are not attempting to combine two or more objects into one image <ref type="bibr" target="#b31">[32]</ref>, but put two objects in correspondence and then interpolate into one. We make no assumptions on the structure of input images in terms of objects and we use no ground truth correspondences.</p><p>Our feature tensor alignment is based on optimal transport theory <ref type="bibr" target="#b58">[59]</ref> and Sinkhorn distance (SD) <ref type="bibr" target="#b10">[11]</ref> in particular. Let A := F (x), A ? := F (x ? ) be the c ? w ? h feature tensors of images x, x ? ? X . We reshape them to c ? r matrices A, A ? by flattening the spatial dimensions, where r := hw. Then, every column a j , a ? j ? R c of A, A ? for j = 1, . . . , r is a feature vector representing corresponding to a spatial position in the original image x, x ? . Let M be the r ? r cost matrix with its elements being the pairwise distances of these vectors:</p><formula xml:id="formula_4">m ij := a i ? a ? j 2<label>(6)</label></formula><p>for i, j ? {1, . . . , r}. We are looking for a transport plan, that is, a r ? r matrix P ? U r , where</p><formula xml:id="formula_5">U r := {P ? R r?r + : P 1 = P ? 1 = 1/r}<label>(7)</label></formula><p>and 1 is an all-ones vector in R r . That is, P is non-negative with row-wise and column-wise sum 1/r, representing a joint probability over spatial positions of A, A ? with uniform marginals. It is chosen to minimize the expected pairwise distance of their features, as expressed by the linear cost function ?P, M ?, under an entropic regularizer:</p><formula xml:id="formula_6">P * = arg min P ?Ur ?P, M ? ? ?H(P ),<label>(8)</label></formula><p>where H(P ) := ? ij p ij log p ij is the entropy of P , ??, ?? is Frobenius inner product and ? is a regularization coefficient. The optimal solution P * is unique and can be found by forming the r ? r similarity matrix e ?M/? and then applying the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b33">[34]</ref>, i.e., iteratively normalizing rows and columns. A small ? leads to sparser P , which improves one-to-one matching but makes the optimization harder <ref type="bibr" target="#b0">[1]</ref>, while a large ? leads to denser P , causing more correspondences and poor matching.</p><p>Interpolation The assignment matrix R := rP * is a doubly stochastic r ? r matrix whose element r ij expresses the probability that column a i of A corresponds to column a ? j of A ? . Thus, we align A and A ? as follows:</p><formula xml:id="formula_7">A := A ? R ?<label>(9)</label></formula><p>A ? := AR.</p><p>Here, column a i of c ? r matrix A is a convex combination of columns of A ? that corresponds to the same column a i of A. We reshape A back to c ? w ? h tensor A by expanding spatial dimensions and we say that A represents A aligned to A ? . We then interpolate between A and the original feature tensor A:</p><formula xml:id="formula_9">mix ? (A, A).<label>(11)</label></formula><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref> (toy example, top right), A is geometrically close to A. The correspondence with A ? and the geometric proximity to A makes A appropriate for interpolation with A. Symmetrically, we can also align A ? to A and interpolate between A ? and A ? :</p><formula xml:id="formula_10">mix ? (A ? , A ? ).<label>(12)</label></formula><p>When mixing feature tensors with alignment (4), we define Mix ? in (2) as the mapping of (A, A ? ) to either <ref type="bibr" target="#b10">(11)</ref> or <ref type="formula" target="#formula_0">(12)</ref>, chosen at random. A is aligned to A ? according to R, giving rise to A. We then interpolate between A, A. Symmetrically, we can align A ? to A and interpolate between A ? , A ? . A, A ? on the left (toy example of 16 points in 2D) shown semi-transparent on the right for reference. </p><formula xml:id="formula_11">x x ? 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 ? mix ? (A, A ? ) mix ? (A, A) mix ? (A ? , A ? ) (a) (b) mix ? (A, A ? ) mix ? (A, A) mix ? (A ? , A ? ) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Visualization and discussion</head><p>Decoder We use a decoder to study images generated with or without feature alignment. Let f : R c?w?h ? R d be a FC layer mapping tensor A to embedding e = f (A).</p><p>We use f ? F as an encoder and a decoder D : R d ? X mapping e back to the image space, reconstructing imag? x = D(e). The autoencoder is trained using only clean images (without mixup) using reconstruction loss L r between x andx, where L r (x, x ? ) := ?x ? x ? ? 2 is the squared Euclidean distance. We use generated images only for visualization purposes below, but we also use the decoder optionally during AlignMixup training in section 4.</p><p>Discussion For different ? ? [0, 1], we interpolate the feature tensors A, A ? of x, x ? without or with alignment, using <ref type="bibr" target="#b10">(11)</ref> or <ref type="formula" target="#formula_0">(12)</ref>, and we generate a new image by decoding the resulting embedding through the decoder D.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we visualize such generated images. Interestingly, by aligning A to A ? and mixing using (11) with ? = 0, the generated image retains the pose of x and the texture of x ? . In <ref type="figure" target="#fig_1">Figure 3</ref>(a) in particular, when x is 'penguin' and x ? is 'dog', the generated image retains the pose of the penguin, while the texture of the dog aligns to the body of the penguin. Similarly, in <ref type="figure" target="#fig_1">Figure 3</ref>(c), the texture from the goldfish is aligned to that of the stork, while the pose of the stork is retained. Vice versa, as shown in <ref type="figure" target="#fig_1">Figure 3</ref>(b,d), by aligning A ? to A and mixing using <ref type="bibr" target="#b11">(12)</ref> with ? = 0, the generated image retains the pose of x ? and the texture of x. By contrast, the image generated from unaligned features appears to be an overlay.</p><p>Randomly sampling several values of ? ? [0, 1] during training generates an abundance of samples, capturing texture from one image and the pose from another. This allows the model to explore beyond the image manifold, thereby improving its generalization and enhancing its performance across multiple benchmarks, as discussed in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>Architecture We use a residual network as our encoder F . The output A is a c ? 4 ? 4 tensor. This is followed by a fully-connected layer as classifier g.</p><p>Autoencoder In <ref type="figure" target="#fig_1">Figure 3</ref>, we have used a decoder to visualize the effect of feature tensor alignment. In our experiments, we also use a decoder optionally during training of AlignMixup, to investigate its effect on representation learning under mixup. This results in a vanilla autoencoder architecture, which we denote as AlignMixup/AE. We use a residual generator <ref type="bibr" target="#b20">[21]</ref> as the decoder D. The encoder and decoder have the same architecture.</p><p>Training We train AlignMixup using only the classification loss L c (5) on mixed examples. For a given mini-batch during training, we mix either x or A (using either <ref type="bibr" target="#b10">(11)</ref> or <ref type="formula" target="#formula_0">(12)</ref> with alignment). We choose between the three cases uniformly at random. For AlignMixup/AE, we either use the reconstruction loss L r on clean examples, training the encoder and decoder, or the classification loss L c (5) on mixed examples, training the encoder and classifier. This gives rise to a fourth case and we choose uniformly at random. The algorithm is in the supplementary material.</p><p>Hyperparameters The hyperparameters used for different datasets are reported in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image classification and robustness</head><p>We use PreActResnet18 <ref type="bibr" target="#b25">[26]</ref> (R-18) and WRN16-8 <ref type="bibr" target="#b65">[66]</ref> as the backbone architecture on CIFAR-10 and CIFAR-100 datasets <ref type="bibr" target="#b34">[35]</ref>. Using the experimental settings of Man-  ifold mixup <ref type="bibr" target="#b57">[58]</ref> (in supplementary material), we reproduce the state-of-the-art (SOTA) mixup methods: baseline network (without mixup), Input mixup <ref type="bibr" target="#b68">[69]</ref>, Manifold mixup <ref type="bibr" target="#b57">[58]</ref>, CutMix <ref type="bibr" target="#b64">[65]</ref>, PuzzleMix <ref type="bibr" target="#b31">[32]</ref>, Co-Mixup <ref type="bibr" target="#b30">[31]</ref>, SaliencyMix <ref type="bibr" target="#b56">[57]</ref>, StyleMix <ref type="bibr" target="#b27">[28]</ref> and StyleCutMix <ref type="bibr" target="#b27">[28]</ref> using official code provided by the authors. We do not compare AlignMixup with AutoMix <ref type="bibr" target="#b71">[72]</ref> and Re-Mix <ref type="bibr" target="#b4">[5]</ref>, since its experimental settings are different from ours and there is no available code. In addition, we use R-18 as the backbone network on TinyImagenet <ref type="bibr" target="#b62">[63]</ref> (TI) and reproduce SaliencyMix <ref type="bibr" target="#b56">[57]</ref>, StyleMix <ref type="bibr" target="#b27">[28]</ref> and StyleCutMix <ref type="bibr" target="#b27">[28]</ref> following the experimental settings of <ref type="bibr" target="#b31">[32]</ref>, and Resnet-50 (R-50) on Ima-geNet <ref type="bibr" target="#b47">[48]</ref>, following the training protocol of <ref type="bibr" target="#b31">[32]</ref>. Using top-1 error (%) as evaluation metric, we show the effectiveness of AlignMixup on image classification and robustness to FGSM <ref type="bibr" target="#b18">[19]</ref> and PGD <ref type="bibr" target="#b40">[41]</ref> attacks.  It is important to note that 40% increase in number of parameters of AlignMixup/AE is due to the residual decoder, which is only used in one out of five cases on clean images without mixup. Computational complexity during inference is the same for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image classification As shown in</head><p>Challenges From <ref type="table" target="#tab_3">Table 1</ref>, we observe that AlignMixup achieves SoTA top-1 error on CIFAR-10 and CIFAR-100. These results are computed using 2000 epochs following the experimental settings of <ref type="bibr" target="#b57">[58]</ref>, which also achieves its best performance at 2000 epochs. While baseline mixup methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b68">69]</ref> perform best at 300 epochs, they do not benefit from long training time. Unlike these methods, which perform mixup in the image space, Manifold mixup <ref type="bibr" target="#b57">[58]</ref> and AlignMixup performs mixup in the feature space. We hypothesize that this takes longer training time until the network learns some meaningful representations. Robustness to FGSM and PGD attacks Following the evaluation protocol of <ref type="bibr" target="#b31">[32]</ref>, we use 8/255 l ? ?-ball for FGSM and 4/255 l ? ?-ball with step size 2/255 for PGD. We reproduce the results of competitors for FGSM and PGD on CIFAR-10 and CIFAR-100; results of baseline, Input, Manifold, Cutmix and Puzzlemix on TI for FGSM are as reported in <ref type="bibr" target="#b31">[32]</ref> and reproduced for SaliencyMix, StyleMix and StyleCutMix. As shown in <ref type="table" target="#tab_4">Table 3</ref>, AlignMixup is more robust comparing to SOTA methods. While AlignMixup is on par with PuzzleMix and Co-Mixup on CIFAR-10 image classification, it outperforms Co-Mixup and PuzzleMix by 5.36% and 2.28% in terms of robustness to FGSM attacks. There is also significant gain of robustness to FGSM on Tiny-ImageNet and to the stronger PGD on CIFAR-100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Overconfidence</head><p>Deep neural networks tend to be overconfident about incorrect predictions far away from the training data and mixup helps combat this problem. Two standard benchmarks to evaluate this improvement are their ability to detect out-of-distribution data and their calibration, i.e., the discrepancy between accuracy and confidence.</p><p>Out-of-distribution detection According to <ref type="bibr" target="#b26">[27]</ref>, indistribution (ID) refers to a test example drawn from the same distribution which the network is trained on, while a sample drawn from any other distribution is out-ofdistribution (OOD). At inference, given a mixture of ID and OOD examples, the network assigns probabilities to the known classes by softmax. An example is then classified as OOD if the maximum class probability is below a certain threshold, else ID. A well-calibrated network should be able to assign a higher probability to ID than OOD examples, making it easier to distinguish the two distributions.</p><p>We compare AlignMixup with SOTA methods trained using R-18 on CIFAR-100 as discussed in subsection 4.   <ref type="table">Table 5</ref>. Weakly-supervised object localization on CUB200-2011. Top-1 loc.: Top-1 localization accuracy (%), MaxBoxAcc-v2: Maximal box accuracy <ref type="bibr" target="#b6">[7]</ref>. Higher is better. Blue: second best. Gain: increase of accuracy.</p><p>32 ? 32 to match the resolution of ID images <ref type="bibr" target="#b64">[65]</ref>. Following <ref type="bibr" target="#b26">[27]</ref>, we measure detection accuracy (Det Acc) using a threshold of 0.5, area under ROC curve (AuROC) and area under precision-recall curve (AuPR). <ref type="table" target="#tab_7">Table 4</ref>, AlignMixup outperforms SOTA methods under all metrics by a large margin, indicating that it is better in reducing over-confident predictions. We further observe that Input mixup is inferior to Baseline, which is consistent with the findings of <ref type="bibr" target="#b64">[65]</ref>. More results are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head><p>Calibration According to <ref type="bibr" target="#b12">[13]</ref>, calibration measures the discrepancy between the accuracy and confidence level of a network's predictions. A poorly calibrated network may make incorrect predictions with high confidence. In the supplementary, we compare AlignMixup with SOTA methods using calibration plots and quantitative experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Weakly-supervised object localization (WSOL)</head><p>WSOL aims to localize an object of interest using only class labels without bounding boxes at training. WSOL works by extracting visually discriminative cues to guide the classifier to focus on salient regions in the image.</p><p>We train AlignMixup using the same procedure as for image classification. At inference, following <ref type="bibr" target="#b64">[65]</ref>, we compute a saliency map using CAM <ref type="bibr" target="#b70">[71]</ref>, binarize it using a threshold of 0.15 and take the bounding box of the mask. We use VGG-GAP <ref type="bibr" target="#b50">[51]</ref> and Resnet-50 <ref type="bibr" target="#b25">[26]</ref> as pretrained on Imagenet <ref type="bibr" target="#b47">[48]</ref> and we fine-tune them on CUB200-2011 <ref type="bibr" target="#b59">[60]</ref>. We follow the evaluation protocol by <ref type="bibr" target="#b6">[7]</ref> and use top-1 localization accuracy with IoU threshold of 0.5 and Maximal Box Accuracy (MaxBoxAcc-v2) to compare AlignMixup with baseline CAM (without mixup), Input mixup <ref type="bibr" target="#b68">[69]</ref>, CutOut <ref type="bibr" target="#b13">[14]</ref> and CutMix <ref type="bibr" target="#b64">[65]</ref>.</p><p>According to <ref type="table">Table 5</ref>, AlignMixup outperforms Input mixup, CutOut and CutMix by 11.4%, 7.3% and 0.6% respectively using VGG-GAP and by 6.9%, 3.8% and 1.4% respectively using Resnet-50 in terms of top-1 localization accuracy. Furthermore, AlignMixup outperforms CutMix by 1.2% and 0.6% using VGG-GAP and Resnet-50 respectively in terms of MaxBoxAcc-v2. It also outperforms dedicated WSOL methods ACoL <ref type="bibr" target="#b69">[70]</ref> and ADL <ref type="bibr" target="#b7">[8]</ref>, which focus on learning spatially dispersed representations. Qualitative localization results are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation study</head><p>All ablations are performed on CIFAR-100 using R-18 as encoder F with feature tensor A being 512?4?4. We study the effect of mixing at different layers (x, A), by aligning A or not before mixing, as well as using a decoder D in an different autoencoder architectures. We report top-1 accu-METHOD/ARCH LAYERS UNALIGNED ALIGNED Baseline 76.76 -Manifold <ref type="bibr" target="#b57">[58]</ref> 80.20 -StyleCutMix <ref type="bibr" target="#b27">[28]</ref> 80. <ref type="bibr" target="#b65">66</ref>   racy (%). All results are in <ref type="table" target="#tab_9">Table 6</ref>. The ablation showing the effect of the number of iterations in Sinkhorn-Knopp algorithm is summarized in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layers</head><p>We study the choice of layers to mix, regardless of feature alignment. According to (2), we may mix at any of two layers, represented by {x, A}. To investigate more diverse cases, we introduce an additional layer f to the encoder of AlignMixup and mix its output, which acts as the latent space of AlignMixup/AE. f could be a FC layer, which outputs a vector e ? R 512 , or a convolutional layer of kernel size 2 ? 2 and stride 2, producing a 128 ? 2 ? 2 tensor E. In both cases, mixup is also represented by <ref type="formula" target="#formula_1">(2)</ref>, where now f 1 := f ? F and f 2 := id. Mixing is now chosen from {x, A, e} or {x, A, E}. In AlignMixup (no decoder), among different choices of unaligned layer sets, mixing {x, e} results in the highest classification accuracy. Furthermore, AlignMixup/AE outperforms baseline and the best performing competitor StyleCutMix for all choices of layers, even when features are unaligned, showing a motivation to use the decoder.</p><p>Tensor alignment We investigate the effect of aligning feature tensors or not before mixing it, by using standard mixup (2) or (11), <ref type="bibr" target="#b11">(12)</ref>, respectively. It is important to note that when e is a vector, we do not align it. In AlignMixup, we observe that aligning tensors A and E before mixing improves classification accuracy significantly. Furthermore, we observe that using an additional FC layer for e brings only minor improvement (81.71 ? 81.92), meaning that the major improvement comes from alignment. Overall, Align-Mixup/AE works the best when x, A, e are mixed, with A being aligned. It outperforms StyleCutMix by 1.52%.</p><p>Alignment resolution Given the best settings of Align-Mixup/AE, we investigate the effect of aligning A at different spatial resolutions. The default is 4 ? 4, denoted as A 4?4 . We also experiment 2 ? 2 (A 2?2 ), obtained by average pooling, and 8 ? 8 (A 8?8 ), by removing downsampling from the last convolutional layer. The accuracy of 8 ? 8 is only slightly better than 4 ? 4 by 0.02%, while being computationally more expensive. Thus, we choose 4 ? 4 as the default. By contrast, aligning at 2 ? 2 is worse than not aligning at all. This may be due to soft correspondences causing loss of information by averaging.</p><p>Autoencoder architecture We compare AlignMixup with two autoencoder architectures: the vanilla autoencoder (AlignMixup/AE), and a variational autoencoder <ref type="bibr" target="#b32">[33]</ref> (AlignMixup/VAE). The latter has two vectors ?, ? ? R 512 instead of e, representing mean and standard deviation, respectively. We also investigate 128 ? 2 ? 2 tensors, denoted as M, ? where the two variables are mixed simultaneously. As for AlignMixup and AlignMixup/AE, we investigate different combinations of layers with or without alignment. All three architectures work best when x, A, e are mixed. Alignment improves consistently on all three architectures. Both AlignMixup and AlignMixup/VAE are inferior to AlignMixup/AE. However, their best settings still outperform Baseline and StyleCutMix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have shown that mixup of a combination of input and latent representations is a simple and very effective pairwise data augmentation method. The gain is most prominent on large datasets and in combating overconfidence in predictions, as indicated by out-of-distribution detection. Interpolation of feature tensors boosts performance significantly, but only if they are aligned.</p><p>Our work is a compromise between a "good" handcrafted interpolation in the image space and a fully learned one in the latent space. A challenge is to make progress in the latter direction without compromising speed and simplicity, which would affect wide applicability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This work was in part supported by the ANR-19-CE23-0028 MEERQAT project and was performed using the HPC resources from GENCI-IDRIS Grant 2021 AD011012528. This work was partially done while Yannis was at Inria. We also thank Konstantinos Tertikas for his amazing help with adapting AlignMixup to transformers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm</head><p>AlignMixup and AlignMixup/AE are summarized in algorithm 1. By default (AlignMixup), for each mini-batch, we uniformly draw at random one among three choices (line 2) over mixup on input (x) or feature tensors (A, using either <ref type="bibr" target="#b10">(11)</ref> or <ref type="bibr" target="#b11">(12)</ref> for mixing). For AlignMixup/AE, there is a fourth choice where we only use reconstruction loss on clean examples (line 7).</p><p>For mixup, we use only classification loss (5) (line 24). Following <ref type="bibr" target="#b57">[58]</ref>, we form, for each example (x, y) in the mini-batch, a paired example (x ? , y ? ) from the same minibatch regardless of class labels, by randomly permuting the indices (lines 1,10). Inputs x, x ? are mixed by (2),(3) (line 12). Feature tensors A and A ? are first aligned and then mixed by (2),(11) (A aligns to A ? ) or (2),(12) (A ? aligns to A) (lines 14,23). </p><formula xml:id="formula_12">15 A ? F (x) , A ? ? F (x ? ) ? feature tensors 16 A ? RESHAPE c?r (A) ? to matrix 17 A ? ? RESHAPE c?r (A ? ) 18 M ? DIST(A, A ? ) ? pairwise distances (6) 19 P * ? SINKHORN(exp(?M/?)) ? tran. plan (8) 20 R ? DETACH(rP * ) ? assignments 21 A ? A ? R ? ? alignment (9) 22 A ? RESHAPE c?w?h ( A) ? to tensor 23 out ? f (mix ? (A, A)) ? (2),(11) 24</formula><p>?i ? Lc(g(out), mix ? (y, y ? )) ? classification loss <ref type="bibr" target="#b4">(5)</ref> In computing loss derivatives, we backpropagate through feature tensors A, A ? but not through the transport plan P * (line 20). Hence, although the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b33">[34]</ref> is differentiable, its iterations take place only in the forward pass. Importantly, AlignMixup is easy to implement and does not require sophisticated optimization like <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>B. Hyperparameter settings CIFAR-10/CIFAR-100 We train AlignMixup using SGD for 2000 epochs with an initial learning rate of 0.1, decayed by a factor 0.1 every 500 epochs. We set the momentum as 0.9 with a weight decay of 0.0001 and use a batch size of 128. The interpolation factor is drawn from Beta(?, ?) where ? = 2.0. Using these settings, we reproduce the results of SOTA mixup methods for image classification, robustness to FGSM and PGD attacks, calibration and out-of-distribution detection. For alignment, we apply the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b33">[34]</ref> for 100 iterations with entropic regularization coefficient ? = 0.1.</p><p>TinyImagenet We follow the training protocol of Kim et al. <ref type="bibr" target="#b31">[32]</ref>, training R-18 as stage-1 encoder F using SGD for 1200 epochs. We set the initial learning rate to 0.1 and decay it by 0.1 at 600 and 900 epochs. We set the momentum as 0.9 with a weight decay of 0.0001 and use a batch size of 128 on 2 GPUs. The interpolation factor is drawn from Beta(?, ?) where ? = 2.0. For alignment, we apply the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b33">[34]</ref> for 100 iterations with entropic regularization coefficient ? = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet</head><p>We follow the training protocol of Kim et al. <ref type="bibr" target="#b31">[32]</ref>, where training R-50 as F using SGD for 300 epochs. The initial learning rate of the classifier and the remaining layers is set to 0.1 and 0.01, respectively. We decay the learning rate by 0.1 at 100 and 200 epochs. We set the momentum as 0.9 with a weight decay of 0.0001 and use a batch size of 100 on 4 GPUs. The interpolation factor is drawn from Beta(?, ?) where ? = 2.0. For alignment, we apply the Sinkhorn-Knopp algorithm <ref type="bibr" target="#b33">[34]</ref> for 100 iterations with entropic regularization coefficient ? = 0.1.</p><p>We also train R-50 on ImageNet for 100 epochs, following the training protocol described in Kim et al. <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CUB200-2011</head><p>For weakly-supervised object localization (WSOL), we use VGG-GAP and R-50 pretrained on Ima-geNet as F . The training strategy for WSOL is the same as image classification and the network is trained without bounding box information. In R-50, following <ref type="bibr" target="#b64">[65]</ref>, we modify the last residual block (layer 4) to have stride 2 instead of 1, resulting in a feature map of spatial resolution 14 ? 14. The modified architecture of VGG-GAP is the same as described in <ref type="bibr" target="#b70">[71]</ref>. The classifier is modified to have 200 classes instead of 1000.</p><p>For fair comparisons with <ref type="bibr" target="#b64">[65]</ref>, during training, we resize the input image to 256 ? 256 and randomly crop the resized image to 224 ? 224. During testing, we directly re-NETWORK RESNET-50 <ref type="bibr">Baseline</ref> 24.03 Input <ref type="bibr" target="#b68">[69]</ref> 22.97 Manifold <ref type="bibr" target="#b57">[58]</ref> 23.30 CutMix <ref type="bibr" target="#b64">[65]</ref> 22.92 PuzzleMix <ref type="bibr" target="#b31">[32]</ref> 22.49 Co-Mixup <ref type="bibr" target="#b30">[31]</ref> 22.39 StyleMix <ref type="bibr" target="#b27">[28]</ref> 24.06 StyleCutMix <ref type="bibr" target="#b27">[28]</ref> 22.71</p><p>AlignMixup (ours) 22.0 Gain +0.39 size to 224?224. We train the network for 600 epochs using SGD. For R-50, the initial learning rate of the classifier and the remaining layers is set to 0.01 and 0.001, respectively. For VGG, the initial learning rate of the classifier and the remaining layers is set to 0.001 and 0.0001, respectively. We decay the learning rate by 0.1 every 150 epochs. The momentum is set to 0.9 with weight decay of 0.0001 and batch size of 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional experiments</head><p>ImageNet classification Following the training protocol of <ref type="bibr" target="#b30">[31]</ref>, <ref type="table" target="#tab_10">Table 7</ref>   Uniform and Gaussian noise as OOD samples. Uniform is drawn from U(0, 1) and Gaussian from N (?, ?) with ? = ? = 0.5. All SOTA mixup methods are reproduced using the same experimental settings. Following <ref type="bibr" target="#b26">[27]</ref>, we measure detection accuracy (Det Acc) using a threshold of 0.5, area under ROC curve (AuROC) and area under precision-recall curve (AuPR). As shown in <ref type="table" target="#tab_12">Table 8</ref>, AlignMixup outperforms SOTA methods under all metrics by a large margin, indicating that it is better in reducing over-confident predictions.</p><p>Calibration We compare AlignMixup with SOTA methods , training R-18 on CIFAR-100 as discussed in subsection 4.2. All SOTA mixup methods are reproduced using the same experimental settings. We compare qualitatively by plotting accuracy vs. confidence. As shown in <ref type="figure">Figure 4</ref>, while Baseline is clearly overconfident and Input and Manifold mixup are clearly under-confident, AlignMixup results in the best calibration among all competitors. We also compare quantitatively, measuring the expected calibration error (ECE) <ref type="bibr" target="#b21">[22]</ref> and overconfidence error (OE) <ref type="bibr" target="#b54">[55]</ref>. As shown in <ref type="table" target="#tab_14">Table 9</ref>, AlignMixup outperforms SOTA methods by achieveing lower ECE and OE, indicating that it is better calibrated.  <ref type="figure">Figure 4</ref>. Calibration plots on CIFAR-100 using PreActResnet18: near diagonal is better. We plot accuracy vs. confidence, that is, probability for the predicted class.   Qualitative results of WSOL Qualitative localization results shown in <ref type="figure" target="#fig_4">Figure 5</ref> indicate that AlignMixup encodes semantically discriminative representations, resulting in better localization performance.</p><p>Object detection Following the settings of CutMix <ref type="bibr" target="#b64">[65]</ref>, we use Resnet-50 pretrained on ImageNet using Align-Mixup as the backbone of SSD <ref type="bibr" target="#b37">[38]</ref> and Faster R-CNN <ref type="bibr" target="#b44">[45]</ref> detectors and fine-tune it on Pascal VOC07 <ref type="bibr" target="#b16">[17]</ref> and MS-COCO <ref type="bibr" target="#b36">[37]</ref>   <ref type="table" target="#tab_3">Table 10</ref>. Ablation of the number of iterations in Sinkhorn-Knopp algorithm using R-18 on CIFAR-100. Top-1 classification accuracy(%): higher is better.</p><p>Iterations in Sinkhorn-Knopp The default number of iterations for the Sinkhorn-Knopp algorithm in solving <ref type="formula" target="#formula_6">(8)</ref> is i = 100. Here, we investigate more choices, as shown in <ref type="table" target="#tab_3">Table 10</ref>. The case of i = 0 is similar to crossattention. In this case, we only normalize either the rows or columns in <ref type="bibr" target="#b6">(7)</ref> once, such that P 1 = 1/r (when A aligned to A ? ) or P ? 1 = 1/r (when A ? aligned to A). We observe that while AlignMixup outperforms the best baseline-StyleCutMix (80.66)-in all cases, it performs best for i = 100 iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Feature tensor alignment and interpolation. Cost matrix M contains pairwise distances of feature vectors in tensors A, A ? . Assignment matrix R is obtained by Sinkhorn-Knopp [34] on similarity matrix e ?M/? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Visualizing alignment. For different ? ? [0, 1], we interpolate feature tensors A, A ? without alignment (top) or aligned feature tensors (bottom) of two images x, x ? and then we generate a new image by decoding the resulting embedding through the decoder D. (a), (c) We align A to A ? and mix with<ref type="bibr" target="#b10">(11)</ref>. (b), (d) We align A ? to A and mix with<ref type="bibr" target="#b11">(12)</ref>. Only meant for illustration: No decoded images are seen by the classifier at training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 : 7 ?i 9 ?</head><label>179</label><figDesc>AlignMixup/AE (parts involved in the AE variant indicated in blue)Input: encoders F ; embedding e, decoder D; classifier gInput: mini-batch B := {(xi, yi)} b i=1 Output: loss values L := {?i} b i=1 1 ? ? unif(S b ) ? random permutation of {1, . . . , b} 2 mode ? unif{clean, input, feat, feat ? } ? mixup? 3 for i ? {1, . . . , b} do 4 (x, y) ? (xi, yi)? current example<ref type="bibr" target="#b4">5</ref> if mode = clean then ? no mixup 6x ? D(e(F (x))) ? encode/decode ? Beta(?, ?) ? interpolation factor 10 (x ? , y ? ) ? (x ?(i) , y ?(i) ) ? paired example<ref type="bibr" target="#b10">11</ref> if mode = input then ? as in[69] 12 out ? F (mix ? (x, x ? )) ? (2),(3) else ? mode ? {feat, feat ? } 13 if mode = feat ? then ? choose (12) over (11) 14 SWAP (x, x ? ), SWAP (y, y ? )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Localization examples using ResNet-50 on CUB200-2011. Red boxes: predicted; green: ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.11 23.24 20.63 43.40 * Input [69] 4.03 3.98 20.21 19.88 43.48 * CutMix [65] 3.27 3.54 19.37 19.71 43.11 * Manifold [58] 2.95 3.56 19.80 19.23 40.76</figDesc><table><row><cell>DATASET</cell><cell cols="2">CIFAR-10</cell><cell>CIFAR-100</cell><cell>TI</cell></row><row><cell>NETWORK</cell><cell cols="4">R-18 W16-8 R-18 W16-8 R-18</cell></row><row><cell cols="5">Baseline 5PuzzleMix [32] 5.19 2.93 2.99 20.01 19.25 36.52  *</cell></row><row><cell>Co-Mixup [31]</cell><cell>2.89</cell><cell cols="3">3.04 19.81 19.57 35.85  *</cell></row><row><cell>SaliencyMix [57]</cell><cell>2.99</cell><cell cols="3">3.53 19.69 19.59 34.81</cell></row><row><cell>StyleMix [28]</cell><cell>3.76</cell><cell cols="3">3.89 20.04 20.45 36.13</cell></row><row><cell>StyleCutMix [28]</cell><cell>3.06</cell><cell cols="3">3.12 19.34 19.28 34.49</cell></row><row><cell>AlignMixup (ours)</cell><cell>2.95</cell><cell cols="3">3.09 18.29 18.77 33.13</cell></row><row><cell cols="2">AlignMixup/AE (ours) 2.83</cell><cell cols="3">3.15 17.82 18.09 32.73</cell></row><row><cell>Gain</cell><cell cols="4">+0.06 -0.10 +1.52 +1.14 +1.76</cell></row><row><cell cols="5">Table 1. Image classification top-1 error (%) on CIFAR-10/100</cell></row><row><cell cols="5">and TI (TinyImagenet). Top-1 error (%): lower is better. Blue:</cell></row><row><cell cols="3">second best. R: PreActResnet, W: WRN.</cell><cell></cell></row></table><note>** : reported by [31].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Image classification top-1 error (%) and computational analysis on ImageNet using Resnet-50 for 300 epochs. Lower is better. Blue: second best. * : reported by authors; ? : reported by PuzzleMix.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 ,</head><label>1</label><figDesc>AlignMixup and AlignMixup/AE is on par or outperforms the SOTA methods by achieving the lowest top-1 error, especially on large datasets. On CIFAR-10, AlignMixup and Align-Mixup/AE is on par with Co-Mixup and Puzzlemix with R-18 and WRN16-8. On CIFAR-100, AlignMixup outperforms StyleCutMix and Manifold mixup by 1.05% and 0.46% with R-18 and WRN16-8, respectively. On TI, AlignMixup outperforms Co-Mixup by 2.72% using R-18. From Table 2, AlignMixup/AE outperforms PuzzleMix by 2.41% on ImageNet. While the overall improvement by SOTA methods on ImageNet over Baseline is around 2%, AlignMixup/AE improves SOTA by another 2.5%.Computational complexityTable 2shows the computational analysis of AlignMixup training as compared with baseline and SOTA mixup methods on ImageNet, in terms of number of parameters and msec/batch on a NVIDIA RTX</figDesc><table><row><cell>ATTACK</cell><cell></cell><cell></cell><cell>FGSM</cell><cell></cell><cell></cell><cell></cell><cell cols="2">PGD</cell><cell></cell></row><row><cell>DATASET</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell><cell>TI</cell><cell cols="2">CIFAR-10</cell><cell cols="2">CIFAR-100</cell></row><row><cell>NETWORK</cell><cell>R-18</cell><cell>W16-8</cell><cell>R-18</cell><cell>W16-8</cell><cell>R-18</cell><cell>R-18</cell><cell>W16-8</cell><cell>R-18</cell><cell>W16-8</cell></row><row><cell>Baseline</cell><cell>89.41</cell><cell>88.02</cell><cell>87.12</cell><cell>72.81</cell><cell>91.85</cell><cell>99.99</cell><cell>99.94</cell><cell>99.97</cell><cell>99.99</cell></row><row><cell>Input [69]</cell><cell>78.42</cell><cell>79.21</cell><cell>81.30</cell><cell>67.33</cell><cell>88.68</cell><cell>99.77</cell><cell>99.43</cell><cell>99.96</cell><cell>99.37</cell></row><row><cell>CutMix [65]</cell><cell>77.72</cell><cell>78.33</cell><cell>86.96</cell><cell>60.16</cell><cell>88.68</cell><cell>99.82</cell><cell>98.10</cell><cell>98.67</cell><cell>97.98</cell></row><row><cell>Manifold [58]</cell><cell>77.63</cell><cell>76.11</cell><cell>80.29</cell><cell>56.45</cell><cell>89.25</cell><cell>97.22</cell><cell>98.49</cell><cell>99.66</cell><cell>98.43</cell></row><row><cell>PuzzleMix [32]</cell><cell>57.11</cell><cell>60.73</cell><cell>78.70</cell><cell>57.77</cell><cell>83.91</cell><cell>97.73</cell><cell>97.00</cell><cell>96.42</cell><cell>95.28</cell></row><row><cell>Co-Mixup [31]</cell><cell>60.19</cell><cell>58.93</cell><cell>77.61</cell><cell>56.59</cell><cell>-</cell><cell>97.59</cell><cell>96.19</cell><cell>95.35</cell><cell>94.23</cell></row><row><cell>SaliencyMix [57]</cell><cell>57.43</cell><cell>68.10</cell><cell>77.79</cell><cell>58.10</cell><cell>81.16</cell><cell>97.51</cell><cell>97.04</cell><cell>95.68</cell><cell>93.76</cell></row><row><cell>StyleMix [28]</cell><cell>79.54</cell><cell>71.05</cell><cell>80.54</cell><cell>67.94</cell><cell>84.93</cell><cell>98.23</cell><cell>97.46</cell><cell>98.39</cell><cell>98.24</cell></row><row><cell>StyleCutMix [28]</cell><cell>58.79</cell><cell>56.12</cell><cell>77.49</cell><cell>56.83</cell><cell>80.59</cell><cell>97.87</cell><cell>96.70</cell><cell>91.88</cell><cell>93.78</cell></row><row><cell>AlignMixup (ours)</cell><cell>54.83</cell><cell>56.20</cell><cell>74.18</cell><cell>55.05</cell><cell>78.83</cell><cell>95.42</cell><cell>96.71</cell><cell>90.40</cell><cell>92.16</cell></row><row><cell>AlignMixup/AE (ours)</cell><cell>52.13</cell><cell>54.86</cell><cell>76.40</cell><cell>55.44</cell><cell>78.98</cell><cell>97.16</cell><cell>95.32</cell><cell>91.69</cell><cell>92.23</cell></row><row><cell>Gain</cell><cell>+4.98</cell><cell>+1.26</cell><cell>+3.31</cell><cell>+1.40</cell><cell>+1.76</cell><cell>+1.80</cell><cell>+0.87</cell><cell>+1.48</cell><cell>+1.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>Robustness to FGSM &amp; PGD attacks. Top-1 error (%): lower is better. Blue: second best. Gain: reduction of error. TI: TinyImagenet. R: PreActResnet, W: WRN.2080 TI GPU. AlignMixup has nearly the same computa- tional overhead as Manifold mixup while achieving 1.82% increase of accuracy. While SOTA methods like Co-Mixup and PuzzleMix are computationally more expensive than AlignMixup by 1.8? and 2.3? respectively, they are out- performed by AlignMixup by 0.6% on average. Align- Mixup/AE brings a further 1.85% gain in accuracy over AlignMixup.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>It is even more challenging in our case, since we mix features at deeper layers comparing with Manifold mixup. Empirically, when trained for 2000 epochs instead of 300 epochs, the top-1 error drops from 21.64 ? 19.80 for Manifold mixup and from 21.38 ? 18.29 for AlignMixup.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>METRIC</cell><cell cols="2">TOP-1 LOC.</cell><cell cols="2">MAXBOXACC-V2</cell></row><row><cell>NETWORK</cell><cell cols="4">VGG-GAP RESNET-50 VGG-GAP RESNET-50</cell></row><row><cell>ACoL [70]</cell><cell>45.9</cell><cell>-</cell><cell>57.4</cell><cell>-</cell></row><row><cell>ADL [8]</cell><cell>52.4</cell><cell>-</cell><cell>61.3</cell><cell>58.4</cell></row><row><cell>Baseline CAM [71]</cell><cell>37.1</cell><cell>49.4</cell><cell>59.0</cell><cell>59.7</cell></row><row><cell>Input [69]</cell><cell>41.7</cell><cell>49.3</cell><cell>57.1</cell><cell>60.6</cell></row><row><cell>CutMix [65]</cell><cell>52.5</cell><cell>54.8</cell><cell>62.6</cell><cell>64.8</cell></row><row><cell>AlignMixup (ours)</cell><cell>53.1</cell><cell>56.2</cell><cell>63.8</cell><cell>65.4</cell></row><row><cell>Gain</cell><cell>+0.6</cell><cell>+1.4</cell><cell>+1.2</cell><cell>+0.6</cell></row></table><note>Out-of-distribution detection using PreActResnet18. Det Acc (detection accuracy), AuROC, AuPR (ID) and AuPR (OOD): higher is better; Blue: second best. Gain: increase in performance. TI: TinyImagenet. Additional results are in the supplementary material.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table /><note>Ablations using R-18 on CIFAR-100. Top-1 classification accuracy (%): higher is better. Arch: autoencoder architecture.AE: vanilla; VAE: variational [33].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>Image classification on ImageNet for 100 epochs using ResNet-50. Top-1 error (%): lower is better. Blue: second best.</figDesc><table /><note>Gain: reduction of error.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Experiments using transformersWe apply mixup to LeViT-128S<ref type="bibr" target="#b19">[20]</ref> on ImageNet for 100 epochs. For Align-Mixup, we align the feature tensors in the last layer of the convolution stem. The top-1 accuracy is: baseline 67.4%, input mixup 68.3%, manifold mixup 67.8%, Cut-Mix 68.7%, AlignMixup 69.9%. Thus, we outperform input mixup and CutMix by 1.6% and 1.2% respectively, which in turn outperform the baseline by 0.9% and 1.3% respectively. This means that the improvement brought by mixing is roughly doubled.</figDesc><table><row><cell>DATASET</cell><cell>LSUN (RESIZE)</cell><cell>TI (RESIZE)</cell></row><row><cell>METRIC</cell><cell>DET AU AUPR AUPR ACC ROC (ID) (OOD)</cell><cell>DET AU AUPR AUPR ACC ROC (ID) (OOD)</cell></row><row><cell>reports classification performance when</cell><cell></cell><cell></cell></row><row><cell>training for 100 epochs on ImageNet. Using the top-1</cell><cell></cell><cell></cell></row><row><cell>error (%) reported for competitors by [31], AlignMixup</cell><cell></cell><cell></cell></row><row><cell>outperforms all methods, including Co-Mixup [31]. Im-</cell><cell></cell><cell></cell></row><row><cell>portantly, while the overall improvement by SOTA meth-</cell><cell></cell><cell></cell></row><row><cell>ods over Baseline is around 1.64%, AlignMixup improves</cell><cell></cell><cell></cell></row><row><cell>SOTA by another 0.4%.</cell><cell></cell><cell></cell></row><row><cell>Out-of-distribution detection We compare AlignMixup</cell><cell></cell><cell></cell></row><row><cell>with SOTA methods, training R-18 on CIFAR-100 as dis-</cell><cell></cell><cell></cell></row><row><cell>cussed in subsection 4.2. At inference, ID examples are test</cell><cell></cell><cell></cell></row><row><cell>images from CIFAR-100, while OOD examples are test im-</cell><cell></cell><cell></cell></row><row><cell>ages from LSUN [64] and Tiny-ImageNet, resizing OOD</cell><cell></cell><cell></cell></row><row><cell>examples to 32 ? 32 to match the resolution of ID im-</cell><cell></cell><cell></cell></row><row><cell>ages [65]. We also use test images from CIFAR-100 with</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>Out-of-distribution detection on different datasets (top) and under different noise (bottom) using PreActResnet18. Det Acc (detection accuracy), AuROC, AuPR (ID) and AuPR (OOD): higher is better. Blue: second best. Gain: increase in performance.</figDesc><table /><note>TI: TinyImagenet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 .</head><label>9</label><figDesc>Calibration using PreActResnet18 on CIFAR-100. ECE: expected calibration error; OE: overconfidence error. Lower is better. Blue: second best. Gain: reduction of error.</figDesc><table><row><cell>Input</cell><cell>mixup [69]</cell></row><row><cell></cell><cell>IoU = 0.27</cell><cell>IoU = 0.41</cell></row><row><cell cols="2">CutMix [65]</cell></row><row><cell></cell><cell>IoU = 0.59</cell><cell>IoU = 0.52</cell></row><row><cell>AlignMixup</cell><cell>(Ours)</cell></row><row><cell></cell><cell>IoU = 0.76</cell><cell>IoU = 0.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>respectively. AlignMixup outperforms CutMix mAP by 0.8% (77.6 ? 78.4) on Pascal VOC07 and 0.7% (35.16 ? 35.84) on MS-COCO. 80.96 81.31 81.42 81.71 81.50 81.34 81.28</figDesc><table><row><cell cols="4">D. Additional ablations</cell></row><row><cell cols="2">ITERATIONS (i) 0</cell><cell>10</cell><cell>20</cell><cell>50 100 200 500 1000</cell></row><row><cell>AlignMixup</cell><cell>80.98</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gromovwasserstein alignment of word embedding spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On adversarial mixup resynthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farnoosh</forename><surname>Ghadiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Better mixing via deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gr?goire</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><surname>Rifai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and improving interpolation in autoencoders via an adversarial regularizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Remix: Towards image-to-image translation with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luanxuan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenan</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Pointmixup: Augmentation for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunlu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><forename type="middle">Tao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengwan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evaluating weakly supervised object localization methods right</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungho</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AutoAugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supermix: Supervising the mixing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Dabouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sobhan</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fariborz</forename><surname>Taherkhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The comparison and evaluation of forecasters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Degroot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fienberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Crosstransformers: spatially-aware few-shot transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning by augmenting single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning generative models with sinkhorn divergences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Levit: A vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scnet: Learning semantic correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Rafael S Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kwan-Yee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonia</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Painter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesan</forename><surname>Niranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam Pr?gel-Bennett Jonathon</forename><surname>Hare</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12047</idno>
		<title level="m">Fmix: Enhancing mixed sample data augmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stylemix: Separating content and style for enhanced data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minui</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Co-mixup: Saliency guided joint mixup with supermodular diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hosan</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Puzzle mix: Exploiting saliency and local statistics for optimal mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jang-Hyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonho</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Sinkhorn-Knopp algorithm: convergence and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Site Li, Ping Jia, and Jane You. Data augmentation via latent space interpolation for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingsheng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Forre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Carioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Florent Perronnin, and Cordelia Schmid. Transformation pursuit for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattis</forename><surname>Paulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Resizemix: Mixing data with preserved object information and true labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiemin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.11101</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">End-toend weakly-supervised semantic alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Transformation invariance in pattern recognition-tangent distance and tangent propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Patrice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victorri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: tricks of the trade</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Yannis Avrithis, and Ondrej Chum. Local features and visual words emerge in activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriane</forename><surname>Sim?oni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Improved mixedexample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael J Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Ricap: Random image cropping and patching data augmentation for deep cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryo</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takashi</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">On mixup training: Improved calibration and predictive uncertainty for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gopinath</forename><surname>Sunil Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmoy</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michalak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning from between-class examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">SaliencyMix: A saliency guided data augmentation strategy for better regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mst</forename><surname>A F M Uddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wheemyung</forename><surname>Monira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taechoong</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung-Ho</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bae</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krista</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Tiny imagenet classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>Standford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Automix: Mixup networks for sample interpolation via cooperative barycenter learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">AlignMixup (ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">AlignMixup (ours)</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
