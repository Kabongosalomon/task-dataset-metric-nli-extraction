<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
							<email>nibingbing@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
							<email>tian.qi1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Huawei Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CIPS-3D: A 3D-Aware Generator of GANs Based on Conditionally-Independent Pixel Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>a) GIRAFFE [42] (b) pi-GAN [11] Content Style Stylization MetFaces Stylization and multiple views (c) All these images are synthesized by CIPS-3D at the resolution of 512 ? 512.</p><p>Figure 1. Three types of 3D-aware GANs. (a): There are apparent artifacts in the images generated by GIRAFFE [42]. (b): The images generated by pi-GAN [11] are blurred and lack details. (c): CIPS-3D can generate photo-realistic high-fidelity images. We fine-tune the base model trained on FFHQ so that the transferred model can generate other types of style images. Then we interpolate the base model and the transferred model to create a new model that can generate stylized images. CIPS-3D enables one to manipulate the pose of the stylized faces (the rightmost images) explicitly. For details, please refer to Secs. 4.4 and 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The style-based GAN (StyleGAN) architecture achieved state-of-the-art results for generating high-quality images, but it lacks explicit and precise control over camera poses. The recently proposed NeRF-based GANs made great progress towards 3D-aware generators, but they are unable to generate high-quality images yet. This paper presents CIPS-3D, a style-based, 3D-aware generator that is composed of a shallow NeRF network and a deep implicit neural representation (INR) network. The generator synthesizes each pixel value independently without any spatial convolution or upsampling operation. In addition, we diagnose the problem of mirror symmetry that implies a suboptimal solution and solve it by introducing an auxiliary discriminator. Trained on raw, single-view images, CIPS-3D sets new records for 3D-aware image synthesis with an impressive FID of 6.97 for images at the 256 ? 256 resolution on FFHQ. We also demonstrate several interesting directions for CIPS-3D such as transfer learning and 3D-aware face stylization. The synthesis results are best viewed as videos, so we recommend the readers to check our github project at https://github.com/ PeterouZh/CIPS-3D.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b19">[20]</ref> can synthesize high-fidelity images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref> but lack an explicit control mechanism to adjust the viewpoint for the generated object. Previous methods alleviate this problem by finding the latent semantic vectors existing in pre-trained 2D GAN models <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>. However, these methods can only roughly change the pose of the object implicitly and fail to render the object from arbitrary camera poses. Several 3D-aware methods have been proposed to enable explicit control over the camera pose <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b62">63]</ref>. However, these models are often limited to low-resolution images with disappointing artifacts.</p><p>Recently, there has been a growing interest in leveraging the neural radiance fields (NeRF) <ref type="bibr" target="#b38">[39]</ref> to build 3D-aware GANs. To our knowledge, there exist two types of 3Daware generators: (i) using a pure NeRF network as the generator <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref>; (ii) generating low-resolution feature maps with a NeRF network and then upsampling with a 2D CNN decoder <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>. The former suffers from a low capacity of the generator because the NeRF network is memoryintensive, which limits the depth of the generator. Thus the synthesized images are blurred and lack sharp details (see <ref type="figure">Fig. 1b</ref>). The latter is susceptible to aliasing due to non-ideal upsampling filters (see <ref type="figure">Fig. 1a</ref>) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>. This paper presents CIPS-3D, an approach to synthesize each pixel value independently, just as its 2D version did <ref type="bibr" target="#b3">[4]</ref>. The generator consists of a shallow 3D NeRF network (to alleviate memory complexity) and a deep 2D implicit neural representation (INR) network (to enlarge the capacity of the generator) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45]</ref>, without any spatial convolution or up-sampling operations. Interestingly, the design of our generator is consistent with the well-known semantic hierarchical principle of GANs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b60">61]</ref>, where the early layers (i.e., the shallow NeRF network in our generator) determine the pose, and the middle and higher layers (i.e., the INR network in our generator) control semantic attributes and color scheme, respectively. The early NeRF network enables us to control the camera pose explicitly.</p><p>We found that CIPS-3D suffers from a mirror symmetry problem, which also exists in other 3D-aware GANs such as GIRAFFE <ref type="bibr" target="#b41">[42]</ref> and StyleNeRF <ref type="bibr" target="#b4">[5]</ref>. Rather than simply attributing this issue to the dataset bias, we explain why this problem exists. Going one step further, we propose to utilize an auxiliary discriminator to regularize the output of the NeRF network, thus successfully solving this problem (see Sec. 3.3). To train CIPS-3D at high resolution, we propose a training strategy named partial gradient backpropagation. Moreover, we provide a more efficient implementation for the Modulated Fully Connected layer (ModFC) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref> to speed up the training (see Sec. 3.2).</p><p>We validate the advantages of our approach on highresolution face datasets including FFHQ <ref type="bibr" target="#b31">[32]</ref>, Met-Faces <ref type="bibr" target="#b29">[30]</ref>, BitmojiFaces <ref type="bibr" target="#b0">[1]</ref>, CartoonFaces <ref type="bibr" target="#b1">[2]</ref>, and an animal dataset, AFHQ <ref type="bibr" target="#b14">[15]</ref>. For 3D-aware image synthesis, CIPS-3D achieves state-of-the-art FID scores of 6.97 and 12.26 on FFHQ at 256 2 and 1024 2 resolution, respectively, surpassing the StyleNeRF <ref type="bibr" target="#b4">[5]</ref> proposed very recently. Moreover, we verify that CIPS-3D works pretty well in transfer learning settings and show its application for 3Daware face stylization (see Secs. 4.4 and 4.5). We will release the code to the public. We hope that CIPS-3D will serve as a good base model for downstream tasks such as 3D-aware GAN inversion and 3D-aware image editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Implicit neural representation is a powerful tool for representing scenes in a continuous and memory-cheap way compared to mesh/voxel-based ones. It is usually implemented by a multilayer perception (MLP). The implicit representation has been widely applied in 3D tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> as well as some 2D tasks such as image generation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">56]</ref> and super-resolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b58">59]</ref>. Equipped with volume rendering <ref type="bibr" target="#b27">[28]</ref>, NeRF-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62]</ref> enable novel view synthesis by learning an implicit function for a specific scene. Recently, there has been a trend combining NeRF with GANs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b48">49]</ref> to design 3D-aware generators <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Like GIRAFFE <ref type="bibr" target="#b41">[42]</ref> and StyleNeRF <ref type="bibr" target="#b4">[5]</ref>, CIPS-3D utilizes NeRF to render features instead of RGB colors. However, our method differs from GIRAFFE and StyleNeRF in several ways. Both GIRAFFE and StyleNeRF adopt a two-stage strategy, where they render low-resolution feature maps first and then upsample the feature maps using a CNN decoder. CIPS-3D synthesizes each pixel independently without any up-sampling and spatial convolution operations. Moreover, CIPS-3D represents 3D shape and appearance with NeRF and INR networks, respectively, which is convenient for transfer learning settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">NeRF Network for 3D Shape</head><p>Shallow NeRF Network A neural radiance field <ref type="bibr" target="#b38">[39]</ref> is a continuous function f whose inputs are 3D coordinates x = (x, y, z) and viewing direction d, and whose outputs are emitted colors c = (r, g, b) and volume density ?. A multilayer perceptron (MLP) is usually used to parameterize the continuous function f :</p><formula xml:id="formula_0">f : R dim(x) ? R dim(d) ? R + ? R 3 , (x, d) ? (?, c). (1)</formula><p>The 3D coordinates are sampled along camera rays. Each ray corresponds to a pixel of the rendered image. Thus to render a high-resolution image of size H ? W , the number of rays is large (i.e., H ? W ). Besides, to obtain accurate 3D shape, many points need to be sampled on each ray. As a result, the NeRF network is memory-intensive.</p><p>To alleviate memory complexity, we adopt a shallow NeRF network to represent 3D shape while assigning the task of synthesizing high-fidelity appearance to a deep 2D INR network. In particular, a shallow NeRF network (see <ref type="figure">Fig. 2b</ref>) containing only three SIREN blocks <ref type="bibr" target="#b53">[54]</ref> is used as the initial layers of the GAN's generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modulated SIREN Block</head><p>The vanilla NeRF is restricted to a specific scene with fixed geometry. Because the generated images are various for GANs, the shape of each image is different. To render different shapes with one NeRF, we condition the NeRF network on a noise vector z s so that different shapes can be obtained by sampling z s . In particular, like StyleGAN <ref type="bibr" target="#b31">[32]</ref>, we use a mapping network m s : Z s ? W s to map z s to w s , and use w s to modulate the feature maps of the NeRF network (see <ref type="figure">Fig. 2a</ref>).</p><p>We adopt the strategy of pi-GAN <ref type="bibr" target="#b10">[11]</ref>: modulating features with FiLM <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b46">47]</ref>, followed by a SIREN activation function <ref type="bibr" target="#b53">[54]</ref>. The modulated SIREN block is given by  <ref type="figure">Figure 2</ref>. The style-based 3D-aware generator with detailed hyperparameters. The NeRF network is shallow to save runtime memory. The INR network is deep to increase the capacity of the generator. We disentangle 3D shape and appearance, where the NeRF network is responsible for the 3D shape and the INR network for appearance. The auxiliary discriminator helps to overcome the problem of mirror symmetry (see Sec. 3.3). For the INR network, each ModFC is followed by a LeakyReLU (not shown here).</p><formula xml:id="formula_1">? (x) = sin (? ? (W x + b) + ?) ,<label>(2)</label></formula><p>where ? = Affine(w s ) and ? = Affine(w s ) represent frequency and phase, respectively. The block contains a FC layer with W and b as the weight matrix and the bias. As shown in <ref type="figure">Fig. 2b</ref>, the NeRF network only contains three SIREN blocks to minimize runtime memory complexity.</p><p>Our experiments show that the viewing direction d will cause inconsistencies of face identities under multiple views (see <ref type="figure">Fig. 11a</ref>). Thus we do not take d as input, which is different from the original NeRF <ref type="bibr" target="#b38">[39]</ref>. Furthermore, instead of predicting the color c, we let the NeRF network predict a more general feature v <ref type="bibr" target="#b41">[42]</ref>. As a result, the proposed NeRF function is given by</p><formula xml:id="formula_2">g : R dim(x) ? R dim(zs) ? R + ? R dim(v) , (x, z s ) ? (?, v),<label>(3)</label></formula><p>where v is a feature vector corresponding to point x. z s is a shape code that is shared by all pixels of a generated image.</p><p>Volume Rendering As described in Eq. (3), the neural radiance field represents a scene as the volume density ? and feature vector v at any point in space. Let o be the camera origin. For each pixel, we cast a ray r(t) = o + td from origin o towards the pixel. We sample points along the camera ray r(t) and transform the 3D coordinates of the points into volume densities and feature vectors using Eq. (3). Using the classical volume rendering <ref type="bibr" target="#b27">[28]</ref>, the overall feature vector V r corresponding to the ray r(t) is given by</p><formula xml:id="formula_3">V r = tf tn T (t)?(r(t))v(r(t))dt, T (t) = exp ? t tn ?(r(s))ds ,<label>(4)</label></formula><p>where t n and t f are near and far bounds, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">INR Network for Appearance</head><p>Deep INR Network To generate images at H ? W resolution, the NeRF network outputs feature maps of shape dim(V r ) ? H ? W , where V r is the feature vector calculated by Eq. (4). Next, we need to convert these feature maps into the RGB space. We adopt an implicit neural representation (INR) network, where each pixel value is calculated independently <ref type="bibr" target="#b3">[4]</ref> given the feature vector V r . As shown in <ref type="figure">Fig. 2c</ref>, the INR network contains nine blocks, and each block contains two modulated fully connected layers (ModFC) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33]</ref>. We add a tRGB (i.e., a fully connected layer) layer after each block to convert the intermediate fea- </p><formula xml:id="formula_4">b n n ? ? ? ( ) 3 r p b H W n n ? ? ? ? ? H W ( ) r b H W n d ? ? ? ? Gradients RGB w/o grad ( ) 3 r b H W n ? ? ? ? Gradients 3 r b n ? ? r b n d ? ? Merge 3 b H W ? ? ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradients</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditionally-Independent Pixel Synthesis</head><p>Inference Inference <ref type="figure">Figure 3</ref>. Partial gradient backpropagation. In the training phase, the gradient calculation is turned on during the forward pass only for the green rays sampled randomly. The remaining rays do not participate in the backpropagation (the grey rays).</p><p>bmm</p><formula xml:id="formula_5">Input in b n d ? ? Normalize out b n d ? ? Out in out b d d ? ? in out b d d ? ? ? Weights in out d d ? 1 in out d d ? ? in b d ? Style 1 in b d ? ?</formula><p>Mod Demod Matrix multiplication <ref type="figure">Figure 4</ref>.</p><p>An efficient implementation of modulated fully connected layer (ModFC) using bmm.</p><p>Partial Gradient Backpropagation Our generator network is a columnar structure without any up-sampling or down-sampling operations. Therefore, directly training it on high-resolution images is challenging due to limited GPU memory. Note that the generator synthesizes each pixel value independently. Leveraging this property, we propose a training strategy named partial gradient backpropagation for training on high-resolution images.</p><p>As shown in <ref type="figure">Fig. 3</ref>, to synthesize an image at H ?W resolution, we randomly sample n r rays (green rays in <ref type="figure">Fig. 3</ref>), and then convert these rays into n r RGB values, with the gradient calculation enabled. On the other hand, the remaining H ? W ? n r rays (grey rays in <ref type="figure">Fig. 3</ref>) are converted to RGB values, but the gradient calculation is disabled to save memory. Finally, all the RGB values are combined into a high-resolution image, which will be presented to the discriminator. This strategy allows us to train the generator on high-resolution images. Compared with the patch-based method <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51]</ref>, partial gradient backpropagation ensures that the discriminator observes full natural images rather than image patches at low resolution.</p><p>Efficient Implementation for ModFC Like the NeRF network, the INR network also adopts a style-based architecture. As shown in <ref type="figure">Fig. 2d</ref>, a mapping network m a : Z a ? W a turns z a into w a , where the stochasticity of appearance comes from the code z a . Then, w a is mapped to style vectors using affine layers (i.e., FC layer). The style vectors are injected into the INR network using Modulated Fully Connected (ModFC) layers.</p><p>CIPS <ref type="bibr" target="#b3">[4]</ref> regards ModFC as a special case of 1 ? 1 convolutional layer and implements ModFC with the offthe-shelf modulated convolutional layer <ref type="bibr" target="#b32">[33]</ref>. The modulated convolution is implemented using grouped convolution, which is not efficient for ModFC. In fact, we can directly utilize the batch matrix multiplication (bmm) to implement ModFC more efficiently. As shown in <ref type="figure">Fig. 4</ref>, ModFC consists of Mod, Demod, and a batch matrix multiplication operation. Mathematically, let W ? R din?dout be the weights of a fully connected layer, S ? R b?din be a batch of style vectors, and X ? R b?n?din be the input with n being the length of the sequence. We first resize W and S to shapes of 1 ? d in ? d out and b ? d in ? 1, respectively. The Mod operation is given by W = W ? S, where ? stands for tensor-broadcasting multiplication and W ? R b?din?dout . The Demod operation is given by</p><formula xml:id="formula_6">W = W ? din (W ?,din,? ) 2 + ? 1 2</formula><p>, where is a small constant and W ? R b?din?dout . Finally, we use W to linearly map the input X ? R b?n?din (i.e., Y = X ? W , and Y ? R b?n?dout ), which is achieved through the batch matrix multiplication function <ref type="bibr" target="#b0">1</ref> . Experiments substantiate that this implementation is more efficient than the implementation using grouped convolution (see <ref type="figure" target="#fig_4">Fig. 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Overcoming Mirror Symmetry Issue with Auxiliary Discriminator</head><p>In practice, we found a mirror symmetry problem for the generator composed of the NeRF network and the INR network. As shown in <ref type="figure">Fig. 5c and Fig. 6</ref>, the direction of the bangs changes suddenly near the yaw angle of ? 2 . Interestingly, GIRAFFE <ref type="bibr" target="#b41">[42]</ref>, composed of a deeper NeRF network and a CNN decoder, also has this disturbing problem (see <ref type="figure">Fig. 5a</ref>). We identify two sources for the mirror symmetry: (i) the positional encoding function <ref type="bibr" target="#b38">[39]</ref>, and (ii) the mirror symmetry of the input coordinates of the NeRF network.</p><p>The positional encoding function ? : R ? R 2L , mapping a scalar to a high-dimensional vector, is given by ?(t; L) = sin 2 0 t? , cos 2 0 t? , . . . , sin 2 L?1 t? , cos 2 L?1 t? , <ref type="bibr" target="#b4">(5)</ref> where t is a scalar, and L is a hyperparameter determining the dimension of the mapping space.   As L increases, the distance between a and its symmetry point c is less than the distance between a and its neighbor b.</p><p>where ?(?; L) is defined by Eq. (5). Then T (?; L = 10) 2 is not distance preserving.</p><p>Proof. As shown in <ref type="figure">Fig. 6</ref>, let a = (cos 70? 180 , 0, sin 70? 180 ), c = (? cos 70? 180 , 0, sin 70? 180 ), and b = (cos 80? 180 , 0, sin 80? 180 ), then we have d(a, b) &lt; d(a, c),</p><p>where d represents Euclidean distance, and a and c are symmetric with respect to the yOz plane. We apply T (?; L) to a, b, and c respectively, and draw the distance between them in <ref type="figure">Fig. 7</ref>. Then we know that  </p><p>which contradicts the fact of Eq. (9). Thus T (?; L = 10) is not distance preserving and we finish the proof. <ref type="bibr" target="#b1">2</ref> T (?; L = 10) is often adopted by NeRF-based networks.</p><p>Eq. (9) shows that after positional encoding, the distance between a and its symmetry point c is less than the distance between a and its neighbor b. This may cause the network to predict similar appearance for a and c, causing mirror symmetry. Therefore, we discard the fixed positional encoding function and adopt a learnable strategy, i.e., the coordinates (x, y, z) are mapped to a high-dimensional space by a fully connected layer followed by a sine activation <ref type="bibr" target="#b54">[55]</ref>. However, mirror symmetry remains.</p><p>After closer inspection, we found that the essence of mirror symmetry lies in the symmetry of the coordinate system. As shown in <ref type="figure">Fig. 6</ref>, the coordinates of a and c are almost the same except that the x coordinate differs by a minus sign. The network tends to leverage the symmetry of coordinates to learn a symmetrical appearance. Note that the symmetry is a double-edged sword, and it facilitates the fitting of symmetrical objects <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b57">58]</ref> such as human faces, cat faces, cars, etc.</p><p>We notice that pi-GAN <ref type="bibr" target="#b10">[11]</ref>, whose generator is a pure NeRF network, does not suffer from mirror symmetry (see <ref type="figure">Fig. 5b</ref>). This substantiates that the discriminator can prevent the NeRF network from falling into the pitfall of mirror symmetry. However, both GIRAFFE's generator (NeRF + CNN decoder) and our generator (NeRF + INR network) suffer from mirror symmetry. It turns out that the discriminator cannot effectively regularize the NeRF network when there is a 2D network between the 3D NeRF network and the discriminator. Therefore, we propose to utilize an auxiliary discriminator to supervise the output of the NeRF network directly. In particular, the output of the NeRF network is mapped to the RGB space by a fully connected layer (see <ref type="figure">Fig. 2b</ref>), and the RGB is presented to the auxiliary discriminator. As shown in <ref type="figure">Fig. 5d</ref>, the mirror symmetry disappeared after applying the auxiliary discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation details</head><p>During training, we let the virtual camera be located on the surface of a unit sphere and look at the origin. The pitch and yaw are randomly sampled from predefined distributions, respectively. Both the main discriminator and the auxiliary discriminator adopt the StyleGAN2 <ref type="bibr" target="#b32">[33]</ref> discriminator architecture, but the auxiliary discriminator has fewer channels. The model is trained with a standard nonsaturating logistic GAN loss with R1 penalty <ref type="bibr" target="#b36">[37]</ref>. We adopt Adam <ref type="bibr" target="#b33">[34]</ref> to train the networks with ? 0 = 0 and ? 1 = 0.999. Following pi-GAN <ref type="bibr" target="#b10">[11]</ref>, we adopt a progressive training strategy where we start training at 64 2 resolution and progressively increase the resolution up to 512 2 . Note that the generator architecture remains fixed, but the resolution of the generator is increased by sampling more rays. We train the model with eight Tesla V100 GPUs <ref type="table">Table 1</ref>. Comparison with SOTA on FFHQ. We computed FID and KID (?10 3 ) between 50k generated images and all training images using the torch-fidelity library <ref type="bibr" target="#b42">[43]</ref>. ? stands for quoting from the paper. CIPS-3D achieves the state-of-the-art for 3Daware GANs, surpassing the very recent method StyleNeRF by clear margins.    and batch size of 32. When the resolution of the generator reaches 512 ? 512, the number of rays participating in the gradient calculation is set to 400 2 to save GPU VRAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>Comparison with SOTA We evaluate image quality using Fr?chet Inception Distance (FID) <ref type="bibr" target="#b23">[24]</ref> and Kernel Inception Distance (KID) <ref type="bibr" target="#b8">[9]</ref>. The baseline models contain the current start-of-the-art 3D-aware GANs: GIRAFFE <ref type="bibr" target="#b41">[42]</ref>, pi-GAN <ref type="bibr" target="#b10">[11]</ref>, and StyleNeRF <ref type="bibr" target="#b4">[5]</ref>. We also present the re-sults of 2D GANs, such as StyleGAN2 <ref type="bibr" target="#b32">[33]</ref> and CIPS <ref type="bibr" target="#b3">[4]</ref>, for your reference. As shown in Tab. 1, our method sets new records for 3D-aware GANs on FFHQ <ref type="bibr" target="#b31">[32]</ref> with impressive FID scores of 6.97 and 12.26 for images at 256 2 and 1024 2 resolution, respectively. Note that our method even outperforms the StyleNeRF <ref type="bibr" target="#b4">[5]</ref> proposed very recently, in terms of FID (6.97 vs. 8.00) and KID (2.87 vs. 3.70) at 256 2 resolution. However, our method is still inferior to the 2D state of the art, StyleGAN2, leaving room for improvement in future work. We present some generated images in <ref type="figure">Fig. 1</ref>. Compared to other 3D-aware GANs, CIPS-3D generates images with sharper details.</p><p>Progressively Growing Training <ref type="figure" target="#fig_6">Fig. 8</ref> shows the FID curves over the course of progressive training. CIPS-3D is clearly better than pi-GAN, especially at higher resolution. Note that it is not easy to train pi-GAN at high resolution. First of all, the generator of pi-GAN is a pure NeRF architecture being memory-intensive. Secondly, as shown in <ref type="figure" target="#fig_6">Fig. 8b</ref>, the convergence of pi-GAN has already deteriorated in the middle resolution (i.e., 128 2 ). Additionally, we found that pi-GAN is sensitive to hyperparameters, and increasing the depth and capacity of the generator may cause pi-GAN to fail to converge. CIPS-3D circumvents these difficulties by designing a new generator architecture consisting of a shallow 3D NeRF network and a deep 2D INR network.</p><p>Efficiency Comparison As shown in <ref type="figure" target="#fig_4">Fig. 9a and 9b</ref>, CIPS-3D has more parameters but is more efficient than pi-GAN that adopts a pure NeRF generator. To increase the number of parameters, we increase the depth of the INR network (18 layers) and the dimension of the fully connected layers (512 dimensions). Although there are more layers, the training speed of CIPS-3D is still higher than that of pi-GAN because the 2D INR network is inherently higher than the 3D NeRF network in terms of training efficiency. <ref type="figure" target="#fig_4">Fig. 9c</ref> compares the speed of ModFC using different implementation methods. Compared with group conv, bmm improves the speed of ModFC considerably (i.e., from 15.18 batches/second to 27.78 batches/second, with batch size of 4096, input/output dimension of 512). We evaluate the speed 1000 times and report the average runtime.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>We conduct ablation studies to help understand the individual components. As shown in the first two rows of Tab. 2, discarding the viewing direction d for the NeRF network improves FID moderately. <ref type="figure">Fig. 11</ref> shows the images generated by these two comparison methods. We deliver two messages. First, incorporating the viewing direction d as input leads to inconsistencies in face identity <ref type="figure">(Fig. 11a)</ref>. Second, the generator will suffer from the mirror symmetry issue regardless of whether the viewing direction d is used as input or not. We have explained the reason for the mirror symmetry in Sec. 3.3 and think it is due to the inherent symmetry of the coordinate system.</p><p>The second and third rows of Tab. 2 indicate that the learnable positional encoding function (FC + sine) deteriorates FID compared to the commonly used fixed positional encoding function. After adopting the auxiliary discriminator, the problem of mirror symmetry is solved, and the FID is improved as well (the third row vs. the fourth row in Tab. 2). Since the fixed positional encoding function (i.e., Eq. <ref type="formula">(7)</ref>) is not distance preserving, it theoretically may strengthen the problem of mirror symmetry, as analyzed in Sec. 3.3. Thus we decided to adopt the learnable positional encoding function. Our final method includes using only coordinates as input, a learnable positional encoding function, and an auxiliary discriminator.</p><p>Next, we study the effect of the number of pixels participating in calculating the gradient on performance. <ref type="figure">Fig. 12</ref> plots the FID curves over the course of training. Ablations were conducted at 128 2 resolution on FFHQ. It is evident that too few pixels participating in gradient backpropagation during training will harm the performance. When the number of pixels for the gradient backpropagation reaches 96 2 , the performance is comparable to that of using all pixels (i.e., 128 2 ). In addition, we experimentally found that using a small number of pixels (e.g., 48 2 ) can still achieve the performance of using all pixels, but at the cost of more training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Transfer Learning</head><p>In <ref type="figure" target="#fig_8">Fig. 10</ref>, we show the output images of the NeRF network and the corresponding output images of the INR network. The INR output images share the same pose as the NeRF images but own much richer textures. This indicates that the NeRF network is responsible for the posture and the INR network is responsible for the texture. Inspired by the FreezeG <ref type="bibr" target="#b2">[3]</ref> which freezes the early layers of the generator, we freeze the NeRF network of our generator trained on FFHQ and only fine-tune the INR network for transfer learning settings. Freezing NeRF is critical for transfer learning because there are too few images in the target domain. It is almost impossible to learn 3D shapes from so few images. However, by reusing the weights of NeRF, we <ref type="table">Table 2</ref>. Ablation studies on FFHQ at 64 2 resolution. We also present the images generated by each compared method in <ref type="figure">Fig. 11a, Fig. 11b, Fig. 5c, and Fig. 5d</ref>, respectively. The proposed auxiliary discriminator eliminates the problem of mirror symmetry. FID is calculated between 2048 generated images and 2048 real images. Please refer to Sec. 4.3 for details.   <ref type="figure">Figure 11</ref>. These images are generated by the ablation methods of the first two rows of Tab. 2. The images of each row are synthesized with different yaw angles. Please zoom in to see the pitch and yaw angles (at the upper left corner of the images). (a): Incorporating the viewing direction d as input leads to inconsistencies in face identity (please compare the leftmost and rightmost images). Interestingly, StyleNeRF <ref type="bibr" target="#b4">[5]</ref> claimed that the mirror symmetry is caused by the viewing direction d. However, in our experiments, both (a) and (b) suffer from the mirror symmetry issue, indicating that the viewing direction d is not the root cause of the mirror symmetry. PE: positional encoding.  <ref type="figure">Figure 12</ref>. Ablations for partial gradient computation on FFHQ at 128 2 resolution. Too few pixels for the gradient backpropagation during training harms the performance. Using more pixels will narrow the performance gap with using all pixels. only need to update the 2D INR network to render textures of other domains.</p><p>We verified the efficacy of the method on four datasets, including MetFaces <ref type="bibr" target="#b29">[30]</ref>, BitmojiFaces <ref type="bibr" target="#b0">[1]</ref>, Cartoon-Faces <ref type="bibr" target="#b1">[2]</ref>, and even the animal dataset, AFHQ <ref type="bibr" target="#b14">[15]</ref>. As shown in <ref type="figure" target="#fig_11">Fig. 13a</ref>, the transferred models generate images of different domains. Moreover, we can easily control the pose of the generated faces by explicitly manipulating the NeRF network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">3D-aware Face Stylization</head><p>We consider interpolating the original base model trained on FFHQ and the transferred models (fine-tuned on target datasets) to create new models to generate images in novel domains. Note that the weights of the NeRF network of the base mode and the transferred models are completely equal. We test two types of interpolating methods: (i) linearly interpolating all INR layers, and (ii) replacing the higher layers of the INR network of the base model with the corresponding layers of the transferred model <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b47">48]</ref>. The former produces images between two domains, as shown in <ref type="figure" target="#fig_11">Fig. 13c</ref>. The images smoothly fade from one domain to another. The latter generates images that combine the structural characteristics of the content images and the style characteristics of the style images (see <ref type="figure" target="#fig_11">Fig. 13b</ref>). Besides, CIPS-3D allows one to manipulate the pose of all the stylized faces explicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitation and Future Work</head><p>CIPS-3D works in a noise-to-image manner. Thus current stylization is limited to randomly generated images. To edit a real image, we need to project the image to the latent space of the generator. For this purpose, we need to study 3D-aware GAN inversion, and we leave this to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper presents a style-based 3D-aware generator that synthesizes pixel values independently without any spatial convolution or up-sampling operation. We find that the symmetry of the input coordinates leads to the problem of mirror symmetry and propose to utilize an auxiliary discriminator to solve this problem. We look forward to applying the proposed generator to more interesting applications such as 3D-aware GAN inversion and image-to-image translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 1 . 6 )Figure 5 .</head><label>165</label><figDesc>Let (X, d x ) and (Y, d y ) be two metric spaces. A mapping function f : X ? Y is called distance preserving if for any a, b ? X, one hasd y (f (a), f (b)) = d x (a, b).(Proposition 1. A positional encoding function T : R 3 ? R 3+6L is given by T (x, y, z; L) = (x, y, z, ?(x; L), ?(y; L), ?(z; L)), (7) 1 The function is bmm(?) in PyTorch. (a) GIRAFFE [42] (mirror symmetry) (b) pi-GAN [11] (no mirror symmetry) (c) CIPS-3D w/o auxiliary discriminator (mirror symmetry) (d) CIPS-3D w/ auxiliary discriminator (no mirror symmetry) The images of each row are synthesized with different yaw angles. Mirror symmetry exists in (a) and (c). (d): The auxiliary discriminator helps to overcome the mirror symmetry.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 . 2 .</head><label>62</label><figDesc>The direction of the bangs changes suddenly near the yaw angle of ? Please zoom in to see the yaw angles (at the upper left corner of the images).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a), T(b)) d(T(a), T(c))Figure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>d T (a; L = 10) , T (b; L = 10) &gt; d T (a; L = 10) , T (c; L = 10) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( 9 )</head><label>9</label><figDesc>Supposing that T (?; L = 10) is distance preserving, according to Eq. (6) and Eq. (8), we get d T (a; L = 10) , T (b; L = 10) &lt; d T (a; L = 10) , T (c; L = 10) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Training at 128 2 resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Progressive training on FFHQ. Generators are initially trained at 64 2 resolution (a), then at 128 2 resolution (b). pi-GAN converges steadily at low resolution, but deteriorates at higher resolution. For efficiency of training, FID is calculated between 2048 generated images and 2048 real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>(a) and (b): CIPS-3D has more generator parameters, but its training speed is higher than that of pi-GAN which adopts a pure NeRF generator. (c): Compared with group conv, bmm improves the speed of ModFC considerably. Please refer to Sec. 4.2 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 .</head><label>10</label><figDesc>Top: Images synthesized by the NeRF network. Bottom: Corresponding images synthesized by the INR network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Ablation study: (x, y, z) + d + Fixed PE function (b) Ablation study: (x, y, z) + Fixed PE function</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FID 32 2</head><label>32</label><figDesc>pixels w/ grad 48 2 pixels w/ grad 64 2 pixels w/ grad 96 2 pixels w/ grad 128 2 pixels w/ grad</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .</head><label>13</label><figDesc>Images synthesized by transferred models. (b) Stylization (multi-views) Content (c) Images synthesized by interpolated models (linear interpolation). Style (a): Fine-tuning the base model trained on FFHQ to generate images in other domains (please refer to Sec. 4.4 for details). (b) and (c): Interpolating the base model and the transferred model to generate stylized images (please refer to Sec. 4.5 for details). CIPS-3D enables us to manipulate the pose of the generated faces explicitly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ture maps to RGB values. The final RGB values are the summation of all intermediate RGB values.</figDesc><table><row><cell cols="2">3D points</cell><cell></cell><cell>Feature</cell><cell>RGB</cell></row><row><cell cols="2">w/ grad</cell><cell></cell><cell>w/ grad</cell><cell>w/ grad</cell></row><row><cell>r</cell><cell>p</cell><cell>3</cell><cell>3D NeRF</cell><cell>2D INR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Network</cell><cell>Network</cell></row><row><cell cols="2">3D points</cell><cell></cell><cell>Feature</cell><cell></cell></row><row><cell cols="2">w/o grad</cell><cell></cell><cell>w/o grad</cell><cell></cell></row><row><cell>Ray Sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bitmojifaces</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/mostafamozafari/bitmoji-faces.2" />
		<imprint>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cartoonfaces</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/arnaud58/photo2cartoon/version/1?select=trainB" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freezeg</surname></persName>
		</author>
		<ptr target="https://github.com/bryandlee/FreezeG.7" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image Generators with Conditionally-Independent Pixel Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirill</forename><surname>Ivan Anokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taras</forename><surname>Demochkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sterkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korzhenkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Style-based 3D Aware Generator for High-resolution Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stylenerf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to The Tenth ICLR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wasserstein Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratul</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<title level="m">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields. In ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs]</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1811.10597</idno>
		<title level="m">GAN Dissection: Visualizing and Understanding Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miko?aj</forename><surname>Bi?kowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danica</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demystifying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gans</surname></persName>
		</author>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.11096</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">R</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anpei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuqiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanbo</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mvsnerf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15595[cs],2021.2</idno>
		<title level="m">Fast Generalizable Radiance Field Reconstruction from Multi-View Stereo</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning Continuous Image Representation with Local Implicit Image Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sifei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learning Implicit Fields for Generative Shape Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02822</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">StarGAN v2: Diverse Image Synthesis for Multiple Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unconstrained Scene Generation with Locally Conditioned Radiance Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">Angel</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">M</forename><surname>Susskind</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Harm de Vries, Aaron Courville, and Yoshua Bengio. Feature-wise transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Local Deep Implicit Functions for 3D Shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avneesh</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06126[cs],2020.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrester</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06447</idno>
		<title level="m">Learning Shape Templates with Structured Implicit Functions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GANSpace: Discovering Interpretable GAN Controls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Escaping Plato&apos;s Cave: 3D Shape From Adversarial Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niloy</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised Image-to-Image Translation via Pre-trained StyleGAN2 Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05713[cs],2020.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jahanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07171[cs],2020.1</idno>
		<title level="m">On the &quot;steerability&quot; of generative adversarial networks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">CodeNeRF: Disentangled Neural Radiance Fields for Object Categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonbong</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lourdes</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ray tracing volume densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian P Von</forename><surname>Kajiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Progressive Growing of GANs for Improved Quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
	</analytic>
	<monogr>
		<title level="m">Stability, and Variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12423</idno>
		<title level="m">Alias-Free Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Style-Based Generator Architecture for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Analyzing and Improving the Image Quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Deep Meta Functionals for Shape Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gidi</forename><surname>Littwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06277</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Alexey Dosovitskiy, and Daniel Duckworth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Martin-Brualla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noha</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barron</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02268[cs],2021.2</idno>
	</analytic>
	<monogr>
		<title level="m">NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo Collections</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Which training methods for GANs do actually Converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03828</idno>
		<title level="m">Sebastian Nowozin, and Andreas Geiger. Occupancy Networks: Learning 3D Reconstruction in Function Space</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pratul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">HoloGAN: Unsupervised learning of 3D representations from natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Liang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17269</idno>
		<idno>2021. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Highfidelity performance metrics for generative models in Py-Torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Obukhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Seitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semen</forename><surname>Zhydenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kyl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elvis Yu-Jing</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.00844[cs],2021.5</idno>
		<title level="m">Do 2D GANs Know 3D Shape? unsupervised 3D shape reconstruction from 2D Image GANs</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong Joon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepsdf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05103</idno>
		<title level="m">Learning Continuous Signed Distance Functions for Shape Representation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11222[cs],2021.2</idno>
		<title level="m">On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Harm de Vries, Vincent Dumoulin, and Aaron Courville. FiLM: Visual Reasoning with a General Conditioning Layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Resolution Dependent GAN Interpolation for Controllable Image Synthesis Between Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doron</forename><surname>Pinkney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05334[cs],2020.8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05172</idno>
		<title level="m">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Interpreting the Latent Space of GANs for Semantic Face Editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Closed-Form Factorization of Latent Semantics in GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.06600</idno>
		<idno>2020. 1</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Implicit Neural Representations with Periodic Activation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Implicit Neural Representations with Periodic Activation Functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">W</forename><surname>Martel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><surname>Lindell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wetzstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Savva Ignatyev, and Mohamed Elhoseiny. Adversarial Generation of Continuous Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Skorokhodov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><forename type="middle">Adrian</forename><surname>Prisacariu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07064[cs],2021.2</idno>
		<title level="m">Neural Radiance Fields Without Known Camera Parameters</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shangzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11130[cs],2020.5</idno>
		<title level="m">Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Ul-traSR: Spatial Encoding is a Missing Key for Implicit Image Function-based Arbitrary-Scale Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12716[cs],2021.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning Object-Compositional Neural Radiance Field for Editable Scene Rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangbang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Semantic Hierarchy Emerges in Deep Generative Representations for Scene Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09267[cs],2020.2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07492[cs],2020.2</idno>
		<title level="m">NeRF++: Analyzing and Improving Neural Radiance Fields</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoutong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02725</idno>
		<title level="m">Visual Object Networks: Image Generation with Disentangled 3D Representation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs, stat]</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
