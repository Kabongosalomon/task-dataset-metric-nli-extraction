<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Super-Resolution System of 4K-Video Based on Deep Learning (Invited Paper)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint International Research Laboratory of Information Display and Visualization</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengcheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint International Research Laboratory of Information Display and Visualization</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changjun</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint International Research Laboratory of Information Display and Visualization</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Joint International Research Laboratory of Information Display and Visualization</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Li</surname></persName>
							<email>he.li@ieee.org</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Engineering</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Super-Resolution System of 4K-Video Based on Deep Learning (Invited Paper)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>video super-resolution</term>
					<term>real-time system</term>
					<term>neural network acceleration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video super-resolution (VSR) technology excels in reconstructing low-quality video, avoiding unpleasant blur effect caused by interpolation-based algorithms. However, vast computation complexity and memory occupation hampers the edge of deplorability and the runtime inference in real-life applications, especially for large-scale VSR task. This paper explores the possibility of real-time VSR system and designs an efficient and generic VSR network, termed EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for temporal coherence. In order to pursue faster VSR processing ability up to 4K resolution, this paper tries to choose lightweight network structure and efficient upsampling method to reduce the computation required by EGVSR network under the guarantee of high visual quality. Besides, we implement the batch normalization computation fusion, convolutional acceleration algorithm and other neural network acceleration techniques on the actual hardware platform to optimize the inference process of EGVSR network. Finally, our EGVSR achieves the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the most advanced VSR network at present, we achieve 85.04% reduction of computation density and 7.92? performance speedups. In terms of visual quality, the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP, etc.) on the public test dataset Vid4 and surpasses other state-of-the-art methods in overall performance score. The source code of this project can be found on https://github.com/Thmen/EGVSR.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Video super-resolution (VSR) is developed from image super-resolution, and it is one of the hot topics in the field of computer vision. VSR technology reconstructs degraded video, restores the definition of video, and improves the subjective visual quality. VSR is of great significance for improving the quality of early low-resolution video resources. At present, high-resolution such as 4K or even 8K display technology is relatively mature, however, the mainstream video sources are still dominated by low-resolution such as 1080P or 720P, which limits the quality of video system from the source end. In the near future, 4K and even higher resolution will surely replace Full HD (FHD) as the mainstream format. Therefore, there is an urgent real-life demand for efficient and lightweight VSR technology to upgrade a mass of low-resolution (LR) videos to highresolution (HR) ones.</p><p>The research object of VSR technology is the image sequence of video resources. The image sequence is composed of a series of static images and forms into continuous frames. Since some objects in the video move at a fast speed and appear as a motion blur effect in a single image, there will be sub-pixel displacements between target frames and its adjacent frames. Therefore, it is crucially important for VSR systems to align the adjacent frames using effective motion compensation algorithms. This is a difficult and challenging problem in the field of current VSR research. In addition, super-resolution technology still has the following formidable challenges and urgent research directions:</p><p>? Large scale and unknown corruption, still lack of effective algorithms.</p><p>? Lightweight and real-time architecture, where deep VSR models are still difficult to deploy on hardware.</p><p>Deep Learning algorithms are considered to be excel at solving many unsupervised problems, and essential to solve the aforementioned challenges. This paper explores the solutions of large-scale VSR and pursues the goal of 4K highresolution in VSR system.</p><p>For large-scale VSR, challenges in the computational complexity and memory consumption impede the real-time and low latency performance of video processing. Although advanced deep models have achieved high quality on VSR, these models are still difficult to be deployed in practical applications due to the huge amount of parameters and calculations. In order to solve this problem, we need to design a lightweight VSR model, or refine the existing VSR model with fewer parameters and sparse structures.</p><p>Generally, in the field of VSR, main research direction lies in the pursuit of video quality, while few focus on fast and real-time VSR methods. Real-time VSR requires to consider both of quality and speed. In this paper, we propose a VSR network that can handle large-scale and high-performance, and investigate hardware-friendly accelerating architectures for VSR network inference, thereby allowing real-time processing without the sacrifice of VSR quality. The contributions of this paper are summarised as follows:</p><p>1. We present a lightweight and efficient VSR network to improve the performance of VSR quality and running speed. 2. We investigate various network acceleration strategies tailored for large-scale VSR system to meet the requirements of real-time inference. 3. We propose an unified method to quantify different metrics of VSR quality for efficient automated evaluation across vast test samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deep Learning Based Video Super Resolution</head><p>From the perspective of technical route, super-resolution (SR) technology can be summarized into three categories: interpolation based SR, super-resolution reconstruction based SR, and learning based SR <ref type="bibr" target="#b0">[1]</ref>. In the last few years, interests in deep learning (DL) based SR algorithms research have risen rapidly. It is difficult for traditional algorithms to make breakthroughs for higher performance, while DL-based SR algorithms have achieved significant improvements of SR quality <ref type="bibr" target="#b1">[2]</ref>. In addition, compared to single-image SR, video SR problems provide more available information from multiple frames, with both spatial dependence of intra-frame and temporal dependence of inter-frame. Therefore, the existing work mainly focuses on how to make an efficient use of spatio-temporal correlation, which refers to explicit motion compensation (MC) and recursive feedback mechanism to fuse additional image information from multi-frames.</p><p>In terms of MC based VSR methods, Liao et al. <ref type="bibr" target="#b2">[3]</ref> used multiple optical flow methods to generate HR candidate objects and integrated them into CNNs. VSRnet <ref type="bibr" target="#b2">[3]</ref> estimated the optical flow through the Druleas algorithm, SOFVSR <ref type="bibr" target="#b3">[4]</ref> reconstructed the coarse-to-fine optical flow through the OFRnet network. Both used multiple continuous frames as the input of CNNs to predict HR frames. Besides, some methods tried to learn MC directly. VESPCN <ref type="bibr" target="#b4">[5]</ref> used a trainable spatial transformer to learn MC between adjacent frames, and input multiple frames into a spatio-temporal network ESPCN <ref type="bibr" target="#b5">[6]</ref> for end-to-end prediction. BRCN <ref type="bibr" target="#b6">[7]</ref> proposed a bidirectional framework that using CNN, RNN, and conditional Generative Adversarial Network (GAN) for model spatial, temporal, and spatio-temporal dependence, respectively. FRVSR <ref type="bibr" target="#b7">[8]</ref> and TecoGAN <ref type="bibr" target="#b8">[9]</ref> used the previous HR predicted frames to reconstruct the subsequent HR frames in a circular manner through two DNNs. Another trend started to use recursive method to capture spatio-temporal correlations without the need for explicit MC. Specifically, DUF <ref type="bibr" target="#b9">[10]</ref> used an end-to-end deep CNN to generate dynamic upsampling filters and residual images to avoid explicit MC processing. EDVR <ref type="bibr" target="#b10">[11]</ref> used the enhanced deformable convolutions and RBPN <ref type="bibr" target="#b11">[12]</ref> utilized a recurrent encoder-decoder module to improve the fusion of multi-frame information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Efficient and Real-time VSR Network</head><p>Following the design principle of CNN networks, "the deeper, the better", VSR networks have been developing towards a larger and wider network architecture. However, large-scale networks bring huge computation, making it difficult to be implemented on present-constrained hardware platforms and deploy practical VSR networks in real-time. Recently, many research studies have investigated optimization and acceleration methods of VSR network. For example, Chao et al. redesigned and optimized network structure in order to accelerate the previous SRCNN model <ref type="bibr" target="#b12">[13]</ref>, therefore, the network complexity of FSRCNN is much lower than that of SRCNN. FAST <ref type="bibr" target="#b13">[14]</ref> used compression algorithm to extract a compact description of the structure and pixel correlation, and accelerated the most advanced SR algorithm by 15 times with a minimum performance loss (only -0.2 dB). The VSRnet proposed by Kappeler et al. used an adaptive MC architecture to deal with motion blur problems, and the processing time of each frame only needs 0.24s on GPU device <ref type="bibr" target="#b14">[15]</ref>. Furthermore, interests in FPGA-based high-performance and parallel computing have grown. In the early work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, researchers first implemented large-scale VSR tasks on FPGA, i.e. 2Kto8K@60Hz 4? video upscale and 4Kto8K@60Hz 2? upscale, however, they still used the non-DL traditional interpolation-based algorithm. The energy-efficient DCNNs devised by Chang et al. optimized the deconvolutional layer, and proposed the FPGA-based CNN accelerator to generate UHD video efficiently <ref type="bibr" target="#b17">[18]</ref>. Under the same occupation of hardware resources, the throughput of the DCNN accelerator is 108 times faster than a traditional implementation. Yongwoo et al. proposed a hardware-friendly VSR network based on FPGA facilitated by quantisation and network compression <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Efficient and Generic VSR System</head><p>The generative and discriminative modules of GANs can play games with each other during the training process to produce better perceptual quality than traditional generative models. Therefore, GANs are widely used in the SR field. We rely on the powerful ability of deep feature learning of GAN models to deal with large-scale and unknown degradation challenges in VSR tasks. In addition, we refer to the design of the TecoGAN <ref type="bibr" target="#b8">[9]</ref> method and introduce the spatio-temporal adversarial structure to help the discriminator understand and learn the distribution of spatio-temporal information, which avoids instability effect in temporal domain encountered by traditional GANs.</p><p>Moreover, to meet the requirements of super-resolving large-scale video up to 4K-resolution, we follow the design principles of efficient CNN model to build a more generic and high-quality video super-resolution network, termed EGVSR (efficient and generic video super-resolution). To allow processing 4K video in real-time, we recall the practical guidelines of an efficient CNN architecture <ref type="bibr" target="#b19">[20]</ref> and build an lightweight network structure for EGVSR. The generator part is divided into FNet module and SRNet module for optical flow estimation and video frame super-resolution, respectively. <ref type="figure" target="#fig_0">Figure 1</ref> shows the framework of EGVSR's generator part and the data flow during inference stage. The structure of FNet refers to the encoder-decoder architecture in RNN to estimate the dense optical flow and provide motion compensation information for adjacent frame alignment operation (Warp). The encoding part uses three encoder units, each of which is composed of {Conv2d? LeakyReLU?Conv2d?LeakyReLU?MaxPool2}, and the decoding part uses three decoder units formed by {Conv2d? LeakyReLU? Conv2d? LeakyReLU? BilineraUp?2}. The design of SRNet module needs to take into account both network capacity and inference speed. We remark that multiple network layers are used to ensure the VSR quality, while the network complexity should be controlled for realtime video processing ability.</p><p>Herein, we refer to the structure of ResNet <ref type="bibr" target="#b20">[21]</ref> network and adopt lightweight residual block (ResBlock) to build SRNet. The structure of ResBlock is {(Conv2d?ReLU? Conv2d)+Res}. Considering the balance between quality and speed, we use 10 ResBlock to build SRNet, and use lightweight sub-pixel convolutional layer in the end of EGVSR network as the output upsampling method, with the structure of {PixelShuffle?4?ReLU?Conv2d}.</p><p>As for the other modules in our EGVSR, we keep the same setup to the previous work <ref type="bibr" target="#b8">[9]</ref> for fair comparison, considering the balance between the inference speed of the EGVSR network and the quality of the VSR. Moreover, a lightweight network is introduced without affecting the quality of the VSR. The design principle is to simplify the EGVSR network as much as possible, and uses the various neural network acceleration techniques mentioned below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Batch Normalization Fusion</head><p>In order to ensure real-time processing capability of our EGVSR system, further optimizations are made in EGVSR system without sacrificing the quality of VSR. Batch Normalization (BN) technology is most commonly used in the field of deep learning to improve the generalization of the network and prevent the side effect of over-fitting. The mathematical formula for the calculation of BN processing can be briefly described as Eq. <ref type="formula" target="#formula_0">(1)</ref>. It can be seen that the calculation of BN is quite complicated, and the mean ( ) and variance ( 2 ) value of a batch of samples need to be counted first. The FNet module in our EGVSR network also makes extensive use of the BN layer. We need to optimize it to improve the speed of network training and inferencing. </p><formula xml:id="formula_0">1 1 , ( ) i i n n i i i i x x x x n n ? ?? ? ? ? ? ? ? = = = + ? + = = ? ?? ?<label>(1)</label></formula><p>First of all, we transform the BN calculation into matrix form, as expressed in <ref type="formula">(2):</ref> (2)</p><p>We can see that the transformed BN layer is similar to the formation of the 1?1 convolution ( ?) = * ? + , then we can utilize the 1?1 convolutional layer to realize and replace the layer of BN. Finally, we can fuse the 1?1 convolutional layer with the previous convolutional layer, so that we can eliminate the need of calculating BN. The optimization of BN fusion will provide a speed improvement of about 5%. The overall transformation process is shown in <ref type="figure" target="#fig_2">Figure 2</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Efficient Upsampling Method</head><p>Upsampling layer is one of the most important portions in SR network, which can be roughly divided into two categories according to different technique routes: traditional interpolation-based upsampling methods and learning-based upsampling methods. All interpolation upsampling methods have an obvious defect, which will cause edge blur to the image, while various learning-based upsampling methods, such as Deconvolution, Sub-pixel Convolution, and Resize Convolution, have powerful feature learning capability, and show their talents in VSR networks.</p><p>In order to compare the above three intra-network upsampling methods to select the best solution, we used the control variate method to evaluate the efficiency of these upsampling layers in actual SR networks. Specifically, ESPCN <ref type="bibr" target="#b5">[6]</ref> network is used as the backbone of SR network. We only changed the upsampling layer while maintaining the other network structures and configurations, and trained multiple groups of SRNet with three different upsampling methods, i.e. A. Resize convolution (using bilinear interpolation); B. Deconvolution; C. Sub-pixel convolution. <ref type="table" target="#tab_0">Table I</ref> shows the detailed network settings. Table II records the performance metrics of different SRNets. It can be seen that the sub-pixel convolution has the best quality performance in both the training and testing stages, except for PSNR metric in testing stage, which is slightly lower than that of deconvolution (-0.02dB). Besides, we test the average running time of different SRNets for 3? superresolving single test image with the size of 800?800 under the same testing environment. Although the SRNet with resize convolution has the least weight parameters, the processing time bottleneck lies in both CPU and GPU platforms, due to the high computational complexity of interpolation. Sub-pixel convolution performs better than method A and B, which is 1.77 times faster (CPU) and 1.58 times faster (GPU) than that of method A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Convolutional Computation Acceleration</head><p>In order to further improve the inference speed of EGVSR network, we explore the core of computation in the neural network. From the perspective of actual engineering deployment, it points out that convolutional computation is the key to CNNs, accounting for more than 90% of the total computation, which consumes most of the calculation time, therefore, it is necessary to improve the computational efficiency of convolution. We should design an efficient convolutional algorithm suitable for hardware deployment.</p><p>According to basic calculation process of the traditional na? ve convolution (ConV), a large number of loop structures (6 loops) are used, and the computation efficiency is quite low. In terms of two-dimensional convolutional layer with a 3?3 kernel, we need to traverse from top left to bottom right on the input feature map based on the traditional sliding window method to obtain the output feature map, as shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. We consider using the matrix multiplication (MatMul) algorithm to improve it, following the process in the second line of <ref type="figure" target="#fig_3">Figure 3</ref>. First, input data of each activation zone is extracted according to the size of convolutional filter, and converted from original 2D matrix with 3?3 size into single row vector. When the length of sliding step is one, there are four activation zones in total, thus four 1D row vectors can be obtained. All vector constitutes a 2D matrix with a size of 4?9. We call this conversion as im2col (i.e. feature map to column vector or image to column), and this optimization method was first proposed by Jia <ref type="bibr" target="#b21">[22]</ref>. Similarly, the 2D convolutional filter with the size of 3?3 is straightened directly and transformed into the 1D column vector with the size of 9?1. Such a conversion does not consume computation, and it is only a rearrangement of the memory in reading order. There will be duplicated data elements in the converted matrix, which will increase the memory usage.</p><p>We find that MatMul computation of two transformed matrices identify with the results of a convolutional computation, and no additional calculation is required. The desired output feature results can be obtained through the inverse col2im conversion. The convolution operation in CNN is essentially a multiple dimensional correlation computation. In our actual hardware implementation, the method mentioned above is adopted to convert convolutional computation into matrix multiplication, which saves inference time by memory space to boost higher computational efficiency.</p><p>We also aim to accelerate our proposed EGVSR network on FPGAs using convolution accelerators. We recall our previous work WinoConv <ref type="bibr" target="#b22">[23]</ref>, a FPGA-based convolution accelerator, and analyse the feasibility of EGVSR's edge deployment on FPGA, where Winograd algorithm <ref type="bibr" target="#b23">[24]</ref> is used to reduce the complexity of convolutional computation, decreased from O( 3 ) to O( 2.376 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evalution of Image Quality</head><p>Firstly, we evaluated and compared the actual superresolution performance of image quality on the standard testing dataset VID4 with previous VSR networks under different technical routes, including: 1).VESPCN <ref type="bibr" target="#b4">[5]</ref> and SOFVSR <ref type="bibr" target="#b3">[4]</ref> networks based on non-GAN method with MC; 2). DUF <ref type="bibr" target="#b9">[10]</ref> network based on non-GAN method without explicit MC; and 3). our EGVSR network based on GAN method and optical flow based MC.</p><p>In order to facilitate comparison, we captured the actual image results from different VSR methods and focus more on the detail textural features. <ref type="figure">Figure 4</ref> exhibits the reconstruction results produced by various VSR networks on VID4 dataset, and the group of detail images on the right side represents the image results from LR (low-resolution), VESPCN, SOFVSR, DUF, EGVSR and GT (ground-truth) respectively. From the subjective results, EGVSR's results are the closest to the GT images and achieve higher image detail reconstruction quality. VESPCN and SOFVSR networks performed relatively fuzzy in the overall picture and seriously lost most image edge details. EGVSR against the DUF network that currently has state-of-the-art performance of image quality in VSR field.</p><p>For a more objective assessment of VSR image quality, we used three most common metrics: PSNR, SSIM and LPIPS. The specific experimental results are shown in <ref type="table" target="#tab_0">Table III</ref>. The objective testing results are consistent with the previous subjective results, and it can be seen that DUF and EGVSR seem equally matched in three metrics. Generally, DUF performs slightly better in PSNR and SSIM metrics, while our EGVSR performs better in LPIPS. Regarding to the evaluation of image super-resolving quality, Blau and Michaeli have proved that the measurement using PSNR or SSIM metric to assess the human visual perception quality has an inherent distortion <ref type="bibr" target="#b24">[25]</ref>. DL-based feature mapping metric LPIPS can capture more high-level image semantic structures, and the LPIPS metric is close to the subjective evaluation of human eyes. Therefore, LPIPS is more accurate than the first two metrics, and our EGVSR has a significant performance improvement of 48.15% compared with DUF in LPIPS, according to the average results on the VID4 dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evalution of Video Quality and Temporal Coherence</head><p>In this section, we will investigate the objective evaluation of video quality for our VSR system. In order to acquire the qualitative comparison result of temporal consistency, we introduced two temporal metrics, tOF and tLP, used in previous work <ref type="bibr" target="#b8">[9]</ref>. tOF measures the pixel-wise difference of motions estimated from sequences, and tLP measures perceptual changes over time using deep feature map:</p><formula xml:id="formula_1">( ) ( ) ( ) ( ) 1 1 1 1 1 1 tOF , , tLP , , t t t t t t t t OF b b OF g g LP b b LP g g ? ? ? ? = ? = ?<label>(3)</label></formula><p>Pixel differences and perceptual variations are critical to quantifying actual temporal coherence, therefore, tOF and tLP will measure the difference between the VSR results and the corresponding GT reference ones. The smaller the score is, the closer it is to the true result, which providing a more pleasant and fluent subjective perceptual experience. In addition to the VESPCN and SOFVSR networks (DUF is excluded, mainly because it is not based on explicit MC), two latest VSR networks, FRVSR <ref type="bibr" target="#b7">[8]</ref> and TecoGAN <ref type="bibr" target="#b8">[9]</ref>, are used. We conducted testing experiment on three datasets, VID4, TOS3 and GVT72. The specific experiment results are shown in <ref type="figure" target="#fig_4">Figure 5</ref> and <ref type="figure" target="#fig_5">Figure 6</ref>. The results of the temporal metrics show that the spatio-temporal adversarial model has better performance than the traditional model. EGVSR network can recover more spatial details with a satisfied temporal coherent, meeting the subjective perception of human eyes.  TecoGAN model stands out in all temporal performance test of TOS3 dataset. There is still a slight gap between our EGVSR and TecoGAN model, where a reduction performance of -4.74%~-11.01% is shown in evaluation result. However, it is difficult to distinguish their difference from the aspect of subjective perception, as shown in <ref type="figure" target="#fig_5">Figure 6</ref>. Besides, EGVSR is even slightly better in some representation details, such as the reconstruction of brick texture details marked in the yellow box of the "Bridge" sample in <ref type="figure" target="#fig_5">Figure 6</ref>. Images generated by EGVSR are closer to the GT ones. EGVSR has an advantage over TecoGAN in some respects, or even overall exceeds TecoGAN on VID4, maintaining a performance advantage of +5.53% to +12.35%. The performance of our EGVSR network in temporal domain is significantly better than that of the previous methods and is comparable to that of TecoGAN, the SOTA VSR model by far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Runtime Speed on GPU</head><p>This section will test the running frame rate of different VSR models during inference. The experimental results are shown in <ref type="table" target="#tab_0">Table IV</ref>. The 2nd line lists the parameters of each VSR networks, line 3 counts the statistics of corresponding computation cost, and the last two lines show the average FPS that can be achieved during 4? VSR in different resolutions. The results show that, the total computation cost required by our EGVSR is only 29.57% of VESPCN, 12.63% of SOFVSR, and 14.96% of FRVSR and TecoGAN. In terms of using CPU only, we achieve the increase of speed-up by 8.25? to 9.05? compared to TecoGAN. As for using GPU for acceleration, we realize the EGVSR system in real-time at a speed of 720p/70.0FPS, 1080p/66.9FPS, 4K/29.6FPS, which has 2.25?, 4.45?, and 7.92? performance speed-up compared to TecoGAN. Due to our lightweight design of VSR network and various optimization strategies, the efficiency of EGVSR on CPU/GPU hardware platform is improved greatly. In contrast, other DL-based VSR methods have limited runtime efficiency when dealing with large-scale VSR tasks such as 1080p and 4K resolution, which cannot meet the runtime ability (above 25FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Overall Performance</head><p>Although the above experimental discussion provides the test result for evaluating the visual quality and running speed of our VSR system, these test experiments are compared on their own dimension independently. An unified quantitative metric is essential for efficient automated evaluation across a large number of test samples. In this section, we consider the balance between visual quality and running speed of VSR network for generating high-resolution video. Therefore, we propose a novel and unified VSR visual quality assessment metric to quantify LPIPS in spatial domain and tOF and tLP in temporal domain. Specifically, Eq. (4) is used to normalize the value of all metrics of each network in different datasets.</p><formula xml:id="formula_2">min max min ( ) ( ) nor M M M M M = ? ?<label>(4)</label></formula><p>The weighted sum method is used to quantify different metrics, and finally the comprehensive visual quality score of VSR network is calculated by </p><p>where, the value of the score ranges from 0 to 1, a higher score indicating that the VSR system achieves a better visual quality. <ref type="figure" target="#fig_7">Figure 7</ref> depicts the comprehensive performance of video quality score and network running speed of various VSR methods. In addition to VESPCN, SOFVSR, DUF, FRVSR, TecoGAN and EGVSR mentioned above, we also obtained the specific performance of EDVR <ref type="bibr" target="#b10">[11]</ref> and RBPN <ref type="bibr" target="#b11">[12]</ref> from the public data. In terms of running speed, the average running of various VSR networks on the GPU for 4? video superresolution with target resolution of 4K is tested. As shown in <ref type="figure" target="#fig_7">Figure 7</ref>, the closer to the upper right corner, the better visual performance and faster running speed can VSR network achieve. The color and size of the bubble represents the computational complexity and parameter number of network, respectively. In summary, the overall visual quality of EGVSR network is at the advanced level, second only to TecoGAN network (lower 0.011/1.14%), while it is the only VSR network that is capable of processing 4K video in realtime (29.61FPS). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. FPGA Deployment Estimation</head><p>We have deployed the prototype design of the convolution accelerator, WinoConv mentioned above, on a Xilinx KC705 development platform. And, in this section, we tested and evaluated the WinoConv convolution accelerator on KC705 under 300MHz. We compared our WinoConv with previous work: LUT-based convolution method <ref type="bibr" target="#b25">[26]</ref> and DSP-based convolution method <ref type="bibr" target="#b26">[27]</ref>. <ref type="table">Table V</ref> shows the hardware synthesis results of different methods to achieve 3?3 convolution.</p><p>Experimental results show that, WinoConv has the lowest computational latency and has great advantages in terms of convolutional computation speed. The delay of LUT-based direct convolution method is unacceptable among three methods. Compared to DSP-based convolution method, our method can reduce the latency at least 1.83?, and yields more speed-up gains with larger convolution size. Besides, we have calculated the max FLOPs by the following formulas: </p><p>The last column of <ref type="table">Table V</ref> indicates the maximal FLOPs provided by different WinoConv accelerators. Combined with the computation cost required by the EGVSR network given in <ref type="table" target="#tab_0">Table IV</ref>, the implementation of the whole EGVSR network on FPGA edge deployment could realize the runtime speed of 720p/99.44FPS, 1080p/44.32FPS, 4K/11.05FPS in the way of theoretical estimation. We remark that implementing the entire VSR system on FPGAs would meet the demands of edge and low-energy computing, as a task in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we have conducted an in-depth study in the VSR field to address the 4K-resolution VSR tasks and efficient VSR processing in real-time. Using various optimization strategies, the proposed EGVSR method reduces the computation load to the lowest requirement, under the premise of high visual quality of VSR, and realizes a real-time 4K VSR implementation on hardware platforms. The balance between quality and speed performance is improved effectively. Even though we have designed the accelerator for convolutional computation on FPGAs, while it is considerable to deploy the whole system on FPGA platform to further achieve the possibility of edge inference for VSR systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of EGVSR network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Batch Normalization fusion processing flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Use matrix multiplication to accelerate convolutional computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Averaged VSR metric evaluations for three dataset with the following metrics?LPIPS, tOF, tLP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>VSR comparisons for detial views of captured image ("Bridge" video sample in TOS3 dataset) in order to compare to previous work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Overall performance comparison of various VSR networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>THE NETWORK SETTING OF THREE CONTROL SRNETS</figDesc><table><row><cell>Structure</cell><cell>Layer</cell><cell>Output Shape</cell><cell>Param#</cell></row><row><cell></cell><cell>1-Conv2d+Tanh</cell><cell>[1,64,800,800]</cell><cell>1,664</cell></row><row><cell>Backbone</cell><cell>2-Conv2d+Tanh</cell><cell>[1,32,800,800]</cell><cell>18,464</cell></row><row><cell></cell><cell>3-Conv2d+Tanh</cell><cell>[1,32,800,800]</cell><cell>9,248</cell></row><row><cell>Upsample-A</cell><cell>4-Interpolation 5-Conv2d</cell><cell>[1,32,2400,2400] [1,1,2400,2400]</cell><cell>0 33</cell></row><row><cell>Upsample-B</cell><cell>5-ConvTranspose2d</cell><cell>[1,1,2400,2400]</cell><cell>801</cell></row><row><cell>Upsample-C</cell><cell>4-Conv2d 5-PixelShuffle</cell><cell>[1,9,800,800] [1,1,2400,2400]</cell><cell>297 0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell>.</cell><cell cols="6">EXPERIMENTAL COMPARISON RESULTS OF VARIOUS</cell></row><row><cell></cell><cell></cell><cell cols="4">UPSAMPLING METHODS</cell><cell></cell></row><row><cell>Up-sample Method</cell><cell>Total Param#</cell><cell>Loss</cell><cell cols="4">Train PSNR PSNR SSIM Test</cell><cell>CPU time (ms)</cell><cell>GPU time (ms)</cell></row><row><cell>A</cell><cell>29,409</cell><cell cols="2">0.0055</cell><cell>22.61</cell><cell>25.45</cell><cell>0.72</cell><cell>415.8 9.860</cell></row><row><cell>B</cell><cell>30,177</cell><cell cols="2">0.0048</cell><cell>23.20</cell><cell>26.52</cell><cell>0.76</cell><cell>253.4 8.203</cell></row><row><cell>C</cell><cell>29,673</cell><cell cols="2">0.0047</cell><cell>23.28</cell><cell>26.50</cell><cell>0.77</cell><cell>234.9 6.234</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="5">OBJECTIVE EVALUTION RESULTS OF IMAGE QUALIT ON</cell></row><row><cell></cell><cell></cell><cell cols="2">VID4 TEST DATASET</cell><cell></cell><cell></cell></row><row><cell cols="6">Sequence Name Fig. 4. Subjective comparison results from various VSR methods (Testing on Vid4 dataset, video sequence name: Calendar/City/Foliage/Walk). Metric VESPCN SOFVSR DUF Ours</cell></row><row><cell></cell><cell>PSNR?</cell><cell>14.67</cell><cell>18.39</cell><cell>23.59</cell><cell>23.60</cell></row><row><cell>Calendar</cell><cell>SSIM?</cell><cell>0.19</cell><cell>0.50</cell><cell>0.80</cell><cell>0.80</cell></row><row><cell></cell><cell>LPIPS?</cell><cell>0.57</cell><cell>0.41</cell><cell>0.33</cell><cell>0.17</cell></row><row><cell></cell><cell>PSNR?</cell><cell>19.38</cell><cell>22.03</cell><cell>27.63</cell><cell>27.31</cell></row><row><cell>City</cell><cell>SSIM?</cell><cell>0.14</cell><cell>0.69</cell><cell>0.79</cell><cell>0.79</cell></row><row><cell></cell><cell>LPIPS?</cell><cell>0.48</cell><cell>0.21</cell><cell>0.27</cell><cell>0.16</cell></row><row><cell></cell><cell>PSNR?</cell><cell>16.22</cell><cell>22.96</cell><cell>26.15</cell><cell>24.79</cell></row><row><cell>Foliage</cell><cell>SSIM?</cell><cell>0.09</cell><cell>0.46</cell><cell>0.77</cell><cell>0.73</cell></row><row><cell></cell><cell>LPIPS?</cell><cell>0.54</cell><cell>0.36</cell><cell>0.35</cell><cell>0.14</cell></row><row><cell></cell><cell>PSNR?</cell><cell>15.28</cell><cell>20.91</cell><cell>29.90</cell><cell>27.84</cell></row><row><cell>Walk</cell><cell>SSIM?</cell><cell>0.32</cell><cell>0.45</cell><cell>0.91</cell><cell>0.86</cell></row><row><cell></cell><cell>LPIPS?</cell><cell>0.34</cell><cell>0.44</cell><cell>0.14</cell><cell>0.09</cell></row><row><cell></cell><cell>PSNR?</cell><cell>16.20</cell><cell>21.02</cell><cell>26.82</cell><cell>25.88</cell></row><row><cell>Average</cell><cell>SSIM?</cell><cell>0.19</cell><cell>0.53</cell><cell>0.82</cell><cell>0.80</cell></row><row><cell></cell><cell>LPIPS?</cell><cell>0.48</cell><cell>0.36</cell><cell>0.27</cell><cell>0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV .</head><label>IV</label><figDesc>THE RUNTIME SPEED OF DIFFERENT VSR NETWORKS ON CPU AND GPU</figDesc><table><row><cell cols="2">Performance</cell><cell>Source</cell><cell>Target</cell><cell></cell><cell>VESPCN</cell><cell>SOFVSR</cell><cell>FRVSR</cell><cell cols="2">TecoGAN</cell><cell>Ours: EGSVR</cell><cell>Speed-up vs.</cell></row><row><cell cols="2">Parameters(M)</cell><cell>--</cell><cell>--</cell><cell></cell><cell>0.879</cell><cell>1.640</cell><cell>2.589</cell><cell></cell><cell>2.589</cell><cell>2.681</cell><cell>TecoGAN</cell></row><row><cell></cell><cell></cell><cell>320?180</cell><cell>720p</cell><cell></cell><cell>96.56</cell><cell>226.12</cell><cell>190.81</cell><cell></cell><cell>190.81</cell><cell>28.55</cell><cell>--</cell></row><row><cell cols="2">FLOPs(G)</cell><cell>480?270</cell><cell>1080p</cell><cell></cell><cell>221.08</cell><cell>508.78</cell><cell>429.30</cell><cell></cell><cell>429.30</cell><cell>64.06</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell>960?540</cell><cell>4K</cell><cell></cell><cell>886.47</cell><cell>2035.11</cell><cell>1718.65</cell><cell></cell><cell>1718.65</cell><cell>257.01</cell><cell>--</cell></row><row><cell></cell><cell></cell><cell>320?180</cell><cell>720p</cell><cell></cell><cell>3.053</cell><cell>1.039</cell><cell>1.152</cell><cell></cell><cell>1.150</cell><cell>9.487</cell><cell>8.25?</cell></row><row><cell cols="2">FPS(CPU)</cell><cell>480?270</cell><cell>1080p</cell><cell></cell><cell>1.201</cell><cell>0.443</cell><cell>0.485</cell><cell></cell><cell>0.485</cell><cell>4.389</cell><cell>9.05?</cell></row><row><cell></cell><cell></cell><cell>960?540</cell><cell>4K</cell><cell></cell><cell>0.289</cell><cell>0.106</cell><cell>0.112</cell><cell></cell><cell>0.112</cell><cell>0.958</cell><cell>8.55?</cell></row><row><cell></cell><cell></cell><cell>320?180</cell><cell>720p</cell><cell></cell><cell>48.48</cell><cell>13.31</cell><cell>31.16</cell><cell></cell><cell>31.15</cell><cell>70.04</cell><cell>2.25?</cell></row><row><cell cols="2">FPS(GPU)</cell><cell>480?270</cell><cell>1080p</cell><cell></cell><cell>24.76</cell><cell>5.993</cell><cell>15.10</cell><cell></cell><cell>15.05</cell><cell>66.90</cell><cell>4.45?</cell></row><row><cell></cell><cell></cell><cell>960?540</cell><cell>4K</cell><cell></cell><cell>6.78</cell><cell>1.734</cell><cell>3.76</cell><cell></cell><cell>3.74</cell><cell>29.61</cell><cell>7.92?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE V.</cell><cell cols="2">SYNTHESIS RESULTS ON FPGA</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Method 2019 [26]</cell><cell></cell><cell cols="2">Method 2017 [27]</cell><cell></cell><cell></cell><cell cols="2">Ours: WinoConv</cell><cell></cell></row><row><cell>Input</cell><cell></cell><cell>LUT-based</cell><cell></cell><cell></cell><cell></cell><cell>DSP-based</cell><cell></cell><cell></cell><cell cols="2">LUT-based</cell><cell>Max</cell></row><row><cell>Size</cell><cell></cell><cell cols="2">Direct Convolution</cell><cell></cell><cell cols="2">Direct Convolution</cell><cell></cell><cell></cell><cell cols="2">Winograd Convolution</cell><cell>FLOPs</cell></row><row><cell></cell><cell>FF</cell><cell>LUT</cell><cell>Latency</cell><cell>DSP</cell><cell>FF</cell><cell>LUT</cell><cell>Latency</cell><cell>FF</cell><cell>LUT</cell><cell>Latency</cell><cell>(T)</cell></row><row><cell>4?4</cell><cell>191</cell><cell>493</cell><cell>39</cell><cell>2</cell><cell>383</cell><cell>658</cell><cell>11</cell><cell>343</cell><cell>827</cell><cell>6</cell><cell>2.839</cell></row><row><cell>5?5</cell><cell>243</cell><cell>635</cell><cell>85</cell><cell>3</cell><cell>681</cell><cell>2055</cell><cell>22</cell><cell>1138</cell><cell>2682</cell><cell>10</cell><cell>0.821</cell></row><row><cell>6?6</cell><cell>253</cell><cell>666</cell><cell>148</cell><cell>4</cell><cell>853</cell><cell>2888</cell><cell>31</cell><cell>1794</cell><cell>4242</cell><cell>12</cell><cell>0.623</cell></row><row><cell>7?7</cell><cell>233</cell><cell>654</cell><cell>229</cell><cell>5</cell><cell>1150</cell><cell>3834</cell><cell>48</cell><cell>5111</cell><cell>10214</cell><cell>16</cell><cell>0.264</cell></row><row><cell>8?8</cell><cell>233</cell><cell>515</cell><cell>328</cell><cell>6</cell><cell>1239</cell><cell>4862</cell><cell>60</cell><cell>8055</cell><cell>16499</cell><cell>17</cell><cell>0.201</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Learning for Image Superresolution: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deep Learning for Single Image Super-Resolution: A Brief Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Xue</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Video Super-Resolution via Deep Draft-Ensemble Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning for video super-resolution through HR optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="514" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz? R</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Frame-Recurrent Video Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Temporally coherent gans for video super-resolution (tecogan)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thuerey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09393</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep video super-resolution network using dynamic upsampling filters without explicit motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3224" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recurrent backprojection network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Accelerating the Super-Resolution Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FAST: A Framework to Accelerate Super-Resolution Processing on Compressed Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on New Trends in Image Restoration and Enhancement</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Video Super-Resolution With Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An FPGAoptimized architecture of anti-aliasing based super resolution for realtime HDTV to 4K-and 8K-UHD conversions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kusano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ikebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Motomura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on ReConFigurable Computing and FPGAs (ReConFig)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Super-resolution for 2K/8K television using wavelet-based image registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sakaida</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="378" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An Energy-Efficient FPGA-based Deconvolutional Neural Networks Accelerator for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Real-Time Convolutional Neural Network for Super-Resolution on FPGA with Applications to 4K UHD 60 fps Video Services</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yongwoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jae-Seok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Munchurl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits &amp; Systems for Video Technology</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Convolution in Caffe: a memo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explore Efficient LUT-based Architecture for Quantized Convolutional Neural Networks on FPGA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 28th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="232" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winograd</surname></persName>
		</author>
		<title level="m">Arithmetic complexity of computations</title>
		<imprint>
			<publisher>Siam</publisher>
			<date type="published" when="1980" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The perception-distortion tradeoff</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6228" to="6237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reconfigurable convolutional kernels for neural networks on FPGAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</title>
		<meeting>the 2019 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluating fast algorithms for convolutional neural networks on FPGAs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="101" to="108" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
