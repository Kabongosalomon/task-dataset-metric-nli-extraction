<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Cloud AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><forename type="middle">?</forename><surname>Ar?k</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Cloud AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Cloud AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Nested Hierarchical Transformer: Towards Accurate, Data-Efficient and Interpretable Visual Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hierarchical structures are popular in recent vision transformers, however, they require sophisticated designs and massive datasets to work well. In this paper, we explore the idea of nesting basic local transformers on non-overlapping image blocks and aggregating them in a hierarchical way. We find that the block aggregation function plays a critical role in enabling cross-block non-local information communication. This observation leads us to design a simplified architecture that requires minor code changes upon the original vision transformer. The benefits of the proposed judiciously-selected design are threefold: (1) NesT converges faster and requires much less training data to achieve good generalization on both ImageNet and small datasets like CIFAR; (2) when extending our key ideas to image generation, NesT leads to a strong decoder that is 8? faster than previous transformer-based generators; and (3) we show that decoupling the feature learning and abstraction processes via this nested hierarchy in our design enables constructing a novel method (named GradCAT) for visually interpreting the learned model. Source code is available https://github.com/google-research/nested-transformer. 6 7 8 9 3 2 4 5</p><p>1 Golden retriever Linear projection Block aggregation 6 7 3 8 4 9 5 Block aggregation Blocking image to patches [6, 7, 8, 9, ?] (#block, seqlen, d) (#block/4, seqlen, d) ? ? (#block/16, seqlen, d) Pseudo code: NesT # embed and block image to (#block,seqlen,d) x = Block(PatchEmbed(input_image)) for i in range(num_hierarchy): # apply transformer layers T_i within each block # with positional encodings (PE) y = Stack([T_i(x[0] + PE_i[0]), ...]) if i &lt; num_hierarchy -1: # aggregate blocks and reduce #block by 4 x = Aggregate(y, i) h = GlobalAvgPool(x) # (1,seqlen,d) to (1,1,d) logits = Linear(h[0,0]) # (num_classes,) def Aggregate(x, i): z = UnBlock(x) # unblock seqs to (h,w,d) z = ConvNormMaxPool_i(x) # (h/2,w/2,d) return Block(z) # block to seqs</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Vision Transformer (ViT) ) model and its variants have received significant interests recently due to their superior performance on many core visual applications <ref type="bibr" target="#b12">(Cordonnier, Loukas, and Jaggi 2020;</ref><ref type="bibr" target="#b36">Liu et al. 2021)</ref>. ViT first splits an input image into patches, and then patches are treated in the same way as tokens in NLP applications. Following, several self-attention layers are used to conduct global information communication to extract features for classification. Recent work <ref type="bibr" target="#b12">Cordonnier, Loukas, and Jaggi 2020)</ref> shows that ViT models can achieve better accuracy than state-of-the-art convnets <ref type="bibr" target="#b51">(Tan and Le 2019;</ref><ref type="bibr" target="#b23">He et al. 2016</ref>) when trained on datasets with tens or hundreds of millions of labeled samples. However, when trained on smaller datasets, ViT usually underperforms its counterparts based on convolutional layers. Addressing this data inefficiency is important to make ViT applicable to other application scenarios, e.g. semi-supervised learning <ref type="bibr" target="#b48">(Sohn et al. 2020</ref>) and generative modeling <ref type="bibr">(Goodfellow et al. 2014;</ref>.</p><p>Lack of inductive bias such as locality and translation equivariance, is one explanation for the data inefficiency of ViT models. <ref type="bibr" target="#b12">Cordonnier, Loukas, and Jaggi (2020)</ref> discovered that transformer models learn locality behaviors in a deformable convolution manner <ref type="bibr" target="#b14">(Dai et al. 2017)</ref>: bottom layers attend locally to the surrounding pixels and top layers favor long-range dependency. On the other hand, global selfattention between pixel pairs in high-resolution images is computationally expensive. Reducing the self-attention range is one way to make the model training more computationally efficient <ref type="bibr" target="#b4">(Beltagy, Peters, and Cohan 2020)</ref>. These type of insights align with the recent structures with local self-attention and hierarchical transformer <ref type="bibr" target="#b21">(Han et al. 2021;</ref><ref type="bibr" target="#b54">Vaswani et al. 2021;</ref><ref type="bibr" target="#b36">Liu et al. 2021</ref>). Instead of holistic global self-attention, these perform attention on local image patches. To promote information communication across patches, they propose specialized designs such as the "haloing operation"  and "shifted window" ). These are based on modifying the self-attention mechanism and often yields in complex architectures. Our design goal on the other hand keeping the attention as is, and introducing the design of the aggregation function, to improve the accuracy and data efficiency, while bringing interpretability benefits. The proposed NesT model stacks canonical transformer blocks to process non-overlapping image blocks individually. Cross-block self-attention is achieved by nesting these transformers hierarchically and connecting them with a proposed aggregation function. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the overall architecture and the simple pseudo code to generate it. Our contributions can be summarized as:</p><p>1. We demonstrate integrating hierarchically nested transformers with the proposed block aggregation function can outperform previous sophisticated (local) selfattention variants, leading to a substantially-simplified architecture and improved data efficiency. This provides a novel perspective for achieving effective cross-block communication.  Each node T_i processes an image block. The block aggregation is performed between hierarchies (num_hierarchy= 3 here) to achieve cross-block communication on the image (feature map) plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">NesT achieves impressive</head><p>architectures. E.g., training a NesT with 6M parameters using a single GPU results in 96% accuracy on CIFAR10 .</p><p>3. We show that when extending this idea beyond classification to image generation, NesT can be repurposed into a strong decoder that achieves better performance than convolutional architectures meanwhile has comparable speed, demonstrated by 64 ? 64 ImageNet generation, which is an important to be able to adopt transformers for efficient generative modeling.</p><p>4. Our proposed architectural design leads to decoupled feature learning and abstraction, which has significant interpretability benefits. To this end, we propose a novel method called GradCAT to interpret NesT reasoning process by traversing its tree-like structure. This providing a new type of visual interpretability that explains how aggregated local transformers selectively process local visual cues from semantic image patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Vision transformer-based models <ref type="bibr" target="#b12">(Cordonnier, Loukas, and Jaggi 2020;</ref><ref type="bibr" target="#b16">Dosovitskiy et al. 2021</ref>) and self-attention mechanisms <ref type="bibr" target="#b43">Ramachandran et al. 2019)</ref> have recently attracted significant interest in the research community, with explorations of more suitable architectural designs that can learn visual representation effectively, such as injecting convolutional layers <ref type="bibr" target="#b49">Srinivas et al. 2021;</ref><ref type="bibr" target="#b59">Yuan et al. 2021)</ref> and building local or hierarchical structures <ref type="bibr" target="#b66">(Zhang et al. 2021b;</ref><ref type="bibr" target="#b57">Wang et al. 2021b</ref>). Existing methods focus on designing a variety of self-attention modifications. Hierarchical ViT structures becomes popular both in vision <ref type="bibr" target="#b54">Vaswani et al. 2021</ref>) and NLP <ref type="bibr" target="#b67">(Zhang, Wei, and Zhou 2019;</ref><ref type="bibr" target="#b45">Santra, Anusha, and Goyal 2020;</ref><ref type="bibr" target="#b35">Liu and Lapata 2019;</ref><ref type="bibr" target="#b40">Pappagari et al. 2019)</ref>. However, many methods often add significant architectural complexity in order to optimize accuracy. One challenge for vision transformer-based models is data efficiency. Although the original ViT  can perform better than convolutional networks with hundreds of millions images for pre-training, such a data requirement is not always practical. Data-efficient ViT (DeiT) <ref type="bibr" target="#b52">(Touvron et al. 2020</ref><ref type="bibr" target="#b53">(Touvron et al. , 2021</ref> attempts to address this problem by introducing teacher distillation from a convolutional network. Although promising, this increases the supervised training complexity, and existing reported performance on data efficient benchmarks <ref type="bibr" target="#b22">(Hassani et al. 2021;</ref><ref type="bibr" target="#b9">Chen et al. 2021</ref>) still significantly underperforms convolutional networks. Since ViT has shown to improve vision tasks beyond image classification, with prior work studying its applicability to generative modeling <ref type="bibr" target="#b41">(Parmar et al. 2018;</ref><ref type="bibr" target="#b10">Child et al. 2019;</ref><ref type="bibr" target="#b28">Jiang, Chang, and Wang 2021;</ref><ref type="bibr" target="#b27">Hudson and Zitnick 2021)</ref>, video understanding <ref type="bibr" target="#b39">(Neimark et al. 2021;</ref><ref type="bibr" target="#b1">Akbari et al. 2021)</ref>, segmentation and detection <ref type="bibr" target="#b56">(Wang et al. 2021a;</ref><ref type="bibr" target="#b34">Liang et al. 2020;</ref><ref type="bibr" target="#b30">Kim et al. 2021)</ref>, interpretability <ref type="bibr" target="#b6">(Chefer, Gur, and Wolf 2021;</ref><ref type="bibr" target="#b0">Abnar and Zuidema 2020)</ref>, a deeper understanding of the data efficiency and training difficulties from the architectural perspective is of significant impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>Main Architecture According to <ref type="figure" target="#fig_0">Fig. 1</ref>, our overall design stacks canonical transformer layers to conduct local self-attention on every image block independently, and then nests them hierarchically. Coupling of processed information between spatially adjacent blocks is achieved through a proposed block aggregation between every two hierarchies. The overall hierarchical structure can be determined by two key hyper-parameters: patch size S ? S and number of block hierarchies T d . All blocks inside each hierarchy share one set of parameters.</p><p>Given an input of image with shape H ?W ?3, each image patch with size S ? S is linearly projected to an embedding in R d . Then, all embeddings are partitioned to blocks and flattened to generate input X ? R b?Tn?n?d , where b is the batch size, T n is the total number of blocks at bottom of the  <ref type="figure">Figure 2</ref>: Example results of the proposed GradCAT. Given the left input image (containing four objects), the figure visualizes the top-4 class traversal results (4 colors) using an ImageNet-trained NesT (with three tree hierarchies). Each tree node denotes the averaged activation value (? l defined in Algorithm 1). The traversals can correctly find the model decision path along the tree to locate an image patch belonging to the objects of given target classes.</p><p>NesT hierarchy, and n is the sequence length (the number of embeddings) at each block. Note that T n ? n = H ? W/S 2 . Inside each block, we stack a number of canonical transformer layers, where each is composed of a multi-head selfattention (MSA) layer followed by a feed-forward fullyconnected network (FFN) with skip-connection <ref type="bibr" target="#b23">(He et al. 2016)</ref> and Layer normalization (LN) <ref type="bibr" target="#b2">(Ba, Kiros, and Hinton 2016)</ref>. Trainable positional embedding vectors <ref type="bibr" target="#b52">(Touvron et al. 2020</ref>) are added to all sequence vectors in R d to encode spatial information before feeding into the block function T :</p><formula xml:id="formula_0">multiple ? y = x + MSA NesT (x , x , x ), x = LN(x) x = y + FFN(LN(y))<label>(1)</label></formula><p>The FFN is composed of two layers: max(0, xW 1 +b)W 2 +b. Given input X ? R b?Tn?n?d , since all blocks at one NesT hierarchy share the same parameters, MSA NesT basically MSA is applied <ref type="bibr" target="#b55">(Vaswani et al. 2017)</ref> to all blocks in parallel:</p><formula xml:id="formula_1">MSA NesT (Q, K, V ) = Stack(block 1 , ..., block Tn ), where block i = MSA(Q, K, V )W O .<label>(2)</label></formula><p>block i has shape b ? n ? d. Lastly, we build a nested hierarchy with block aggregation -every four spatially connected blocks are merged into one. The overall design makes NesT easy to implement, requiring minor code changes to the original ViT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block Aggregation</head><p>From a high-level view, NesT leads to hierarchical representations, which share similarity with several pyramid designs <ref type="bibr" target="#b66">(Zhang et al. 2021b;</ref><ref type="bibr" target="#b57">Wang et al. 2021b</ref>). However, most of these works use global self-attention throughout the layers, interleaved with (spatial) down-sampling. In contrast, we show that NesT, which leverages local attention, can lead to significantly improved data efficiency. In local self-attention, non-local communication is important to maintain translational equivariance . To this end, Halonet  allows the query to attend to slightly larger regions than the assigned block. Swin Transformer ) achieves this by shifting the block partition windows between consecutive self-attention layers to connect adjacent blocks; applying special masked self-attention to guarantee spatial continuity. However, both add complexity to the self-attention layers and such sophisticated architectures are not desired from implementation perspective. On the other hand, every block in NesT processes information independently via standard transformer layers, and only communicate and mix global information during the block aggregation step via simple spatial operations (e.g. convolution and pooling). One key ingredient of block aggregation is to perform it in the image plane so that information can be exchanged between nearby blocks. This procedure is summarized in <ref type="figure" target="#fig_0">Fig. 1</ref>. The output X l ? R b?#block?n?d at hierarchy l is unblocked to the full image plane A l ? R b?H ?W ?d . A number of spatial operations are applied to down-sample feature maps A l ? R b?H /2?W /2?d . Finally, the feature maps are blocked back to X l+1 ? R b?#block/4?n?d for hierarchy l + 1. The sequence length n always remains the same and the total number of blocks is reduced by a factor of 4, until reduced to 1 at the top (i.e. #block/4 (T d ?1) = 1). Therefore, this process naturally creates hierarchically nested structure where the "receptive field" expands gradually. d ? d depends on the specific model configuration.</p><p>Our block aggregation is specially instantiated as a 3 ? 3 convolution followed by LN and a 3 ? 3 max pooling. <ref type="figure">Figure  A2</ref> in Appendix explains the core design and the importance of applying it on the image plane (i.e. full image feature maps) versus the block plane (i.e. partial feature maps corresponding to 2 ? 2 blocks that will be merged). The small information exchange through the small convolution and max. pooling kernels across block boundaries are particularly important. We conduct comprehensive ablation studies to demonstrate the importance of each of the design components.</p><p>Note that the resulting design shares some similarities with recent works that combine transformer and convolutional networks <ref type="bibr" target="#b59">Yuan et al. 2021;</ref><ref type="bibr" target="#b3">Bello 2021)</ref> as specialized hybrid structures. However, unlike these, our proposed method aims to solve cross-block communications in local self-attention, and the resulting architecture is simple as a stacking of basic transformer layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transposed NesT for Image Generation</head><p>The data efficiency and straightforward implementation of NesT makes it desirable for more complex learning tasks. With transpose the key ideas from NesT to propose a decoder for generative modeling, and show that it has better performance than convolutional decoders with comparable speed. Remarkably, it is nearly a magnitude faster than the transformer-based decoder TransGAN <ref type="bibr" target="#b28">(Jiang, Chang, and Wang 2021)</ref>.</p><p>Creating such a generator is straightforward by transposing NesT (see <ref type="table" target="#tab_15">Table A6</ref> of Appendix for architecture details). The input of the model becomes a noise vector and the output is a full-sized image. To support the gradually increased number of blocks, the only modification to NesT is replacing the block aggregation with appropriate block de-aggregation, i.e. up-sampling feature maps (we use pixel shuffle <ref type="bibr" target="#b47">(Shi et al. 2016)</ref>). The feature dimensions in all hi- <ref type="bibr" target="#b4">3)</ref>. The number of blocks increases by a factor of 4. Lastly, we can unblock the output sequence tensor to an image with shape H ? W ? 3. The remaining adversarial training techniques are based on <ref type="bibr">(Goodfellow et al. 2014;</ref> as explained in experiments. Analogous to our results for image classification, we show the importance of careful block de-aggregation design, in making the model significantly faster while achieving better generation quality.</p><formula xml:id="formula_2">erarchies are (b, nd) ? (b, 1, n, d) ? (b, 4, n, d ), ..., ? (b, #blocks, n,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GradCAT: Interpretability via Tree Traversal</head><p>Different from previous work, the nested hierarchy with the independent block process in NesT resembles a decision tree in which each block is encouraged to learn non-overlapping features and be selected by the block aggregation. This unique behavior motivates us to explore a new method to explain the model reasoning, which is an important topic with significant real world impact in convnets <ref type="bibr" target="#b46">(Selvaraju et al. 2017;</ref><ref type="bibr" target="#b50">Sundararajan, Taly, and Yan 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: GradGAT</head><p>Define: A l denotes the feature maps at hierarchy l. Yc is the logit of predicted class c.</p><p>[?]2?2 indexes one of 2 ? 2 partitions of input maps.</p><formula xml:id="formula_3">Input: {A l |l = 2, ..., T d }, ?T d = AT d , P = [] Output: The traversal path P from top to bottom for l = [T d , ..., 2] do h l = ? l ? (? ?Yc ? l ) # obtain target activation map? h l = AvgPool 2?2 (h l ) ? R 2?2 n * l = arg max? l , P = P + [n * l ] # pick the maximum index ? l = A l [n * l ]2?2</formula><p># obtain the partition for the index end for</p><p>We present a gradient-based class-aware tree-traversal (GradCAT) method (Algorithm 1). The main idea is to find the most valuable traversal from a child node to the root node that contributes to the classification logits the most. Intuitively, at the top hierarchy, each of four child nodes processes one of 2?2 non-overlapping partitions of feature maps A T d . We can use corresponding activation and class-specific gradient features to trace the high-value information flow recursively from the root to a leaf node. The negative gradient ? ?Yc A l provides the gradient ascent direction to maximize the class c logit, i.e., a higher positive value means higher importance. <ref type="figure">Fig. 2</ref> illustrates a sample result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We first show the benefit of NesT for data efficient learning and then demonstrate benefits for interpretability and generative modeling. Finally, we present ablation studies to analyze the major constituents of the methods. Experimental setup. We follow previous work  to generate three architectures that have comparable capacity (in number of parameters and FLOPS), noted as tiny (NesT-T), small (NesT-S), and base (NesT-B). Most recent ViT-based methods follow the training techniques of DeiT <ref type="bibr" target="#b52">(Touvron et al. 2020)</ref>. We follow the settings with minor modifications that we find useful for local self-attention (see Appendix for all architecture and training details). We do not explore the specific per-block configurations (e.g. number of heads and hidden dimensions), which we believe can be optimized through architecture search <ref type="bibr" target="#b51">(Tan and Le 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons to Previous Work</head><p>CIFAR. We compare NesT to recent methods on CIFAR datasets <ref type="bibr" target="#b32">(Krizhevsky, Hinton et al. 2009</ref>) in <ref type="table" target="#tab_2">Table 1</ref>, to investigate the data efficiency. It is known that transformer-based methods usually perform poorly on such tasks as they typically require large datasets to be trained on. The models that perform well on large-scale ImageNet do not necessary work perform on small-scale CIFAR, as the full self-attention based models require larger training datasets. DeiT <ref type="bibr" target="#b52">(Touvron et al. 2020</ref>) performs poorly and does not improve given  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Interpretability</head><p>GradGAT results. <ref type="figure">Fig. 3 (left)</ref> shows the explanations obtained with the proposed GradGAT. For GradGAT, each tree node corresponds to a value that reflects the mean activation strength. Visualizing the tree traversal through image blocks, we can get insights about the decision making process of NesT. The traversal passes through the path with the highest values. As can be seen, the decision path can correctly locate the object corresponding to the model prediction. The Lighter example is particularly interesting because the ground truth class -lighter/matchstick -actually defines the bottom-right matchstick object, while the most salient visual features (with the highest node values) are actually from the upper-left red light, which conceptually shares visual cues with a lighter. Thus, although the visual cue is a mistake, the output prediction is correct. This example reveals the potential of using GradGAT to conduct model diagnosis at different tree hierarchies.  <ref type="bibr" target="#b70">(Zhou et al. 2016)</ref>. We follow <ref type="bibr" target="#b17">(Gildenblat 2021)</ref> in using an improved version of Rollout. NesT with standard CAM, outperforms others that are specifically designed for this task. <ref type="figure">Fig. 4</ref> shows a qualitative comparison (details in the Appendix), exemplifying that NesT can generate clearer attention maps which converge better on objects.</p><p>Overall, decoupling local self-attention (transformer blocks) and global information selection (block aggregation), which is unique to our work, shows significant potential for making models easier to interpret.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Modeling with Transposed NesT</head><p>We evaluate the generative ability of Transposed NesT on ImageNet <ref type="bibr" target="#b44">(Russakovsky et al. 2015)</ref> where all images are resized to 64 ? 64 resolution. We focus on the unconditional image generation setting to test the effectiveness of different decoders. We compare Transposed NesT to TransGAN <ref type="bibr" target="#b28">(Jiang, Chang, and Wang 2021)</ref>, that uses a full Transformer as the generator, as well as a convolutional baseline following the  widely-used architecture from ) (its computationally expensive self-attention module is removed). <ref type="figure">Fig. 5</ref> shows the results. Transposed NesT obtains significantly faster convergence and achieves the best FID and Inception score (see <ref type="figure" target="#fig_3">Fig. A6</ref> of Appendix for results). Most importantly, it achieves 8? throughput over TransGAN, showing its potential for significantly improving the efficiency of transformer-based generative modeling. More details are explained in the Appendix.</p><p>It is noticeable from <ref type="figure">Fig. 5</ref> (middle) that appropriate unsampling (or block de-aggregation) impacts the generation quality. Pixel shuffle <ref type="bibr" target="#b47">(Shi et al. 2016)</ref> works the best and the margin is considered surprisingly large compared to other alternatives widely-used in convnets. This aligns with our main findings in classification, suggesting that judiciously injecting spatial operations is important for nested local transformers to perform well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>We summarize key ablations below (more in Appendix).</p><p>Fast convergence. NesT achieves fast convergence, as shown in <ref type="figure" target="#fig_3">Fig. 6 (top)</ref> for Imagenet training with {30, 60, 100, 300} epochs. NesT-B merely loses 1.5% when reducing the training epoch from 300 to 100. The results suggest that NesT can learn effective visual features faster and more efficiently. Less sensitivity to data augmentation. NesT uses several kinds of data augmentation following <ref type="bibr" target="#b52">(Touvron et al. 2020)</ref>. As shown in <ref type="figure" target="#fig_3">Fig. 6 (right)</ref> and <ref type="figure">Fig. A4</ref>, our method shows higher stability in data augmentation ablation studies compared to DeiT. Data augmentation is critical for global selfattention to generalize well, but reduced dependence on domain or task dependent data augmentation helps with generalization to other tasks. Impact of block aggregation. Here we show that the design of block aggregation is critical for performance and data efficiency. We study this from four perspectives: <ref type="bibr" target="#b22">(1)</ref> whether unblocking to the full image plane is necessary;</p><p>(2) how to use convolution; <ref type="formula">(3)</ref>   The results show that: (1) when performing these spatial operations, it is important to apply it on the holistic image plane versus the block plane although both can reasonably introduce spatial priors; (2) small kernel convolution is sufficient and has to be applied ahead of pooling; (3) max. pooling is far better than other options, such as stride-2 sub-sampling and average pooling; (4) sub-sampling the query sequence length (similar to performing sub-sampling on the block plane as illustrated in <ref type="figure">Fig. A2</ref>), as used by Halonet , performs poorly on data efficient benchmarks. We also experiment PatchMerge from Swin Transformer ) on both CIFAR and ImageNet. Our block aggregation closes the accuracy gap on ImageNet, suggesting that a conceptually negligible difference in aggregating nested transformers can lead to significant differences in model performance.  <ref type="figure">Figure 7</ref>: Demonstration of the impact of block aggregation on CIFAR and ImageNet. NesT-T is used. Conv3x3 has stride 2. AvgPool3x3 on ImageNet is followed by Conv1x1 to change hidden dimensions of self-attention. Four plausible block aggregation designs are shown in x-axis, and applied on the image plane and block plane both for comparison. Note that Ours in x-axis is Conv3x3 followed by LN and MaxPool3x3 (stride 2). More alternatives are validated in <ref type="figure">Fig. A3</ref> of Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We have shown that aggregating nested transformers can match the accuracy of previous more complex methods with significantly improved data efficiency and convergence speed. In addition, we have shown that this idea can be extended to image generation, where it provides significant speed gains. Finally, we have shown that the decoupled feature learning and feature information extraction in this nested hierarchy design allows for better feature interpretability through a new gradient-based class-aware tree traversal method. In future work we plan to generalize this idea to non-image domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NesT Architecture and Training Details</head><p>Architecture configuration. The focus on this paper is how to aggregating nested transformers and its extended usage. We do not focus on the specific per-block hyper-parameters (e.g. number of heads and number of MSA layers). We mainly follow previous work to obtain right architectures that has similar capacity (e.g. number of parameters and throughput).</p><p>Recall that the overall hierarchy can be determined by two key hyper-parameters: patch size S ? S and hierarchy depth T d . Just like how ResNet <ref type="bibr" target="#b23">(He et al. 2016)</ref> adapts to small and large input sizes, NesT also has different configuration for small input size and large input size. We follow <ref type="bibr" target="#b52">(Touvron et al. 2020;</ref><ref type="bibr" target="#b36">Liu et al. 2021)</ref> to configure the number of head, hidden dimensions for the tiny, small and base versions. For 32 ? 32 image size, we follow <ref type="bibr" target="#b52">(Touvron et al. 2020)</ref>. Specifically, we setup the same number of repeated MSA NesT per block hierarchy. In each hierarchy, the number of hidden dimensions and the number of heads are the same as well. For 224 ? 224 image size, we follow . Therefore, different hierarchy has a gradually increased number of head, hidden dimensions, and number of repeated MSA NesT layers. <ref type="table" target="#tab_2">Table A1</ref> specifies details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NesT Hierarchy Variants</head><p>We study flexible variants to understand how the hierarchical structure of NesT impacts accuracy. When increasing the hierarchy depth by one, every four blocks are splitted to process four image partitions (see <ref type="figure" target="#fig_0">Figure 1</ref> of the main paper). A deeper hierarchy structure makes each block focus on a narrower range of pixel information (i.e., shorter sequence length).</p><p>We test combinations with S = {1, 2} and depth={2, 3, 4, 5} on NesT-{T, S, B}. <ref type="figure" target="#fig_0">Figure A1</ref> compares different variants on two CIFAR datasets. Shallower NesT has clear accuracy drop (although the total number of self-attention layers are the same). It is because, when depth= 1, the model degenerates to a global self-attention method, such as ViT . Depth= 5 has marginal decrease, because the sequence length of all blocks is only n = 2 ? 2. Note that we denote NesT 4 -B, S = 1 as the base NesT with hierarchy depth 4 and word patch size 1 ? 1. We use the configuration NesT 4 , S = 1 as the default for the most CIFAR experiments (subscription is omitted sometimes). Data Augmentation. We apply the commonly used data augmentation and regularization techniques as <ref type="bibr" target="#b52">(Touvron et al. 2020)</ref>, which include a mixture of data augmentation (MixUp <ref type="bibr" target="#b62">(Zhang et al. 2018a</ref>), CutMix <ref type="bibr" target="#b60">(Yun et al. 2019)</ref>, RandAugment , RandomErasing <ref type="bibr" target="#b69">(Zhong et al. 2020)</ref>) and regularization like Stochastic Depth <ref type="bibr" target="#b26">(Huang et al. 2016)</ref>. Repeated augmentation <ref type="bibr" target="#b25">(Hoffer et al. 2020)</ref> in DeiT is not used. In addition, for ImageNet models, we also add color jittering similar to <ref type="bibr">(Chen et al. 2020a,b)</ref> which seems to reduce dependency on local texture cues and slightly improves generalization (?0.3% on ImageNet).</p><p>Training details. We use a base learning rate 2.5e-6 per device. We use the AdamW optimizer <ref type="bibr" target="#b37">(Loshchilov and Hutter 2018)</ref> and set the weight decay 0.05. The warm-up epoch is 20. The initial learning rate is linearly scaled by a factor of total_batch_size/256. For ImageNet training, the total batch size can be 1024 or 2048 when using distributed training on the TPU hardware. We use [0.2, 0.3, 0.5] stochastic death rates for NesT-T, NesT-S, and NesT-B models, respectively. All transformer layer weights and block aggregation weights are initialized using truncated normal distribution.</p><p>We use a 0.1 stochastic depth drop rate for all CIFAR models and the warmup is 5 epoch. The CIFAR results of compared transformer based methods, besides CCT-7/3?1 <ref type="bibr" target="#b22">(Hassani et al. 2021)</ref>, in <ref type="table" target="#tab_2">Table 1</ref> of the main paper are trained by us. We train these models using their suggested hyperparameters and we find it works nearly optimal on CIFAR by searching from a set of learning rate and weight decay combinations. <ref type="figure">Figure A3</ref> shows detailed studies of different block aggregation functions to complete results in <ref type="figure">Figure 7</ref> of the main paper. Although many of them has tiny difference, it is interesting that the impact to performance is non-trivial. In addition, we find perform query down-sampling inside self attention makes transformers more difficult to train because the skip connection also needs proper down-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Block Aggregation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>Data augmentation sensitivity. NesT uses several kinds of data augmentation types following DeiT <ref type="bibr" target="#b52">(Touvron et al. 2020)</ref>. As shown in <ref type="figure">Figure A4</ref>, our method shows the high stability in augmentation ablation studies compared with DeiT. We speculate the underline reason is that learning effective vision cues is much easier in local attention than in global attention, so Swin Transformer also shows comparable stability. More comparison on ImageNet will be left as future work. Weak teacher distillation. We also explore the teacher distillation proposed by <ref type="bibr" target="#b52">(Touvron et al. 2020)</ref>, which suggests the inductive bias introduced by convnet teachers are helpful for ViT data efficiency. <ref type="table" target="#tab_12">Table A4</ref> provides detailed distillation study on CIFAR datasets. With such a weak teacher distillation, NesT-B is able to achieve 84.9% CIFAR100 accuracy with 300 epochs and even 86.1% with 1000 epoch training using 2 GPUs.</p><p>Convnet teacher distillation <ref type="bibr" target="#b52">(Touvron et al. 2020</ref>) is effective to further improve our method as well as DeiT. As explained in <ref type="bibr" target="#b52">(Touvron et al. 2020)</ref>, the inductive biases of convnets have positive implicit effects to the transformer training. Because data augmentation can improve teacher performance, we question the inductive biases brought by data augmentation is useful or not. Based on our experiments, it seems data augmentation negatively affect the effectiveness of teacher distillation. If the teacher and target model are both trained with strong augmentation, the performance decreases either for a small teacher or a big teacher. In other words, our study suggests that training a high accuracy teacher using strong augmentation negatively impact the distillation <ref type="table" target="#tab_2">Table A1</ref>: Architecture details of NesT. In each block, the structure is defined using the protocol <ref type="bibr">[d, h] ? a, b, where [d, h]</ref> refers to [hidden dimensions, number of heads]; a refers to the number of repeated transformer layers V in Equation 1 of the main paper; b refers to the total number of blocks in that hierarchy. Tiny, Small, and Base models have different setup and they are specified below. Note that once the hierarchy is fixed. The sequence length of all blocks are consistent. Configurations of each block for CIFAR and ImageNet are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input size</head><p>Seq. length</p><p>NesT Hierarchy (Froward direction is 5 to 1) <ref type="bibr">192,</ref><ref type="bibr">384,</ref><ref type="bibr">768]</ref> and h = <ref type="bibr" target="#b4">[3,</ref><ref type="bibr">6,</ref><ref type="bibr">12]</ref>  effectiveness. Future verification on ImageNet will be left for future work. Number of heads. We realize different architecture design uses different number of heads for MSA. We attempt to understand the effectiveness of the different configurations. We experiment number of head from 1 to 96 given a fixed d = 768 hidden dimension using NesT 4 -B. <ref type="table" target="#tab_14">Table A5</ref> shows CIFAR10 results on NesT. It is interesting find the number of heads affects less to the final performance.</p><formula xml:id="formula_4">1 2 3 4 5 32 ? 32 S = 1 d = [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generative Modeling</head><p>NesT can become a decoder with minor changes. For fair comparison in terms of model capacity, we configure NesT following the architecture design of TransGAN for image generation. <ref type="table" target="#tab_15">Table A6</ref> specifies the architectural details. The block aggregation layer is swapped to a block de-aggregation layer to achieve the gradually increased image size. Pixel shuffle (PS) <ref type="bibr" target="#b47">(Shi et al. 2016</ref>) is leveraged to increase the image size at block de-aggregation by a factor of two while the hidden dimension is reduced to a quarter of the input. We adopt the same discriminator architecture as <ref type="bibr" target="#b29">(Karras et al. 2020)</ref> where R1 gradient penalty <ref type="bibr" target="#b38">(Mescheder, Geiger, and Nowozin 2018)</ref> is applied during training. Adam <ref type="bibr" target="#b31">(Kingma and Ba 2014)</ref> is utilized for optimization with ? 1 = 0 and ? 2 = 0.99. The learning rate is 0.0001 for both the generator and discriminator with mini-batches of size 256. We use Fr?chet Inception Distance (FID) <ref type="bibr" target="#b24">(Heusel et al. 2017)</ref> for assessing image quality, which has been shown to be consistent with human judgments of realism <ref type="bibr" target="#b24">(Heusel et al. 2017;</ref><ref type="bibr" target="#b65">Zhang et al. 2020</ref><ref type="bibr" target="#b64">Zhang et al. , 2021a</ref>. Lower FID values indicate closer distances between synthetic and real data distributions. <ref type="figure" target="#fig_3">Figure A6</ref> shows the inception score of different compared methods on 64 ? 64 image generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Interpretabilty</head><p>GradGAT results. GradGAT always traverses from the root node to one of the leaf node. Since each node of the bottom layer only corresponds to a small non-overlapping patch of  the whole image, visualizing GradCAT is less meaningful when the targeting object is large and centered. We find it is true for the majority of ImageNet images although we find our results fairly promising for most ImageNet images that have small objects. Exploring more comprehensive studies on image datasets with non-centered objects is be left for future work. The proposed GradCAT is partially inspired by how Grad-CAM <ref type="bibr" target="#b46">(Selvaraju et al. 2017)</ref> in convnets uses gradient information to improve visual attention. Nevertheless, the actual detailed design and serving purposes are distinct. Class attention map results. <ref type="figure">Figure 4</ref> of the main paper compares the qualitative results of CAM, including Grad-CAM++ <ref type="bibr" target="#b5">(Chattopadhay et al. 2018)</ref> with ResNet50 <ref type="bibr" target="#b23">(He et al. 2016)</ref>, DeiT with Rollout attention <ref type="bibr" target="#b0">(Abnar and Zuidema 2020)</ref>, and our NesT CAM <ref type="bibr" target="#b70">(Zhou et al. 2016)</ref>. We follow <ref type="bibr" target="#b17">(Gildenblat 2021)</ref> to use an improved version of Rollout, which is better than the original version. When converting CAM generated by different methods to bounding boxes, the best threshold of each method varies. We search the best threshold [0, 1] using 0.05 as the interval to find the best number for each method on the ImageNet 50k validation set. It is promising to find that NesT CAM can outperform methods for this task and our baselines. We only use the single forward to obtain bounding boxes.  <ref type="figure">Figure A2</ref>: Illustration of block aggregation and a comparison when applying to the block plane versus on the image plane. Although both perform convolution and pooling spatially, performing block aggregation on the image plane allows information communication among blocks (different color palettes) that belong to different merged blocks at the upper hierarchy.   <ref type="figure">Figure A3</ref>: Study the impact of block aggregation on CIFAR and ImageNet. NesT-T is used. We study from different perspectives as explained in the text of the main paper. We verify ImageNet with NesT-T in the bottom-right figure using a subset of representative block aggregation options found on CIFAR datasets. Patch merge ) and 2x2 sub-sampling ) are used by previous methods. Since NesT for ImageNet has different hidden dimensions at different hierarchies, AVG3 on ImageNet is followed by a 1x1 convolution to map hidden dimensions. The chosen combinations are specified in x-axis. The leftmost x-axis point (C3-LN-MX3) of each figure is ours. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR10 accuracy</head><p>DeiT-S Swin-T NesT-S <ref type="figure">Figure A4</ref>: Data augmentation ablation studies on CIFAR10, either removing augmentation individually (middle) or removing (from left to right of x-axis) consecutively (right). None means all are used.  <ref type="figure">Figure A5</ref>: More output visualization of the proposed GradGAT.  <ref type="figure" target="#fig_3">Figure A6</ref>: Left: Inception score of 64 ? 64 ImageNet generation of different architectures. Right: Inception score with different un-pooling options. The models used to report the results are the same models in <ref type="figure">Figure 5</ref> of the main paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(Left) Illustration of NesT with nested transformer hierarchy; (right) the simple pseudo code to generate the architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. A5of Appendix shows more examples. Class attention map (CAM) results. In contrast to ViT) which uses class tokens, NesT uses global average pooling before softmax. This enables conveniently applying CAM-like<ref type="bibr" target="#b70">(Zhou et al. 2016</ref>) methods to interpret how well learned representations measure object features, as the activation coefficients can be directly without approximate algorithms.Fig. 3(right)shows quantitative evaluation of weakly-supervised object localization, which is a common evaluation metric for CAM-based methods<ref type="bibr" target="#b70">(Zhou et al. 2016</ref>), including GradCAM++ (Chattopadhay et al. 2018) with ResNet50 (He et al. 2016), DeiT with Rollout attention (Abnar and Zuidema 2020), and our NesT CAM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Left: Output visualization of the proposed GradGAT. Tree nodes annotate the averaged responses to the predicted class. We use a NesT-S with three tree hierarchies. Right: CAM-based weakly supervised localization comparison on the ImageNet validation set. ? indicates results obtained by us. Visualization of CAM-based attention results. All models are trained on ImageNet. CAM (vanilla) with NesT achieves accurate attention patterns on object regions, yielding finer attention to objects than DeiT Rollout<ref type="bibr" target="#b0">(Abnar and Zuidema 2020)</ref> and less noise than ResNet50 GradCAM++<ref type="bibr" target="#b5">(Chattopadhay et al. 2018</ref>).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Top: Training convergence. NesT achieves better performance than DeiT with the same total epoch of training (each point is a single run). Bottom: Data augmentation ablations. Results of DeiT-B (Touvron et al. 2020) are reported by its paper. NesT shows less reliance on data augmentation.self-attention.Fig. 7andFig. A3of Appendix compare the results of different plausible designs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ImageNet classification accuracy with a significantly simplified architectural design. E.g., training a NesT with 38M/68M parameters obtains 83.3%/83.8% ImageNet accuracy.The favorable data efficiency of NesT is embodied by its fast convergence, such as achieving 75.9%/82.3% training with 30/100 epochs. Moreover, NesT achieves matched accuracy on small datasets compared with popular convolutional arXiv:2105.12723v4 [cs.CV] 30 Dec 2021</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy on CIFAR with input size 32?32. The compared convolutional architectures are optimized models for CIFAR. All transformer-based architectures are trained from random initialization with the same data augmentation. DeiT uses S = 2. Swin and our NesT uses S = 1. means model tends to diverge.</figDesc><table><row><cell>Arch. base</cell><cell>Method</cell><cell cols="2">C10 (%) C100 (%)</cell></row><row><cell>Convolutional</cell><cell>Pyramid-164-48 WRN28-10</cell><cell>95.97 95.83</cell><cell>80.70 80.75</cell></row><row><cell></cell><cell>DeiT-T</cell><cell>88.39</cell><cell>67.52</cell></row><row><cell></cell><cell>DeiT-S</cell><cell>92.44</cell><cell>69.78</cell></row><row><cell>Transformer</cell><cell>DeiT-B</cell><cell>92.41</cell><cell>70.49</cell></row><row><cell>full-attention</cell><cell>PVT-T</cell><cell>90.51</cell><cell>69.62</cell></row><row><cell></cell><cell>PVT-S</cell><cell>92.34</cell><cell>69.79</cell></row><row><cell></cell><cell>PVT-B</cell><cell>85.05</cell><cell>43.78</cell></row><row><cell></cell><cell>CCT-7/3?1</cell><cell>94.72</cell><cell>76.67</cell></row><row><cell></cell><cell>Swin-T</cell><cell>94.46</cell><cell>78.07</cell></row><row><cell></cell><cell>Swin-S</cell><cell>94.17</cell><cell>77.01</cell></row><row><cell>Transformer</cell><cell>Swin-B</cell><cell>94.55</cell><cell>78.45</cell></row><row><cell>local-attention</cell><cell>NesT-T</cell><cell>96.04</cell><cell>78.69</cell></row><row><cell></cell><cell>NesT-S</cell><cell>96.97</cell><cell>81.70</cell></row><row><cell></cell><cell>NesT-B</cell><cell>97.20</cell><cell>82.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison on the ImageNet dataset. All models are trained from random initialization. ViT-B/16 uses an image size 384 and others use 224.</figDesc><table><row><cell>Arch. base</cell><cell>Method</cell><cell cols="2">#Params Top-1 acc. (%)</cell></row><row><cell></cell><cell>ResNet-50</cell><cell>25M</cell><cell>76.2</cell></row><row><cell>Convolutional</cell><cell>RegNetY-4G</cell><cell>21M</cell><cell>80.0</cell></row><row><cell></cell><cell>RegNetY-16G</cell><cell>84M</cell><cell>82.9</cell></row><row><cell>Transformer full-attention</cell><cell>ViT-B/16 DeiT-S DeiT-B</cell><cell>86M 22M 86M</cell><cell>77.9 79.8 81.8</cell></row><row><cell></cell><cell>Swin-T</cell><cell>29M</cell><cell>81.3</cell></row><row><cell></cell><cell>Swin-S</cell><cell>50M</cell><cell>83.0</cell></row><row><cell>Transformer</cell><cell>Swin-B</cell><cell>88M</cell><cell>83.3</cell></row><row><cell>local-attention</cell><cell>NesT-T</cell><cell>17M</cell><cell>81.5</cell></row><row><cell></cell><cell>NesT-S</cell><cell>38M</cell><cell>83.3</cell></row><row><cell></cell><cell>NesT-B</cell><cell>68M</cell><cell>83.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison on ImageNet benchmark with ImageNet-22K pre-training. bigger model size. PVT<ref type="bibr" target="#b57">(Wang et al. 2021b)</ref> has also a full self-attention based design, though with a pyramid structure. PVT-T seems to perform better than DeiT-T when model size is small, however, the performance largely drops and becomes unstable when scaling up, further suggesting that full self-attention at bottom layers is not desirable for data efficiency. Other transformer-based methods improve slowly with increasing model size, suggesting that bigger models are more challenging to train with less data. We attribute this to to their complex design (i.e. shifted windows with masked MSA) requiring larger training datasets, while NesT benefiting from a judiciously-designed block aggregation. We also include comparisons with convolutional architectures that are specifically optimized for small CIFAR images and show that NesT can give better accuracy without any small dataset specific architecture optimizations (while still being larger and slower, as they do not incorporate convolutional inductive biases). The learning capacity and performance of NesT get better with increased model size. Most variants of NesT inFig. A1of Appendix outperform compared methods with far better throughput. E.g., NesT 3 -T (S = 2) leads to 94.5% CIFAR10 accuracy with 5384 images/s throughout, 10? faster than the best compared result 94.6% accuracy. More details can be found in Appendix. ImageNet. We test NesT on standard ImageNet 2012 benchmarks<ref type="bibr" target="#b15">(Deng et al. 2009</ref>) with commonly used 300 epoch training on TPUs inTable 2. The input size is 224 ? 224 and no extra pre-training data is used. DeiT does not use teacher distillation, so it can be viewed as ViT with better data augmentation and regularization. ImageNet-22K. We scale up NesT to ImageNet-22K following the exact training schedules in<ref type="bibr" target="#b16">Dosovitskiy et al. 2021)</ref>. The pre-training is 90 epoch on 224?224 ImageNet21K images and finetuning is 30 epoch on 384?384 ImageNet images.Table 3compares the results. NesT again achieves competitive results, with a significantly more straightforward design.</figDesc><table><row><cell></cell><cell cols="3">ViT-B/16 Swin-B Nest-B</cell></row><row><cell>ImageNet Acc. (%)</cell><cell>84.0</cell><cell>86.0</cell><cell>86.2</cell></row></table><note>NesT matches the performance of prior work with a signif- icantly more straightforward design (e.g. NesT-S matches the accuracy of Swin-B, 83.3%). The results of NesT suggest that correctly aggregating the local transformer can improve the performance of local self-attention.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Comparison of NesT hierarchy variants with different depth, word size S ? S, and model size. The right table specifies the resulting sequence length given hierarchy depth and S combinations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">for model T, S, and B</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>8 ? 8 [d, h] ? 4, 1 [d, h] ? 4, 4</cell><cell>[d, h] ? 4, 16</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>4 ? 4 [d, h] ? 3, 1 [d, h] ? 3, 4</cell><cell cols="2">[d, h] ? 3, 16 [d, h] ? 3, 64</cell><cell>-</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>2 ? 2 [d, h] ? 2, 1 [d, h] ? 2, 4</cell><cell cols="4">[d, h] ? 2, 16 [d, h] ? 2, 64 [d, h] ? 2, 256</cell></row><row><cell></cell><cell cols="2">224 ? 224</cell><cell cols="5">d = [96, 96, 128], h = [3, 6, 12], and k = [8, 20, 20] for model T, S, and B</cell></row><row><cell></cell><cell>S = 4</cell><cell></cell><cell cols="2">14 ? 14 [d, h] ? 2, 1 [2d, 2h] ? 2, 4 [4d, 4h] ? k, 16</cell><cell>-</cell><cell>-</cell><cell></cell></row><row><cell>CIFAR100 Accuracy CIFAR10 Accuracy</cell><cell>76 78 80 82 94 95 96 97</cell><cell>200 200</cell><cell>500 500 Throughput (images / s) 1000 Depth=2 Depth=3 Depth=4 Depth=5 1000</cell><cell>3000 NesT-T, S=1 NesT-S, S=1 NesT-B, S=1 3000</cell><cell>NesT-T, S=2 NesT-S, S=2 NesT-B, S=2 6000 6000</cell><cell>Depth 2 3 4 5</cell><cell>Node seq. length n S = 1 S = 2 256 64 64 16 16 4 4 -</cell></row><row><cell cols="2">Figure A1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A2 :</head><label>A2</label><figDesc>Test accuracy on CIFAR with input size 32 ? 32. Compared convnets are optimized models for CIFAR. All transformer based models are trained from random initialization with the same data augmentation. The number of parameters (millions), GPU memory (MB), and inference throughput (images/s) on single GPU are compared. We minimize the word size S ? S for each transformer based method. DeiT uses S = 2. Swin and our NesT uses S = 1. means models tends to diverge.</figDesc><table><row><cell>Arch. base</cell><cell>Method</cell><cell>#Params</cell><cell cols="4">GPU Throughput C10 (%) C100 (%)</cell></row><row><cell>Convnet</cell><cell>Pyramid-164-48 (Han, Kim, and Kim 2017) WRN28-10 (Zagoruyko and Komodakis 2016)</cell><cell cols="2">1.7M 126M 36.5M 202M</cell><cell>3715.9 1510.8</cell><cell>95.97 95.83</cell><cell>80.70 80.75</cell></row><row><cell></cell><cell>DeiT-T (Touvron et al. 2020)</cell><cell cols="2">5.3M 158M</cell><cell>1905.3</cell><cell>88.39</cell><cell>67.52</cell></row><row><cell></cell><cell>DeiT-S (Touvron et al. 2020)</cell><cell cols="2">21.3M 356M</cell><cell>734.7</cell><cell>92.44</cell><cell>69.78</cell></row><row><cell>Transformer</cell><cell>DeiT-B (Touvron et al. 2020)</cell><cell cols="2">85.1M 873M</cell><cell>233.7</cell><cell>92.41</cell><cell>70.49</cell></row><row><cell>full-attention</cell><cell>PVT-T (Wang et al. 2021b)</cell><cell cols="2">12.8M 266M</cell><cell>1478.1</cell><cell>90.51</cell><cell>69.62</cell></row><row><cell></cell><cell>PVT-S (Wang et al. 2021b)</cell><cell cols="2">24.1M 477M</cell><cell>707.2</cell><cell>92.34</cell><cell>69.79</cell></row><row><cell></cell><cell>PVT-B (Wang et al. 2021b)</cell><cell cols="2">60.9M 990M</cell><cell>315.1</cell><cell>85.05</cell><cell>43.78</cell></row><row><cell></cell><cell>CCT-7/3?1 (Hassani et al. 2021)</cell><cell>3.7M</cell><cell>94M</cell><cell>3040.2</cell><cell>94.72</cell><cell>76.67</cell></row><row><cell></cell><cell>Swin-T (Liu et al. 2021)</cell><cell cols="2">27.5M 183M</cell><cell>2399.2</cell><cell>94.46</cell><cell>78.07</cell></row><row><cell></cell><cell>Swin-S (Liu et al. 2021)</cell><cell cols="2">48.8M 311M</cell><cell>1372.5</cell><cell>94.17</cell><cell>77.01</cell></row><row><cell>Transformer</cell><cell>Swin-B (Liu et al. 2021)</cell><cell cols="2">86.7M 497M</cell><cell>868.3</cell><cell>94.55</cell><cell>78.45</cell></row><row><cell>local-attention</cell><cell>NesT-T</cell><cell cols="2">6.2M 187M</cell><cell>1616.9</cell><cell>96.04</cell><cell>78.69</cell></row><row><cell></cell><cell>NesT-S</cell><cell cols="2">23.4M 411M</cell><cell>627.9</cell><cell>96.97</cell><cell>81.70</cell></row><row><cell></cell><cell>NesT-B</cell><cell cols="2">90.1M 984M</cell><cell>189.8</cell><cell>97.20</cell><cell>82.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A3 :</head><label>A3</label><figDesc>Comparison on the ImageNet benchmark. The number of parameters (millions), GFLOPS, and inference throughput (images/s) evaluated on a single GPU are also compared. All models are trained from random initialization without extra pre-training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A4 :</head><label>A4</label><figDesc>Teacher distillation studies on CIFAR datasets. Left: The top two rows of the table are teacher supervised accuracy on CIFAR100. The bottom two rows show accuracy using these trained teachers with standard or strong augmentation. Right: Teacher (PN-164-48 with standard augmentation) distillation effects on DeiT and the proposed NesT. DeiT and NesT are always trained with strong augmentation.</figDesc><table><row><cell cols="4">Teacher Target model Standard Strong</cell><cell>Distillation</cell><cell></cell></row><row><cell cols="2">--PN-164-270 PN-164-48</cell><cell>80.7 83.4</cell><cell>81.5 84.9</cell><cell>Dataset</cell><cell>C10 C100 C10 C100</cell></row><row><cell>PN-164-48 PN-164-270</cell><cell>NesT-B NesT-B</cell><cell>84.5 84.9</cell><cell>83.7 83.8</cell><cell>DeiT-B NesT-B</cell><cell>92.4 70.5 95.5 81.5 97.2 82.6 97.1 84.5</cell></row><row><cell>Notation of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>spatial operations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>C3: 3 ? 3 Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>LN: LayerNorm</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MX3: 3 ? 3 MaxPool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S2: 2 ? 2 sub-sampling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>AVG3: 3 ? 3 AvgPool</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>D4: 4 ? 1 1D Conv</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PM: Patch merge</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table A5 :</head><label>A5</label><figDesc>Study the impact of number of heads in MSA on the CIFAR10 dataset with NesT 4 -B. When #head=96, the hidden dimension used for computing Attention is only 8. However, it can still lead to similar accuracy. 96.85 96.92 97.07 97.21 97.01 97.03 97.08</figDesc><table><row><cell>#head in MSA</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell>24</cell><cell>48</cell><cell>96</cell></row><row><cell cols="2">Hidden dimension d 768</cell><cell>384</cell><cell>256</cell><cell>128</cell><cell>64</cell><cell>32</cell><cell>16</cell><cell>8</cell></row><row><cell>Accuracy</cell><cell cols="8">97.1 Tricycle Lighter</cell></row><row><cell></cell><cell></cell><cell cols="2">Safety pin</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Dough</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table A6 :</head><label>A6</label><figDesc>Architecture details of NesT as image generator. d = 1024 and h = 4. The input is a reshaped noise vector. At the last hierarchy, there are 64 image blocks. Since the sequence length is 8 ? 8, it is easy to see that the output image size is 64 ? 64. At hierarchy 1, the hidden dimension is 1024/64 = 16. Then a LayerNorm followed by Conv1x1 maps the hidden dimension to the output with shape 64 ? 64 ? 3.</figDesc><table><row><cell></cell><cell>15</cell><cell cols="4">Comparison of architectures</cell><cell>15.0</cell><cell>Comparison of block de-aggregration</cell></row><row><cell>Inception score</cell><cell>10 11 12 13 14</cell><cell cols="4">200 400 600 800 1000 Iterations (1000x) ConvNet TransGAN Transposed NesT Inception score</cell><cell>5.0 7.5 10.0 12.5</cell><cell>Iterations (1000x) 200 400 600 800 1000 PS-C3 C3-PS NN-C3 C3-NN (failed) PS (Ours)</cell></row><row><cell></cell><cell cols="2">Input size</cell><cell>Seq. length</cell><cell>1</cell><cell>NesT Hierarchy (Froward direction is 4 to 1) 2 3</cell><cell>4</cell></row><row><cell></cell><cell cols="5">1 ? n ? d 8 ? 8 [d/64, h] ? 2, 64 [d/16, h] ? 3, 16 [d/4, h] ? 3, 4 [d, h] ? 5, 1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide more experimental results below to complete the experimental sections of the main paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Quantifying attention flow in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00928.2</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178.2</idno>
	</analytic>
	<monogr>
		<title level="m">Audio and Text</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450.3</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Lambdanetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150.1</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Grad-cam++: Generalized gradientbased visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chattopadhay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno>WACV. 5</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15679</idno>
		<title level="m">Generic Attentionmodel Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno>ICML. 10</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semisupervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12533.2</idno>
		<title level="m">Visformer: The Vision-friendly Transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509.2</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
		<idno>CVPR. 6</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the relationship between self-attention and convolutional layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno>ICCV. 1</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno>CVPR. 5</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note>ICLR. 1, 2, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring Explainability for Vision Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gildenblat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Generative adversarial networks. NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep pyramidal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno>CVPR. 12</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<title level="m">Transformer in transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abuduweili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05704.2</idno>
		<title level="m">Escaping the Big Data Paradigm with Compact Transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. 1, 3, 5</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>NeurIPS. 11</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Augment Your Batch: Improving Generalization Through Instance Repetition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno>CVPR. 10</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>ECCV. 10</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.01209.2</idno>
		<title level="m">Generative Adversarial Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07074</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno>CVPR. 11</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">HOTR: End-to-End Human-Object Interaction Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>ICLR. 11</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707.2</idno>
		<title level="m">LocalViT: Bringing Locality to Vision Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Polytransform: Deep polygon transformer for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>CVPR. 2</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Hierarchical transformers for multi-document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13164.2</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Which training methods for GANs do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<idno>ICML. 11</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neimark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zohar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Asselmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.00719.2</idno>
		<title level="m">Video transformer network</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hierarchical transformers for long document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pappagari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zelasko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Carmiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="838" to="844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno>ICML. 2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno>CVPR. 12</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stand-alone self-attention in vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV. 5</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Hierarchical transformer for task oriented dialog systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Santra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anusha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08067.2</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>In CVPR. 4, 6</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno>NeurIPS. 1</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605.2</idno>
		<title level="m">Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<idno>ICML. 4</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239.2</idno>
		<title level="m">Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scaling Local Self-Attention For Parameter Efficient Visual Backbones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. NeurIPS. 3</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808.3</idno>
		<title level="m">Cvt: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<idno>ICCV. 10</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>BMVC. 12</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>ICLR. 10</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. 1, 4</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Cross-Modal Contrastive Learning for Text-to-Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baldridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno>CVPR. 11</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Consistency regularization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno>ICLR. 11</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15358</idno>
		<title level="m">Multi-Scale Vision Longformer: A New Vision Transformer for High-Resolution Image Encoding</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">HIBERT: Document level pre-training of hierarchical bidirectional transformers for document summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06566.2</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno>CVPR. 6</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno>AAAI. 10</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Arch. base Method Size #Params GFLOPS Throughput Top-1 acc</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Convnet ResNet</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page">224</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Regnety-4g</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
	<note>Radosavovic et al. 2020</note>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Regnety-16g</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
	<note>Radosavovic et al. 2020</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B ;</forename><surname>Vit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dosovitskiy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">384</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-T (</forename><surname>Swin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-S (</forename><surname>Swin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-B (</forename><surname>Swin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">224</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
