<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ERANNs: Efficient Residual Audio Neural Networks for Audio Pattern Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Verbitskiy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mechanics and Mathematics</orgName>
								<orgName type="institution">Novosibirsk State University</orgName>
								<address>
									<settlement>Novosibirsk</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Deepsound</orgName>
								<address>
									<settlement>Novosibirsk</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Berikov</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Data Analysis Laboratory</orgName>
								<orgName type="institution">Sobolev Institute of Mathematics SB RAS</orgName>
								<address>
									<settlement>Novosibirsk</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viacheslav</forename><surname>Vyshegorodtsev</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Deepsound</orgName>
								<address>
									<settlement>Novosibirsk</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ERANNs: Efficient Residual Audio Neural Networks for Audio Pattern Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>audio pattern recognition</term>
					<term>sound classification</term>
					<term>audio tagging</term>
					<term>residual convolutional neural networks</term>
					<term>transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio pattern recognition (APR) is an important research topic and can be applied to several fields related to our lives. Therefore, accurate and efficient APR systems need to be developed as they are useful in real applications. In this paper, we propose a new convolutional neural network (CNN) architecture and a method for improving the inference speed of CNN-based systems for APR tasks. Moreover, using the proposed method, we can improve the performance of our systems, as confirmed in experiments conducted on four audio datasets. In addition, we investigate the impact of data augmentation techniques and transfer learning on the performance of our systems. Our best system achieves a mean average precision (mAP) of 0.450 on the AudioSet dataset. Although this value is less than that of the state-of-the-art system, the proposed system is 7.1x faster and 9.7x smaller. On the ESC-50, UrbanSound8K, and RAVDESS datasets, we obtain state-of-the-art results with accuracies of 0.961, 0.908, and 0.748, respectively. Our system for the ESC-50 dataset is 1.7x faster and 2.3x smaller than the previous best system. For the RAVDESS dataset, our system is 3.3x smaller than the previous best system. We name our systems "Efficient Residual Audio Neural Networks".</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Studies on audio pattern recognition (APR) have been gradually increasing, and APR systems can be used in several fields that are related to our everyday lives. The primary research subtopics of APR are environmental sound classification <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, sound event detection <ref type="bibr" target="#b2">[3]</ref>, and audio tagging (predicting the presence or absence of sounds in audio signals) <ref type="bibr" target="#b3">[4]</ref>. In general, we are surrounded by environmental sounds; therefore, there are several applications of APR systems, e.g., smart room monitoring <ref type="bibr" target="#b4">[5]</ref> and video content highlight generation <ref type="bibr" target="#b5">[6]</ref>. Moreover, over the past decade, several studies have focused on musical genre classification <ref type="bibr" target="#b6">[7]</ref>. Therefore, APR systems can be used to analyze music preferences of users of media services to improve the recommendation process. Another application of APR systems is speech emotion classification <ref type="bibr" target="#b7">[8]</ref>. In addition, APR systems can be used in the medical field, e.g., to classify respiratory diseases <ref type="bibr" target="#b8">[9]</ref>. In this study, we cover three APR subtasks, for which our systems are evaluated: audio tagging (AudioSet <ref type="bibr" target="#b3">[4]</ref>), environmental sound classification (ESC-50 <ref type="bibr" target="#b0">[1]</ref> and UrbanSound8K <ref type="bibr" target="#b1">[2]</ref>), and speech emotion classification (RAVDESS <ref type="bibr" target="#b7">[8]</ref>).</p><p>The AudioSet dataset <ref type="bibr" target="#b3">[4]</ref>, which is the largest dataset containing two million audio recordings with 527 classes, is used to train and evaluate modern APR systems. Recently, several researchers conducted studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>] to obtain high performance on the aforementioned dataset. Despite a considerable amount of effort to make APR systems more accurate for AudioSet, the systems still have significantly lower performance compared with modern systems used for image recognition tasks on datasets that are of a similar size. For instance, the state-of-the-art model achieves a mean average precision (mAP) of 0.485 on AudioSet <ref type="bibr" target="#b9">[10]</ref>; however, previous image classification systems achieve a top-1 accuracy of 0.9 <ref type="bibr" target="#b13">[14]</ref> on ImageNet <ref type="bibr" target="#b14">[15]</ref>. Therefore, the APR field needs to be explored further to develop more accurate systems.</p><p>In addition to the performance of APR systems, we need to focus on the computational complexity of the systems. In recent years, several studies focused on techniques to reduce the computational complexity of neural networks for diverse tasks <ref type="bibr" target="#b15">[16]</ref>, including APR tasks <ref type="bibr" target="#b11">[12]</ref>, as it is important to implement systems for low-powered devices and reduce the use of computing resources in data centers. However, the best system for AudioSet <ref type="bibr" target="#b9">[10]</ref> has low efficiency, which makes it practically infeasible to use this system in real life. Thus, there is a need to develop APR systems, which are also compact and fast.</p><p>Considering the aforementioned issues, we suggest several techniques to improve the performance and efficiency of APR systems. First, we develop a new convolutional neural network (CNN) architecture with the widening factor <ref type="bibr" target="#b16">[17]</ref>, which is used to change the number of parameters and floating-point operations (FLOPs) of systems. Moreover, we propose a simple method to improve the inference efficiency of CNN-based systems for APR tasks. This method is based on an increase in the stride size (from 2 to 4) of few convolutional layers. Second, we describe and compare several data augmentation techniques for audio pattern recognition tasks, which are applied to improve the accuracy of systems on evaluation sets: the temporal cropping, SpecAugment <ref type="bibr" target="#b17">[18]</ref>, modified mixup <ref type="bibr" target="#b18">[19]</ref>, and pitch shifting <ref type="bibr" target="#b19">[20]</ref>. To estimate the impact of diverse techniques on the performance and computational complexity of APR systems, we conduct experiments on four benchmark audio datasets for tagging and classification tasks: AudioSet <ref type="bibr" target="#b3">[4]</ref>, ESC-50 <ref type="bibr" target="#b0">[1]</ref>, UrbanSound8K <ref type="bibr" target="#b1">[2]</ref>, and RAVDESS <ref type="bibr" target="#b7">[8]</ref>. In addition, transfer learning is applied to ESC-50, UrbanSound8K, and RAVDESS: we transfer our systems, which are pre-trained on AudioSet. Thus, we examine the impact of transfer learning on the performance of systems on small APR datasets. Our APR systems are known as "Efficient Residual Audio Neural Networks" (ERANNs).</p><p>Owing to the design and application of various techniques, ER-ANNs can obtain high results on all the datasets. The proposed system achieves an mAP of 0.450 on the AudioSet dataset. Although this value is less than that of the state-of-the-art system (0.485) <ref type="bibr" target="#b9">[10]</ref>, our system is 7.1x faster and has 9.7x fewer parameters. Moreover, the best system is pre-trained on ImageNet <ref type="bibr" target="#b14">[15]</ref>; however, our system is trained from scratch. On the ESC-50, UrbanSound8K, and RAVDESS datasets, we obtain state-of-the-art accuracies of 0.961, 0.908, and 0.748, respectively. Previous state-of-the-art systems achieve accuracies of 0.956 <ref type="bibr" target="#b9">[10]</ref>, 0.891 <ref type="bibr" target="#b12">[13]</ref>, and 0.721 <ref type="bibr" target="#b10">[11]</ref>, respectively. Our system for ESC-50 is 1.7x faster and 2.3x smaller than the previous best system. For RAVDESS, our system is 3.3x smaller than the previous best system. The remainder of this paper is organized as follows. Section 2 describes previous methods used for APR tasks. Section 3 presents the feature extraction process, data augmentation techniques, and the proposed CNN architecture. Section 4 provides detailed experimental results and a comparison of our APR systems with previous systems. Section 5 concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Traditional APR systems comprised classical generative or discriminative models, e.g., Gaussian mixture models (GMMs) <ref type="bibr" target="#b20">[21]</ref>, and used time-frequency representations, e.g., the log mel spectrogram or mel-frequency cepstral coefficients (MFCCs), as input. However, later, methods with neural networks, in particular with CNNs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13]</ref>, significantly outperformed conservative machine learning methods for APR tasks. Moreover, CNNbased systems outperformed recurrent neural network RNN-based systems, e.g., for sound event detection tasks <ref type="bibr" target="#b22">[23]</ref>. Nowadays, CNN-based systems are widely used for APR tasks.</p><p>Systems with transformers are gaining popularity for APR tasks. A transformer-based system achieves the state-of-the-art performance with an mAP of 0.485 <ref type="bibr" target="#b9">[10]</ref> on the AudioSet dataset. However, this system has high computational complexity (526.6 million parameters), which makes it difficult to apply this system in real life.</p><p>There are two main approaches for CNN-based systems. The first approach states to use models with 2D convolutional layers and time-frequency representations, e.g., the log mel spectrogram, as input to the first 2D convolutional layer <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref>. The second approach defines end-to-end systems <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>, where the raw audio signal is used as input. End-to-end systems, as a rule, comprise two parts <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19]</ref>: the first part includes 1D convolutional layers, and the second part includes 2D convolutional layers. The first part is applied to extract 2D features, which replace time-frequency representations. Despite the learnable extraction of time-frequency features in end-to-end systems, systems with the log mel spectrogram perform better on APR tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Moreover, there are end-to-end systems, which contain only 1D convolutional layers <ref type="bibr" target="#b23">[24]</ref>. In this article, we use the first approach and the log mel spectrogram as input to the first 2D convolutional layer. We compare the aforementioned approaches below.</p><p>Residual neural networks (ResNets) <ref type="bibr" target="#b24">[25]</ref> have shortcut connections among convolutional layers. Shortcut connections help partially avoid the vanishing gradient problem. Over the past years, the application of shortcut connections in CNN architectures became a widespread technique in the area of machine learning, including APR <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13]</ref>. WideResNets <ref type="bibr" target="#b16">[17]</ref> have an additional hyperparameter-the widening factor for the width of convolutional layers-to change the computational complexity of models. With optimal values of the widening factor, WideResNets have better performance and fewer parameters than original ResNets. The widen-ing factor was introduced in several CNN-based systems for simple scaling, for instance, for image recognition tasks <ref type="bibr" target="#b15">[16]</ref> (EfficientNet) and APR tasks <ref type="bibr" target="#b11">[12]</ref> (AemNet-DW). We also use shortcut connections and the widening factor in our CNN architecture.</p><p>Transfer learning is a widespread technique in computer vision tasks. A model is trained for tasks with a large dataset and transferred to similar tasks with smaller datasets: all parameters of the model for the new task are initialized from the pre-trained model, except parameters of few last layers. For APR tasks, transfer learning was used by other researchers <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref> (pretraining on AudioSet) that significantly improved accuracy on APR tasks with small datasets. In this article, we use fine-tuning as the transfer learning strategy (we optimize all parameters of transferred models). Models pre-trained on the AudioSet dataset are transferred to the other three APR tasks with small datasets: ESC-50, UrbanSound8K, and RAVDESS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED APR SYSTEM</head><p>This section describes the feature extraction process, data augmentation techniques, and the proposed CNN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Extraction</head><p>For each experiment, we use the log mel spectrogram as input to the first 2D convolutional layer of our models.</p><p>We use a sampling rate (sr) of 44.1 kHz for all audio signals. The short-time Fourier transform (STFT) with the Hann window of size 1380 (? 31 ms) and the hop size of H = 345 (? 8 ms) is applied to extract spectrograms. The number of time frames Ts is calculated as</p><formula xml:id="formula_0">Ts = sr ? t H + 1 = 44100 ? t 345 + 1,<label>(1)</label></formula><p>where t is the duration of an audio signal (in seconds). Integer t ? 5 yields a value of Ts = 128 ? t. For integer t &gt; 5, we pad the audio signal with zeros to obtain Ts = 128 ? t. If t is not an integer, we pad the audio signal with zeros to gain integer duration. Thus, Ts / 2 j ? N for j ? 7. This is a convenient way to avoid size mismatch in shortcut connections of models with different sets of stride sizes of convolutional layers for input audio signals with various lengths (in our models, the maximum product of stride sizes for the temporal dimension is 128). This substantiates the selection of H = 345. The size of the window is selected to ensure an overlap of 0.75. We adopt the number of mel bins M = 128. We suppose that M = 128 is the best choice as a trade-off between computational complexity and performance of models. In <ref type="bibr" target="#b10">[11]</ref>, the model with M = 128 performed better than the model with M = 64 on the AudioSet dataset. Large values of M can significantly decrease the efficiency of models.</p><p>Moreover, we determine the lower cut-off frequency fmin = 50 Hz to remove low-frequency noise and the upper cut-off frequency fmax = 14,000 Hz to remove the aliasing effect.</p><p>Thus, the log mel spectrogram of each audio recording is represented as a 2D tensor with a shape of 128 ? Ts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Augmentation Techniques</head><p>We use several data augmentation techniques to prevent models from overfitting during training and improve the performance of models on evaluation sets. All data augmentation techniques are applied during training on mini-batches of audio signals generated from original training sets but not before training to obtain more extensive training sets.</p><p>The first technique is the temporal cropping. We use tc-second sections of audio recordings during training of our models. Sections are cut from random places. While evaluating models, full audio recordings without the temporal cropping are used to obtain final predictions.</p><p>In our experiments, we use SpecAgment <ref type="bibr" target="#b17">[18]</ref> for frequency and time masking on spectrograms of training audio clips. This technique was used in previous studies for APR tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref>. We use SpecAugment with two time masks with a maximum length of 8 ? tc frames and two frequency masks with maximum length of 16 bins.</p><p>Moreover, we apply modified mixup <ref type="bibr" target="#b18">[19]</ref>, which considers the sound pressure level of two mixed audio signals. We use mixup with ? = 1.0 in all the experiments. In <ref type="bibr" target="#b18">[19]</ref>, models trained using modified mixup had better performance than models trained with standard mixup on the ESC-50 and UrbanSound8K datasets. Moreover, we show that modified mixup is better for the AudioSet dataset below. In previous studies, for AudioSet, only the standard mixup was used <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Pitch shifting <ref type="bibr" target="#b19">[20]</ref> is used for RAVDESS during training because this data augmentation technique is effective for speech, as described in the following. This data augmentation technique is applied with a probability of 0.5 for each audio signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">ERANNs Architecture</head><p>Each tensor between convolutional blocks has a shape of Fi ? Ti ? Ci, where Fi is "the i-th frequency size", Ti is "the i-th temporal size" and Ci is the number of channels. All input tensors after feature extraction have a shape of F0 ? T0 ? C0, where F0 = M = 128, T0 = Ts, and C0 = 1.</p><p>We modify sizes of kernel, stride, and padding for the temporal dimension in few convolutional layers to make models faster. We introduce "the decreasing temporal size parameter" sm into our architecture, and for sm &gt; 0, temporal sizes Ti are reduced stronger based on increased stride sizes of convolutional layers. Thus, increasing the value of sm, we reduce FLOPs of our models because sizes of tensors between convolutional layers have fewer sizes.</p><p>The proposed architecture is presented in <ref type="table" target="#tab_0">Table 1</ref>. </p><formula xml:id="formula_1">Extraction 128 ? T0 ? 1 Feature extractor BatchNorm Stage 0 128 ? T0 ? 8 ? W ARB(1, 1, 8 ? W ) ? 4 Stage 1 64 ? T1 ? 16 ? W ARB(2, s1, 16 ? W ) ? 1 ARB(1, 1, 16 ? W ) ? 3 Stage 2 32 ? T2 ? 32 ? W ARB(2, s2, 32 ? W ) ? 1 ARB(1, 1, 32 ? W ) ? 3 Stage 3 16 ? T3 ? 64 ? W ARB(2, s3, 64 ? W ) ? 1 ARB(1, 1, 64 ? W ) ? 3 Stage 4 8 ? T4 ? 128 ? W ARB(2, 2, 128 ? W ) ? 1 ARB(1, 1, 128 ? W ) ? 3 1 ? 1 ? 128 ? W Global pooling Stage 5 128 ? W FC1, Leaky ReLU N FC2, sigmoid / softmax</formula><p>The input of our ERANNs is a mini-batch with audio signals.</p><p>Data augmentation for training data is implemented for each minibatch before the feature extractor. SpecAugment is applied after the extraction of spectrograms.</p><p>In the table, W is the widening factor and si are stride sizes, which depend on the decreasing temporal size parameter sm that can have four values:</p><formula xml:id="formula_2">sm ? {0, ..., 3}.<label>(2)</label></formula><p>Stride sizes si for i = 1, ..., 3 for convolutional blocks ARB(2, si, c) can be calculated as</p><formula xml:id="formula_3">si = 2 i &gt; sm 4 i ? sm.<label>(3)</label></formula><p>Temporal sizes Ti for i = 0, ..., 3 are calculated as</p><formula xml:id="formula_4">T0 = Ts, Ti = T0 i j=1 sj , i ? 1.<label>(4)</label></formula><p>ARB(x, y, c) is "the Audio Residual Block", where x is the stride size for the frequency dimension, y is the stride size for the temporal dimension, and c is the number of output channels. Each ARB contains two or three 2D convolutional layers, two batch normalization layers, and two activation functions. The Audio Residual Block is the modified version of the basic block of ResNet-V2 <ref type="bibr" target="#b25">[26]</ref>. The ARB architecture is described in <ref type="figure" target="#fig_1">Fig. 1 (Table 2</ref>).   </p><formula xml:id="formula_5">BatchNorm1 ? ? ? Leaky ReLU ? ? ? Conv2D1 K1(x) ? K1(y) x ? y 1 ? 1 BatchNorm2 ? ? ? Leaky ReLU ? ? ? Conv2D2 K2(x) ? K2(y) 1 ? 1 P (x) ? P (y) Conv2DRes 1 ? 1 x ? y 0 ? 0. Conv2D 1 K 1 (x) x K 2 (y), c</formula><p>We do not use dimension reduction as in bottleneck blocks because <ref type="bibr" target="#b10">[11]</ref> stated that ResNet38 with basic blocks performs better than ResNet54 with bottleneck blocks on the AudioSet dataset.</p><p>The first ARB does not have BatchNorm and Leaky ReLU at the start. The batch normalization layer after the feature extractor is used as a replacement for data standardization (over the frequency dimension).</p><p>For global pooling, we use a sum of average and max pooling as in <ref type="bibr" target="#b10">[11]</ref> to combine their advantages. After global pooling, we use two fully connected layers, FC1 and FC2. We apply softmax for sound classification tasks and sigmoid for audio tagging tasks at the end to obtain predictions.</p><p>Leaky ReLU with parameter 0.01 is applied as a replacement for ReLU because the use of ReLU can lead to the dying ReLU problem <ref type="bibr" target="#b26">[27]</ref>.</p><p>The proposed architecture of ERANNs does not depend on the duration of input audio signals caused by the presence of global pooling before the last fully connected layers. For instance, the system with fixed values of W , sm, and N can be trained with 8-second audio signals and applied to audio signals of any length. Audio signals are padded with zeros, as described in Section 3.1, where the number of zeros does not exceed 44,100 (1 second). FLOPs and the inference speed of the system with fixed values of hyperparameters depend linearly on the length of input audio signals, excluding the computational cost of the last fully connected layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head><p>In this section, we demonstrate the comparison of the performance and computational complexity of ERANNs with various values of hyperparameters W and sm for four datasets: AudioSet <ref type="bibr" target="#b3">[4]</ref>, ESC-50 <ref type="bibr" target="#b0">[1]</ref>, UrbanSound8K <ref type="bibr" target="#b1">[2]</ref>, and RAVDESS <ref type="bibr" target="#b7">[8]</ref>. Moreover, we compare the performance of models for AudioSet, which are trained with various data augmentation techniques. Models pre-trained on AudioSet are transferred to the other three datasets to obtain higher performance. Finally, we compare our ERANNs with previous state-of-the-art APR systems for all the datasets. We show that the proposed techniques are effective for APR tasks.</p><p>Each system with fixed values of hyperparameters is abbreviated as ERANN-sm-W , where sm is the decreasing temporal size parameter and W is the widening factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Models parameters are optimized by minimizing the categorical cross-entropy loss with the Adam optimizer <ref type="bibr" target="#b27">[28]</ref> and a mini-batch size of 32. We use the one-cycle learning rate policy <ref type="bibr" target="#b28">[29]</ref> for the Au-dioSet dataset with a maximum learning rate of 0.001. For ESC-50, UrbanSound8K, and RAVDESS, we use a constant learning rate of 0.0002 for training from scratch and a constant learning rate of 0.0001 for fine-tuning.</p><p>In addition, final models for evaluation sets are obtained using an exponential moving average (EMA) with a decay rate of 0.999 to increase the sustainability of models for evaluation sets and avoid the considerable influence of last training iterations. Thus, the value of the parameter?j of the model for the evaluation set for the j?th iteration is calculated as</p><formula xml:id="formula_7">?j = ?0, if j = 0 ??j?1 + (1 ? ?) ?j, otherwise,<label>(6)</label></formula><p>where ?j is the value of the corresponding learnable parameter for the j?th training iteration and ? = 0.999 is the decay rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">AudioSet</head><p>AudioSet <ref type="bibr" target="#b3">[4]</ref> is a large-scale audio dataset. The dataset includes over 2 million audio recordings (2,085,999 audio recordings in the training set and 20,371 audio recordings in the evaluation set) with 527 sound classes. Audio recordings are extracted from YouTube videos, and the duration of most audio clips is 10 seconds. We successfully downloaded 87.4% of audio clips of the training set and 88.2% of audio clips of the evaluation set. All audio recordings are converted into the monophonic format at a sampling rate of 44.1 kHz.</p><p>The AudioSet dataset is a multilabel dataset (tagging task). Therefore, sigmoid is used to obtain predictions of models.</p><p>We use a balanced sampling strategy for the AudioSet dataset during training <ref type="bibr" target="#b10">[11]</ref> because this dataset is highly unbalanced. As described in <ref type="bibr" target="#b10">[11]</ref>, the balanced sampling strategy significantly improves the performance of models.</p><p>Our models are trained for 500,000 iterations with the full training set without cross-validation. Final models are obtained using early stopping (models are evaluated on the evaluation set at an interval of 5000 iterations). All experiments are repeated three times with different random seeds to obtain final results (the mean of three results). To evaluate and compare APR systems on the evaluation set, we use the mean average precision (mAP) as the main evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Ablation Studies</head><p>We conduct an ablation study to examine the impact of data augmentation techniques and various values of hyperparameters on the performance and computational complexity of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Data Augmentation Techniques:</head><p>We compare the performance of models trained with various data augmentation techniques. We conduct experiments with different types of mixup for ERANN-2-5 (tc = 8): 1-training without mixup, 2-standard mixup on the waveform, 3-modified mixup on the waveform, and 4-modified mixup on the log mel spectrogram. We also show advantages of using SpecAugment for ERANN-2-5 (tc = 8) and the temporal cropping for ERANN-1-6 (tc ? {4, 8, 10}) during training. The results of all the experiments are listed in <ref type="table" target="#tab_2">Table 3</ref>. The model trained using the modified mixup on the waveform and SpecAugment has the best performance. It can be concluded that tc = 8 is the best value in terms of the duration of cropped sections of training audio recordings among the values of 4, 8, and 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Hyperparameters:</head><p>We compare the performance, the number of parameters, and the inference speed of ERANNs with various values of two hyperparameters. Models are compared with three values of W with sm = 4 and four values of sm with W = 6. We stop at W = 6, so that the model is not too large, and stop at W = 4, so the model is not too simple. To evaluate the inference speed of models, we calculate the number of resulting predictions for 10-second audio recordings per second. We use NVIDIA Tesla V100 and compare the inference speed of models for mini-batch sizes of B = 1 and B = 32. Finally, we compare our APR systems with AST systems <ref type="bibr" target="#b9">[10]</ref>, which have state-of-theart performance on the AudioSet dataset. The results are demonstrated in <ref type="table" target="#tab_3">Table 4</ref>. Increasing sm reduces FLOPs and increases the inference speed of models because tensors between convolutional layers have fewer sizes. As listed in <ref type="table" target="#tab_3">Table 4</ref>, ERANN-1-6 and ERANN-2-6 are 1.68x and 1.46x faster (for B = 32) than ERANN-0-6, respectively. Moreover, ERANN-1-6 and ERANN-2-6 have better performance than ERANN-0-6.</p><p>The system AST <ref type="bibr" target="#b9">[10]</ref> has better performance than ERANN-1-6; however, our system has 1.62x fewer parameters and is 1.45x faster. ERANN-1-6 is significantly more efficient than the state-of-the-art system AST (Ensemble-M) <ref type="bibr" target="#b9">[10]</ref>: ERANN-1-6 is 7.10x faster (for B = 1) and 9.66x smaller.</p><p>As can be seen from <ref type="table" target="#tab_3">Table 4</ref>, using the proposed method, we can not only increase the inference speed of our APR systems, but also save and even improve their performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Results</head><p>The comparison of our ERANNs with previous APR systems is shown in <ref type="table" target="#tab_4">Table 5</ref>. Moreover, we compare end-to-end systems ("e2e" in the table) and systems that use 2D time-frequency representations. Our system ERANN-1-6 does not achieve the state-of-the-art results on the AudioSet dataset; however, our system is significantly more efficient than the state-of-the-art system, AST <ref type="bibr" target="#b9">[10]</ref>. Moreover, AST was pre-trained on ImageNet <ref type="bibr" target="#b14">[15]</ref>; our system is trained from scratch. The proposed APR system ERANN-2-5 with an mAP of 0.446 is 2.11x smaller than CNN14 and Wavegram-Logmel-CNN <ref type="bibr" target="#b10">[11]</ref> and has better performance. Moreover, our systems outperform other previous systems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> by a large margin.</p><p>We can see that end-to-end systems AemNet-DW <ref type="bibr" target="#b11">[12]</ref> are significantly smaller than previous state-of-the-art systems, and our systems, which use 2D time-frequency representation as input. On the other hand, systems AemNet-DW are significantly less accurate.  <ref type="bibr" target="#b11">[12]</ref> 1.2 0.329 AemNet-DW (WM=2) <ref type="bibr" target="#b11">[12]</ref> 3.0 0.340 Wavegram-CNN <ref type="bibr" target="#b10">[11]</ref> 81.0 0.389 CNN14 <ref type="bibr" target="#b10">[11]</ref> ? 80.7 0.431 Wavegram-Logmel-CNN <ref type="bibr" target="#b10">[11]</ref> ? 81.1 0.439 AST <ref type="bibr" target="#b9">[10]</ref> ? 88.1 0.459 AST (Ensemble-M) <ref type="bibr">[</ref> For each experiment, we repeat 5-fold cross-validation (official division into folds suggested by the authors of ESC-50) three times and calculate the average accuracy score (15 trainings for each model). All models are trained with four folds for 20,000 iterations from scratch and 2000 iterations using fine-tuning. We also use early stopping to obtain final models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Ablation Studies</head><p>For ESC-50, we also determine the impact of various values of hyperparameters and pre-training on the AudioSet dataset on the performance and computational complexity of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Hyperparameters and AudioSet Pre-training:</head><p>We compare the computational complexity and performance of models with various values of hyperparameters and the performance of models trained from scratch and fine-tuned models. For all the experiments, we adopt tc = 4 and use SpecAugment and modified mixup on the waveform during training. The comparison is shown in <ref type="table" target="#tab_6">Table 6</ref>. ERANNs with W ? 5 are large for the small dataset to train from scratch. High values of W increase the fast overfitting risk. ERANN-1-3 has higher accuracy than ERANN-0-3 by 0.01 and is 1.38x faster. The system ERANN-1-3 has 1.79x fewer pa-rameters and is 1.94x faster than the system ERANN-1-4; however, ERANN-1-3 achieves similar accuracy. ERANN-2-6 trained using transfer learning is 1.63x faster than ERANN-0-6, while ERANN-2-6 has better performance. Our best APR system for ESC-50 is fine-tuned ERANN-2-5. It has better performance, 2.30x fewer parameters, and is 1.67x faster than the previous best system AST <ref type="bibr" target="#b9">[10]</ref>.</p><p>The use of pre-trained models significantly improve performance, while models are trained 10x faster: 20,000 iterations for training from scratch and 2000 for training using fine-tuning.</p><p>For ESC-50, the proposed method helps greatly reduce the computational complexity as well as increase the performance of our APR systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Results</head><p>The comparative analysis of ERANNs with previous APR systems for the ESC-50 dataset is shown in <ref type="table" target="#tab_7">Table 7</ref>. Our system ERANN-2-5 trained using transfer learning achieves an accuracy of 0.961, outperforming the previous state-of-the-art system AST <ref type="bibr" target="#b9">[10]</ref>, which has an accuracy of 0.956.</p><p>Moreover, we can observe a significant difference between the performance of end-to-end systems and systems that use 2D timefrequency representations. At the same time, the end-to-end system EnvNet-v2 <ref type="bibr" target="#b18">[19]</ref> has a large number of parameters; however, AemNet-DW <ref type="bibr" target="#b11">[12]</ref> is very efficient compared with other systems listed in the table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">UrbanSound8K</head><p>The UrbanSound8K dataset includes 8,732 audio clips with 10 classes of urban sounds. Audio signals have various durations, which do not exceed 4 seconds. We convert all the recordings into the monophonic format at a sample rate of 44.1 kHz.</p><p>This dataset is divided into 10 folds by its authors that we applied to train and evaluate our models. All models are trained with 20,000 iterations from scratch and 5000 iterations using fine-tuning. All experiments are repeated three times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Results</head><p>For UrbanSound8K, we use modified mixup on the waveform and SpecAugment as data augmentation techniques. We do not employ the temporal cropping because the duration of audio signals significantly varies (from 0.05 to 4 seconds).</p><p>In <ref type="table" target="#tab_8">Table 8</ref> we compare our ERANNs with various values of hyperparameters and previous systems for UrbanSound8K. We compare our systems with the previous systems, which were evaluated using 10-fold cross-validation without custom generation of segments as another way of evaluating systems can lead to unreasonably high performance <ref type="bibr" target="#b21">[22]</ref>. Comparing ERANNs with sm = 0 and sm ? 1, we observe that the proposed technique is effective and helps improve the performance of systems for UrbanSound8K. Moreover, transfer learning significantly improves the performance of ERANNs.</p><p>Our best fine-tuned system ERANN-2-6 achieves an accuracy of 0.908, which is better than the performance of the previous best system <ref type="bibr" target="#b12">[13]</ref> with an accuracy of 0.891. Moreover, ERANN-2-4 with an accuracy of 0.897 outperforms the previous state-of-the-art system and is 1.24x smaller.</p><p>The comparison of the two approaches of CNN-based systems is the same as for ESC-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">RAVDESS</head><p>The RAVDESS dataset <ref type="bibr" target="#b7">[8]</ref> includes speech and song recordings of 24 professional actors with eight diverse emotions. We use the speech set that comprises 1440 audio recordings with an average duration of 4 seconds.</p><p>For this dataset, we evaluate ERANNs considering 4-fold crossvalidation. Folds are formed, so that the specific actor is in one fold for robust results. As for ESC-50, all our models are trained for the same number of iterations, and we repeat all the experiments three times and report the average value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Results</head><p>For RAVDESS, we use modified mixup on the waveform, the temporal cropping (tc = 3), and SpecAugment during training. We also use pitch shifting. <ref type="table" target="#tab_9">Table 9</ref> shows the comparison of our ERANNs with various values of hyperparameters and with previous APR systems for RAVDESS. Moreover, the table compares ERANNs trained with and without pitch shifting ("P-S" in the table).</p><p>As listed in <ref type="table" target="#tab_9">Table 9</ref>, using pitch shifting during training significantly improves the performance of systems. Pre-training on the AudioSet dataset does not improve the performance of systems on RAVDESS. Moreover, our ERANNs with sm &gt; 0 have worse performance than ERANNs with sm = 0.</p><p>We can conclude that the proposed method helps improve the performance of our systems but not for all APR tasks. Nevertheless, our ERANN-0-4 with an accuracy of 0.748 outperforms the previous state-of-the-art system <ref type="bibr" target="#b10">[11]</ref> with an accuracy of 0.721 by a large margin. Moreover, our ERANN-0-4 is 3.32x smaller than the previous state-of-the-art system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>In this article, we have proposed a method for decreasing the computational complexity of CNN-based systems for APR tasks. The proposed method has helped reduce the computational complexity and improve the performance of ERANNs on three audio datasets (AudioSet, ESC-50, and UrbanSound8K). Moreover, this technique can be applied in other CNN architectures to improve their efficiency for APR tasks.</p><p>Furthermore, we have shown the impact of various data augmentation techniques and transfer learning on the performance of APR systems. For instance, we have demonstrated the advantage of using modified mixup for AudioSet; however, in previous studies, only standard mixup was used for this dataset.</p><p>Owing to the presence of two hyperparameters in our architecture, ERANNs are flexible, and the system with specific values of W and sm can be selected considering the best trade-off between the computational complexity and the performance on specific APR tasks with the limitation of computational resources. Another advantage of the proposed systems is that ERANNs do not depend on the duration of input audio signals. Moreover, ERANNs achieve high mean average precision on AudioSet and state-of-the-art accuracies on ESC-50, UrbanSound8K, and RAVDESS. Furthermore, the proposed APR systems have less computational complexity than previous state-of-the-art systems for the aforementioned audio datasets.</p><p>The main shortcomings of our APR systems are as follows. First, ERANNs do not outperform the best system AST <ref type="bibr" target="#b9">[10]</ref> on AudioSet, and second, ERANNs in this study have significantly more computational complexity than the modern end-to-end APR system AemNet-DW <ref type="bibr" target="#b11">[12]</ref>. Moreover, we have not considered sound event detection <ref type="bibr" target="#b2">[3]</ref>, which are, in addition to tagging and classification tasks, an important subtask of APR.</p><p>It is also worth mentioning that the reported results were obtained on validation sets and not on independent test sets (as in most previous works), and therefore, they are likely to be biased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We have proposed a new convolutional neural network for audio pattern recognition tasks and a simple technique for decreasing the computational complexity of CNN-based systems. Moreover, we have demonstrated the impact of various data augmentation techniques and transfer learning on the performance of APR systems. Our systems have high results on the AudioSet dataset and new state-of-the-art results on the ESC-50, UrbanSound8K, and RAVDESS datasets. The proposed systems are more efficient than previous best systems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>in ? x? 1 ? y? 1 c = c in ? x=y=1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Scheme of ARB(x,y,c) Functions K1(z), K2(z), and P (z) for ARBs, where z is the stride size, are defined as ( K1(z), K2(z), P (z) ) = ( 3, 3, 1 ), if z = 1 ? z = 2 ( 6, 5, 2 ), if z = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Proposed ERANNs architecture</figDesc><table><row><cell>Stage name</cell><cell>Output size</cell><cell>Layers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Proposed ARB(x,y,c) architecture Kernel Stride Padding</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of data augmentation techniques for AudioSet</figDesc><table><row><cell></cell><cell cols="4">tc SpecAugment mixup type mAP</cell></row><row><cell>ERANN-2-5</cell><cell>8</cell><cell></cell><cell>1</cell><cell>0.430</cell></row><row><cell>ERANN-2-5</cell><cell>8</cell><cell></cell><cell>2</cell><cell>0.443</cell></row><row><cell>ERANN-2-5</cell><cell>8</cell><cell></cell><cell>3</cell><cell>0.446</cell></row><row><cell>ERANN-2-5</cell><cell>8</cell><cell></cell><cell>4</cell><cell>0.439</cell></row><row><cell>ERANN-2-5</cell><cell>8</cell><cell>?</cell><cell>3</cell><cell>0.441</cell></row><row><cell>ERANN-1-6</cell><cell>4</cell><cell></cell><cell>3</cell><cell>0.417</cell></row><row><cell>ERANN-1-6</cell><cell>8</cell><cell></cell><cell>3</cell><cell>0.450</cell></row><row><cell cols="2">ERANN-1-6 10</cell><cell></cell><cell>3</cell><cell>0.440</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of the computational complexity and the performance of ERANNs on AudioSet (OOM is Out Of Memory)</figDesc><table><row><cell></cell><cell>Params</cell><cell cols="2">Audios/sec</cell><cell>mAP</cell></row><row><cell></cell><cell>? 10 6</cell><cell>B=1</cell><cell>B=32</cell><cell></cell></row><row><cell>ERANN-2-4</cell><cell>24.5</cell><cell>137</cell><cell>246</cell><cell>0.430</cell></row><row><cell>ERANN-2-5</cell><cell>38.2</cell><cell>96</cell><cell>149</cell><cell>0.446</cell></row><row><cell>ERANN-0-6</cell><cell>54.4</cell><cell>53</cell><cell>74</cell><cell>0.447</cell></row><row><cell>ERANN-1-6</cell><cell>54.5</cell><cell>71</cell><cell>108</cell><cell>0.450</cell></row><row><cell>ERANN-2-6</cell><cell>54.9</cell><cell>82</cell><cell>124</cell><cell>0.448</cell></row><row><cell>ERANN-3-6</cell><cell>56.5</cell><cell>79</cell><cell>128</cell><cell>0.436</cell></row><row><cell>AST [10]</cell><cell>88.1</cell><cell>49</cell><cell cols="2">OOM 0.459</cell></row><row><cell>AST (Ensemble-M) [10]</cell><cell>526.6</cell><cell>10</cell><cell cols="2">OOM 0.485</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of APR systems for AudioSet e2e Params mAP</figDesc><table><row><cell>? 10 6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the computational complexity and the performance of ERANNs on ESC-50</figDesc><table><row><cell></cell><cell cols="2">Params Audios/sec</cell><cell cols="2">Accuracy</cell></row><row><cell></cell><cell>? 10 6</cell><cell>B=32</cell><cell cols="2">Scratch Fine-tune</cell></row><row><cell>ERANN-0-3</cell><cell>13.5</cell><cell>421</cell><cell>0.882</cell><cell>-</cell></row><row><cell>ERANN-1-3</cell><cell>13.6</cell><cell>583</cell><cell>0.892</cell><cell>-</cell></row><row><cell>ERANN-0-4</cell><cell>24.0</cell><cell>300</cell><cell>0.892</cell><cell>-</cell></row><row><cell>ERANN-1-4</cell><cell>24.1</cell><cell>428</cell><cell>0.891</cell><cell>-</cell></row><row><cell>ERANN-2-4</cell><cell>24.3</cell><cell>482</cell><cell>0.880</cell><cell>0.949</cell></row><row><cell>ERANN-2-5</cell><cell>37.9</cell><cell>294</cell><cell>0.879</cell><cell>0.961</cell></row><row><cell>ERANN-0-6</cell><cell>54.1</cell><cell>150</cell><cell>-</cell><cell>0.953</cell></row><row><cell>ERANN-1-6</cell><cell>54.2</cell><cell>218</cell><cell>-</cell><cell>0.954</cell></row><row><cell>ERANN-2-6</cell><cell>54.6</cell><cell>245</cell><cell>-</cell><cell>0.959</cell></row><row><cell>AST [10]</cell><cell>87.3</cell><cell>176</cell><cell>-</cell><cell>0.956</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of APR systems for ESC-50</figDesc><table><row><cell></cell><cell cols="2">e2e Params</cell><cell cols="2">Accuracy</cell></row><row><cell></cell><cell></cell><cell>? 10 6</cell><cell cols="2">Scratch Fine-tune</cell></row><row><cell>EnvNet-v2 [19]</cell><cell></cell><cell>101.3</cell><cell>0.818</cell><cell>-</cell></row><row><cell>ESResNet [22]</cell><cell>?</cell><cell>30.6</cell><cell>0.832</cell><cell>0.915</cell></row><row><cell>AemNet-DW [12]</cell><cell></cell><cell>0.9</cell><cell>0.749</cell><cell>0.923</cell></row><row><cell>CNN14 [11]</cell><cell>?</cell><cell>79.8</cell><cell>0.833</cell><cell>0.947</cell></row><row><cell>ESResNeXt [13]</cell><cell>?</cell><cell>30.1</cell><cell>-</cell><cell>0.952</cell></row><row><cell>AST [10]</cell><cell>?</cell><cell>87.3</cell><cell>0.887</cell><cell>0.956</cell></row><row><cell>ERANN-1-3</cell><cell>?</cell><cell>13.6</cell><cell>0.892</cell><cell>-</cell></row><row><cell>ERANN-2-5</cell><cell>?</cell><cell>37.9</cell><cell>0.879</cell><cell>0.961</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparison of APR systems for UrbanSound8K e2e ParamsAccuracy</figDesc><table><row><cell></cell><cell></cell><cell>? 10 6</cell><cell cols="2">Scratch Fine-tune</cell></row><row><cell>EnvNet-v2 [19]</cell><cell></cell><cell>101.1</cell><cell>0.783</cell><cell>-</cell></row><row><cell>AemNet-DW [12]</cell><cell></cell><cell>0.9</cell><cell>0.763</cell><cell>0.835</cell></row><row><cell>ESResNet [22]</cell><cell>?</cell><cell>30.6</cell><cell>0.828</cell><cell>0.854</cell></row><row><cell>ESResNeXt [13]</cell><cell>?</cell><cell>30.0</cell><cell>-</cell><cell>0.891</cell></row><row><cell>ERANN-0-3</cell><cell>?</cell><cell>13.5</cell><cell>0.823</cell><cell>-</cell></row><row><cell>ERANN-1-3</cell><cell>?</cell><cell>13.5</cell><cell>0.829</cell><cell>-</cell></row><row><cell>ERANN-0-4</cell><cell>?</cell><cell>24.0</cell><cell>0.826</cell><cell>-</cell></row><row><cell>ERANN-1-4</cell><cell>?</cell><cell>24.1</cell><cell>0.835</cell><cell>-</cell></row><row><cell>ERANN-2-4</cell><cell>?</cell><cell>24.2</cell><cell>0.826</cell><cell>0.897</cell></row><row><cell>ERANN-2-5</cell><cell>?</cell><cell>37.9</cell><cell>0.813</cell><cell>0.903</cell></row><row><cell>ERANN-0-6</cell><cell>?</cell><cell>54.0</cell><cell>-</cell><cell>0.904</cell></row><row><cell>ERANN-1-6</cell><cell>?</cell><cell>54.1</cell><cell>-</cell><cell>0.906</cell></row><row><cell>ERANN-2-6</cell><cell>?</cell><cell>54.5</cell><cell>-</cell><cell>0.908</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Comparison of APR systems for RAVDESS P-S e2e ParamsAccuracy</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>? 10 6</cell><cell cols="2">Scratch Fine-tune</cell></row><row><cell>CNN14 [11]</cell><cell>-</cell><cell>?</cell><cell>79.7</cell><cell>0.692</cell><cell>0.721</cell></row><row><cell>ERANN-1-3</cell><cell>?</cell><cell>?</cell><cell>13.5</cell><cell>0.669</cell><cell>-</cell></row><row><cell>ERANN-1-3</cell><cell></cell><cell>?</cell><cell>13.5</cell><cell>0.731</cell><cell>-</cell></row><row><cell>ERANN-0-4</cell><cell>?</cell><cell>?</cell><cell>24.0</cell><cell>0.705</cell><cell>-</cell></row><row><cell>ERANN-0-4</cell><cell></cell><cell>?</cell><cell>24.0</cell><cell>0.748</cell><cell>-</cell></row><row><cell>ERANN-1-4</cell><cell></cell><cell>?</cell><cell>24.1</cell><cell>0.741</cell><cell>-</cell></row><row><cell>ERANN-2-4</cell><cell></cell><cell>?</cell><cell>24.2</cell><cell>0.722</cell><cell>0.730</cell></row><row><cell>ERANN-2-5</cell><cell></cell><cell>?</cell><cell>37.9</cell><cell>0.720</cell><cell>0.737</cell></row><row><cell>ERANN-0-6</cell><cell></cell><cell>?</cell><cell>54.0</cell><cell>-</cell><cell>0.743</cell></row><row><cell>ERANN-1-6</cell><cell></cell><cell>?</cell><cell>54.1</cell><cell>-</cell><cell>0.735</cell></row><row><cell>ERANN-2-6</cell><cell></cell><cell>?</cell><cell>54.5</cell><cell>-</cell><cell>0.734</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>This work was partly supported by NVIDIA Inception Program for AI Startups; FASIE grant 0060584; RFBR grant 19-29-01175. We also thank the anonymous reviewers from Pattern Recognition Letters, whose suggestions helped improve the manuscript quality.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ESC: Dataset for Environmental Sound Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM International Conference on Multimedia</title>
		<meeting>the 23rd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1015" to="1018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Dataset and Taxonomy for Urban Sound Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jacoby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Multimedia</title>
		<meeting>the 22nd ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1041" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sound event detection in domestic environments with weakly labeled data and soundscape synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Detection and Classification of Acoustic Scenes and Events</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sound classification in a smart room environment: an approach using GMM and HMM methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Serignat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaillol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Speech Technology and Human-Computer Dialogue</title>
		<meeting>the IEEE Conference on Speech Technology and Human-Computer Dialogue</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep networks for audio event classification in soccer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bazzica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="474" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Musical Genre Classification of Audio Signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzanetakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="293" to="302" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A Respiratory Sound Database for the Development of Automated Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rocha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="33" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="571" to="575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PANNs: Large-Scale Pretrained Audio Neural Networks for Audio Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient End-to-End Audio Embeddings Generation for Audio Classification on Target Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lopez-Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Del Hoyo Ontiveros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<biblScope unit="page" from="601" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ESResNe(X)t-fbsp: Learning Robust Time-Frequency Transformation of Audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Meta Pseudo Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="11" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 36th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>in IN-TERSPEECH</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning from Between-class Examples for Deep Sound Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">New phase-vocoder techniques for pitch-shifting, harmonizing and other exotic effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Laroche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dolson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA&apos;99 (Cat. No.99TH8452)</title>
		<meeting>the 1999 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. WASPAA&apos;99 (Cat. No.99TH8452)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="91" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An MFCC-GMM approach for event detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Broeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karsmakers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vanrumste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Hamme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WAS-PAA)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ESResNet: Environmental Sound Classification Based on Visual Domain Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4933" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A comparison of Deep Learning methods for environmental sound detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end environmental sound classification using a 1D convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abdoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cardinal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Lameiras</forename><surname>Koerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="252" to="263" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dying ReLU and Initialization: Theory and Numerical Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Computational Physics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1671" to="1706" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page" from="369" to="386" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
