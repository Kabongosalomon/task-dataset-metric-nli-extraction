<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">R 3 Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cnzimingfzm@cmbchina.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Feng</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">MoE Key Lab of Artificial Intelligence</orgName>
								<orgName type="institution" key="instit1">AI Institute</orgName>
								<orgName type="institution" key="instit2">Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Merchants Bank Credit Card Center</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>He</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Anhui COWAROBOT CO., Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">R 3 Det: Refined Single-Stage Detector with Feature Refinement for Rotating Object</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Corresponding author is Junchi Yan.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rotation detection is a challenging task due to the difficulties of locating the multi-angle objects and separating them effectively from the background. Though considerable progress has been made, for practical settings, there still exist challenges for rotating objects with large aspect ratio, dense distribution and category extremely imbalance. In this paper, we propose an end-to-end refined single-stage rotation detector for fast and accurate object detection by using a progressive regression approach from coarse to fine granularity. Considering the shortcoming of feature misalignment in existing refined singlestage detector, we design a feature refinement module to improve detection performance by getting more accurate features. The key idea of feature refinement module is to re-encode the position information of the current refined bounding box to the corresponding feature points through pixel-wise feature interpolation to realize feature reconstruction and alignment. For more accurate rotation estimation, an approximate SkewIoU loss is proposed to solve the problem that the calculation of SkewIoU is not derivable. Experiments on three popular remote sensing public datasets DOTA, HRSC2016, UCAS-AOD as well as one scene text dataset ICDAR2015 show the effectiveness of our approach. Tensorflow and Pytorch version codes are available at https://github.com/Thinklab-SJTU/ R3Det_Tensorflow and https://github.com/ SJTU-Thinklab-Det/r3det-on-mmdetection, and R 3 Det is also integrated in our open source rotation detection benchmark: https://github.com/ yangxue0827/RotationDetection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is one of the fundamental tasks in computer vision, and many high-performance general-purpose object detectors have been proposed. Current popular detection methods can be in general divided into two types: two-stage object detectors <ref type="bibr" target="#b10">[14,</ref><ref type="bibr" target="#b9">13,</ref><ref type="bibr" target="#b36">40,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b23">27]</ref> and singlestage object detectors <ref type="bibr" target="#b27">[31,</ref><ref type="bibr" target="#b34">38,</ref><ref type="bibr" target="#b24">28]</ref>. Two-stage methods have achieved promising results on various benchmarks, while the single-stage approach maintains faster detection speed.</p><p>However, current general horizontal detectors have fundamental limitations for many practical applications. For instance, scene text detection, retail scene detection and remote sensing object detection whereby the objects can appear in various orientations. Therefore, many rotation detectors based on a general detection framework have been proposed in the above fields. In particular, three challenges are pronounced, as analyzed as follows: 1) Large aspect ratio. The Skew Intersection over Union (SkewIoU) score between large aspect ratio objects is sensitive to change in angle, as sketched in <ref type="figure" target="#fig_3">Figure 3</ref>(b).</p><p>2) Densely arranged. As illustrated in <ref type="figure" target="#fig_7">Figure 6</ref>, many objects usually appear in densely arranged forms.</p><p>3) Arbitrary orientations. Objects in images can appear in various orientations, which requires the detector to have accurate direction estimation capabilities. This paper is devoted to design an accurate and fast rotation detector. To maintain high detection accuracy and speed for large aspect ratio objects, we have adopted a refined single-stage rotation detector. First, we find that rotating anchors can perform better in dense scenes, while horizontal anchors can achieve higher recalls in fewer quantities. Therefore, a progressive regression form from coarse to fine is adopted in the refined single-stage detector, that is, the horizontal anchors are used in the first stage for faster speed and higher recall, and then the refined rotat- ing anchors are used in the subsequent refinement stages to adapt to intensive scenarios. Second, we also notice that existing refined single-stage detectors <ref type="bibr" target="#b53">[57,</ref><ref type="bibr" target="#b6">7]</ref> have feature misalignment problems 1 , which greatly limits the reliability of classification and regression during the refined stages. We design a feature refinement module FRM that uses the feature interpolation to obtain the position information correspond to the refined anchors and reconstruct the whole feature map by pixel-wise manner to achieve feature alignment. FRM can also reduce the number of refined bounding box after the first stage, thus speeding up the model. Experimental results have shown that feature refinement is sensitive to location and its improvement in detection results is very noticeable. Finally, an approximate SkewIoU loss is devised to address the indifferentiable problem of SkewIoU calculation for more accurate rotation estimation. Combing these three techniques as a whole, our approach achieves state-of-the-art performance with considerable speed on four public rotating sensitive datasets including DOTA, HRSC2016, UCAS-AOD, and ICDAR2015. Specifically, this work makes the following contributions:</p><p>1) For large aspect ratio object detection, an accurate and fast rotation singe-stage detector is devised in a refined manner, for high-precision detection. In contrast to the recent learning based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">19,</ref><ref type="bibr" target="#b52">56]</ref> for feature alignment, which lacks an explicit mechanism to compensate the misalignment, we propose a direct and effective pure computing based approach which is further extended to handle the rotation case. To our best knowledge, it is the first work for solving the feature misalignment problem for rotation detection.</p><p>2) For densely arranged objects, we develop an efficient <ref type="bibr" target="#b0">1</ref> Mainly refers to misalignment between region of interest (RoI) and the feature, see details in <ref type="figure" target="#fig_5">Figure 4</ref>  coarse-to-fine progressive regression approach to better exploring the two forms of anchors in a more flexible manner, tailored to each detection stage. Compared with the previous methods <ref type="bibr" target="#b31">[35,</ref><ref type="bibr" target="#b49">53,</ref><ref type="bibr">12,</ref><ref type="bibr" target="#b48">52,</ref><ref type="bibr" target="#b47">51]</ref> using one single anchor form, our method is more flexible and efficient.</p><p>3) For arbitrarily-rotated objects, a derivable approximate SkewIoU loss is devised for more accurate rotation estimation. Compared with the over-approximation of SkewIoU loss in recent work <ref type="bibr" target="#b5">[6]</ref>, our method retains the accurate SkewIoU amplitude and only approximates the gradient direction of SkewIoU loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Two-Stage Object Detectors. Most existing two-stage methods are region based. In a region based framework, category-independent region proposals are generated from an image in the first stage, followed with feature extraction from these regions, and then category-specific classifiers and regressors are used for classification and regression in the second stage. Finally, the detection results are obtained by using post-processing methods such as nonmaximum suppression (NMS). Faster-RCNN <ref type="bibr" target="#b36">[40]</ref>, R-FCN <ref type="bibr" target="#b7">[8]</ref>, and FPN <ref type="bibr" target="#b23">[27]</ref> are classic structures in a two-stage approach that can detect object quickly and accurately in an end-to-end manner. Single-Stage Object Detectors. For their efficiency, single-stage detection methods are receiving more and more attention. OverFeat <ref type="bibr" target="#b39">[43]</ref> is one of the first single-stage detectors based on convolutional neural networks. It performs object detection in a multiscale sliding window fashion via a single forward pass through the CNN. Compared with region based methods, Redmon et al. <ref type="bibr" target="#b34">[38]</ref> propose YOLO, a unified detector casting object detection as a regression problem from image pixels to spatially separated bounding boxes and associated class probabilities. To preserve realtime speed without sacrificing too much detection accuracy, Liu et al. <ref type="bibr" target="#b27">[31]</ref> propose SSD. The work <ref type="bibr" target="#b24">[28]</ref> solves the class imbalance problem by proposing RetinaNet with Focal loss and further improves the accuracy of single-stage detector. Rotation Object Detectiors. Remote sensing, scene text and retail scene are the main application scenarios of the rotation detector. Due to the complexity of the remote sensing image scene and the large number of small, cluttered and rotated objects, two-stage rotation detectors are still dom- inant for their robustness. Among them, ICN <ref type="bibr" target="#b0">[1]</ref>, ROI-Transformer [10], SCRDet <ref type="bibr" target="#b50">[54]</ref> and Gliding Vertex <ref type="bibr" target="#b46">[50]</ref> are state-of-the-art detectors. However, they use a more complicated structure causing speed bottleneck. For scene text detection, there are many efficient rotation detection methods, including both two-stage methods (R 2 CNN <ref type="bibr" target="#b16">[20]</ref>, RRPN <ref type="bibr" target="#b31">[35]</ref>, FOTS <ref type="bibr" target="#b28">[32]</ref>), as well as single-stage methods (EAST <ref type="bibr" target="#b57">[61]</ref>, TextBoxes++ <ref type="bibr" target="#b21">[25]</ref>). For retail scene detection, DRN <ref type="bibr" target="#b33">[37]</ref> and PIoU <ref type="bibr" target="#b5">[6]</ref> Loss are the latest two rotation detectors used in retail scene detection, and two rotation retail datasets are proposed respectively. Refined Object Detectors. To achieve better detection accuracy, many cascaded or refined detectors are proposed. The Cascade RCNN <ref type="bibr" target="#b2">[3]</ref>, HTC <ref type="bibr" target="#b3">[4]</ref>, and FSCascade <ref type="bibr" target="#b18">[22]</ref> perform multiple classifications and regressions in the second stage, which greatly improved the detection accuracy. The same idea is also used in single-stage detectors, such as RefineDet <ref type="bibr" target="#b53">[57]</ref>. Unlike the two-stage detectors, which use RoI Pooling <ref type="bibr" target="#b9">[13]</ref> or RoI Align <ref type="bibr" target="#b11">[15]</ref> for feature alignment. The currently refined single-stage detector is not well resolved in this respect. An important requirement of the refined single-stage detector is to maintain a full convolutional structure, which can retain the advantage of speed, but methods such as RoI Align cannot satisfy it whereby fully-connected layers have to be introduced. Although some works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b15">19,</ref><ref type="bibr" target="#b52">56]</ref> use deformable convolution <ref type="bibr" target="#b8">[9]</ref> for feature alignment, whose offset parameters are often obtained by learning the offset between the pre-defined anchor box and the refined anchor. These deformable-based feature alignment methods are too implicit and can not ensure that features are truely aligned. Feature misalignment still limits the performance of the refined single-stage detector. Compared to these methods, our method can clearly find the corresponding feature area by calculation and achieve the purpose of feature alignment by feature map reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>We give an overview of our method as sketched in <ref type="figure" target="#fig_0">Figure 1</ref>. The embodiment is a refined single-stage rotation detector based on the RetinaNet <ref type="bibr" target="#b24">[28]</ref>, namely Refined Ro-tation RetinaNet (R 3 Det). The refinement stage (which can be added and repeated by multiple times) is added to the network to refine the bounding box, and the feature refinement module FRM is added during the refinement stage to reconstruct the feature map. In a single-stage rotating object detection task, continuous refinement of the predicted bounding box can improve the regression accuracy, and feature refinement is a necessary process for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Rotation RetinaNet</head><p>Base Setting. RetinaNet is one of the most advanced single-stage detectors available today. It consists of two parts: backbone network, classification and regression subnetwork. For RetinaNet-based rotation detection, we use five parameters (x, y, w, h, ?) to represent arbitrary-oriented rectangle. Ranging in [??/2, 0), ? denotes the acute angle to the x-axis, and for the other side we refer it as w. Therefore, it calls for predicting an additional angular offset in the regression subnet, whose rotation bounding box is:</p><p>Loss Function. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, each box set has the same center point, height and width. The angle difference between the two box sets is the same, but the aspect ratio is different. As a result, the smooth L1 loss value of the two sets is the same (mainly from the angle difference), but the SkewIoU is quite different. The red and orange arrows in <ref type="figure" target="#fig_3">Figure 3</ref>(b) show the inconsistency between SkewIoU and smooth L1 Loss. We can draw conclusion that smooth L1 loss function is still not suitable for rotation detection, especially for objects with large aspect ratios, which are sensitive to SkewIoU. What's more, the evaluation metric of rotation detection is also dominated by SkewIoU.</p><p>The IoU related loss is an effective regression loss function that can solve above problem and is already widely used in horizontal detection, such as GIoU <ref type="bibr" target="#b37">[41]</ref>, DIoU <ref type="bibr" target="#b55">[59]</ref>, etc. However, the SkewIoU calculation function between two rotating boxes is underivable, which means that we cannot directly use the SkewIoU as the regression loss function. Inspired by SCRDet <ref type="bibr" target="#b50">[54]</ref>, we propose a derivable approximate SkewIoU loss, the multi-task loss is defined as fol-  lows:</p><formula xml:id="formula_0">L = ? 1 N N n=1 obj n L reg (v n , v n ) |L reg (v n , v n )| |f (SkewIoU )| + ? 2 N N n=1 L cls (p n , t n ) (2) L reg (v , v) = L smooth?l1 (v ? , v ? ) ? IoU (v {x,y,w,h} , v {x,y,w,h} )<label>(3)</label></formula><p>where N indicates the number of anchors, obj n is a binary value (obj n = 1 for foreground and obj n = 0 for background, no regression for background). v represents the predicted offset vectors, v denotes the targets vector of ground-truth. While t n indicates the label of object, p n is the probability distribution of various classes calculated by sigmoid function. SkewIoU denotes the overlap of the prediction box and ground-truth. The hyper-parameter ? 1 , ? 2 control the trade-off and are set to 1 by default. The classification loss L cls is implemented by focal loss <ref type="bibr" target="#b24">[28]</ref>. |.| is used to obtain the modulus of the vector and is not involved in gradient back propagation. f (.) represents the loss function related to SkewIoU. IoU (.) represents the horizontal bounding box IoU calculation function.</p><p>Compared to the traditional regression loss, the new regression loss can be divided into two parts,</p><formula xml:id="formula_1">Lreg(v n ,vn)</formula><p>|Lreg(v n ,vn)| de- termines the direction of gradient propagation (a unit vector), which is an important part to ensure that the loss function is derivable. |f (SkewIoU )| is responsible for adjusting the loss value (magnitude of gradient), and it is unnecessary to be derivable (a scalar). Taking into account the inconsistency between SkewIoU and smooth L1 loss, we use Equation 3 as the dominant gradient function for regression loss. Through such a combination, the loss function is derivable, while its size is highly consistent with SkewIoU. Experiments show that the detector based on this approximate SkewIoU loss can achieve considerable gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Refined Rotation RetinaNet</head><p>Refined Detection. The SkewIoU score is sensitive to the change in angle, and a slight angle shift causes a rapid decrease in the IoU score, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Therefore, the refinement of the prediction box helps to improve the recall rate of the rotation detection. We join multiple refinement stages with different IoU thresholds. In addition to using the foreground IoU threshold 0.5 and background IoU threshold 0.4 in the first stage, the thresholds of first refinement stage are set 0.6 and 0.5, respectively. If there are multiple refinement stages, the remaining thresholds are 0.7 and 0.6. The overall loss for refined detector is defined as follows:</p><formula xml:id="formula_2">L total = N i=1 ? i L i (4)</formula><p>where L i is the loss value of the i-th refinement stage and trade-off coefficients ? i are set to 1 by default. Feature Refinement Module. Many refined detectors still use the same feature map to perform multiple classifications and regressions, without considering the feature misalignment caused by the location changes of the bounding box. <ref type="figure" target="#fig_5">Figure 4</ref>(c) depicts the box refining process without feature refinement, resulting in inaccurate features, which can be disadvantageous for those categories that have a large aspect ratio or a small sample size. Here we propose to re-encode the position information of the current refined bounding box (orange rectangle) to the corresponding feature points (red point 2 ), thereby reconstructing the entire feature map by pixel-wise manner to achieve the alignment of the features. The whole process is shown in <ref type="figure" target="#fig_5">Figure 4</ref>(d). To accurately obtain the location feature information correspond to the refined bounding box, we adopt the bilinear feature interpolation method, as shown in <ref type="figure" target="#fig_5">Figure 4</ref>(b). Feature interpolation can be formulated as follows:</p><formula xml:id="formula_3">F =F lt * A rb + F rt * A lb + F rb * A lt + F lb * A rt<label>(5)</label></formula><p>where A denotes the Area in <ref type="figure" target="#fig_5">Figure 4</ref></p><formula xml:id="formula_4">(b), F ? R C?1?1</formula><p>represents the feature vector of the point on the feature map. Based on the above result, a feature refinement module is devised, whose structure and pseudo code is shown in <ref type="figure" target="#fig_6">Figure 5</ref> and Algorithm 3.2, respectively. Specifically, the feature map is added by two-way convolution to obtain a new feature (large kernel, LK). Only the bounding box with the highest score of each feature point is preserved in the refinement stage to increase the speed (box filtering, BF), meanwhile ensuring that each feature point corresponds to only one refined bounding box. The filtering of bounding boxes is a necessary step for feature reconstruction (FR). For each feature point of the feature map, we obtain the corresponding feature vector on the feature map according to the five coordinates of the refined bounding box (one center point and four corner points). A more accurate feature vector is obtained by bilinear interpolation. We add the five   feature vectors and replace the current feature vector. After traversing the feature points, we reconstruct the whole feature map. Finally, the reconstructed feature map is added to the original feature map to complete the whole process. The refinement stage can be added and repeated by multiple times. The feature reconstruction process of each refinement stage is simulated as follows:</p><formula xml:id="formula_5">F i+1 = F RM (B i , S i , {P 2 , ..., P 7 })<label>(6)</label></formula><p>where F i+1 represents the feature map of the i + 1 stage, B i , S i represent the bounding box and confidence score of the ith stage prediction, respectively. Discussion for comparison with RoIAlign. The core to solve feature misalignment for FRM is feature reconstruction. Compared with RoI Align that has been adopted in many two-stage rotation detectors including R 2 CNN and RRPN, FRM has the following differences that contribute to R 3 Det's higher efficiency, as shown in   samples five feature points, about one-fortieth of RoI Align, which gives FRM a huge speed advantage.</p><p>2) RoI Align need to obtain the feature corresponding to RoI (instance level) before classification and regression. In contrast, FRM first obtains the features corresponding to the feature points, and then reconstructs the entire feature map (image level). As a result, the FRM based method can maintain a full convolution structure that leads to higher efficiency and fewer parameters, compared with the RoI Align based method that involves a fully-connected structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Protocls</head><p>The public dataset DOTA <ref type="bibr" target="#b45">[49]</ref> is comprised of 2,806 large aerial images from different sensors and platforms. Objects in DOTA exhibit a wide variety of scales, orientations, and shapes. These images are then annotated by experts using 15 object categories. The short names for categories are defined as (abbreviation-full name): PL-Plane, BD-Baseball diamond, BR-Bridge, GTF-Ground field track, SV-Small vehicle, LV-Large vehicle, SH-Ship, TC-Tennis court, BC-Basketball court, ST-Storage tank, SBF-Soccer-ball field, RA-Roundabout, HA-Harbor, SP-Swimming pool, and HC-Helicopter. The fully annotated DOTA benchmark contains 188,282 instances, each of which is labeled by an arbitrary quadrilateral. There are two detection tasks for DOTA: horizontal bounding boxes (HBB) and oriented bounding boxes (OBB). Half of the original images are randomly selected as the training set, 1/6 as the validation set, and 1/3 as the testing set. We divide the images into 600 ? 600 subimages with an overlap of 150 pixels and scale it to 800 ? 800. With all these processes, we obtain about 27,000 patches.</p><p>The HRSC2016 dataset <ref type="bibr" target="#b30">[34]</ref>    For all datasets, the models are trained by 20 epochs in total, and learning rate is reduced tenfold at 12 epochs and 16 epochs, respectively. The initial learning rates for Reti-naNet is 5e-4. The number of image iterations per epoch for DOTA, ICDAR2015, HRSC2016 and UCAS-AOD are 54k, 10k, 5k and 5k, and doubled if data augmentation and multi-scale training are used. The experiments in this paper are initialized by ResNet50 <ref type="bibr" target="#b12">[16]</ref> by default unless otherwise specified. Weight decay and momentum are 0.0001 and 0.9, respectively. We employ MomentumOptimizer over 4 GPUs with a total of 4 images per minibatch (1 images per GPU). The anchors have areas of 32 2 to 512 2 on pyramid levels P3 to P7, respectively. At each pyramid level we use anchors at seven aspect ratios {1, 1/2, 2, 1/3, 3, 5, 1/5} and three scales {2 0 , 2 1/3 , 2 2/3 }. We also add six angles {?90 ? , ?75 ? , ?60 ? , ?45 ? , ?30 ? , ?15 ? } for rotating anchor-based method (RetinaNet-R).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Robust Baseline Methods</head><p>In this paper, we use three robust baseline models with different anchor settings. ReitnaNet-H: The advantage of a horizontal anchor is that it can use less anchor but match more positive samples by calculating the IoU with the horizontal circumscribing rect-angle of the ground truth, but it introduces a large number of non-object or regions of other objects. For an object with a large aspect ratio, its prediction rotating bounding box tends to be inaccurate, as shown in <ref type="figure" target="#fig_7">Figure 6(d)</ref>. ReitnaNet-R: In contrast, in <ref type="figure" target="#fig_7">Figure 6</ref>(e), the rotating anchor avoids the introduction of noise regions by adding angle parameters and has better detection performance in dense scenes. However, the number of anchor has multiplied, about 6 times in this paper, thus making the model less efficient. R 3 Det * : This is a refined detector without feature refinement. Considering the number of original anchors determines the speed of the model, we adopt a progressive regression form from coarse to fine. Specifically, we first use horizontal anchor to reduce the number of anchors and increase the object recall rate in the first stage, and then use the rotating refined anchor to overcome the problems caused by dense scenes in subsequent stages, as shown in <ref type="figure" target="#fig_7">Figure  6</ref>(f).</p><p>RetinaNet-H and RetinaNet-R have similar overall mAP (62.79% versus 62.76%) according to <ref type="table" target="#tab_1">Table 1</ref>, while with their respective characteristics. The horizontal anchorbased approach clearly has an advantage in speed, while the rotating anchor-based method has better regression capabilities in dense object scenarios and objects with large aspect ratio, such as small vehicle, large vehicle, and ship. R 3 Det * achieves 63.52% performance, better than RetinaNet-H and RetinaNet-R. Although the category of dense and large aspect ratio has been improved a lot, it is still not as good as RetinaNet-R (such as LV and SH). RetianNet-R's advantages in this regard will also be reflected in <ref type="table" target="#tab_2">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Feature Refinement Module. <ref type="table" target="#tab_1">Table 1</ref> shows that R 3 Det * can improve performance by about 0.8% which is not significant. We believe that the main reason is that the feature misalignment problem. FRM reconstructs the feature map based on the refined anchor, which increases the overall performance by 2.79% to 66.31% according to <ref type="table" target="#tab_1">Table 1</ref>. In order to further verify the effectiveness of FRM, we have also verified it in other datasets, including the text dataset ICDAR2015, and remote sensing dataset HRSC2016 and UCAS-AOD. FRM still shows a stronger performance advantage. As shown in <ref type="table" target="#tab_3">Table 2</ref>, the FRMbased method is improved by 1.69%, 0.12% (1.03)%, and 1.14% respectively under the same experimental configuration. Compared with (deformable) learning based method, our learning-free FRM is more accurate and effective according to <ref type="table" target="#tab_4">Table 3</ref>. When we randomly disturb the order of the four weights in the interpolation formula, the final performance of the model will be greatly reduced, rows 3-4 of <ref type="table" target="#tab_4">Table 3</ref>. The same conclusions have also appeared in the experiments of quantitative operations, see rows 5-6 of <ref type="table" target="#tab_4">Ta-ble 3</ref>. This phenomenon reflects the location sensitivity of the feature points and explains why the performance of the model can be greatly improved after the feature is correctly refined. Number of Refinement Stages. Refinement strategy can significantly improve the performance of rotation detection, especially the introduction of feature refinement. <ref type="table" target="#tab_1">Table  1</ref> explores the relationship between the number of refinements and model performance. R 3 Det ? has joined the two refinement stages and bring more gain. To further explore the impact of the number of stages, several experimental results are summarized in <ref type="table" target="#tab_6">Table 4</ref>. Experiments show that three or more refinements will not bring additional improvements to overall performance. We also find that ensemble multi-stage results can further improve detection performance. Approximate SkewIoU Loss. We use two different detectors and three different SkewIoU functions to verify the effectiveness of the approximate SkewIoU, as shown in <ref type="table" target="#tab_7">Table  5</ref>. RetinaNet-based detectors will have a large number of low-SkewIoU prediction bounding box in the early stage of training, and will produce very large loss after the log function, and training is prone to non-convergence. Compared with the linear function, the derivative of the expbased function is related to SkewIoU, that is, more attention is paid to the training of difficult samples, so it has a higher performance improvement. Compare with PIoU, we can achieve considerable gains on a higher baseline and far exceed PIoU in final performance, 73.79% versus 60.5% as shown in <ref type="table" target="#tab_9">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the State-of-the-Art</head><p>Results on DOTA. We compare our results with the stateof-the-arts in DOTA as depicted in <ref type="table" target="#tab_9">Table 6</ref>. Two-stage detectors are still dominant in DOTA, and the latest two-stage detection methods, such as ROI Transformer, SCRDet, and CenterMap OBB have performed well. However, they all use complex model structures in exchange for performance improvements, which are extremely low in terms of detection efficiency. The advantage of the two-stage method on the DOTA dataset lies in the multi-stage regression and the use of low-level feature maps (P2) that are friendly to small objects. Compared to all published single-stage methods, our method achieves the best performance without using multi-scale training and testing, at 73.79%. By using a stronger backbone and multi-scale training and testing, as used in the most advanced two-stage method Cen-terMask OBB, R 3 Det performs competitive performance, about 76.47%. Results on HRSC2016 and UCAS-AOD. The HRSC2016 is a challenging dataset that contains lots of large aspect ratio ship instances with arbitrary orientation. We use RRPN and R 2 CNN for comparative experiments, which <ref type="table">Method   Backbone  MS  PL  BD  BR  GTF  SV  LV  SH  TC  BC  ST  SBF  RA  HA  SP  HC  mAP   Two-</ref>    are originally used for scene text detection. Experimental results show that these two methods under-perform in the remote sensing dataset, only 73.07% and 79.08% respectively. Although RoI Transformer achieves 86.20% mAP, its detection speed is still not ideal, and only about 6fps without accounting for the post-processing operations.</p><p>RetinNet-H, RetinaNet-R and R 3 Det * are the three baseline models used in this paper. RetinaNet-R achieves the best detection results, around 89.14%, which is consistent with the performance of the ship category in the DOTA dataset. This further illustrates that the rotation-based approach has advantages in large aspect ratio object detection. Under ResNet101 backbone, R 3 Det can achieve better performance than Gliding Vertex, DRN, SDB and above methods. Besides, our method can achieve 86.67% accuracy and 20fps speed, given MobileNetv2 <ref type="bibr" target="#b38">[42]</ref> as backbone with input image size 600?600. <ref type="table" target="#tab_11">Table 8</ref> illustrates the comparison of performance on UCAS-AOD dataset, our results are the best out of all the existing published methods, at 96.17%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented an end-to-end refined single-stage detector designated for rotating objects with large aspect ratio, dense distribution and arbitrary orientations, which are common in practice like aerial, retail and scene text image. Seeing the shortcoming of feature misalignment in the current refined single-stage detector, we design a feature refinement module to improve detection performance.</p><p>The key idea of FRM is to re-encode the position information of the current refined bounding box to the corresponding feature points through pixel-wise feature interpolation to achieve feature reconstruction and alignment. For more accurate rotation estimation, an approximate SkewIoU loss is proposed to solve the problem that the calculation of SkewIoU is not derivable. We perform careful ablation studies and comparative experiments on multiple rotation detection datasets including DOTA, HRSC2016, UCAS-AOD, and ICDAR2015, and demonstrate that our method achieves state-of-the-art detection accuracy with high efficiency. <ref type="figure">Figure 7</ref>: Performance versus speed on HRSC2016 <ref type="bibr" target="#b30">[34]</ref> dataset. As can be seen, our algorithm significantly surpasses competitors in accuracy, whilst running very fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Speed Comparison</head><p>Due to the high-resolution test images and extra processing such as image cropping and test results merging, DOTA is not suitable for speed comparison. Therefore, we compare the speed and accuracy with the other six methods on the HRSC2016 dataset under the same test environment. The time of post process (i.e. R-NMS) is included. We also explore the impact of different backbones and image sizes on the per performance of the proposed model. The detailed experimental results are shown in <ref type="figure">Figure 7</ref>. Our method can achieve 92.83% accuracy and 20fps speed, given Mo-bileNetv2 as backbone with input image size 600 ? 600. The HRSC2016 dataset contains only one category, so the speed comparsion is not obvious between RetinaNet-R and R 3 Det according to (5 + C) ? A, but the gap will gradually widen as the number of categories increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results on ICDAR2015.</head><p>Scene text detection is also one of the main application scenarios for rotation detection. As shown in <ref type="table" target="#tab_13">Table 9</ref>, our method achieves 84.96% while maintaining 13.5fps in the ICDAR2015 dataset, better than most mainstream algorithms except for FOTS, which adds a lot of extra training data (like ICDAR 2017 MLT <ref type="bibr" target="#b32">[36]</ref>) and uses large test image. With these heavy settings, our method R 3 Det ? achieved 89.21% and still maintains 9fps. The experimental results still show that the proposed techniques are general that can be useful for both aerial images and scene text images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">FRM is Suitable for Rotation Detection</head><p>When applying FRM to horizontal detection, the feature vectors of the four corner points (green points in <ref type="figure">Figure   Method</ref> FRM   The sample points for horizontal detection are significantly further away from the object, and the sampling points for rotation detection are tighter.</p><p>8(a)) obtained in FRM are likely to be far from the object, resulting in very inaccurate features being sampled. However, in the rotation detection task, the four corner points (red points in <ref type="figure" target="#fig_8">Figure 8</ref>(b)) of the rotating bounding box are very close to the object. We have experimented on COCO dataset and the results are not satisfactory, but we have achieved considerable gains on many rotating datasets. <ref type="figure" target="#fig_9">Figure 9</ref> shows the training loss curve after using approximate SkewIoU loss and Smooth L1 loss. It can be clearly seen from the variance and mean of the two curves that the training is more stable after using approximate SkewIoU loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Training Loss Curve</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Visualization on Different Datasets</head><p>We visualize the detection results of R 3 Det on different types of datasets, including remote sensing datasets ( <ref type="figure" target="#fig_0">Figure  12</ref> and <ref type="figure" target="#fig_0">Figure 11</ref>), and scene text datasets ( <ref type="figure" target="#fig_0">Figure 10</ref>).  (2018AAA0100704), NSFC (61972250, U19B2035). The author Xue Yang is supported by Wu Wen Jun Honorary Doctoral Scholarship, AI Institute, Shanghai Jiao Tong University.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Broad Societal Implications</head><p>This paper aims to advance the technology of rotation detection, which has wide applications in face, remote sensing, scene text and retail scene. The more accurate and efficient detection can help people more conveniently and costeffectively record the key information while in the mean time, the privacy of individuals may be put at risk. Hence we shell take additional measures to protect privacy along the development of such technology.  <ref type="figure" target="#fig_0">Figure 12</ref>: Detection results on the OBB task on DOTA. Our method performs better on those with large aspect ratio , in arbitrary direction, and high density. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The architecture of the proposed Refined Rotation Single-Stage Detector (RetinaNet as an embodiment). The refinement stage can be repeated by multiple times. 'A' indicates the number of anchors on each feature point, and 'C' indicates the number of categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between SkewIoU and Smooth L1 Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>The SkewIoU scores vary with the angle deviation. The red and green rectangles represent the ground truth and the prediction bounding box, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Refine box with misaligned feature due to bounding box location changes. (d) Refine box with aligned features by reconstructing the feature map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Root cause analysis of feature misalignment and the core idea for our proposed feature refinement module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Feature Refinement Module FRM. It mainly includes three parts: refined bounding box filtering (BF), large kernel (LK) and feature reconstruction (FR).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Visualization on DOTA. Here 'H' and 'R' represent the horizontal and rotating anchors, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Schematic diagram of sampling points for FRM in horizontal detection and rotation detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Training loss curve after using approximate SkewIoU loss and Smooth L1 loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Text detection results on the ICDAR2015 benchmarks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 11 :</head><label>11</label><figDesc>Ship detection results on the HRSC2016 benchmarks. The red and green bounding box indicate the ground truth and prediction box, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Det * 65.02 67.31 67.31 63.52 R 3 Det 65.81 72.76 70.14 66.31 R 3 Det ? 67.45 73.98 70.27 67.66 R 3 Det ? 68.04 72.72 76.03 69.50</figDesc><table><row><cell>Method</cell><cell>FRM</cell><cell>approximate</cell><cell>SV.</cell><cell>LV.</cell><cell>SH.</cell><cell>mAP</cell></row><row><cell></cell><cell cols="2">BF&amp;FR LK SkewIoU loss</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RetinaNet-R</cell><cell></cell><cell></cell><cell cols="4">64.64 71.01 68.62 62.76</cell></row><row><cell>RetinaNet-H</cell><cell></cell><cell></cell><cell cols="4">63.50 50.68 65.93 62.79</cell></row><row><cell>R 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Ablative study of each component in our method on the DOTA dataset. R 3 Det ? indicates that R 3 Det with two refinement stages. BF, LK and FR denote box filtering, large kernel and feature reconstruction.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 7 .</head><label>7</label><figDesc>and reducing the sampling point greatly affects the performance of the detector. FRM only +1.69) 89.26 (+0.12) 96.01 (+1.03) 96.17 (+1.14)</figDesc><table><row><cell>Method</cell><cell>FRM</cell><cell cols="2">ICDAR2015</cell><cell cols="2">HRSC2016</cell><cell>UCAS-AOD</cell></row><row><cell></cell><cell cols="2">BF&amp;FR LK Recall Precision</cell><cell>Hmean</cell><cell>mAP (07)</cell><cell>mAP (12)</cell><cell>mAP</cell></row><row><cell>R 3 Det  *</cell><cell>81.64</cell><cell>84.97</cell><cell>83.27</cell><cell>89.14</cell><cell>94.98</cell><cell>95.03</cell></row><row><cell>R 3 Det</cell><cell>83.54</cell><cell>86.43</cell><cell>84.96 (</cell><cell></cell><cell></cell><cell></cell></row></table><note>1) RoI Align has more sampling points (the default num- ber is 7 ? 7 ? 4 = 196),</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison between R 3 Det * and R 3 Det on three datasets. 07 or 12 means 2007 or 2012 evaluation metric.</figDesc><table><row><cell>Align Mode</cell><cell>Feature Refinement Interpolation Formula</cell><cell cols="2">Feature Extraction mAP</cell></row><row><cell></cell><cell></cell><cell cols="2">Random Bilinear 64.37</cell></row><row><cell>FRM</cell><cell>Flt  *  1 + Frt  *  0 + Frb  *  0 + Flb  *  0</cell><cell>Quantification</cell><cell>64.02</cell></row><row><cell>FRM</cell><cell>Flt  *  0 + Frt  *  0 + Frb  *  1 + Flb  *  0</cell><cell>Quantification</cell><cell>64.19</cell></row><row><cell>deformable</cell><cell>-</cell><cell>-</cell><cell>63.56</cell></row></table><note>FRM Flt * Arb + Frt * Alb + Frb * Alt + Flb * Art Bilinear 66.31 FRM Flt * Alt + Frt * Art + Frb * Arb + Flb * Alb Random Bilinear 64.28 FRM Flt * Alb + Frt * Arb + Frb * Art + Flb * Alt</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Experiments with our feature alignment tech-</cell></row><row><cell>nique (FRM) and learnable deformable module [9] with</cell></row><row><cell>different interpolation formulas. Feature interpolation</cell></row><row><cell>has location-sensitive properties.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>63.50 50.68 65.93 51.93 62.79 2 2 42.72 65.81 72.76 70.14 56.07 66.31 3 3 45.14 67.09 73.70 70.21 56.96 67.29 4 4 44.20 65.30 72.99 70.16 55.70 67.02 3 2 ? 3 45.08 67.45 73.98 70.27 57.30 67.66</figDesc><table><row><cell cols="2">#Stages Test stage</cell><cell>BR</cell><cell>SV</cell><cell>LV</cell><cell>SH</cell><cell>HA</cell><cell>mAP</cell></row><row><cell>1</cell><cell>1</cell><cell>39.25</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>contains images from two</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for number of stages on DOTA dataset. 2 ? 3 indicates the ensemble result, which is the collection of all outputs from refinement stages.</figDesc><table><row><cell>Method</cell><cell cols="4">baseline ? ln(SkewIoU ) 1 ? SkewIoU exp(1 ? SkewIoU ) ? 1</cell></row><row><cell>RetinaNet-H</cell><cell>62.79</cell><cell>NAN</cell><cell>65.06 (+2.27)</cell><cell>65.34 (+2.55)</cell></row><row><cell>R 3 Det  ?</cell><cell>67.66</cell><cell>NAN</cell><cell>68.97 (+2.31)</cell><cell>69.50 (+2.84)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Experiments with different SkewIoU functions.</figDesc><table><row><cell>The train-</cell></row></table><note>scenarios including ships on sea and ships close inshore. All the images are collected from six famous harbors. The image sizes range from 300?300 to 1, 500?900.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>74.30 47.70 70.30 64.90 67.80 70.00 90.80 79.10 78.20 53.60 62.90 67.00 64.20 50.20 68.20 RADet [24] ResNeXt101 79.45 76.99 48.05 65.83 65.46 74.40 68.86 89.70 78.14 74.97 49.92 64.63 66.14 71.58 62.16 69.09 RoI-Transformer [10] ResNet101 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56 80.65 52.09 68.36 68.36 60.32 72.41 90.85 87.94 86.86 65.02 66.68 66.25 68.24 65.21 72.61 FADet [23] ResNet101 90.21 79.58 45.49 76.41 73.18 68.27 79.56 90.83 83.40 84.68 53.40 65.42 74.17 69.69 64.86 73.28 Gliding Vertex [50] ResNet101 89.64 85.00 52.26 77.34 73.01 73.14 86.82 90.74 79.02 86.81 59.55 70.91 72.94 70.86 57.32 75.02 Mask OBB [46] ResNeXt101 89.56 85.95 54.21 72.90 76.52 74.16 85.63 89.85 83.81 86.48 54.89 69.64 73.94 69.06 63.32 75.83.62 53.42 76.03 74.01 77.16 79.45 90.83 87.15 84.51 67.72 60.33 74.61 71.84 65.55 75.75 CenterMap OBB [47] ResNet101 89.83 84.41 54.60 70.25 77.66 78.32 87.19 90.66 84.89 85.27 56.46 69.23 74.13 71.56 66.06 76.03</figDesc><table><row><cell></cell><cell cols="3">ICN [1] 81.40 CAD-Net [55] ResNet101 ResNet101 87.8</cell><cell>82.4</cell><cell>49.4</cell><cell>73.5</cell><cell>71.1</cell><cell>63.5</cell><cell>76.7</cell><cell>90.9</cell><cell>79.2</cell><cell>73.3</cell><cell>48.4</cell><cell>60.9</cell><cell>62.0</cell><cell>67.0</cell><cell>62.2</cell><cell>69.9</cell></row><row><cell>stage</cell><cell>Cascade-FF [18] SCRDet [54]</cell><cell>ResNet152 ResNet101</cell><cell cols="16">89.9 89.98 33 80.4 51.7 77.4 68.2 75.2 75.6 90.8 78.8 84.4 62.3 64.6 57.7 69.4 50.1 71.8</cell></row><row><cell></cell><cell>FFA [11]</cell><cell>ResNet101</cell><cell>90.1</cell><cell>82.7</cell><cell>54.2</cell><cell>75.2</cell><cell>71.0</cell><cell>79.9</cell><cell>83.5</cell><cell>90.7</cell><cell>83.9</cell><cell>84.6</cell><cell>61.2</cell><cell>68.0</cell><cell>70.7</cell><cell>76.0</cell><cell>63.7</cell><cell>75.7</cell></row><row><cell cols="19">APE [63] IENet [29] PIoU [6] P-RSDet [60] O 2 -DNet [48] DRN [37] 89.96 Single-stage ResNeXt101 ResNet101 80.20 64.54 39.82 32.07 49.71 65.01 52.58 81.45 44.66 78.51 46.54 56.73 64.40 64.24 36.75 57.14 DLA-34 80.9 69.7 24.1 60.2 38.3 64.4 64.8 90.9 77.2 70.4 46.5 37.1 57.1 61.9 64.0 60.5 ResNet101 89.02 73.65 47.33 72.03 70.58 73.71 72.76 90.82 80.12 81.32 59.45 57.87 60.79 65.21 52.59 69.82 Hourglass104 89.31 82.14 47.33 61.21 71.32 74.03 78.62 90.76 82.23 81.36 60.93 60.17 58.21 66.98 61.03 71.04 Hourglass104 89.71 82.34 47.22 64.10 76.22 74.43 85.84 90.57 86.18 84.89 57.65 61.93 69.30 69.63 58.48 73.23 R 3 Det  ? (Ours) ResNet101 88.76 83.09 50.91 67.27 76.23 80.39 86.72 90.78 84.68 83.24 61.98 61.35 66.91 70.63 53.94 73.79</cell></row><row><cell></cell><cell>R 3 Det (Ours)</cell><cell>ResNet152</cell><cell cols="16">89.80 83.77 48.11 66.77 78.76 83.27 87.84 90.82 85.38 85.51 65.67 62.68 67.53 78.56 72.62 76.47</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Detection accuracy on different objects (AP) and overall performance (mAP) evaluation on DOTA. R 3 Det ? indicates that two refinement stages have been added. MS indicates that multi-scale training or testing is used.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">Image Size mAP (07) mAP (12) Speed</cell></row><row><cell>R 2 CNN [20]</cell><cell>ResNet101</cell><cell>800*800</cell><cell>73.07</cell><cell>79.73</cell><cell>5fps</cell></row><row><cell>RC1 &amp; RC2 [34]</cell><cell>VGG16</cell><cell>-</cell><cell>75.7</cell><cell>-</cell><cell>-</cell></row><row><cell>RRPN [35]</cell><cell>ResNet101</cell><cell>800*800</cell><cell>79.08</cell><cell>85.64</cell><cell>1.5fps</cell></row><row><cell>R 2 PN [58]</cell><cell>VGG16</cell><cell>-</cell><cell>79.6</cell><cell>-</cell><cell>-</cell></row><row><cell>RetinaNet-H</cell><cell>ResNet101</cell><cell>800*800</cell><cell>82.89</cell><cell>89.27</cell><cell>14fps</cell></row><row><cell>RRD [26]</cell><cell>VGG16</cell><cell>384*384</cell><cell>84.3</cell><cell>-</cell><cell>-</cell></row><row><cell>RoI-Transformer [10]</cell><cell>ResNet101</cell><cell>512*800</cell><cell>86.20</cell><cell>-</cell><cell>6fps</cell></row><row><cell>Gliding Vertex [50]</cell><cell>ResNet101</cell><cell>-</cell><cell>88.20</cell><cell>-</cell><cell>-</cell></row><row><cell>DRN [37]</cell><cell>Hourglass104</cell><cell>-</cell><cell>-</cell><cell>92.70</cell><cell>-</cell></row><row><cell>SBD [33]</cell><cell>ResNet50</cell><cell>-</cell><cell>-</cell><cell>93.70</cell><cell>-</cell></row><row><cell>R 3 Det  *</cell><cell>ResNet101</cell><cell>800*800</cell><cell>89.14</cell><cell>94.98</cell><cell>4fps</cell></row><row><cell>RetinaNet-R</cell><cell>ResNet101</cell><cell>800*800</cell><cell>89.18</cell><cell>95.21</cell><cell>8fps</cell></row><row><cell></cell><cell>ResNet101</cell><cell>300*300</cell><cell>87.14</cell><cell>93.22</cell><cell>18fps</cell></row><row><cell></cell><cell>ResNet101</cell><cell>600*600</cell><cell>88.97</cell><cell>94.61</cell><cell>15fps</cell></row><row><cell>R 3 Det</cell><cell>ResNet101 MobileNetV2</cell><cell>800*800 300*300</cell><cell>89.26 77.16</cell><cell>96.01 84.31</cell><cell>12fps 23fps</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>600*600</cell><cell>86.67</cell><cell>92.83</cell><cell>20fps</cell></row><row><cell></cell><cell>MobileNetV2</cell><cell>800*800</cell><cell>88.71</cell><cell>94.45</cell><cell>16fps</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Accuracy and speed on HRSC2016. 07 (12) means using the 2007 (2012) evaluation metric.</figDesc><table><row><cell>Method</cell><cell cols="2">mAP Plane</cell><cell>Car</cell></row><row><cell cols="4">YOLOv2 [39] 87.90 96.60 79.20</cell></row><row><cell cols="4">R-DFPN [53] 89.20 95.90 82.50</cell></row><row><cell>DRBox [30]</cell><cell cols="3">89.95 94.90 85.00</cell></row><row><cell>S 2 ARN [2]</cell><cell cols="3">94.90 97.60 92.20</cell></row><row><cell>RetinaNet-H</cell><cell cols="3">95.47 97.34 93.60</cell></row><row><cell>ICN [1]</cell><cell>95.67</cell><cell>-</cell><cell>-</cell></row><row><cell>FADet [23]</cell><cell cols="3">95.71 98.69 92.72</cell></row><row><cell>R 3 Det</cell><cell cols="3">96.17 98.20 94.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Performance evaluation on UCAS-AOD datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Performance on ICDAR2015. Tick means the module is enabled</figDesc><table><row><cell>(a) Horizontal detection</cell><cell>(b) Rotation detection</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 764-773, 2017. 3, 6 [10] Jian Ding, Nan Xue, Yang Long, Gui-Song Xia, and Qikai Lu. Learning roi transformer for oriented object detection in aerial images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2849-2858, 2019. 3, 8 [11] Kun Fu, Zhonghan Chang, Yue Zhang, Guangluan Xu, Keshu Zhang, and Xian Sun. Rotation-aware and multi-scale convolutional neural network for object detection in remote sensing images. ISPRS Journal of Photogrammetry and Remote Sensing, 161:294-308, 2020. 8 [12] Kun Fu, Yang Li, Hao Sun, Xue Yang, Guangluan Xu, Yuting Li, and Xian Sun. A ship rotation detection model in remote sensing images based on feature fusion pyramid network and deep reinforcement learning. Remote Sensing,</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t x = (x ? x a )/w a , t y = (y ? y a )/h a t w = log(w/w a ), t h = log(h/h a ), t ? = ? ? ? a t x = (x ? x a )/w a , t y = (y ? y a )/h a t w = log(w /w a ), t h = log(h /h a ), t ? = ? ? ? a(1)where x, y, w, h, ? denote the box's center coordinates, width, height and angle, respectively. Variables x, x a , x are for the ground-truth box, anchor box, and predicted box, respectively (likewise for y, w, h, ?).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The red and green points should be totally overlapping to each other, while here the red point is intentionally offset in order to distinguishingly visualize the entire process.(a) (b) (c) (d) RetinaNet-H (e) RetinaNet-R (f) R 3 Det *</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards multi-class object detection in unconstrained remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleonora</forename><surname>Seyed Majid Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Bahmanyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="150" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Single shot anchor refinement network for oriented object detection in optical remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songze</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaonan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="87150" to="87161" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4974" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Dual refinement networks for accurate and fast object detection in real-world scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08638</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Piou loss: Towards accurate oriented object detection in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kean</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selective refinement network for high performance face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Stan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8231" to="8238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deformable convolutional</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="1922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep direct regression for multi-oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu-Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="745" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascade detector with feature fusion for arbitrary-oriented objects in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liping</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho-Deok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12736</idno>
		<title level="m">Philipp Benz, Jinsun Park, and In So Kweon. Propose-and-attend single shot detector</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">R2cnn: rotational region cnn for orientation robust scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09579</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Icdar 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelos</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><forename type="middle">Ramaseshan</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 13th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rethinking classification and localization for cascade r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11914</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feature-attentioned object detection in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3886" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Radet: Refine feature pyramid network and multi-layer attention network for arbitrary-oriented object detection of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghua</forename><surname>Shang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">389</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3676" to="3690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotation-sensitive regression for oriented scene text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghui</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5909" to="5918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Ienet: Interacting embranchment one stage anchor free detector for orientation aerial object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youtian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengming</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Guan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00969</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning a rotation invariant detector with rotatable bounding box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Lei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09405</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fots: Fast oriented text spotting with a unified network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dagui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Omnidirectional scene text detection with sequential-free box discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianwen</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lele</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhepeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02371</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A high resolution optical satellite image dataset for ship recognition and some new baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Pattern Recognition Applications and Methods</title>
		<meeting>the International Conference on Pattern Recognition Applications and Methods</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nibal</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imen</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic refinement network for oriented and densely packed object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kekai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolei</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="11207" to="11216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
	</analytic>
	<monogr>
		<title level="m">Micha?l Mathieu, Rob Fergus, and Yann LeCun</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Detecting oriented text in natural images by linking segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoguang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2550" to="2558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mask obb: A semantic attentionbased mask oriented bounding box representation for multicategory object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wensheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page">2930</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning center probability map for detecting objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng-Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Oriented objects as pairs of middle lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongqi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.10694</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dota: A large-scale dataset for object detection in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gliding vertex on the horizontal bounding box for multi-oriented object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Building detection in high spatial resolution remote sensing imagery with the u-rotation detection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiurui</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="6036" to="6058" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object detection with head direction in remote sensing images based on rotational region cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2507" to="2510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Automatic ship detection in remote sensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid networks. Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Scrdet: Towards more robust detection for small, cluttered and rotated objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="8232" to="8241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Cad-net: A context-aware detection network for objects in remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="10015" to="10024" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Cascade retinanet: Maintaining consistency for single-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingpeng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06881</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Toward arbitrary-oriented ship detection with rotated region proposal and discrimination networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zenghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengnan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1745" to="1749" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Distance-iou loss: Faster and better learning for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongguang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwei</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12993" to="13000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Objects detection for remote sensing images based on polar coordinates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.02988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">East: An efficient and accurate scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Orientation robust object detection in aerial images using deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haigang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3735" to="3739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Adaptive period embedding for representing oriented objects in aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueqing</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
