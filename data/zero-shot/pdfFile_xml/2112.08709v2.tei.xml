<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DOCmT5: Document-Level Pretraining of Multilingual Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Hsuan</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ratnakar</surname></persName>
							<email>vratnakar@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
							<email>melvinp@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Washington ? Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DOCmT5: Document-Level Pretraining of Multilingual Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce DOCmT5, a multilingual sequence-to-sequence language model pretrained with large scale parallel documents. While previous approaches have focused on leveraging sentence-level parallel data, we try to build a general-purpose pretrained model that can understand and generate long documents. We propose a simple and effective pretraining objective -Document reordering Machine Translation (DrMT), in which the input documents that are shuffled and masked need to be translated. DrMT brings consistent improvements over strong baselines on a variety of document-level generation tasks, including over 12 BLEU points for seen-languagepair document-level MT, over 7 BLEU points for unseen-language-pair document-level MT and over 3 ROUGE-1 points for seen-languagepair cross-lingual summarization. We achieve state-of-the-art (SOTA) on WMT20 De-En and IWSLT15 Zh-En document translation tasks. We also conduct extensive analysis on various factors for document pretraining, including (1) the effects of pretraining data quality and (2) the effects of combining monolingual and cross-lingual pretraining. We plan to make our model checkpoints publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multilingual pretrained language models have been useful for a wide variety of NLP tasks. pretraining on large-scale multilingual corpora facilitates transfer across languages and benefits low-resource languages.</p><p>Previously, sentence-level or word-level crosslingual objectives have been considered for pretraining large language models (LLM), but not much effort has been put in document-level objectives for pretraining. In this work, we propose a multilingual sequence-to-sequence language model pretrained with cross-lingual structure-aware document-level objectives. DOCmT5 is built on top of mT5 <ref type="bibr">(Xue et al., 2021)</ref> and is further trained with parallel documents across multiple language pairs. To encourage the model to gain a deep understanding of the document structure and cross-lingual relationships, we consider a challenging translation scenario as a second-stage pretraining task: the input sentences are shuffled in a random order and random spans are masked. To effectively translate the input document, the model needs to reconstruct the document in the original order, making the model learn sentence relationships, and also recover the masked spans. This objective is effective on documentlevel generation tasks such as machine translation and cross-lingual summarization, outperforming previous best systems.</p><p>To enable cross-lingual pretraining at a large scale, we created a synthetic parallel document corpus. To avoid expensive human annotation, we use off-the-shelf neural machine translation (NMT) models to translate the documents in the mC4 corpus <ref type="bibr">(Xue et al., 2021)</ref> into English. In our experimental results, this corpus is more effective for pretraining than existing large-scale automatically aligned corpora (e.g., CCAligned <ref type="bibr" target="#b7">(El-Kishky et al., 2020)</ref>).</p><p>We also conduct extensive ablation studies and provide insights on document-level pretraining. We show that simple document-level pretraining is more useful than sentence-level pretraining for generative tasks. We also show that data quality matters when performing multilingual document pretraining. Finally, we don't observe improvements from combining mono-lingual and cross-lingual objectives when evaluating on two document-level translation tasks.</p><p>In summary, this paper makes the following contributions:</p><p>? We build a state-of-the-art multilingual document-level sequence-to-sequence language model pretrained with a structure-aware cross-lingual objective.  <ref type="figure">Figure 1</ref>: Overview of our proposed Document-Reordering Machine Translation (DrMT) pretraining. For each input document, the sentences are shuffled in random order and then randomly selected spans will be masked. The prediction target of DOCmT5 is to generate the translation of the input document.</p><p>? Our proposed model achieves strong results on cross-lingual summarization and documentlevel machine translation for seen and unseen language paris, including SOTA on WMT20 De-En and IWSLT2015 Zh-En tasks.</p><p>? We also conduct extensive experiments to study what works and what doesn't work in document-level multilingual pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multilingual Pretraining</head><p>Multilingual pretrained models provide a set of parameters that can be quickly finetuned for different downstream tasks <ref type="bibr" target="#b25">(Ruder et al., 2021)</ref>. Some popular models are: mBERT <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and XLM-R <ref type="bibr" target="#b4">(Conneau et al., 2020)</ref> which pretrain with masked language modeling objective using only monolingual data, mT5 <ref type="bibr">(Xue et al., 2021)</ref> and mBART  which use a sequenceto-sequence language model and pretrain on largescale mono-lingual corpora across many languages. Our proposed model uses mT5 as a backbone and further utilizes pseudo-parallel documents to learn better cross-lingual representations.</p><p>To capture cross-lingual information, translation language modeling <ref type="bibr" target="#b5">(Conneau and Lample, 2019)</ref> and its variants (VECO <ref type="bibr" target="#b20">(Luo et al., 2021)</ref>, ERNIE-M <ref type="bibr" target="#b23">(Ouyang et al., 2021)</ref>) was proposed to leverage sentence-level parallel data. AMBER  use two explicit alignment objectives that align representations at the word and sentence level. HICTL  pretrains on parallel sentences with word and sentence-level contrastive losses. mBART50 <ref type="bibr" target="#b30">(Tang et al., 2021)</ref>, mT6 <ref type="bibr">(Chi et al., 2021)</ref> and nmT5 <ref type="bibr" target="#b9">(Kale et al., 2021)</ref> focus on second-stage of pretraining using large-scale sentence-level translation data. Our model goes beyond the sentence and focuses on document-level understanding.</p><p>While sentence-level pretraining has received a lot of attention, document-level pretraining has been under-studied. Unicoder <ref type="bibr" target="#b11">(Huang et al., 2019)</ref> replaces alternating sentences in a document with translations and pretrains with masked language modeling. MARGE  adopts the retriever-generator paradigm and pretrains with an unsupervised translation objective on automatically retrieved documents. M2M100  pretrains sequence-to-sequence language models on automatically mined parallel sentences and documents. Our model considers a challenging supervised translation objective on parallel documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multilingual Parallel Data Sources</head><p>OPUS-100 <ref type="bibr" target="#b0">(Aharoni et al., 2019;</ref><ref type="bibr" target="#b35">Zhang et al., 2020a)</ref> is collected from a variety of domains and is human labeled but it is at the sentence level. ML50 <ref type="bibr" target="#b30">(Tang et al., 2021)</ref> is collected from different machine translation challenges and other publicly available corpora such as OPUS, but most of the data is at the sentence level. CCMatrix <ref type="bibr" target="#b27">(Schwenk et al., 2021b)</ref> and Wikimatrix <ref type="bibr" target="#b26">(Schwenk et al., 2021a)</ref> use multilingual sentence embedding to automatically mine parallel sentences. Perhaps the most closest to our proposed corpus is CCAligned <ref type="bibr" target="#b7">(El-Kishky et al., 2020)</ref>, which is also automatically mined but its quality is in question <ref type="bibr" target="#b16">(Kreutzer et al., 2021)</ref>   MTmC4 corpus does not require human annotation and instead was produced by NMT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Document-level Machine Translation</head><p>There are different ways to incorporate document context into translation model. Just to name a few, previous works have explored concatenation-based methods <ref type="bibr" target="#b31">(Tiedemann and Scherrer, 2017;</ref><ref type="bibr" target="#b13">Junczys-Dowmunt, 2019;</ref><ref type="bibr" target="#b29">Sun et al., 2020;</ref><ref type="bibr" target="#b19">Lopes et al., 2020)</ref>, multi-source context encoder <ref type="bibr" target="#b36">(Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Jean et al., 2017)</ref>, and hierarchical networks <ref type="bibr" target="#b38">(Zheng et al., 2020;</ref><ref type="bibr" target="#b37">Zhang et al., 2020b;</ref>. This line of research focuses on architectural modifications of neural translation models. We focus on how to design a generalized pretraining objective and furthermore, our model can be finetuned for various downstream tasks (e.g. summarization) without task-specific changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multilingual Pretraining</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">mC4</head><p>For pretraining, we use mC4 <ref type="bibr">(Xue et al., 2021)</ref>, a large scale corpus extracted from Common Crawl that covers over 100 languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">MTmC4: Creating Parallel Documents with mC4</head><p>To create large-scale parallel documents, we take mC4 as a starting point and use in-house NMT models to translate documents from 25 languages into English. Each sentence in each document is translated independently. For each language, we sample 1 million documents, if there are more than that to start with, in mC4. Detailed data statistics for all the languages can be found in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Document Reordering Machine Translation (DrMT)</head><p>We start by introducing two related pretraining objectives:</p><p>? NMT Pretraining: <ref type="bibr" target="#b30">Tang et al. (2021)</ref> and <ref type="bibr" target="#b9">Kale et al. (2021)</ref> proposed to perform a second-stage of pretraining using sentencelevel MT data. The objective here is to perform sentence-level translation without any other changes to the input.</p><p>? Monolingual Document Reordering (Dr) Pretraining: This objective, proposed by mBART , changes the order of the sentences in each document. This is then followed by the original span corruption objective in T5. The decoder is required to generate the original document in order.</p><p>We combine these two objectives and propose DrMT. In DrMT, we introduce two types of noise on the input: (i) sentences in the document are randomly shuffled and (ii) randomly sampled spans are masked. In order to correctly translate the content, the model needs to decipher the corrupted document in order first. This enforces the models to gain deep understanding of the document structure. More formally, suppose we have N language pairs and each language has a set of parallel documents, the whole collection of document pairs are = { 1 , 2 , ..., }. And a pair of ( , ) is an instance in one of the language documents . The overall learning objective is maximizing the likelihood of given a corrupted ( ), that is</p><formula xml:id="formula_0">? ? ? ( , )? log ( | ( )).</formula><p>(1)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DOCmT5</head><p>We use mT5 as the backbone model. mT5 is a sequence-to-sequence language model pretrained with the span corruption objective in which random spans in the input are masked and the decoder is required to reconstruct the masked spans (see <ref type="bibr" target="#b24">Raffel et al. (2020)</ref> and <ref type="bibr">Xue et al. (2021)</ref> for further details). Our system, DOCmT5, incorporates a second-stage pretraining with a structure-aware cross-lingual objective(3.2) on pseudo parallel documents. Detailed comparisons with previous multilingual language models can be found in <ref type="table" target="#tab_2">Table  1</ref>. We provide two variants of DOCmT5 with both Base and Large model settings:</p><p>? DOCmT5-5 This model is pretrained with 5 languages: {De, Ru, Tr, Vi and Es}. For all of the pretraining objective baselines in this paper, we pretrain with this set of languages, unless specified otherwise.</p><p>? DOCmT5-25 This model is pretrained with 25 languages. We show the full list of languages and their sizes in <ref type="table" target="#tab_3">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>We use mT5-Base 1 and mT5-Large 2 checkpoints at 1M steps as our pretrained models. We perform a second-stage of pretraining for an additional 0.5M steps using batches of 256 examples each of max length 1024. The learning rate is determined by 1 https://console.cloud.google.com/ storage/browser/t5-data/pretrained_ models/mt5/base/ 2 https://console.cloud.google.com/ storage/browser/t5-data/pretrained_ models/mt5/large/ a inverse square root scheduler as defined in T5, with the learning rate set to 1? ? where n is the number of training step. We use the same span corruption objective as T5, with 15% of random tokens masked and an average noise span length of 3. For finetuning, we use a constant learning rate of 0.001 and dropout rate of 0.1 for all tasks until convergence. We adopt greedy decoding during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Second-Stage Pretraining on 5 Languages</head><p>Language models pretrained with huge numbers of languages suffer from curse of multilinguality. In order to make a fair comparison, we create a strong mT5 model by continuing to pretrain on the same 5 languages of mC4 as in DOCmT5-5 with the same number of steps using the original span corruption objective in mT5. Models pretrained with this objective is denoted as cont-5langs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Monolingual Document Reordering (Dr)</head><p>We briefly mention this objective in Sec-tion3.2. We use the mC4 corpus for this pretraining objective. Models pretrained with this objective is denoted as Dr (Document Reordering).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Document TLM (DocTLM)</head><p>In <ref type="bibr" target="#b5">Conneau and Lample (2019)</ref>, the authors propose the translation language modeling(TLM) objective, which concatenates parallel sentences and applies masked language modeling to learn cross-lingual knowledge.</p><p>Here we extend it to the document level by concatenating parallel documents. Instead of masking single tokens, we follow the span corruption objective in T5 and mask consecutive spans. The models are pretrained with this objective on MTmC4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Document NMT (DocNMT)</head><p>We consider a standard document-level machine translation for pretraining. The source document is the input and the target translation is the output. We use MTmC4 for this pretraining objective.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Cross-Lingual Summarization</head><p>We evaluate DOCmT5 on cross-lingual summarization as it is challenging for the model to summarize a long document and translate the salient information at the same time. We use Wikilingua, a cross-lingual summarization dataset, in which a document from an arbitrary language must be summarized in English. We adopt the GEM <ref type="bibr" target="#b9">(Gehrmann et al., 2021)</ref> version where the data is re-split to avoid train-test overlap between languages. We use a special prefix for cross-lingual summarization: "Summarize X to Y", where X and Y are the source and target language names respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Results on Seen Language Pairs</head><p>We show the finetuning results of language pairs that are in the second stage of pretraining in Table 3. We use the same four languages that are in Wikilingua's original release {Es, Ru, Tr, Vi}.</p><p>The Dr objective brings substantial improvements over cont-5langs in all four languages, justifying the importance of structure-aware objectives. As for cross-lingual objectives, DocTLM is better than DocNMT in almost all languages except for Russian. DOCmT5-5 substantially outperforms Doc-NMT and DocTLM, showing that our proposed pretraining objective leads to improved cross-lingual learning. The results of DOCmT5-25 are inferior to DOCmT5-5 and this is possibly due to capacity dilution <ref type="bibr" target="#b1">(Arivazhagan et al., 2019)</ref>. As we increase the capacity, we see that DOCmT5-25-Large outperforms DOCmT5-5-Large. DOCmT5-25-Large is the best overall model outperforming the strong prior system: mBART.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results on Unseen Language Pairs</head><p>We show the finetuning results of language pairs that are not in the second-stage of pretraining stage in <ref type="table" target="#tab_6">Table 4</ref>. We use three languages {Fr, Id, Hi} 3 . Once again, we see that the Dr objective brings substantial improvements over cont-5langs. Surprisingly, without directly pretraining on the same language pairs, DOCmT5-5 leads to substantial improvements over strong baselines. This shows that our pretraining objectives are able to generalize to other languages. DOCmT5-25 pretrains on French and Hindi but not Indonesian and hence we observe improvements of average results over DOCmT5-5. The improvements of DOCmT5 are not so substantial and sometimes even hurt performance in highresource languages: French and Indonesian, which have 44556 and 33237 training examples respectively and there are only 6942 examples in Hindi. DOCmT5-25-Large obtains the best results in almost all 3 languages except for French.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained Model d-BLEU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous Systems</head><p>NTT <ref type="bibr" target="#b15">(Kiyono et al., 2020)</ref> 43.80 PROMT <ref type="bibr" target="#b22">(Molchanov, 2020)</ref> 39.60 OPPO <ref type="bibr" target="#b28">(Shi et al., 2020)</ref> 42.20  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mono</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Document-Level Machine Translation</head><p>We evaluate DOCmT5 on document translation. We split each document into chunks with a max length of 512 tokens. During inference, the decoded chunks are concatenated together to form the final document. We use prefix "Translate X to Y" for translation, where X and Y are the source and target language names respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Seen Language Pair: WMT20 De-En</head><p>WMT20 De-En is a document-level machine translation task. We use parallel training data from <ref type="bibr">3</ref> We choose French to study the transfer ability of the cross-lingual models on high-resource and same-script (latin) languages. Indonesian is for studying high-resource and different-script language. Hindi is for studying low-resource and different-script language.  WMT20 without using additional monolingual data. From the results in Table 5 4 , we see that Dr provides large gains. DocNMT outperforms DocTLM. This is probably due to the fact that DocNMT is more close to the document-level translation task. DOCmT5-5 once again outperforms Dr and other strong cross-lingual baselines. DOCmT5-5 is better than DOCmT5-25 again because of capacity dilution as noted in <ref type="bibr" target="#b0">Aharoni et al. (2019)</ref>. As expected, DOCmT5-5-Large outperforms DOCmT5-5 and to the best of our knowledge, achieves the SOTA. Note that previous systems use one or more of the following techniques: additional monolingual data, back-translation, ensembling or re-ranking tailored to a single translation pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretrained Model d-BLEU</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Unseen Language Pair: IWSLT 2015 Zh-En</head><p>We use IWSLT 2015 Zh-En, another documentlevel machine translation task, to examine the multilingual transferability of DOCmT5 when the target transfer language (Chinese in this case) is of a very different script. Chinese is only in the firststage pretraining of mT5 but not in our second-stage pretraining. We use parallel training data from IWSLT15 without using additional monolingual data. <ref type="figure" target="#fig_0">Following HAN (Werlen et al., 2018)</ref>, we use 2010-2013 TED as the test set. The results are in <ref type="table" target="#tab_10">Table 6</ref>. DOCmT5-5 outperforms the strong crosslingual and mono-lingual baselines, demonstrating impressive transfer capability . DOCmT5-25 includes Chinese as one of the second-stage pretraining languages therefore obtains better numbers than DOCmT5-5. Unsurprisingly, large models are better than their corresponding base models.</p><p>To the best of our knowledge, DOCmT5-25-Large achieves the SOTA on this task. We qualitatively analyze the translations of different systems in Appendix A.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Document Translation Without Finetuning</head><p>We further show that DOCmT5 is able to perform document translation without finetuning, i.e., evaluate the model right after second-stage pretraining without any finetuning on task-specific data. We show the results in <ref type="table" target="#tab_12">Table 7</ref>. While the monolingual pretrained models completely fail to produce meaningful translations, DOCmT5-5 is able to achieve over 20 BLEU points in De-En and 15 in Ru-En. Not surprisingly, DOCmT5-5-Large further improves to over 35 and 29 respectively. DOCmT5-25 includes Pl-En and Ja-En in the second-stage pretraining and therefore obtains competitive results on these two language pairs with either base or large model. Although DOCmT5-5 is not pretrained on Pl-En, the large model gets over 14 BLEU on this task. One hypothesis is that Polish uses the Latin script and shares common subwords with German and Spanish, allowing our model to transfer knowledge across languages. On the other hand, the DOCmT5-5-Base model fails to produce meaningful translations for Pl-En. This shows the importance of size when performing multilingual pretraining. The best model is DocNMT which obtains over 40 BLUE points in both De-En and Ru-En, outperforming DOCmT5-5 and DOCmT5-25. This is reasonable because DOCmT5 shuffles documents in pretraining and this is misaligned with the document translation task inputs. The impressive perfor- mance of both DocNMT and DOCmT5 shows that our MTmC4 corpus is of very high-quality and is likely better than the parallel data provided by the specific tasks in question. Further analysis of the quality of this data will be an interesting avenue for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Are Document-level Models Better Than Sentence-level Models?</head><p>To demonstrate the benefits of pretraining with longer context, we pretrain mT5 using translation language modeling (TLM) on five languages: {De, Es, Tr, Vi, Ru} with two different inputs. In DocTLM, we concatenate the parallel documents into a single training sequence. As for SenTLM, we break down the document into individual sentences and find the alignments in the parallel document pair. Then we concatenate the single aligned sentence pair as a training sequence. We finetune these second-stage pretrained models on Wikilingua and WMT20 De-En. The results are shown in <ref type="figure" target="#fig_0">Figure 2</ref> and <ref type="table" target="#tab_14">Table 8</ref>. We see that document-level models offer small improvements on summarization and very significant improvements on document-level translation, showing that the longer context is indeed useful.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effect of Data Quality in Second-stage Pretraining</head><p>In our experiments, we observe big differences between different parallel corpora. We compare against the CCAligned corpus -a large automatically mined corpus from Common Crawl which is found to be very noisy <ref type="bibr" target="#b16">(Kreutzer et al., 2021)</ref>. In contrast, MTmC4 is produced by using high-quality translation systems. We pretrain mT5-Base on five languages: {De, Es, Tr, Vi, Ru} with these two corpora using DocNMT and DocTLM. We demonstrate the Wikilingua results in <ref type="figure">Figure 3</ref> and WMT20 De-En results in <ref type="figure">Figure 4</ref>. Using our curated MTmC4 is consistently better regardless of pretraining objectives or tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Does Combining Mono-Lingual and Cross-Lingual Pretraining Help?</head><p>Here we try to see if combining both monolingual and cross-lingual objectives helps. We try two different continual pretraining strategies for combining Dr and DrMT. We use five languages: {De, Ru, Tr, Vi, Es}. (i) Dr ? DrMT: We first pretrain mT5 with Dr on mC4 for 0.5M steps and then pretrain with DrMT on MTmC4 for 0.5M steps. (ii) Dr + DrMT: We mix these two objectives with a 50-to-50% ratio and pretrain for 0.5M steps. In <ref type="table">Table 9</ref>, we show that (i) slightly improves over only DrMT in both tasks and (ii) slightly improves on WMT20 De-En but seems to hurt performance on ISWLT15 Zh-En.  <ref type="table">Table 9</ref>: Methods of combining mono-lingual and cross-lingual and their finetuning results on WMT20 De-En and IWSLT15 Zh-En.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">How Many Pretraining Steps is Required for DrMT?</head><p>To answer this question, we take different pretraining checkpoints of DOCmT5-5 and DOCmT5-25 and finetune with WMT20 De-En. The results are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. After 50k steps of pretraining with DrMT, both systems outperform the cont-5langs. After 300k steps, both systems roughly converge and perform similarly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present DOCmT5, a novel document-level multilingual pre-trained model. Our proposed objective, DrMT, is simple and effective and leads to large gains over strong baselines (e.g. mBART and MARGE) on crosslingual summarization and document-level translation. DOCmT5 achieved SOTA on two competitive document-level translation tasks: WMT20 De-En and IWSLT15 Zh-En. We further analyze various factors that contribute to successful document-level pre-training. We plan to release the pre-trained model to facilitate future work on document-level language understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices A Analysis of Document Translation</head><p>We take a deeper look at the translations produced by various systems to understand what makes DOCmT5 better. We demonstrate an example in <ref type="table" target="#tab_10">Table 6</ref>. We take the best system (DOCmT5-25-Large) and the cont-5langs baseline. We observe that DOCmT5 uses time tenses better than the baseline, producing more coherent sentences (redcolored texts). Additionally, DOCmT5 handles a compositional sentence more elegantly, instead of just using "and" (blue-colored texts). Finally, we observe that cont-5langs often makes minor translation mistakes while our DOCmT5 makes much fewer of them.</p><p>?11???? ????????????????????? ?????????????? ?BBC? ?? ??????????? ???????????????? "??????" ??????? ?? ????????? ??????????????? "????????????????" ??? ?? ?????????? ???????? ?6???????????? ???????????? ?? ????5??????? ???????????? ???????????????? ?????? ????????? ??????????? ???????????????? ...</p><p>And when I was 11 years old, I remember waking up one morning to the sound of a happy voice in the house. My father was listening to the BBC on his little gray radio. He had a smile on his face, which is rare, because most of the news was depressing. "The Taliban are gone!" My father shouted. I didn't know what that meant, but I could see that my father was very, very happy. "You can go to a real school now," he said. And I will never forget that morning. A real school. When I was six years old, the Taliban occupied Afghanistan and made it illegal for girls to go to school. So for the next five years, I was a woman in a man's suit and went to a secret school with my sister, who was not allowed to go out alone. This was the only way we were educated. We had to go in different directions every day so no one would suspect where we were going. ...</p><p>And I was 11 years old, and I remember awakefully waking up in the morning and hearing the familiar sound. My father was listening to the BBC news on his little radio. He was smiling, and it was rare, because most of the news was going to frustrate him. "Taliban go." The father went out. I don't know what that meant, but I can see that the father was very, very happy. "You can go to a real school now." He said. I'll never forget that morning. A real school. And I was six years old, and Taliban took Afghanistan and banned girls' schooling. So five years after that, my chick went to a secret school with my sister. And she wasn't allowed to go on a trip. It was the only way that we were educated. We walked on different roads every day so that nobody could suspect where we were. ...</p><p>When I was 11, I remember waking up one morning to the sound of joy in my house. My father was listening to BBC News on his small, gray radio. There was a big smile on his face which was unusual then, because the news mostly depressed him. "The Taliban are gone!" my father shouted. I didn't know what it meant, but I could see that my father was very, very happy. "You can go to a real school now," he said. A morning that I will never forget. A real school. You see, I was six when the Taliban took over Afghanistan and made it illegal for girls to go to school. So for the next five years, I dressed as a boy to escort my older sister, who was no longer allowed to be outside alone, to a secret school. It was the only way we both could be educated. Each day, we took a different route so that no one would suspect where we were going. ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Document</head><p>DOCmT5-25 translation mT5 translation Target Translation <ref type="figure">Figure 6</ref>: A comparison example of Zh-En document translation. DOCmT5 is able to produce consistent time tenses while mT5 baseline fails. DOCmT5 also produces longer and conherent sentences. Best viewed in color.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>SenTLM and DocTLM finetuning results on Wikilingua. The numbers are average of four languages: {Es, Tr, Ru, Vi}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>MTmC4 and CCAlgined finetuning results on Wikilingua. The numbers are average of four languages: {Es, Tr, Ru, Vi}. MTmC4 and CCAlgined finetuning results on WMT20 De-En.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>finetuning results of WMT20 De-En along with pretraining steps. We use DOCmT5-5-base.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2112.08709v2 [cs.CL] 5 May 2022</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Document</cell></row><row><cell>????????? ???????????? ????????</cell><cell></cell><cell>??? &lt;MASK&gt; ?? &lt;MASK&gt; ??? ???????? &lt;MASK&gt; ???????</cell><cell>reordering Machine Translation</cell><cell>Family is irreplaceable. You have to devote your time to nurturing those precious relationships.</cell></row><row><cell></cell><cell>Sentence</cell><cell></cell><cell></cell></row><row><cell>Multilingual</cell><cell>Shuffling</cell><cell></cell><cell></cell></row><row><cell>Common Crawl</cell><cell>+</cell><cell></cell><cell>DOC mT5</cell></row><row><cell>Documents</cell><cell>Span</cell><cell></cell><cell></cell></row><row><cell>Tu prop?sito en la vida es encontrar tu prop?sito.</cell><cell>Corruption</cell><cell>Dale todo tu &lt;MASK&gt; y alma.</cell><cell></cell><cell>Your purpose in life is to find your purpose. Give your</cell></row><row><cell>Dale todo tu coraz?n y alma.</cell><cell></cell><cell>Tu prop?sito en la vida es &lt;MASK&gt; tu prop?sito.</cell><cell></cell><cell>whole heart and soul to it.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>. Our Language Architecture Parameters # Languages Monolingual Data Cross-Lingual Data Parallel Docs</head><label></label><figDesc></figDesc><table><row><cell>mBERT</cell><cell>Encoder-only</cell><cell>180M</cell><cell>104</cell><cell>Wikipedia</cell><cell></cell></row><row><cell>RemBERT</cell><cell>Encoder-only</cell><cell>980M</cell><cell>110</cell><cell>Wikipedia and Common Crawl</cell><cell></cell></row><row><cell>XLM</cell><cell>Encoder-only</cell><cell>570M</cell><cell>100</cell><cell>Wikipedia</cell><cell>Misc.</cell></row><row><cell>XLM-R</cell><cell cols="2">Encoder-only 270M -550M</cell><cell>100</cell><cell>Common Crawl (CCNet)</cell><cell></cell></row><row><cell>mBART</cell><cell>Encoder-decoder</cell><cell>680M</cell><cell>25</cell><cell>Common Crawl (CC25)</cell><cell></cell></row><row><cell>mBART50</cell><cell>Encoder-decoder</cell><cell>680M</cell><cell>50</cell><cell>Common Crawl (CC25)</cell><cell>ML50</cell></row><row><cell>MARGE</cell><cell>Encoder-decoder</cell><cell>960M</cell><cell>26</cell><cell>Wikipedia or CC-News</cell><cell></cell></row><row><cell>mT5</cell><cell cols="2">Encoder-decoder 300M -13B</cell><cell>101</cell><cell>Common Crawl (mC4)</cell><cell></cell></row><row><cell>nmT5</cell><cell cols="2">Encoder-decoder 800M -3B</cell><cell>101</cell><cell>Common Crawl (mC4)</cell><cell>OPUS-100</cell></row><row><cell cols="3">DOCmT5 (ours) Encoder-decoder 580M -800M</cell><cell>25</cell><cell>Common Crawl (mC4)</cell><cell>MTmC4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of DOCmT5 to previous multilingual language models.</figDesc><table><row><cell cols="4">Language Size/GB Language Size/GB</cell></row><row><cell>De?</cell><cell>44</cell><cell>Ar</cell><cell>58</cell></row><row><cell>Es?</cell><cell>52</cell><cell>Az</cell><cell>42</cell></row><row><cell>Tr?</cell><cell>45</cell><cell>Bn</cell><cell>66</cell></row><row><cell>Ru?</cell><cell>58</cell><cell>Bn</cell><cell>66</cell></row><row><cell>Vi?</cell><cell>50</cell><cell>Fa</cell><cell>54</cell></row><row><cell>Fi</cell><cell>47</cell><cell>Ko</cell><cell>87</cell></row><row><cell>Fr</cell><cell>43</cell><cell>Lt</cell><cell>48</cell></row><row><cell>Hi</cell><cell>20</cell><cell>Mr</cell><cell>125</cell></row><row><cell>It</cell><cell>40</cell><cell>Nl</cell><cell>38</cell></row><row><cell>Ja</cell><cell>120</cell><cell>Pl</cell><cell>45</cell></row><row><cell>Pt</cell><cell>40</cell><cell>Th</cell><cell>63</cell></row><row><cell>Ro</cell><cell>53</cell><cell>Uk</cell><cell>66</cell></row><row><cell>Zh</cell><cell>41</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the MTmC4 corpus. ? indicates that the language is used in DOCmT5-5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>15.40 / 32.4033.29 / 12.35 / 27.50 3433.56 / 12.77 / 28.46 37.66 / 16.68 / 31.37 3235.11 / 14.09 / 29.58</head><label></label><figDesc>33.10 / 11.90 / 27.80 34.40 / 13.00 / 28.10 32.00 / 11.10 / 26.40 34.45 / 12.85 / 28.67    Mono-Lingual mT5 29.97 / 10.65 / 25.70 27.91 / 8.90 / 22.60 29.98 / 11.96 / 24.56 24.38 / 7.39 / 19.59 28.06 / 9.72 / 23.11 w. cont-5langs 34.50 / 12.83 / 28.37 30.20 / 10.30 / 24.77 32.12 / 13.71 / 26.40 28.95 / 9.74 / 23.76 31.44 / 11.64 / 25.82 w. Dr 36.22 / 14.18 / 30.31 32.29 / 11.64 / 26.63 34.25 / 14.93 / 28.50 30.07 / 10.46 / 25.00 33.20 / 12.80 / 27.61 DocNMT 33.45 / 12.56 / 29.04 30.93 / 11.01 / 25.82 33.32 / 14.10 / 27.54 27.60 / 9.26 / 22.52 31.40 / 11.59 / 26.12 w. DocTLM 35.40/ 13.76 / 29.71 30.26 / 10.33 / 24.78 34.85 / 15.35 / 28.88 30.35 / 10.86 / 25.03 32.71 / 12.57 / 27.10 DOCmT5-5 36.60 / 14.55 / 30.64 32.90 / 12.09 / 27.41 37.02 / 16.64 / 30.97 32.13 / 11.81 / 26.72 34.66 / 13.77 / 28.93 DOCmT5 </figDesc><table><row><cell>Pretrained Model</cell><cell>Es-En</cell><cell>Ru-En</cell><cell>Tr-En</cell><cell>Vi-En</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell>Previous Systems</cell><cell></cell><cell></cell><cell></cell></row><row><cell>mBART</cell><cell cols="2">38.30 / Cross-Lingual</cell><cell></cell><cell></cell><cell></cell></row><row><cell>w.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :36.28 / 14.27 / 30.78 3435.15 / 13.70 / 29.47 34.16 / 13.26 / 27.93 34.99 / 13.65 / 29.22</head><label>3</label><figDesc>Results of four seen langauges paris {Es, Tr, Ru, Vi} on Wikilingua. Each cell demonstrates three metrics: ROUGE-1, ROUGE-2 and ROUGE-L in order. The mBART results are taken from the GEM<ref type="bibr" target="#b9">(Gehrmann et al., 2021)</ref> paper for a strong baseline model. . </figDesc><table><row><cell>Pretrained Model</cell><cell>Fr-En</cell><cell>Id-En</cell><cell>Hi-En</cell><cell>Average</cell></row><row><cell></cell><cell></cell><cell>Mono-Lingual</cell><cell></cell><cell></cell></row><row><cell>mT5</cell><cell cols="3">29.66 / 9.96 / 24.37 29.08 / 9.87 / 23.83 26.18 / 8.51 / 20.91</cell><cell>28.30 / 9.44 / 23.03</cell></row><row><cell>w. cont-5langs</cell><cell cols="4">32.78 47</cell></row><row><cell>DOCmT5-5</cell><cell cols="4">34.02 / 12.57 / 28.21 34.31 / 13.09 / 28.56 32.24 / 11.84 / 26.06 33.52 / 12.50 / 27.61</cell></row><row><cell>DOCmT5-5-Large</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>/ 11.79 / 27.29 32.21 / 11.65 / 26.36 28.93 / 10.06 / 23.37 31.30 / 11.16 / 25.67 w. Dr 34.47 / 12.67 / 28.58 34.05 / 12.87 / 27.96 31.13 / 11.18 / 25.16 33.21 / 12.24 / 27.23 Cross-Lingual w. DocNMT 33.22 / 12.33 / 27.97 31.97 / 11.80 / 27.11 29.33 / 10.12 / 23.86 31.50 / 11.41 / 26.31 w. DocTLM 32.79 / 11.75 / 27.12 33.35 / 12.24 / 27.37 30.48 / 11.24 / 24.92 32.20 / 11.74 / 26.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Results of three unseen langauges paris {Fr, Id, Hi} on Wikilingua.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table /><note>Finetuning results on WMT20 De-En.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table><row><cell>Unseen language pair results on IWSLT</cell></row><row><cell>2015 Zh-En. Chinese is in the second-stage pretrain-</cell></row><row><cell>ing language set of DOCmT5-25 but not in those of</cell></row><row><cell>DOCmT5-5. DOCmT5-25-Large achieves SOTA.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Document translation without finetuning on WMT20 De-En, Ru-En, Pl-En and Ja-En.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>SenTLM and DocTLM finetuning results on WMT20 De-En.</figDesc><table><row><cell></cell><cell>DocNMT -CCAligned</cell><cell>DocNMT -MTmC4</cell><cell></cell></row><row><cell></cell><cell>DocTLM -CCAligned</cell><cell>DocTLM -MTmC4</cell><cell></cell></row><row><cell>ROUGE-1</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ROUGE-L</cell><cell></cell><cell></cell><cell></cell></row><row><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For all the document translation experiments in this paper, the numbers are calculated using sacreBLEU https:// github.com/mjpost/sacrebleu in document level.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Alexis Conneau, Jon Clark and Mihir Sanjay Kale for the helpful discussions. We also thank Sebastian Ruder, Noah Constant and Ankur Bapna for providing feedback on the manuscript.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Massively multilingual neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roee</forename><surname>Aharoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3874" to="3884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Arivazhagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Lepikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><forename type="middle">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Cherry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.05019</idno>
		<title level="m">Massively multilingual neural machine translation in the wild: Findings and challenges</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modeling discourse structure for document-level neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Automatic Simultaneous Translation</title>
		<meeting>the First Workshop on Automatic Simultaneous Translation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="30" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2021. mT6: Multilingual pretrained text-to-text transformer with translation pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewen</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saksham</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heyan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="1671" to="1683" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana, Dominican Republic. Association for Computational Linguistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?douard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8440" to="8451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="7059" to="7069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A massive collection of cross-lingual web-document pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5960" to="5969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond english-centric multilingual machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shruti</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Kishky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandeep</forename><surname>Baines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Celebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">107</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The GEM benchmark: Natural language generation, its evaluation and metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tosin</forename><surname>Adewumi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karmanya</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pawan</forename><surname>Sasanka Ammanamanchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anuoluwapo</forename><surname>Aremu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miruna-Adriana</forename><surname>Khyathi Raghavi Chandu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Clinciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustubh</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyu</forename><surname>Dhole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">Chinenye</forename><surname>Du?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Gangal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsunori</forename><surname>Garbacea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufang</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangfeng</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shailza</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mounica</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khyati</forename><surname>Maddela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saad</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahamood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Bodhisattwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Mcmillan-Major</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mille</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.gem-1.10</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Natural Language Generation</title>
		<editor>Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo?o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimorina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou</editor>
		<meeting>the 1st Workshop on Natural Language Generation<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="96" to="120" />
		</imprint>
	</monogr>
	<note>and Metrics (GEM 2021</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explicit alignment objectives for multilingual bidirectional encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3633" to="3643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unicoder: A universal language encoder by pretraining with multiple cross-lingual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1252</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><surname>Lauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05135</idno>
		<title level="m">Does neural machine translation benefit from larger context? arXiv preprint</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Microsoft translator at wmt 2019: Towards large-scale document-level neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Junczys-Dowmunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Machine Translation</title>
		<meeting>the Fourth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="225" to="233" />
		</imprint>
	</monogr>
	<note>Task Papers, Day 1)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">2021. nmT5 -is parallel data still relevant for pre-training massively multilingual language models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.87</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="683" to="691" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Tohoku-aip-ntt at wmt 2020 news translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Kiyono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takumi</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuto</forename><surname>Konno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
	<note>Makoto Morishita, and</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Quality at a glance: An audit of web-crawled multilingual datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Kreutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Wahab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasanbayar</forename><surname>Daan Van Esch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allahsera</forename><surname>Ulzii-Orshikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Tapo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Subramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claytone</forename><surname>Sokolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sikasote</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12028</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pre-training via paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilingual denoising pretraining for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Document-level neural MT: A systematic comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ant?nio</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amin Farajian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rachel</forename><surname>Bawden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><forename type="middle">F T</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conference of the European Association for Machine Translation</title>
		<meeting>the 22nd Annual Conference of the European Association for Machine Translation<address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
	<note>European Association for Machine Translation</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Veco: Variable and flexible cross-lingual pretraining for language understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuli</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3980" to="3994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Promt systems for wmt 2020 shared news translation task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Molchanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="248" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ERNIE-M: Enhanced multilingual representation by aligning cross-lingual semantics with monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="27" to="38" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Dominican Republic</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">XTREME-R: Towards more challenging and nuanced multilingual evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.802</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10215" to="10245" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Wikimatrix: Mining 135m parallel sentences in 1620 language pairs from wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1351" to="1361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">CCMatrix: Mining billions of high-quality parallel sentences on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.507</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6490" to="6500" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OPPO&apos;s machine translation systems for WMT20</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingxun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Zhengshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Conference on Machine Translation</title>
		<meeting>the Fifth Conference on Machine Translation</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="282" to="292" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Capturing longer context for document-level neural machine translation: A multi-resolutional approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08961</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multilingual translation from denoising pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chau</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3450" to="3466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural machine translation with extended context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Discourse in Machine Translation</title>
		<meeting>the Third Workshop on Discourse in Machine Translation</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On learning universal representations across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongxiang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luxi</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Documentlevel neural machine translation with hierarchical attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lesly</forename><surname>Miculicich Werlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjay</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2947" to="2954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="page" from="483" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving massively multilingual neural machine translation and zero-shot translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1628" to="1639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Improving the transformer translation model with document-level context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feifei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Long-short term masking transformer: A simple but effective baseline for document-level neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1081" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards making the most of context in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaixiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
