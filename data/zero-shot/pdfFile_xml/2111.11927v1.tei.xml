<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Graph Networks for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-11-23">23 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Dai</surname></persName>
							<email>daiwenrui@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabo</forename><surname>Chen</surname></persName>
							<email>chenyabo@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botao</forename><surname>Wang</surname></persName>
							<email>botaow@qti.qualcomm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">AI Research Shanghai</orgName>
								<address>
									<region>Qualcomm</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
							<email>sunyu@qti.qualcomm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">AI Research Shanghai</orgName>
								<address>
									<region>Qualcomm</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Guo</surname></persName>
							<email>mguo@qti.qualcomm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">AI Research Shanghai</orgName>
								<address>
									<region>Qualcomm</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junni</forename><surname>Zou</surname></persName>
							<email>zoujunni@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
							<email>xionghongkai@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Graph Networks for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-23">23 Nov 2021</date>
						</imprint>
					</monogr>
					<note>HAN LI ET AL.: HGN FOR 3D HUMAN POSE ESTIMATION 1 2 HAN LI ET AL.: HGN FOR 3D HUMAN POSE ESTIMATION</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent 2D-to-3D human pose estimation works tend to utilize the graph structure formed by the topology of the human skeleton. However, we argue that this skeletal topology is too sparse to reflect the body structure and suffer from serious 2D-to-3D ambiguity problem. To overcome these weaknesses, we propose a novel graph convolution network architecture, Hierarchical Graph Networks (HGN). It is based on denser graph topology generated by our multi-scale graph structure building strategy, thus providing more delicate geometric information. The proposed architecture contains three sparse-to-fine representation subnetworks organized in parallel, in which multi-scale graph-structured features are processed and exchange information through a novel feature fusion strategy, leading to rich hierarchical representations. We also introduce a 3D coarse mesh constraint to further boost detail-related feature learning. Extensive experiments demonstrate that our HGN achieves the state-of-the-art performance with reduced network parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D human pose estimation aims to predict the 3D spatial coordinates of body joints from a monocular image and has been widely exploited in various applications such as abnormal behavior detection, sports analysis and automated driving. In recent years, 2D human pose estimation performance has been greatly improved owing to more refined network structure design and richer 2D human pose datasets. Recent works show that using such detected 2D joints positions, the 3D human pose can also be efficiently and accurately regressed <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>. To further boost performance, many attempts have been made to explicitly utilize the human skeletal topology and use graph convolutional networks (GCNs) to exploit the spatial configurations for 3D human pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41]</ref>. However, the graph topology of human skeleton is usually formed by few number of joints (e.g. 17 joints in Human3.6M <ref type="bibr" target="#b15">[16]</ref> dataset) and is sparse. In this paper, we raise a critical issue: are such few number of joints enough for reflecting the body structure and estimating 3D human pose?</p><p>We analyze the drawbacks of sparse graph representation from two perspectives. First, regressing the 3D human pose based on 2D joints positions is an ill-posed problem since multiple valid 3D poses can be projected to the same 2D pose. This inherent ambiguity problem will be more serious when the defined human skeleton structure is oversimple, thus hindering performance improvement. Second, each part of the human body is sparsely represented by only one joint, which will impede the expression of local information and lead to positioning failure when facing complex motions and occluded scenes.</p><p>In this paper, we address these issues by exploiting denser graph topology, proposing a novel architecture named Hierarchical Graph Networks (HGN). Specifically, we propose a novel graph structure building strategy that utilizes the human shape information to obtain finer human structure representation. Then, starting from a sparse representation subnetwork, we gradually add sparse-to-fine representation subnetworks, and connect the multiscale subnetworks in parallel. Since the mapping relation between sparse and fine representations is hard to model by manually designed graph pooling and unpooling, we also propose a multi-scale feature fusion strategy to learn the suitable mapping and exchange information across the parallel subnetworks . In <ref type="figure" target="#fig_0">Figure 1</ref>, we illustrate several typical GCN architectures for 2D-to-3D human pose estimation. The advantage of our HGN is that more delicate features are able to be extracted from the sparse-to-fine human structures.</p><p>Though promising it is, merely increasing the complexity of the graph structure without giving meaning to its nodes can only bring a limited performance improvement. Inspired by recent dense mesh vertices estimation methods <ref type="bibr" target="#b6">[7]</ref>, we conduct dense vertices coarsening to obtain the pseudo-groundtruth of the coarse mesh vertices and utilize it as an additional constraint. Since sparse mesh can represent the shape information of human body, it contains more abundant and detailed information. Adding additional mesh constraints makes our model extract detail-related features, which is of great help to the evaluation of some joints with high degrees of freedom.</p><p>In a nutshell, this paper makes the following contributions:</p><p>? We build a novel hierarchical graph network with multi-scale feature fusion. It is based on denser graph topology generated by our multi-scale graph structure building strategy and contains more delicate geometric information.</p><p>? We generate coarse mesh vertices pseudo-groundtruth by dense vertices coarsening and utilize it as an additional constraint to make the model pay more attention to detailed information. Straight-forward architecture <ref type="bibr" target="#b41">[41]</ref>. (b) Graph Stacked Hourglass <ref type="bibr" target="#b37">[37]</ref> (c) Graph U-Nets <ref type="bibr" target="#b10">[11]</ref>. (d) Ours Hierarchical Graph Networks. (b) and (c) also leverage multi-scale features but their largest scale graph is formed by human skeleton topology, while our architecture introduces denser graph topology and organizes the network in a sparse-to-fine way. Note that our architecture may look like HR-Net <ref type="bibr" target="#b30">[31]</ref> but the network organization mode and feature fusion strategy are totally different.</p><p>? Experiment results demonstrate the superior performance of our HGN compared to other state-of-art GCN-based methods. The key idea of designing more delicate human structure representation may shed light on future research direction.</p><p>2 Related Work 3D Human pose estimation. Current 3D human pose estimation can be categorized into two types: one-stage approach and two-stage approach. The one-stage approach takes RGB images as input for 3D pose estimation. With the rapid development of deep learning, recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">43]</ref> take advantages of Convolutional Neural Networks (CNNs) for the image-to-3D human pose estiamtion. Zhou et al. <ref type="bibr" target="#b43">[43]</ref> propose a weakly-supervised transfer learning method to make full use of mixed 2D and 3D labels, which augments the 2D pose estimation sub-network with a 3D depth regression sub-network to regress the depth. Sun et al. <ref type="bibr" target="#b32">[33]</ref> employ soft-argmax operation to regress the 3D coordinates of body joints in a differentiable way. Pavlakos et al. <ref type="bibr" target="#b27">[28]</ref> exploit voxel to discretize representations of the space around the human body and use 3D heatmaps to estimate 3D human pose. The second category of approaches breaks the problem down into two steps: first predicting 2D human joints from the input image, and then lifting 2D joints to predict 3D pose. Our approach falls into this category. Martinez et al. <ref type="bibr" target="#b22">[23]</ref> propose a simple yet effective baseline for 3D human pose estimation, it uses only 2D joints as input but gets highly accurate results, showing the importance of 2D joints information for 3D human pose estimation. Since the skeleton's topology can be viewed as a graph structure, there has been increasing use of Graph Convolutional Networks (GCNs) for 2D-to-3D pose estimation tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>.</p><p>Graph Convolutional Networks. In recent years, Graph Convolutional Networks (GCNs) have been widely used to process graph-structured data, it can be regarded as a generaliza-tion of traditional CNNs. In general, GCNs can be divided into two categories: spectral GCN <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref> and spatial GCN <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>, in which our approach falls into the second category. In the early day, Kipf and Welling <ref type="bibr" target="#b17">[18]</ref> introduce the "vanilla" GCN, which performs the transformation and aggregation of graph-structured data via a simple graph convolution. Based on "vanilla" GConv, Zhao et al. <ref type="bibr" target="#b41">[41]</ref> propose Semantic Graph Convolution (SemGConv), which can learn local and global semantic relations among nodes in the graph by adding a parameter matrix. Zou et al. <ref type="bibr" target="#b44">[44]</ref> exploit a high-order GCN to learn long-range dependencies among body joints. However, they all adopt a straight-forward network architecture which simply stacks residual graph convolution blocks with same graph topology. To extract multi-scale features, Cai et al. <ref type="bibr" target="#b3">[4]</ref> designed a U-nets like graph networks architecture and Xu et al. <ref type="bibr" target="#b37">[37]</ref> proposed Graph Stacked Hourglass Networks. Unlike the above methods, we argue that the human skeletal graph used in these works is too sparse and propose hierarchical graph networks by exploiting denser graph topology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Let G = {V, E} denote a graph where V is a set of N nodes and E is the collection of edges. A ? {0, 1} N?N is the adjacency matrix of G, and we have a ii = 1 and a i j = 1 if j is the neighboring node of i. Each node i is associated with a D-dimensional feature vector x i ? R D , and the node representations are collected into a matrix X ? R D?N . The vanilla GCN <ref type="bibr" target="#b17">[18]</ref> is</p><formula xml:id="formula_0">X = ? WX? ,<label>(1)</label></formula><p>where X ? R D ?N is the updated feature matrix and ? (?) is a non-linear function.? denotes a symmetrically normalized version of A and W ? R D ?D is a learnable matrix that transforms node representations. SemGConv <ref type="bibr" target="#b41">[41]</ref> further learns the semantic relationships of neighboring nodes by adding another learnable weighting matrix T ? R N?N .</p><formula xml:id="formula_1">X = ? WX? i T ? ,<label>(2)</label></formula><p>where is an element-wise product operation and ? i (?) is softmax nonlinearity which normalizes the input matrix across all choices of i. Following previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b41">41]</ref>, we use two different transformation matrices for the representation of each node i and its neighbors respectively in actual implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Graph Networks</head><p>As mentioned in Section 1, the 2D-to-3D human pose estimation is an ill-posed problem, and it will become more serious when occlusion occurs. We believe that the above problem can be alleviated if we leverage denser graph structures to depict the human skeleton, thus increasing the complexity of the constraint relationship between human joints. To achieve this goal, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we propose Hierarchical Graph Networks (HGN) consisting of three subnetworks with different scales of graph structure. Novel graph structure building strategy and multi-scale feature fusion strategy are also designed for HGN.</p><p>Multi-scale graph structure building. We build three different human structure graphs and assign them to the three subnetworks of HGN in a sparse-to-fine way. The sparsest graph for the bottom subnetwork in <ref type="figure" target="#fig_1">Figure 2</ref> is defined on the standard human skeletal graph G P = {V P , E P }. Similar to existing methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b44">44]</ref>, the number of nodes J is set to 17 in this paper. For the middle and top subnetworks, we coarsen the human mesh graph </p><formula xml:id="formula_2">G M = {V M , E M },</formula><p>which represents human shape information and contains 6890 vertices, multiple times using Heavy Edge Matching (HEM) <ref type="bibr" target="#b8">[9]</ref> and obtain a set of various scales of graphs {G c M } C c=0 , where c indicates the coarsening level. Although more complex graphs can be allocated, we choose the two coarsest graphs G C?1 M and G C M whose the number of nodes V c equal to 96 and 48, respectively, to maintain a suitable model size.</p><p>Multi-scale feature fusion. Since the human skeletal graph G P and the coarse human mesh graphs G C?1 M and G C M are constructed in different ways, it is difficult to describe the specific corresponding relationship between them in a manual way or simply graph downsampling and upsampling operations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b37">37]</ref>. As an alternative, we design a multi-scale feature fusion strategy to learn their mapping relations.</p><p>Inspired by the exchange units proposed by Sun et al. <ref type="bibr" target="#b30">[31]</ref>, we conduct multi-scale fusions such that the parallel subnetworks can exchange different scales information from each other. Given the input feature with different number of nodes {X 0 , X 1 , . . . , X s }, where the subscript denotes the graph scale, we can obtain the fused feature {Y 0 , Y 1 , . . . , Y s } whose scale and widths are the same to the input by</p><formula xml:id="formula_3">Y k = s ? i=1 a(X i , k), k = 0, 1, . . . , s,<label>(3)</label></formula><p>where function a(X i , k) consists of upsampling or downsampling X i from graph scale i to scale k. We use 1 ? 1 convolution for both upsampling and downsampling since the mapping relations are proved to be learned well in such a simple way. Network architecture. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the input 2D pose is mapped to the latent space by a pre-processing graph convolution layer. Then, the multi-scale features extracted in the three subnetworks are fused repeatedly. The subnetworks have 4, 4, 2 residual blocks <ref type="bibr" target="#b13">[14]</ref>, respectively from bottom to up. Each residual block consists of two graph convolution layers and is followed by a non-local layer <ref type="bibr" target="#b34">[35]</ref> to capture both local and global information, and all the graph convolution layers are followed by batch normalization <ref type="bibr" target="#b14">[15]</ref> and a ReLU non-linear layer <ref type="bibr" target="#b25">[26]</ref>. Finally, the features are fed into the output convolution layer and mapped to the output space. It is noted that our HGN does not rely on a specific graph convolution method, so both SemGConv and Vanilla GConv introduced in Section 3.1 can be implemented in our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mesh Constraint</head><p>Since graphs G C?1 M and G C M are derived from mesh graph G M , we can also generate coarse mesh vertices pseudo-groundtruth by dense vertices coarsening and leverage it as another constraint to further refine the local feature representation. Specifically, we generate 3D human mesh M with groundtruth vertices location by fitting SMPL parameters to the 3D groundtruth poses using SMPLify-X <ref type="bibr" target="#b1">[2]</ref>, and then obtain the pseudo-groundtruth of two coarsest meshes M C?1 ? R V C?1 ?3 and M C ? R V C ?3 by a pre-defined indices mapping operation. The final loss function is a combination of 3D pose and 3D coarse mesh constraint: </p><formula xml:id="formula_4">L(P, M C?1 , M C ) = ? P J ? i=1 p i ? p i 2 2 3D pose loss +? M V C?1 ? i=1 m C?1 i ? m C?1 i 2 2 + V C ? i=1 m C i ? m C i 2 2 3D coarse mesh loss ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head><p>Dataset. We mainly evaluate our proposed method on the Human3.6M dataset <ref type="bibr" target="#b15">[16]</ref>, which is widely used in the 3D human pose estimation task. It provides 3.6 million color images taken from four synchronous cameras in different positions and perspectives, by recording 11 subjects actors performing 15 different actions, such as eating and walking. There are 7 subjects annotated with 3D joints. For fair comparison, we follow previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41]</ref> and choose 5 subjects (S1, S5, S6, S7, S8) for training and the other 2 subjects (S9, S11) for test. Besides, the MPI-INF-3DHP <ref type="bibr" target="#b23">[24]</ref> test set provides images in three different scenarios: studio with a green screen (GS), studio without green screen (noGS) and outdoor scene (Outdoor). We apply our model to this dataset to test the generalization capabilities of our proposed method.</p><p>Evaluation. For the Human3.6M benchmark, there are two evaluation protocols used in previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41]</ref>. Protocol #1 uses the mean per-joint position error (MPJPE) as evaluation metric, which computes the mean Euclidean distance error per-joints between the predicted 3D joints and the ground truth after the origin (pelvis) alignment. Protocol #2 aligns the predicted 3D joints with the ground truth by rigid transformation and then computes the error. This metric is abbreviated as PA-MPJPE. Both of these two metrics are measured in millimeter (mm). Implement details. We implement our method within the PyTorch framework. During the training stage, we choose the Adam optimizer <ref type="bibr" target="#b16">[17]</ref> with the learning rate initialized to 0.001 and decayed by 0.9 per 20 epochs. We train each model for 100 epochs using a minibatch size of 64. We initialize weights of the GCNs using the initialization method described  in <ref type="bibr" target="#b11">[12]</ref>. To avoid overfitting, we also adopt Max-norm regularization. In the following experiment, unless specified, the SemGConv is used as the graph convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We conduct a series of ablation studies to better understand how each component affects the performance. The 2D ground truth is taken as input.</p><p>Effects of mesh constraint. We first diagnose how the mesh constraint affects the performance. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our method achieves the best performance when setting the weight of coarse mesh constraint ? M to 0.01. Compared with not adding mesh constraint (? M = 0), we achieve 0.99mm (38.31 to 37.32) and 0.53mm (29.04 to 28.51) gain under two protocols respectively. The performance gain is mainly due to that the mesh constraint enriches the fine-grained representation. However, if we increase ? M , the performance dramatically drops. We believe that this is because the pose-related information will be covered by shape-related information using higher mesh constraint weight. Therefore, it makes sense to set a small weight for mesh constraint for better pose estimation.</p><p>Effects of denser graph topology. We then inspect how denser graph topology benefits the representation.We first set ? M to 0.01 and carry out experiments with three variants of our HGN. 1) SemGCN (Baseline): only the bottom subnetwork in <ref type="figure" target="#fig_1">Figure 2</ref> defined on G P is reserved. This straight-forward architecture is equivalent to Semantic Graph Convolutional Networks (SemGCN) <ref type="bibr" target="#b41">[41]</ref>, and we treat it as our baseline. 2) HGN w/o G C M : we remove the top subnetwork and change the graph structure in the middle subnetwork from G C M to G C?1 M , so that the network can output 3D pose and coarse mesh M C?1 containing 96 vertices in parallel. 3) HGN w/o G C?1 M : we remove the top subnetwork and do not modify the other subnetworks to output 3D pose and coarse mesh M C containing 48 vertices. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>, in which the mean per vertex position error (MPVPE) for mesh prediction measurement is also listed as a reference. We find that all networks with delicate graph structures outperform the baseline for a large margin, which proves the benefits of introducing denser graph topology. Our HGN achieves the best results in MPJPE (37.32mm), indicating that our proposed model has a strong ability to leverage sparse-to-fine graph structures.</p><p>Furthermore, we set ? M to 0 to remove the influence of mesh constraint and model parameters. Experiments are made by fixing the number of channels and model parameters, <ref type="table">Table 3</ref>: Ablation study on the effect of denser graph topology with ? M set to 0. The number of channels and model parameters are fixed (to 128 and 0.43M) for evaluations, respectively.   <ref type="figure">Figure 3</ref>: Mean estimation errors on different body parts (left) and actions (right). Ground truth 2D keypoints are used as input. We show the result for <ref type="bibr" target="#b41">[41]</ref> and ours.</p><p>respectively. <ref type="table">Table 3</ref> shows that our HGN still achieves the overall best performance. Effects of hierarchical network structures. To verify that our architecture has better performance than some typical structures, we keep the GCN type same and the model size comparable for fair comparison. <ref type="table" target="#tab_3">Table 4</ref> shows that the model using our architecture performs better than Sequential Residual blocks (denoted as SeqRes) <ref type="bibr" target="#b41">[41]</ref> and Graph Stacked Hourglass (denoted as GraphSH) <ref type="bibr" target="#b37">[37]</ref> architecture with fewer parameters. It is noted that our method boosts the performance for a large margin when using traditional Vanilla GConv, which demonstrates the great advantage of our architecture itself.</p><p>Understanding the performance improvement. We present the average estimation errors of different body parts and actions as well as the overall mean errors in <ref type="figure">Figure 3</ref>. Among all the actions, our method obtains larger gains for those with serious self-occlusion, such as 'Sitting' (4.78mm), 'SittingDown' (4.85mm), 'Greeting' (4.74mm), etc, while the overall gain is 3.16mm. For body parts, our method brings much improvement for 'head' (10.64mm), 'right shoulder' (6.88mm) and 'left shoulder' (5.83mm) etc because the vertices of coarse mesh, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>, are denser in the upper part of the human body. Those results prove our opinion that more complex graph structure can bring benefit to the depiction of the human skeleton and some joints with high degrees of freedom.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison With the State-of-the-Art</head><p>We use the cascaded pyramid network (CPN) <ref type="bibr" target="#b4">[5]</ref> as 2D pose detector to obtain 2D input joints for benchmark evaluation. CPN is pre-trained on COCO-dataset and fine-tuned on Human3.6M. Following previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">43]</ref>, we also perform horizontal flip augmentation. The results are shown in <ref type="table" target="#tab_4">Tables 5 and 6</ref> for the two protocols, respectively. Note that some other methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">29]</ref> that focus on video-based 3D pose estimation are complementary to our method and can be used to improve the performance. <ref type="table" target="#tab_4">Tables 5 and 6</ref> show that the performance improvement of our model is significant, outperforming all other GCN-based methods and some representative methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref> using  extra MPII data <ref type="bibr" target="#b0">[1]</ref>. GraphSH <ref type="bibr" target="#b37">[37]</ref> achieves comparable performance, but it uses more parameters (3.70M) while our model has only 1.04M parameters, and we have proved in Section 4.2 that our method surpasses GraphSH a lot with the same GCN type and channels. The results demonstrate the great advantage of our HGN. <ref type="figure" target="#fig_3">Figure 4</ref> shows qualitative results for those actions with serious self-occlusion. Compared with baseline, our HGN can alleviate the depth ambiguity caused by self-occlusion. <ref type="figure" target="#fig_4">Figure 5</ref> demonstrates more qualitative results of our HGN on the Human3.6M and MPII datasets. Here, MPII contains in-the-wild images that are unseen for the model trained on Human3.6M. These results further validate the strong generalization ability of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalization Ability</head><p>The MPI-INF-3DHP test set <ref type="bibr" target="#b23">[24]</ref> provides images in three different scenarios: studio with a green screen (GS), studio without green screen (noGS) and outdoor scene (Outdoor). We apply our model to this dataset to test the generalization capabilities of our proposed method and employ 3D-PCK and AUC as evaluation metrics. As shown in Tab. 7, our model yields 85.2 in PCK and 52.1 in AUC while only using the Human3.6M dataset for training, which outperforms all the previous state-of-the-arts. These results validate the strong generalization capability of our architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel architecture named Hierarchical Graph Networks (HGN). The main contributions are two folds. First, we build a novel sparse-to-fine architecture with multi-scale feature fusion based on the denser graphs generated by a multi-scale graph structure building strategy for better feature extraction. Second, we leverage the human coarse mesh as an additional constraint, refining the local feature representation. Extensive experiment results reveal the benefit of our design.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Several typical GCN architectures for 2D-to-3D human pose estimation: (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The overall pipeline of HGN. The input 2D joints J passes three subnetworks of HGN to obtain multi-scale features and output the predicted 3D Pose P and coarse mesh M C and M C?1 in parallel. The nodes of graph built for three subnetworks from bottom to up is 17, 48 and 96, respectively, using the multi-scale graph structure building processing shown in the upper part. The down-right part illustrates the Residual GCN (ResGCN) block and the up-right part shows the proposed multi-scale feature fusion strategy used in HGN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where ? P = 1, ? M = 0.01. P = {p i |i = 1, . . . , J} are the predicted 3D pose and M c = {m c i |i = 1, . . . ,V c , c = C ? 1,C} are the predicted 3D coarse meshes. p i and m c i are the groundtruth /pesudo-groudtruth corresponding top i andm c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results of actions with self-occlusion, i.e., SittingDown, Photo, and Sitting. 3D groud truth, ours, and SemGCN are shown in black, red, and blue, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results of our method on Human3.6M [16] (bottom) and MPII [1] (top).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation study on effects of mesh constraint. Different weights of mesh constraint ? M in Equation (4) are set and ? M = 0 means removing the mesh constraint.</figDesc><table><row><cell cols="2">? P ? M</cell><cell cols="2">MPJPE (mm) PA-MPJPE (mm)</cell></row><row><cell>1</cell><cell>0</cell><cell>38.31</cell><cell>29.04</cell></row><row><cell>1</cell><cell>0.001</cell><cell>37.68</cell><cell>28.79</cell></row><row><cell>1</cell><cell>0.01</cell><cell>37.32</cell><cell>28.51</cell></row><row><cell>1</cell><cell>0.1</cell><cell>38.69</cell><cell>29.09</cell></row><row><cell>1</cell><cell>1</cell><cell>39.64</cell><cell>29.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on effects of denser graph topology with ? M set to 0.01.</figDesc><table><row><cell>Method</cell><cell cols="2">MPJPE PA-MPJPE (mm) (mm)</cell><cell cols="2">MPVPE M C?1 M C</cell><cell># Params</cell></row><row><cell>SemGCN (Baseline) [41]</cell><cell>40.78</cell><cell>31.01</cell><cell>-</cell><cell>-</cell><cell>0.43M</cell></row><row><cell>HGN w/o G C M HGN w/o G C?1 M</cell><cell>38.69 37.83</cell><cell>29.07 28.71</cell><cell>66.28 -</cell><cell>-65.92</cell><cell>0.82M 0.81M</cell></row><row><cell>HGN</cell><cell>37.32</cell><cell>28.51</cell><cell>62.74</cell><cell>61.76</cell><cell>1.04M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of GraphSH<ref type="bibr" target="#b37">[37]</ref> and our method using SemGConv and vanilla GConv.</figDesc><table><row><cell></cell><cell cols="2">(a) SemGConv</cell><cell></cell><cell></cell><cell cols="2">(b) Vanilla GConv</cell><cell></cell></row><row><cell>Method</cell><cell cols="3">Channels MPJPE (mm) # Params</cell><cell>Method</cell><cell cols="3">Channels MPJPE (mm) # Params</cell></row><row><cell>SeqRes [41]</cell><cell>128</cell><cell>40.78</cell><cell>0.43M</cell><cell>SeqRes [41]</cell><cell>128</cell><cell>65.90</cell><cell>0.30M</cell></row><row><cell>GraphSH [37]</cell><cell>64</cell><cell>39.20</cell><cell>0.44M</cell><cell>GraphSH [37]</cell><cell>64</cell><cell>59.10</cell><cell>0.22M</cell></row><row><cell>Ours</cell><cell>64</cell><cell>38.74</cell><cell>0.29M</cell><cell>Ours</cell><cell>64</cell><cell>42.92</cell><cell>0.21M</cell></row><row><cell>Ours</cell><cell>128</cell><cell>37.32</cell><cell>1.04M</cell><cell>Ours</cell><cell>128</cell><cell>40.66</cell><cell>0.71M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Quantitative evaluation results using MPJPE in millimeter on Human3.6M under Protocol #1. No rigid alignment or transform is applied in post-processing. Detected 2D keypoints are used as input. ? uses temporal information. + uses extra data from MPII dataset. Best results are highlighted in bold. #1 Dire. Disc. Eat Greet Phone Photo Pose Puch. Sit SitD. Smoke Wait WalkD Walk WalkT Avg. Pavllo et al. [29] ? 45.2 46.7 43.3 45.6 48.1 55.1 44.6 44.3 57.3 65.8 47.1 44.0 49.0 32.8 33.9 46.8 Cai et al. [4] ? 44.6 47.4 45.6 48.8 50.8 59.0 49.7 47.2 43.9 57.9 61.9 46.6 51.3 37.1 39.4 48.8 Martinez et al. [23] 51.8 56.2 58.1 59.0 69.5 78.4 55.2 58.1 74.0 94.6 62.3 59.1 65.1 49.5 52.4 62.9 Pavlakos et al. [28] + 48.5 54.4 54.5 52.0 59.4 65.3 49.9 52.9 65.8 71.1 56.6 52.9 60.9 44.7 47.8 56.2 Zhao et al.</figDesc><table><row><cell>Protocol [41]</cell><cell>48.2 60.8 51.8 64.0 64.6 53.6 51.1 67.4 88.7 57.7 73.2 65.6 48.9 64.8 51.9 60.8</cell></row><row><cell>Ci et al. [8] +</cell><cell>46.8 52.3 44.7 50.4 52.9 68.9 49.6 46.4 60.2 78.9 51.2 50.0 54.8 40.4 43.3 52.7</cell></row><row><cell>Liu et al. [21]</cell><cell>46.3 52.2 47.3 50.7 55.5 67.1 49.2 46.0 60.4 71.1 51.5 50.1 54.5 40.3 43.7 52.4</cell></row><row><cell>Zou et al. [44]</cell><cell>49.0 54.5 52.3 53.6 59.2 71.6 49.6 49.8 66.0 75.5 55.1 53.8 58.5 40.9 45.4 55.6</cell></row><row><cell cols="2">Xu &amp; Takano [37] 45.2 49.9 47.5 50.9 54.9 66.1 48.5 46.3 59.7 71.5 51.4 48.6 53.9 39.9 44.1 51.9</cell></row><row><cell>Ours</cell><cell>47.8 52.5 47.7 50.5 53.9 60.7 49.5 49.4 60.0 66.3 51.8 48.8 55.2 40.5 42.6 51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Quantitative evaluation results using PA-MPJPE in millimeter on Human3.6M under Protocol #2. Rigid alignment is applied in post-processing. Detected 2D keypoints are used as input. ? uses temporal information. + uses extra data from MPII dataset. Best results are highlighted in bold. Eat Greet Phone Photo Pose Purch. Sit SitD. Smoke Wait WalkD Walk WalkT Avg. Pavllo et al. [29] ? 34.2 36.8 33.9 37.5 37.1 43.2 34.4 33.5 45.3 52.7 37.7 34.1 38.0 25.8 27.7 36.8 Cai et al. [4] ? 35.7 37.8 36.9 40.7 39.6 45.2 37.4 34.5 46.9 50.1 40.5 36.1 41.0 29.6 33.2 39.0 Martinez et al. [23] 39.5 43.2 46.4 47.0 51.0 56.0 41.4 40.6 56.5 69.4 49.2 45.0 49.5 38.0 43.1 47.7 Pavlakos et al. [28] + 34.7 39.8 41.8 38.6 42.5 47.5 38.0 36.6 50.7 56.8 42.6 39.6 43.9 32.1 36.5 41.8 Ci et al. [8] + 36.9 41.6 38.0 41.0 41.9 51.1 38.2 37.6 49.1 62.1 43.1 39.9 43.5 32.2 37.0 42.2 Liu et al. [21] 35.9 40.0 38.0 41.5 42.5 51.4 37.8 36.0 48.6 56.6 41.8 38.3 42.7 31.7 36.2 41.2 Zou et al. [44] 38.6 42.8 41.8 43.4 44.6 52.9 37.5 38.6 53.3 60.0 44.4 40.9 46.9 32.2 37.9 43.7 Ours 35.8 39.7 36.3 40.6 40.2 45.9 36.8 35.8 47.3 53.7 40.7 36.4 43.1 29.8 32.8 39.6</figDesc><table><row><cell>Protocol #2</cell><cell>Dire. Disc.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results on the test set of MPI-INF-3DHP [24] by scene. The results are shown in PCK and AUC.</figDesc><table><row><cell></cell><cell cols="6">Trainning data GS noGS Outdoor ALL (PCK ?) ALL (AUC ?)</cell></row><row><cell>Martinez et al. [23]</cell><cell>H36M</cell><cell cols="3">49.8 42.5 31.2</cell><cell>42.5</cell><cell>17.0</cell></row><row><cell>Mehta et al. [24]</cell><cell>H36M</cell><cell cols="3">70.8 62.3 58.8</cell><cell>64.7</cell><cell>31.7</cell></row><row><cell>Yang et al. [39]</cell><cell cols="2">H36M+MPII -</cell><cell>-</cell><cell>-</cell><cell>69.0</cell><cell>32.0</cell></row><row><cell>Zhou et al. [43]</cell><cell cols="4">H36M+MPII 71.1 64.7 72.7</cell><cell>69.2</cell><cell>32.5</cell></row><row><cell>Luo et al. [22]</cell><cell>H36M</cell><cell cols="3">71.3 59.4 65.7</cell><cell>65.6</cell><cell>33.2</cell></row><row><cell>Ci et al. [8]</cell><cell>H36M</cell><cell cols="3">74.8 70.8 77.3</cell><cell>74.0</cell><cell>36.7</cell></row><row><cell>Zhou et al. [42]</cell><cell cols="4">H36M+MPII 75.6 71.3 80.3</cell><cell>75.3</cell><cell>38.0</cell></row><row><cell>Xu and Takano [37]</cell><cell>H36M</cell><cell cols="3">81.5 81.7 75.2</cell><cell>80.1</cell><cell>45.8</cell></row><row><cell>Ours</cell><cell>H36M</cell><cell cols="3">87.0 84.9 82.7</cell><cell>85.2</cell><cell>52.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2D human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="3686" to="3693" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3D pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Occlusion-aware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wending</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pose2Mesh: Graph convolutional network for 3D human pose and mesh recovery from a 2D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="769" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence (AAAI-18)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6821" to="6828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph U-Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SIA-GCN: A spatial information aware graph neural network with 2D convolutions for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deying</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HDNet: Human depth estimation for multi-person camera-space localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="633" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comprehensive study of weight sharing in graph networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="318" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on 3D vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Camera distance-aware topdown approach for 3D multi-person pose estimation from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10133" to="10142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3D human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tiny-Hourglassnet: An efficient design for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenrui</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junni</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1491" to="1495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxiang</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2602" to="2611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep kinematics analysis for monocular 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="899" to="908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph stacked hourglass networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Takano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16105" to="16114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7444" to="7452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning skeletal graph neural networks for hard 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ailing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minhao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07181</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3D human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3425" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-order graph convolutional networks for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
