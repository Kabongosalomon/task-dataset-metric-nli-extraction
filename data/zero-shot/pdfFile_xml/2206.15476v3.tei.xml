<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Dragoi</surname></persName>
							<email>mdragoi@bitdefender.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Burceanu</surname></persName>
							<email>eburceanu@bitdefender.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuela</forename><surname>Haller</surname></persName>
							<email>ehaller@bitdefender.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Politehnica University of Bucharest</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Manolache</surname></persName>
							<email>amanolache@bitdefender.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florin</forename><surname>Brad</surname></persName>
							<email>fbrad@bitdefender.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bitdefender</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Romania</surname></persName>
						</author>
						<title level="a" type="main">AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning (ML), leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006+, a traffic dataset for network intrusion detection. This type of data meets the premise of shifting the input distribution: it covers a large time span (10 years), with naturally occurring changes over time (e.g. users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testing splits. We validate the performance degradation over time with diverse models, ranging from classical approaches to deep learning. Finally, we show that by acknowledging the distribution shift problem and properly addressing it, the performance can be improved compared to the classical training which assumes independent and identically distributed data (on average, by up to 3% for our approach). Dataset and code are available at https://github.com/ bit-ml/AnoShift/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Analyzing and developing Machine Learning algorithms under gradual distribution shifts is a problem of high interest in the research community. There is a growing enthusiasm for building benchmarks over existing or new datasets <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b20">20]</ref>, that formulate a setup for isolating the shifting aspect and create a better ground for this research field. A better understanding of the distribution shift problem might lead to findings of underlying fundamental aspects, shedding new light on robustness and generalization problems. We argue that the distribution shift occurs naturally and gradually in a continuous data stream (e.g. monitoring network traffic), allowing an in-depth analysis of the problem. On the other side, artificially generated scenarios usually exhibit sudden changes that do not simulate the natural shift problem. Yet, the annotation process for streaming data is quite difficult and expensive, considering the massive amount of data. The IID (gray) testing split comes from the same temporal span as the TRAIN set (white), while NEAR (yellow) and FAR (blue) splits are from different time spans, with NEAR being closer to the training set than FAR. b) To highlight the utility of the proposed chronological protocol, we exemplify the continuous evolution of data, illustrating the distributions of normal and anomaly samples over the considered 10 years. We exemplify the evolution of the percent of recent connections that have the same source and destination IP addresses as the current connection (feature 9 -Dst host srv count).</p><p>From a practical point of view, continuous IT infrastructure monitoring has become essential for computer security and resilience. Recent anomaly detection and intrusion detection systems (IDS) obtain strong results on specific datasets but drastically fail in real-world scenarios <ref type="bibr" target="#b47">[47]</ref>. Our experimental analysis proved a natural change of the Kyoto-2006+ data over the 10 years period when the data was collected. The shift is noticeable both over the input distribution and considering the performance of several anomaly detection systems. Several reasons behind the observed shift are: users leaving or coming to the network, per user interest changes leading to network interaction changes, updates to the software versions, patching old vulnerabilities but revealing new attack vectors for intruders.</p><p>To better assess the models' capabilities, we introduce a chronology based evaluation protocol, distinctly evaluating performance on test data splits (IID, NEAR and FAR - <ref type="figure" target="#fig_0">Fig. 1</ref>) with different temporal distances towards the training set (TRAIN - <ref type="figure" target="#fig_0">Fig. 1</ref>). We observe that the performance of anomaly detection models consistently degrades when tested on data from longer time horizons. Moreover, we prove that a basic distillation technique overcomes a classic IID (assuming independent and identically distributed data) training under gradual data shifts, proving that the awareness of the shift problem might lead to better solving the task.</p><p>Summarized, our main contributions are the following:</p><p>? We analyzed a large and commonly used dataset for the unsupervised anomaly detection task in network traffic (Kyoto-2006+) and demonstrated that it is affected by distribution shifts. The per-feature distributions and t-SNE show multiple changes over the years, and the Optimal Transport Dataset Distance gave us an estimate of its magnitude. ? We propose a chronology-based benchmark, which focuses on splitting the test data based on its temporal distance to the training set, introducing three testing splits: IID, NEAR, FAR <ref type="figure" target="#fig_0">(Fig. 1</ref>). This testing scenario proves to capture the in-time performance degradation of anomaly detection methods for classical to masked language models. This benchmark aims to enable a better estimate of the model's performance, closer to the real world performance. ? We prove that properly acknowledging the distribution shift may lead to better performing anomaly detection models than classical IID training. When facing distribution shift, a basic distillation technique positively impacts the performance by up to 3% on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Relation to benchmarks targeting distribution shift Recently, there has been an increased amount of effort and focus in this direction, with several benchmarks emerging. They empha-size the non-stationary nature of the data, with various underlying reasoning. The most common approach is to search for gaps in the input data distribution that appear with time <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b22">22]</ref>, taking into perspective that the world is continuously evolving; therefore, the data acquired continuously from it should exhibit the same behavior. Our work aligns with this perspective by working with traffic logs from a large university network over 10 years. In <ref type="bibr" target="#b26">[26]</ref>, the authors focus on how the appearance of basic objects changes from year to year, while <ref type="bibr" target="#b22">[22]</ref> emphasizes the seasonal patterns that appear in news language (e.g. elections, hurricanes). A second axis exploited for noticing shifts in data is the spatial one. In <ref type="bibr" target="#b6">[6]</ref>, geolocalization is used in conjunction with the time for guiding the shift. In <ref type="bibr" target="#b20">[20]</ref>, the gap is based on higher level characteristics, like x-ray data from different hospitals, but also on geolocalization. In searching for the autonomous driving robustness, a more complex variation is provided in <ref type="bibr" target="#b44">[44]</ref> following the weather, time of day, and congestion levels. Nevertheless, all works analyze the distribution shift for supervised tasks, focusing on NLP or Computer Vision. In <ref type="bibr" target="#b22">[22]</ref>, the authors monitor the evolution of the perplexity metric, with models learned in a self-supervised manner as a masked language model. They emphasize the need to link the shift analysis to a downstream task, several supervised ones in their case. Differently, AnoShift, our benchmark proposal, tackles an unsupervised anomaly detection task under non-stationary data.</p><p>Relation to traffic anomalies Models tackling Network Intrusion Detection are covered by lots of surveys <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b19">19]</ref>, structured around dataset variations, anomaly types, and methods variation. A fair amount of the approaches are supervised <ref type="bibr" target="#b34">[34]</ref>, based on tree classifiers <ref type="bibr" target="#b48">[48]</ref>, modeling the task as a binary or multi-class anomaly (intrusion) classification. But we are interested here in the unsupervised setup <ref type="bibr" target="#b31">[31]</ref>. Usually, the best models are quite simple, most of them are shallow <ref type="bibr" target="#b17">[17]</ref>, based on OC-SVM <ref type="bibr" target="#b39">[39]</ref> or Isolation Forest <ref type="bibr" target="#b27">[27]</ref>, or very small neural nets <ref type="bibr" target="#b31">[31]</ref>. Several solutions introduce deep learning approaches for intrusion detection <ref type="bibr" target="#b33">[33]</ref>, transforming the data into images <ref type="bibr" target="#b13">[13]</ref>, or modeling the problem using GNNs <ref type="bibr" target="#b29">[29]</ref>.</p><p>An important problem we identified in this area is that the datasets used for the task are easily saturated, mainly because they either lack variety (e.g. simulated traffic patterns for anomalies) or have a very few annotated anomalies, or are small-scale, covering only several days <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b7">7,</ref><ref type="bibr" target="#b34">34]</ref>. In contrast, Kyoto-2006+ <ref type="bibr" target="#b43">[43]</ref> spans over 10 years (2006-2016), containing continuous natural traffic logs from a large university network, within a sub-net of honeypots. Most of those datasets cover basic networks, but there are some oriented towards IOT traffic <ref type="bibr" target="#b38">[38]</ref>, or even to the autonomous driving field, Internet of Vehicles <ref type="bibr" target="#b48">[48]</ref>. But another reason for saturation, is the IID training setup, as we will show in this work. These generalization problems are very acute, leading to weak performances for those algorithms when applied on real world data, or on a new dataset <ref type="bibr" target="#b47">[47]</ref>. With AnoShift, we highlight the IID training problem, by proposing a different training and evaluation setup based on temporal distances, closer to a realistic case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Chronological protocol</head><p>We introduce a chronological protocol for building train and testing splits that can highlight the temporal evolution of data. Taking into consideration the timestamps of our data, we propose to build a training split (TRAIN) along with three different testing splits (IID, NEAR and FAR), comprising multiple years of data ( <ref type="figure" target="#fig_0">Fig. 1a)</ref>). The TRAIN and IID splits are extracted from the first period of time, and the IID tests should highlight the expected performance when there is no distribution shift between train and test. The NEAR and FAR splits are each extracted from different periods of time, where NEAR is closer to the training data and FAR is farther away. We expect standard models to exhibit better performance on NEAR compared to FAR, which we experimentally prove in Sec. 4.2.</p><p>Our proposed benchmark will provide a better estimate of the expected performance when the model is deployed in the wild and exposed to the inevitable distribution shift of the data. To the best of our knowledge, AnoShift is the first to provide a proper scenario for studying the generalization capabilities of unsupervised learning models for anomaly detection.</p><p>Our work revolves around Network Intrusion Detection Systems (NIDS), tackling the problem of distribution shifts that naturally appear in network traffic data. We work over the popular Kyoto-2006+ dataset (Sec. 3.1), which was collected over ten years, providing us with enough data to capture the temporal evolution. Starting from Kyoto-2006+, we introduce our AnoShift Benchmark (Sec. 3.2) that proposes one training and three testing splits, which highlight the difficulty of dealing with data temporarily distant from the training set.  Considering that TRAIN and IID splits are sampled from the same time span, we have jointly represented them. Note that while for TRAIN , NEAR and FAR we extract the same number of normal samples per year, the IID split contains 10 times less normal samples. The anomaly samples are extracted such that we maintain the normal vs. anomaly proportion of the original data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Kyoto-2006+</head><p>Kyoto-2006+ <ref type="bibr" target="#b43">[43]</ref> is a reference dataset for Anomaly Detection over network traffic data <ref type="bibr" target="#b35">[35]</ref>. It is built on 10 years of real traffic data (Nov. 2006 -Dec. 2015), captured by a system of 348 honeypots in 5 sub-networks inside the Kyoto University. Briefly, a honeypot is a real or virtual machine simulating a regular computer (having an OS and multiple services running on it). Its purpose is to deceive an attacker into taking advantage of the vulnerabilities present on the honeypot machine (e.g. software not updated). A honeypot does not request any connection on its own. So in such a scenario, almost all traffic coming to a honeypot machine is unsolicited and therefore considered malicious. By design, this type of dataset has a large percent of anomalies (89.5% anomalies in Kyoto-2006+) compared to other anomaly detection datasets. The 14 conventional features of the dataset include 2 categorical ones like connection service type or flag of the connection and 12 numerical like the connection duration or the number of source bytes. We put more details about Kyoto-2006+ in Appendix A. This dataset is spread across a very large period of time, and it contains exclusively real-world traffic, without simulated events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AnoShift benchmark</head><p>To keep the natural distribution shift of the network traffic data, we sample a fixed number of normal samples per year (#months?25k for TRAIN, NEAR, and FAR and #months?2.5k for IID). The number of anomalous samples is chosen such that we maintain the proportion of normal vs. anomaly samples from the original yearly subset. We illustrate this process in <ref type="figure" target="#fig_2">Fig. 2</ref>. In <ref type="figure" target="#fig_0">Fig. 1 b)</ref> we illustrate the continuous evolution of the data features over the considered 10 years, comparing the distribution for one feature (feature 9 -Dst host srv count). Such behavior can be observed for the majority of features, a fact highlighted by our in-depth analysis from Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Experimental setup</head><p>Preprocess network traffic data We use the 14 conventional features from the new version of the Kyoto dataset <ref type="bibr">[2006]</ref><ref type="bibr">[2007]</ref><ref type="bibr">[2008]</ref><ref type="bibr">[2009]</ref><ref type="bibr">[2010]</ref><ref type="bibr">[2011]</ref><ref type="bibr">[2012]</ref><ref type="bibr">[2013]</ref><ref type="bibr">[2014]</ref><ref type="bibr">[2015]</ref> and convert 3 of the 12 numerical features to categorical values by using an exponentially-scaled binning method between 0 and the maximum value of each feature, such that the bins have a higher density for smaller values and get increasingly wider towards larger values. We used a basis of 1.1, which results in 233 bins, where the width of the ith bin is given by:</p><formula xml:id="formula_0">bin i = [1.1 i ? 1, 1.1 i+1 ? 1]</formula><p>. We keep the original percentage features (9 out of 12 numerical features), which are discretized in 100 values. Therefore, our preprocessing results in a fixed vocabulary size and each possible token is known apriori. See in <ref type="figure" target="#fig_4">Fig. 3</ref> a preprocessed sample. Our processing of the original dataset does not pose any privacy concerns since it does not contain any sensitive information, such as IP address. However, data binning constitutes another potential limitation in our work. Metrics for anomaly detection To analyze the performance of various models on our proposed benchmark, we use the labels (normal and anomaly) provided by the Kyoto-2006+ dataset. As we deal with imbalanced sets, we study the ROC-AUC metric and also evaluate the PR-AUC metric, for both inliers and outliers (note that for a random classifier, PR-AUC for a specific class is close to the ratio of data in that specific class). We report the IID, NEAR and FAR performances as the arithmetic mean of performances over their associated yearly splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Distribution shift analysis</head><p>We perform an in-depth analysis of the proposed benchmark from three points of view. First, we study the inherent non-stationarity of the considered data, highlighting the natural shift between the years, considering both simple, per feature metrics and more complex metrics between distributions (Sec. 4.1). Second, we analyze various anomaly detection models, highlighting the performance decrease when dealing with testing data that is temporarily distant from the training set (Sec. 4.2). Third, we discuss the importance of acknowledging the data shift and emphasize the positive impact of a basic distillation technique over the standard IID approach (Sec. 4.3). We add supplementary discussions on the method in Appendix A.1.</p><p>We run our experiments on an internal cluster with multiple GPU types: GTX 1080 Ti, GTX Titan X, RTX 2080 Ti, RTX Titan. We estimate that we need 5 days to reproduce the experiments on 1 GPU. The CPU training for OC-SVMs, IsolationForest, and LOF benchmarks takes 3 days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inherent non-stationarity</head><p>Visualization of the data shifts For a visual interpretation of the yearly shift, we have considered the unsupervised t-SNE <ref type="bibr" target="#b46">[46]</ref> to illustrate the high dimensional data structure (PCA visualization available in Appendix A). In <ref type="figure">Fig. 4</ref> we introduce the comparison between pairs of yearly splits and the whole figure can be interpreted as a similarity matrix, each cell (i, j) illustrating the similarity between point clouds of year i vs. year j. Each row illustrates the point clouds of the corresponding year over all the other point clouds. At the same time, each column presents the point clouds of the corresponding year below all the other point clouds for a better understanding of the distribution shifts. We observe that point clouds move away as we increase the temporal gap between their corresponding years. This confirms our intuition that the analyzed network traffic data is continuously shifting in time and emphasizes the need for a benchmark as AnoShift that can efficiently test the robustness of models under this inherent non-stationarity of natural data.</p><p>Per-feature shift We further analyze whether the dataset's statistics at the feature level are changing from one year to another. Recall that we have 2 categorical features and 12 numerical ones. We extract the normalized histogram per year for each feature and compute the Jeffreys divergence <ref type="bibr" target="#b15">[15]</ref> between those histograms. The Jeffreys divergence is a commonly used symmetrization for Kullback-Leibler divergence <ref type="bibr" target="#b21">[21]</ref>: KL(p, q) + KL(q, p), and it is proven to be both symmetric and non-negative. We highlight that such an analysis can only illustrate simple scenarios, studying the distribution change from the perspective of single feature changes. With all the considered baselines from Sec.4.2, we have observed a significant decrease in performance for the years 2014 and 2015, leading to the intuition that this subset may have substantial differences from the others. Consequently, in <ref type="figure" target="#fig_5">Fig. 5</ref>, we illustrate the Jeffreys divergence for two features that we find to have a large 2014-2015 distance, but also for a third one that has significant high values in the distance map on other years than the two. <ref type="figure">Figure 4</ref>: Comparison between yearly splits using t-SNE visualization. We observe that the discrepancy between point clouds increases with the temporal distance between splits, colors becoming more separated over time. The analysis is performed considering 2k randomly sampled points per split. Follow the 2007 row: see the orange cluster on top of clusters associated to the other years. It is very similar to its neighbours 2006-2008, and the similarity diminishes in time (see 2015).</p><p>General shift We next explore the distribution differences between dataset splits over time by using the Optimal Transport Dataset Distance method (OTDD) <ref type="bibr" target="#b3">[3]</ref>. OTDD relies on optimal transport, a geometric method for computing distances between probability distributions for comparing datasets. This analysis shows how the splits move away from each other over time (see <ref type="figure" target="#fig_6">Fig. 6</ref>). Compared with the per feature approach, this method allows us to gain a better intuition for the performance on a new split, giving us a single distance based on all features. We observe how the inliers (first image) nicely distances in OTDD value, directly correlated with the distance in time. As for the outliers (third image), it is noticeable that they are quite different between the splits of the first years. We notice that the distances between inliers and outliers (in the middle) show that FAR years' outliers are similar to TRAIN years' inliers, an observation that we empirically confirm in Tab. 1, where all models suffer from a steep descent in performance (bellow random). We run the method with the default parameters for DatasetDistance, over the standardized input of Kyoto, with one-hot encoded categorical variables, 3 times, with a randomly sampled 5k entries per year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Impact on IID models</head><p>We introduce the AnoShift benchmark to understand better the impact of data shifts that naturally appear over time on the performance of anomaly detection models. We hope that the proposed splits  will push forward the research in this direction and help build more robust models that can deal with mild to severe distribution changes between test and training sets. In this context, in the current section, we will study the performance degradation of various anomaly detection approaches, from IID to NEAR and FAR testing splits.</p><p>Anomaly detection models We have considered several unsupervised baselines, ranging from more classical approaches, like Isolation Forest <ref type="bibr" target="#b27">[27]</ref>, OC-SVM <ref type="bibr" target="#b39">[39]</ref>, LocalOutlierFactor(LOF) <ref type="bibr" target="#b5">[5]</ref> and recent ECOD <ref type="bibr" target="#b24">[24]</ref> and COPOD <ref type="bibr" target="#b23">[23]</ref>, to deep learning ones, like SO-GAAL <ref type="bibr" target="#b28">[28]</ref>, deepSVDD <ref type="bibr" target="#b36">[36]</ref>, AE <ref type="bibr" target="#b1">[1]</ref> for anomalies, LUNAR <ref type="bibr" target="#b14">[14]</ref>, InternalConstrastiveLearning <ref type="bibr" target="#b41">[41]</ref> and our proposed transformer for anomalies model, based on the BERT <ref type="bibr" target="#b11">[11]</ref> architecture. For part of the baselines, we have employed the PyOD library <ref type="bibr" target="#b49">[49]</ref>.</p><p>BERT for anomalies We use a simplified BERT architecture, without pretraining, with around 340k trainable parameters. We train the BERT model as a Masked Language Model (MLM), using a data collator that randomly masks a fraction p of the input sequence and optimizing a cross-entropy loss function between the model predictions at mask positions and the original tokens. We derive a sequence anomaly score by randomly masking a fraction p of tokens in the sequence and averaging the probabilities of the correct tokens at mask positions given by the classification layer over the vocabulary. At evaluation time, we average the score over 10 mask samplings. A detailed description of the model is introduced in Appendix A. In our experiments, we used p = 15%. Notice that the ROC-AUC is dropping over time in all cases, except for BERT and SO-GAAL methods, showing this is a property of the method, rather than a problem with the dataset. More precisely, those methods model the outliers very well in the NEAR split (see PR-AUC-out), while the PR-AUC-in is dropping, confirming the distribution shift over time (see Appendix A). The variance for FAR is the highest and almost all methods perform under-random on it. Best scores per split are shown in bold: NEAR-best is BERT, but interestingly, IID-best is LOF, and FAR-best is COPOD. PR-AUC for inliers and outliers are available in Appendix A- <ref type="figure" target="#fig_0">Fig. 10</ref>  In <ref type="table" target="#tab_0">Table 1</ref> we report the results of our experiments. Each baseline model was trained 3 times with a basic set of hyperparameters, and we reported the average results and the standard deviation. Both the OC-SVM and the LUNAR model were trained solely on 5% of the TRAIN set to reduce the computational burden. For all of the considered models, except ECOD, we observe a performance degradation between NEAR and FAR splits, highlighting that these anomaly detection models cannot cope with the distribution shift. In the case of ECOD, the performances of both NEAR and FAR splits are below random, making their relative order irrelevant. The IID evaluation, which is the most popular methodology, proves to give an illusion of high performance, as the performance quickly degrades once we consider a testing set from a different period. The evolution is also presented in Appendix A- <ref type="figure" target="#fig_0">Fig. 10</ref>, illustrating ROC-AUC along with PR-AUC for inliers and outliers. We observe a rapid degradation for inliers PR-AUC, indicating that normal data distribution is continuously changing, and the outliers detection may not be reliable. These experiments highlight the issues of current anomaly detection models and prove the benefits of the AnoShift benchmark.</p><p>Performance on FAR With all tested baselines, we notice a significant decrease in performance for 2014-2015 years for inliers, which motivates us to further investigate the particularities of this subset. We observe a large distance in the Jeffreys divergence between 2014-2015 and the rest of the years for 2 features: service type and the number of bytes sent by the source IP (see <ref type="figure" target="#fig_5">Fig. 5</ref>). From the OTDD analysis in <ref type="figure" target="#fig_6">Fig. 6</ref>, we observe that: first, the inliers from FAR are very distanced to training years; and second, the outliers from FAR are quite close to the training inliers. One root cause of those events can be the steep increase of the "DNS" traffic percentage (from 4% to 37%, in 2013, and 2014 respectively). This contributes to the distribution shift on FAR, explaining the low performance.</p><p>Monthly evaluation In <ref type="figure" target="#fig_8">Fig. 7</ref>, we take a closer look at the BERT's performance at month granularity and break down performance on inliers and outliers. First, notice how the inliers' performance gradually degrades over time, to an abrupt drop at farther months. This doubles the analysis from Sec. 4.1, where we notice the difference between the TRAIN years and FAR (through Jeffreys and OTDD experiments). Second, we observe that on IID years, the anomalies are modeled quite poorly by our language model, resulting in a slightly lower IID performance in comparison with NEAR.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Addressing the shifted data</head><p>We next compare the performance of a BERT model in 3 training regimes: iid, finetune, and knowledge distillation, for subsets of 300k entries from each year. We use 2006-2010 as training data and evaluate 2011-2015 as individual splits. First, in the a) iid mode, we use sets of data starting from 2006 and gradually add each successive split from the train period, initializing a new model for each subset. Next, in the b) finetune mode, we start from the iid model trained on 2006 and gradually finetune it on each successive year in the train period. Finally, in the c) distillation mode, we start from the iid model of 2006 and reinitialize a same-sized model for each new split, which becomes a student for the previous model by combining the MLM loss with a KL divergence loss with the teacher predictions on the current split. The best performance is achieved by the final distilled model for every test split (see <ref type="figure" target="#fig_9">Fig. 8</ref>), outperforming iid and finetune by over 3% on average in ROC-AUC. It is worth noting that the effects of distillation are visible over time, with the iid method outperforming it in the first two iterations over the train splits. At all stages, the distillation method obtains the best performance on FAR data, providing a more robust training alternative to distribution shifts in data. The metrics are available in Appendix A-Tab. 3 and pseudocode for the training modes is available in Appendix A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussions</head><p>MLM as anomaly detector Even though the BERT model greatly exceeds the number of parameters and the complexity of other classical baselines, its generalization performance on farther data is extremely low. The anomaly performance in our case is based on the perplexity score when predicting several masked features in the sample. So if the features are not correlated, the MLM model might be unable to learn something useful, which might result in learning some specific training set biases, failing to generalize on temporarily distant data (eg. lower score on FAR wrt other baselines). We did not investigate this, but we consider it an interesting direction for future work.</p><p>MLM with the training vocabulary In a real world setup, we expect that the fraction of tokens that are previously unseen during training increases with temporal distance. The evaluation score might get artificially inflated due to mapping of unseen features to the UNK token, as for farther points it is easier to predict UNK instead of the right word. Alongside the requirement of a discrete vocabulary, this is another limitation of vocabulary based methods as opposed to other classical approaches. We did not investigate these effects, but it might constitute an interesting direction for future work.</p><p>Other considered datasets To emphasize the Kyoto-2006+ value, we briefly discuss here the other considered datasets and why we choose it in the end. We performed an in depth analysis over a large number of datasets, looking for two characteristics, essential for a distribution shift benchmark: it spreads over a large enough time-span, such that the distribution shift will naturally occur, rather than being synthetically injected, exhibiting sudden changes, and it is not solved already (existing methods do not report perfect scores on it). We first looked over a wide range of known 1. network traffic datasets for intrusion detection, and after analysing them we concluded that most are artificially created, with injected samples, in very restricted scenarios. Only Kyoto-2016 was a proper one, extended over a long enough period of time for showing a natural distribution shift. We next focused our attention on 2. system logs, since the time-span is usually more extensive in these dataset and the natural distribution shift is more probable to occur. But under our analysis (t-SNE, Jeffreys divergence, OTDD, multiple baselines), these datasets did not exhibit a clear distribution shift over time, so we decided to further analyse them until concludent results. Finally, we looked over 3. general multi-variate timeseries datasets, but the most popular ones are quite small and almost perfectly solved already. We leave this exact numbers for the considered datasets in the Appendix A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our approach highlights the true dimension of distribution shifts that appear in naturally and continuously evolving data streams. We analyze it in Kyoto-2006+ network traffic dataset that spans over 10 years from multiple angles: visually with t-SNE, statistically with histogram distances, and by measuring its magnitude with an Optimal Transport approach. Next, we propose AnoShift, a chronology-based benchmark for anomaly detection, to enable the development of models that generalize better and are more robust to shifts in data. Further, we show that by acknowledging the shift and addressing it, the performance can be improved, obtaining a +3% performance boost using a basic distillation technique. 3.2.1 -paragraph 2.; Appendix A.1 (c) Did you discuss any potential negative societal impacts of your work?</p><p>[No] Our work does not have a negative societal impact. Our benchmark proposal is tailored for finding intrusions in a computer network (not at the user level, but at the network level), by detecting anomalous traffic, in a more robust way than before, closer to the real scenario. One use-case is in the IT department of an company or university, where a person monitors the traffic alerts and prioritizes certain alerts based on the predictions of the robust models trained on our proposed benchmark. [Yes] We included a GitHub repository with code resources and a repository with the preprocessed data in the suplimentary material -see Appendix. B (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? <ref type="bibr">[No]</ref> We had several emails with the authors, describing them our purpose and asking for additional information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>Kyoto-2006+ When an attack occurs, the honeypot saves the network access pattern and other metadata and, at some point, might decide to reboot the system and rewrite back the original configuration. The authors deployed another machine in the network to generate normal traffic data, with a mailing server and a DNS service for a single domain. All traffic data from this server was labeled as clean (the logs also include other protocols for managing the machine over ssh or http and https). The 14 conventional features of the dataset includes 2 categorical features: connection service type and flag of the connection and 12 numerical features: connection duration, number of source and destination bytes, number of connections with corresponding IP addresses in a timeframe of two seconds and the percentage of connections accessing the same service and their rate of "SYN" errors, the prevalence of the connection's source IP address and the requested service in the past 100 connections to the current destination IP (Kyoto features). The malicious traffic is further labeled using three software solutions: an Intrusion Detection Systems at the network level, an Antivirus product, and a shellcodes and exploits detector. In addition to those, the authors labeled other entries based on their prior history of connections from a specific IP and destination port. Additional features include source and destination IP addresses and ports, timestamp and protocol. We note that the generality of the original dataset imposes several limitations for our benchmark. More specific, the diversity of the normal traffic in a honeypot setup is quite restricted. Also, since the labeling is done using existing software and rules, the dataset's anomalies might be underestimated.</p><p>Visualization of the data shifts with PCA For completeness, in <ref type="figure">Fig. 9</ref> we illustrate the distribution shift between years using a PCA visualization of the point clouds associated with each year. We observe similar results as the t-SNE visualization presented in <ref type="figure">Fig. 4</ref>.</p><p>Performance evolution over time In Tab. 2 and <ref type="figure" target="#fig_0">Fig. 10</ref> we present the full evaluation of considered baseline models on IID, NEAR and FAR splits.</p><p>Training strategies for data shift In Tab. 3 we present the full ROC-AUC, and PR-AUC for inliers and outliers, for all three training strategies: iid, finetune and distill.</p><p>BERT for Anomalies We propose a simplified BERT architecture for detecting anomalies. The network input is tokenized by a WordLevel tokenizer which obtains tokens for the individual events in a system log sequence and, conversely, for the individual features of network traffic. Therefore, we have fixed-length sequences for Kyoto-2006. We train the BERT model as a Masked Language Model (MLM), using a data collator that randomly masks p% of the input sequence, by optimizing a crossentropy loss function between the model predictions at mask positions and the original tokens. We derive a sequence anomaly score by randomly masking p% of tokens in the sequence and averaging the probabilities of the correct tokens at mask positions given by the classification layer over the vocabulary. The model is not pretrained and consists of two hidden layers of size 120, an intermediate size of 192 and 6 attention heads. It has a hidden dropout and attention dropout probabilities of 0.1, an epsilon of 1e ? 12 for the normalization layer and a 0.02 standard deviation range for the truncated normal weight initialization. Our architecture totals 342135 trainable parameters. For training we mask p = 15% of the input sequence and at evaluation time, we average over n = 10 mask samplings.?</p><formula xml:id="formula_1">j i = w j , if mask i (j) = 0 [M ASK], if mask i (j) = 1<label>(1)</label></formula><p>We repeat the masking process n times and average over all repeats to improve consistency. The anomaly score formula is depicted in equation 2, where we denote by P M the classification layer of the model of parameters ? M , by M asks p k the set of random binary masks of length k and mask probability p, where w t are the initial tokens in the sequence and? t i is the j-th token in the sequence under mask i. <ref type="figure">Figure 9</ref>: Comparison between yearly splits using PCA visualization. Similar to the t-SNE visualization, we observe that the discrepancy between point clouds increases with the temporal distance between splits, colors becoming more separated over time.</p><formula xml:id="formula_2">anomaly_score([w 1 , w 2 , ..., w t ]) = i=1..n maski?M asks p t j=1..t (1 ? P (? j i )) n<label>(2)</label></formula><formula xml:id="formula_3">P (? j i ) = 1, if mask i (j) = 0 P M (w j |? M , [? 1 i , ...,? t i ]), if mask i (j) = 1<label>(3)</label></formula><p>We motivate this metric with the observation that inlier data should consist of common tokens with a high retrieval probability given by the distribution of the training set, while outliers should usually have either rare tokens or unusual combinations of features, within the context given by the unmasked tokens.</p><p>Impure training Till now, we have considered anomaly detection models that were trained solely on the clean data of the TRAIN split. Further, we will study the performance evolution between IID, NEAR and FAR when the BERT model is trained on corrupt data containing mislabeled outliers in different percentages. In <ref type="figure" target="#fig_0">Fig. 11</ref> we present the ROC-AUC for the 3 testing splits. We observe that the distribution shift is noticeable in the model performance even in this corrupt training setup. This validates the usefulness of the proposed chronological protocol when dealing with potentially mislabeled samples and highlights that the observed degradation over time is not a consequence of such dataset issues.</p><p>Broader Impact Our benchmark proposal is tailored for finding intrusions in a computer network (not at the user level, but at the network level), by detecting anomalous traffic, in a more robust way than before, closer to the real scenario. One use-case is in the IT department of an company  or university, where a person monitors the traffic alerts and prioritizes certain alerts based on the predictions of the robust models trained on our proposed benchmark. Our work does not have a negative societal impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Discussions and future work</head><p>The inliers' natural distribution For being able to annotate large amount of data, the network datasets stay either in a clean space, where almost everything is normal, or in a "dark" one, where every connection is considered infected. Kyoto-2006+ lies in the second case, where the normal traffic is not very general, covering several behaviours. It might be interesting as future directions to find a way to combine the two cases towards a more general and unbiased dataset.</p><p>Pre-process through binning We have performed the numerical to categorical conversion in order to make the dataset suitable for BERT based models, whose vocabularies would become too large otherwise. For a fair comparison, we consider it proper to use the same preprocessing for all the methods. We binarize 3 numerical features, transforming them into categorical ones (out of 12 total features). Namely, we convert only the float features: connection duration, number of source bytes and number of destination bytes into categorical ones (bins of values). We also perform experiments without preprocessing those numerical features (Tab. 4). Notice that the performance varies depending on the method and split, and it is not clear that one feature set is better than the other, over all the methods. Nevertheless, we see the same trend of performance drop across the three splits, supporting the claim of our work. Additionally, we observe that the evaluation on data without preprocessing numerical features instead of categorical ones achieves a better score on FAR. This might be explained by the fact that numerical binning induces a higher rarity of tokens in the FAR split compared to IID and NEAR, and therefore resulting in more uncertainty for the models.</p><p>Gap between supervised and unsupervised learning We evaluate several supervised learning methods for anomaly detection modeled as a binary classification task, on our AnoShift benchmark. We test several classical baselines: (SVM <ref type="bibr" target="#b9">[9]</ref>, RandomForest <ref type="bibr" target="#b25">[25]</ref>, XGBoost <ref type="bibr" target="#b8">[8]</ref>), but also some attention-based deep learning methods (BERT with a classification head <ref type="bibr" target="#b10">[10]</ref>, TabNet <ref type="bibr" target="#b4">[4]</ref>,  SAINT <ref type="bibr" target="#b42">[42]</ref>). We report in Tab. 5 ROC-AUC, AUC-PR for Inliers and for Outliers for the supervised baselines, where we also included our unsupervised BERT baseline for comparison. We plotted the results in <ref type="figure" target="#fig_0">Fig. 12</ref>. We observe highly saturated scores on IID and NEAR and a major performance degradation on FAR. The highest performing methods on IID and NEAR, XGBoost, BERT and Saint, achieve the lowest scores on FAR across all baselines.  <ref type="figure" target="#fig_0">Figure 12</ref>: Performance evaluation for several supervised learning baselines in a binary classification task on the Kyoto-2006+ dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pseudo-code for BERT training</head><p>We present the three algorithms for each training strategies: IID 1, Finetune 2, and Distilation 3. We show here the detailed process of how we choose the Kyoto-2006+ dataset and why we consider it to be one of the few relevant in the distribution-shift context for stream-like data. We performed an <ref type="table">Table 5</ref>: Performance evolution over time for supervised methods: IID vs NEAR vs FAR. We report the ROC-AUC, PR-AUC for inliers, and PR-AUC for outliers metrics. The performance degrades over time also in this supervised setting. Notice there is a large (and consistent) gap between the supervised methods and the unsupervised BERT baseline. Best score per split in bold.</p><formula xml:id="formula_4">Algorithm 1 IID training M odel ? init_model() optimizer ? AdamW () set ? shuf f</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Baselines</head><p>IID NEAR FAR  System logs datasets We next focused our attention on system logs, since the time-span is usually more extensive in these dataset and the natural distribution shift is more probable to occur. But under our analysis (t-SNE, Jeffreys divergence, OTDD, multiple baselines), these datasets did not exhibit a clear distribution shift over time, so we decided to further analyse them until concludent results. We used Drain and Spell as log parsers, and we report in Tab. 8 the results using the LogAnomaly <ref type="bibr" target="#b30">[30]</ref> baseline.</p><p>Multi-variate timeseries datasets We next looked over general multi-variate timeseries datasets, but the most popular ones are quite small and almost perfectly solved already (see Tab. 9).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Appendix</head><p>Raw Kyoto dataset documentation The dataset used in our proposed benchmark consists of a preprocessing of the Traffic Data from Kyoto University's Honeypots and results from a discretization of the numerical features in the original dataset, such that a language-modelling approach can be easily applied. The original dataset consists of 14 conventional features and 10 additional features. The conventional features in the original dataset includes connection duration, type of service, number of source and destination bytes, server rate errors percentage and flag of connection. We keep all the conventional features and apply a exponentially-scaled binning over the continuous values (duration, number of source and destination bytes) which results in 233 bins and a discretization of the percentage features in 100 distinct values. As an observation, some of the 10 additional features (the source and destination IP addresses, source and destination port numbers) might be useful when designed models (eg. graphs) focusing on connections between the nodes in the system. Intended uses We hope that our proposed benchmark shifts the general direction of treating network intrusion detection towards a timely fashion that suffers from distributional shift, hereby providing a better suited evaluation protocol for upcoming research in this field.</p><p>URL to dataset download We redirect our readers to the repository of the raw Kyoto dataset published by the Kyoto University at https://www.takakura.com/Kyoto_data/ and provide a repository of data under our proposed processing at https://share.bitdefender.com/s/ 9D4bBE7H8XTdYDB, with subsets of 300000 instances and heldout sets of 30000 instances for each split, maintaining the original inlier to outlier ratio from the original data in each split, as well as the full processed splits, with each full split except 2006 being provided in two parts. We make the remark that the 2006 split contains fewer instance, due to data collection debuting in November.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Code for dataset loading</head><p>We publish our code as a public GitHub repository https://github.com/bit-ml/AnoShift/, containing the data preprocessing script that transforms the original data in our format, sample data manipulation notebooks, license and additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Author responsibility for violation of rights</head><p>There is no sensitive data leaked in the preprocessed dataset. The authors are not aware of any possible violation of rights and take responsibility for the published data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Dataset hosting and long-term preservation</head><p>The authors take full responsibility for the availability of the processed data in the provided repository. However, no statement can be made about the availability of the raw Kyoto-2006+ data published by the Kyoto University, as it depends on Takakura.com. To avoid further problems, we have published our preprocessed version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Licence</head><p>We release our code under a BSD 3-Clause License, therefore allowing the redistribution and use in source and binary forms, with or without modification, under the 3 clauses specified by the Berkeley Software Distribution License:</p><p>1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer. 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution. 3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>a) The proposed AnoShift splits over Kyoto-2006+ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>a) Yearly splits of the network traffic data from Kyoto-2006+ dataset, highlighting the proportion of normal and anomaly samples. b) Proposed train and test splits in our AnoShift benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. 4.1. The TRAIN and IID samples are collected from [2006 ? 2010], while the NEAR and FAR splits consist of [2011 ? 2013] and [2014 ? 2015]intervals. The protocol is illustrated inFig. 1 and Fig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Examples of preprocessed Kyoto-2006+ instances. See Appendix A for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FeatureFigure 5 :</head><label>5</label><figDesc>Jeffreys divergence between Kyoto years. First two images represent features with a large 2014-2015 distance. The 3 rd one is for a feature with significant difference between the histograms across years. Note that it is difficult to predict the performance of the method on a new split, only based on those per feature distances between distributions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Optimal Transport Dataset Distance for Kyoto. See distances between inliers (first), inliers and outliers (second), and outliers (third). The distances from inliers generally increase as you move further from the diagonal, showing large distances between TRAIN and FAR data. Moreover, notice in second image how outliers in the FAR splits are quite similar with inliers from TRAIN, also explaining the abrupt performance drop on farther data (Tab. 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>BERT for anomaly, evaluated on each month. We show the ROC-AUC, PR-AUC for inliers, and PR-AUC for outliers. The performance for the inliers is slowly decreasing during IID and NEAR splits, dropping suddenly just before the FAR split, showing how the language model fails to recognize inliers once it moves further apart from the training data. On the other hand, there are parts of the IID split where the outliers are quite poorly modeled, explaining the slightly poor performance of BERT on IID when compared with NEAR split. iid PR-AUC PR-AUC outliers differences</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>ROC-AUC, PR-AUC-in, PR-AUC-out for Finetune and Distill strategies, relative to the iid. The performance is averaged over all training subsets. Even though the strategies have a high variance in general, the distill is clearly more robust over time when compared to iid and finetune.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Checklist 1 .</head><label>1</label><figDesc>For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Sec. 3.1 -paragraph 2.; Sec.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [N/A] (b) Did you include complete proofs of all theoretical results? [N/A] 3. If you ran experiments (e.g. for benchmarks)... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] in the abstract and the appendix. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] see Sec. 4 and its subsections. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] For each of the tested methods in the main experiment, we run it 3 times, with different seeds. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] see Sec. 4 -paragraph 2 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] We used the existing Kyoto-2006+ dataset as raw data, as detailed in Sec. 3 (b) Did you mention the license of the assets? [N/A] The authors do not mention any kind of licence for the data, it is just publicly available. (c) Did you include any new assets either in the supplemental material or as a URL?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>(e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] see in Sec. 3.2.1 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Performance evolution over time: IID vs NEAR vs FAR. We follow the evolution of ROC-AUC and PR-AUC for inliers and outliers. We observe a large performance gap between the considered splits, correlated with the temporal distance from the training set. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Performance evolution of our BERT model on IID, NEAR and FAR splits, when training on a corrupt set of samples, containing different percentages of mislabeled data points. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Full</head><label></label><figDesc>Kyoto-2006+ dataset As previously described, AnoShift contains subsets of the full data, for allowing faster prototyping. We evaluate BERT for anomalies on the full Kyoto-2006+ yearly sets and observe that the ROC-AUC results are consistent with the subsets. The evaluation is performed on held-out test sets for each year and the results are available in Tab. 6. The subsets as well as the full sets used in our experiments are available at https://share.bitdefender.com/s/ 9D4bBE7H8XTdYDB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>Our split proposal documentation We propose a yearly split of the dataset and group adjacent years into Train, NEAR data and FAR data, which we use to highlight the performance degradation of several benchmarks in time, due to the distributional shift of the data which we demonstrate with a comprehensive analysis. In our proposed split, Train data consists of the first 4 years (2006-2010), Near data of the following 3 years (2011-2013) and Far data of the last two available years(2014- 2015). We publish the data in splits of single years, in csv format. The columns 0 to 13 are the discretized conventional features in the Kyoto-2006+ dataset, preserving the original order, column 14 contains the complete timestamp. Columns 15, 16 and 17 correspond to the first 3 additional features in the original data, namely IDS_detection, Malware_detection and Ashula_detection, which indicates presence of alert triggers from the 3 IDS solutions: Symantec IDS, clamav and Ashula shellcode detector. Column 19 in the preprocessed dataset corresponds to the protocol used by the connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance evolution over time, for classical and deep methods: IID vs NEAR vs FAR.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>and in Tab. 2. ? 0.06 71.43 ? 0.29 49.57 ? 0.09 IsoForest [27] 86.09 ? 0.54 75.26 ? 4.66 27.16 ? 1.69 ? 0.88 79.29 ? 3.33 34.96 ? 0.14 Deep SO-GAAL [28] 50.48 ? 1.13 54.55 ? 3.92 49.35 ? 0.51 deepSVDD [36] 73.43 ? 0.94 69.61 ? 0.83 31.81 ? 4.54 AE [1] for anomalies 81.00 ? 0.22 44.06 ? 0.57 19.96 ? 0.21 LUNAR [14] (train 5%) 85.75 ? 1.95 49.03 ? 2.57 28.19 ? 0.9 InternalContrastiveLearning [41] 84.86 ? 2.14 52.26 ? 1.18 22.45 ? 0.52 BERT [11] for anomalies 84.54 ? 0.07 86.05 ? 0.25 28.15 ? 0.06</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>ROC-AUC ?</cell></row><row><cell cols="2">Type Baselines</cell><cell>IID</cell><cell>NEAR</cell><cell>FAR</cell></row><row><cell>Classical</cell><cell cols="2">OC-SVM [39] (train 5%) 76.86 ECOD [24] 84.76 COPOD [23] 85.62 LOF [5] 91.50</cell><cell>44.87 54.24</cell><cell>49.19 50.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance evolution over time for unsupervised methods: IID vs NEAR vs FAR. We report beside the ROC-AUC metric, also the PR-AUC for inliers and PR-AUC for outliers. With bold are the best results per split. InternalContrastiveLearning<ref type="bibr" target="#b41">[41]</ref> 84.86 ? 2.14 52.26 ? 1.18 22.45 ? 0.52 BERT [11] for anomalies 84.54 ? 0.07 86.05 ? 0.25 28.15 ? 0.06 ? 0.96 52.48 ? 4.56 10.15 ? 0.10 Deep SO-GAAL [28] 58.65 ? 5.36 43.52 ? 11.62 10.68 ? 2.42 deepSVDD [36] 71.24 ? 0.44 43.80 ? 2.87 9.72 ? 0.65 AE [1] for anomalies 73.76 ? 0.09 26.16 ? 0.15 8.51 ? 0.01 LUNAR [14] (train 5%) 78.91 ? 1.69 29.36 ? 2.58 9.33 ? 0.11 InternalContrastiveLearning [41] 76.96 ? 2.12 27.28 ? 0.59 8.81 ? 0.05 BERT [11] for anomalies 74.61 ? 0.13 58.94 ? 0.69 8.22 ? 0.02 ? 0.28 87.71 ? 0.74 92.67 ? 0.13 deepSVDD [36] 62.06 ? 0.42 85.05 ? 0.86 81.03 ? 2.31 AE [1] for anomalies 78.99 ? 0.28 72.97 ? 0.38 75.71 ? 0.05 LUNAR [14] (train 5%) 88.01 ? 1.03 80.91 ? 0.62 79.45 ? 0.30 InternalContrastiveLearning [41] 89.08 ? 0.87 81.93 ? 0.39 77.55 ? 0.50 BERT [11] for anomalies 89.83 ? 0.07 95.96 ? 0.06 78.38 ? 0.02</figDesc><table><row><cell cols="2">Type Unsupervised Baselines</cell><cell>IID</cell><cell>NEAR</cell><cell>FAR</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ROC-AUC (%) ?</cell></row><row><cell>Classical</cell><cell>OC-SVM [39] (train 5%) IsoForest [27] ECOD [24] COPOD [23] LOF [5]</cell><cell cols="3">76.86 ? 0.06 71.43 ? 0.29 49.57 ? 0.09 86.09 ? 0.54 75.26 ? 4.66 27.16 ? 1.69 84.76 44.87 49.19 85.62 54.24 50.42 91.50 ? 0.88 79.29 ? 3.33 34.96 ? 0.14</cell></row><row><cell></cell><cell>SO-GAAL [28]</cell><cell cols="3">50.48 ? 1.13 54.55 ? 3.92 49.35 ? 0.51</cell></row><row><cell></cell><cell>deepSVDD [36]</cell><cell cols="3">73.43 ? 0.94 69.61 ? 0.83 31.81 ? 4.54</cell></row><row><cell>Deep</cell><cell>AE [1] for anomalies LUNAR [14] (train 5%)</cell><cell cols="3">81.00 ? 0.22 44.06 ? 0.57 19.96 ? 0.21 85.75 ? 1.95 49.03 ? 2.57 28.19 ? 0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PR-AUC inliers (%) ?</cell></row><row><cell>Classical</cell><cell>OC-SVM [39] (train 5%) IsoForest [27] ECOD [24] COPOD [23] LOF [5]</cell><cell cols="3">70.84 ? 0.13 41.38 ? 0.29 15.12 ? 0.04 83.68 ? 3.47 57.06 ? 10.27 9.16 ? 0.18 84.47 22.98 13.78 87.86 29.25 14.55 84.11 PR-AUC outliers (%) ?</cell></row><row><cell>Classical</cell><cell>OC-SVM [39] (train 5%) IsoForest [27] ECOD [24] COPOD [23] LOF [5]</cell><cell cols="3">67.94 ? 0.21 85.70 ? 0.16 87.27 ? 0.02 81.46 ? 2.52 87.13 ? 2.08 78.33 ? 1.41 78.37 74.48 85.9 78.19 77.99 85.98 83.86 ? 0.98 92.34 ? 1.26 81.99 ? 0.05</cell></row><row><cell></cell><cell>SO-GAAL [28]</cell><cell>70.38</cell><cell></cell></row><row><cell>Deep</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Training strategies: ROC-AUC (%) for IID training vs Finetune vs Distil on Kyoto-2006+ Train ON: 2006 -&gt; 2007? 2008? 2009? 2010?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Compare different numeric feature preprocessing. Notice that it is not clear that one feature set is better than the other, over all the methods. Nevertheless, we see the same trend of performance drop across the splits, supporting the claim of our work.</figDesc><table><row><cell>Method</cell><cell>Feature binarization</cell><cell>IID</cell><cell>NEAR</cell><cell>FAR</cell></row><row><cell></cell><cell></cell><cell cols="3">ROC-AUC (%) ?</cell></row><row><cell>OC-SVM [39]</cell><cell>w</cell><cell cols="3">76.86 71.43 49.57</cell></row><row><cell></cell><cell>w/o</cell><cell cols="3">81.87 71.24 50.95</cell></row><row><cell>IsoForest [27]</cell><cell>w</cell><cell cols="3">86.09 75.26 27.16</cell></row><row><cell></cell><cell>w/o</cell><cell cols="3">94.41 95.13 32.81</cell></row><row><cell>ECOD [24]</cell><cell>w</cell><cell cols="3">84.76 44.87 49.19</cell></row><row><cell></cell><cell>w/o</cell><cell cols="3">79.38 69.80 60.84</cell></row><row><cell>COPOD [23]</cell><cell>w</cell><cell cols="3">85.62 54.24 50.42</cell></row><row><cell></cell><cell>w/o</cell><cell cols="3">79.03 65.67 60.12</cell></row><row><cell>AE [1]</cell><cell>w</cell><cell cols="3">81.00 44.06 19.96</cell></row><row><cell></cell><cell>w/o</cell><cell cols="3">89.59 86.76 30.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>le(concat(set 1 , ..., set n )) for epoch ? [1, ..., num_epochs] do for batch ? set do mask ? random(batch.shape) &lt; 0.15 Sample a binary mask of batch size with 0.15 probability predictions = M odel(batch * mask) loss_batch = loss(predictions, batch) AdamW () for set ? [set 1 , ..., set n ] do for epoch ? [1, ..., num_epochs] do for batch ? set do mask ? random(batch.shape) &lt; 0.15</figDesc><table><row><cell></cell><cell>Reconstruction loss for masked tokens</cell></row><row><cell>compute loss gradients</cell><cell></cell></row><row><cell>perform optimizer step</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>Algorithm 2 Finetune strategy</cell><cell></cell></row><row><cell>M odel ? init_model()</cell><cell></cell></row><row><cell>loss ? CrossEntropy()</cell><cell></cell></row><row><cell cols="2">optimizer ? Sample a binary mask of batch size with</cell></row><row><cell>0.15 probability</cell><cell></cell></row><row><cell>predictions = M odel(batch  *  mask)</cell><cell></cell></row><row><cell>loss_batch = loss(predictions, batch)</cell><cell>Reconstruction loss for masked tokens</cell></row><row><cell>compute loss gradients</cell><cell></cell></row><row><cell>perform optimizer step</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>end for</cell><cell></cell></row><row><cell>A.3 Other considered datasets</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Algorithm 3 Distillation strategy mlm_loss ? CrossEntropy() distil_loss ? KL_divergence() optimizer ? AdamW () T eacher ? train IID on set 1 for set ? [set 2 , ..., set n ] do Student ? init_model() for epoch ? [1, ..., num_epochs] do for batch ? set do mask ? random(batch.shape) &lt; 0.15</figDesc><table><row><cell>Sample a binary mask of batch size with</cell></row><row><cell>0.15 probability</cell></row><row><cell>pred_s = Student(batch  *  mask)</cell></row><row><cell>pred_t = T eacher(batch  *  mask)</cell></row><row><cell>loss_batch = mlm_loss(pred_s, batch) + distil_loss(pred_s, pred_t)</cell></row><row><cell>compute loss gradients</cell></row><row><cell>perform optimizer step</cell></row><row><cell>end for</cell></row><row><cell>end for</cell></row><row><cell>Student ? T eacher</cell></row><row><cell>end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Network traffic datasets.</figDesc><table><row><cell></cell><cell>Number</cell><cell></cell><cell></cell></row><row><cell></cell><cell>of</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell cols="2">samples Time-span</cell><cell>Other details</cell></row><row><cell>CIC-IDS2017</cell><cell>3 mil</cell><cell>5 days</cell><cell>Different attack types per day</cell></row><row><cell cols="2">CSE-CIC-IDS2018 4.5 mil</cell><cell>17 days</cell><cell>Different attack types per day</cell></row><row><cell>UNSW-NB15</cell><cell>2.5 mil</cell><cell>2 days</cell><cell>too small</cell></row><row><cell>BoT-IoT</cell><cell>73 mil</cell><cell>4 days</cell><cell>too small</cell></row><row><cell>ToN-IoT</cell><cell>22 mil</cell><cell>6 days</cell><cell>too small</cell></row><row><cell>NSL-KDD</cell><cell>0.15 mil</cell><cell>45 days</cell><cell>max reported ROC-AUC 99%</cell></row><row><cell>LANL</cell><cell>1.6 mil</cell><cell>58 days</cell><cell>max reported ROC-AUC 99%</cell></row><row><cell>AAD</cell><cell>1.8 mil</cell><cell>90 days</cell><cell>internally build dataset, max ROC-AUC 98%</cell></row><row><cell>Kyoto-2006+</cell><cell>806M</cell><cell>10 years</cell><cell></cell></row><row><cell cols="4">created, with injected samples, in very restricted scenarios. Only Kyoto-2016 was a proper dataset,</cell></row><row><cell cols="4">extended over a long enough period of time for showing a natural distribution shift.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>System logs datasets.</figDesc><table><row><cell></cell><cell>Number</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset -Preprocessor</cell><cell cols="6">samples Time-span IID (%) NEAR (%) FAR (%) Split proportion</cell></row><row><cell>HDFS -Drain</cell><cell>11 mil</cell><cell>40h</cell><cell>54</cell><cell>66</cell><cell>57</cell><cell>6-6-6</cell></row><row><cell>BGL -Spell/Drain</cell><cell cols="2">4.7 mil 214 days</cell><cell>67/68</cell><cell>43/73</cell><cell>45/35</cell><cell>2-3-2</cell></row><row><cell cols="3">Thunderbird -Spell/Drain 211 mil 244 days</cell><cell>72/71</cell><cell>72/72</cell><cell>76/75</cell><cell>3-3-3</cell></row><row><cell>Liberty</cell><cell cols="2">266 mil 315 days</cell><cell></cell><cell></cell><cell></cell><cell>grouped anomalies</cell></row><row><cell>Spirit-CMU -Spell</cell><cell cols="2">272 mil 570 days</cell><cell>80</cell><cell>67</cell><cell>72</cell><cell>6-5-3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Multi-variate timeseries datasets.</figDesc><table><row><cell></cell><cell>Number of</cell><cell></cell><cell>max reported unsup</cell></row><row><cell>Dataset</cell><cell>samples</cell><cell>Time-span</cell><cell>ROC-AUC (%)</cell></row><row><cell>SMAP -Soil Moisture Active Passive</cell><cell>0.5 mil</cell><cell>7-14 days</cell><cell>99</cell></row><row><cell>SWaT -Secure Water Treatment</cell><cell>0.9 mil</cell><cell>11 days</cell><cell>85</cell></row><row><cell>WADI -Water Distribution</cell><cell>0.96 mil</cell><cell>16 days</cell><cell>90</cell></row><row><cell>SMD -Server Machine Dataset</cell><cell>1.4 mil</cell><cell>35 days</cell><cell>99</cell></row><row><cell>MSDS -Multi-Source Distributed System</cell><cell>0.3 mil</cell><cell>days -months</cell><cell>91</cell></row><row><cell>PSM -Pooled Server Metrics</cell><cell></cell><cell>147 days</cell><cell>98</cell></row><row><cell>MSL -Mars Science Laboratory</cell><cell>0.13 mil</cell><cell>-</cell><cell>99</cell></row><row><cell>NAB -Numenta Anomaly Benchmark</cell><cell>0.37 mil</cell><cell>-</cell><cell>99</cell></row><row><cell cols="3">MBA -MIT-BIH Supraventricular Arrhythmia 0.2 mil 78 half-hour ECGs</cell><cell>99</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Razvan Pascanu for guiding us on how to approach the subject and Ioana Pintilie for helping us with baselines for the rebuttal.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Feature: &apos;Source bytes</title>
		<imprint>
			<date type="published" when="2006-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An introduction to outlier analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Outlier analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of network anomaly detection techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohiuddin</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdun</forename><surname>Naser Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiankun</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Netw. Comput. Appl</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric dataset distances via optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Melis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tabnet: Attentive interpretable tabular learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6679" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">LOF: identifying density-based local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD International Conference on Management of Data</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Online continual learning with natural distribution shifts: An empirical study with visual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ZYELL-NCTU nettraffic-1.0: A large-scale dataset for real-world network anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao-En</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Jun</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Han</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Huang</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Consumer Electronics ICCE-TW</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Xgboost: extreme gradient boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Benesty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Khotilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailong</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>R package version 0.4-2</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Kdd-cup 1999, UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hybrid approach for improving intrusion detection based on deep learning and machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Merna</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Abbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowayda</forename><forename type="middle">A</forename><surname>Sadek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Computer Vision, AICV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Lunar: Unifying local outlier detection methods via graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Goodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">See-Kiong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wee Siong</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="6737" to="6745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An invariant form for the prior probability in estimation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><surname>Jeffreys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. Lond</title>
		<imprint>
			<date type="published" when="1946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Jalal Al-Muhtadi, and Mario Lemes Proen?a Jr. A comprehensive survey on network anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilberto</forename><surname>Fernandes</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P C</forename><surname>Joel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><forename type="middle">Fernando</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carvalho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Telecommun. Syst</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evaluating shallow and deep neural networks for network intrusion detection systems in cyber security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rahul-Vigneswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Soman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabaharan</forename><surname>Poornachandran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computing, Communication and Networking Technologies, ICCCNT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cyber security data sources for dynamic network research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">D</forename><surname>Kent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Dynamic Networks and Cyber-Security</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Survey of intrusion detection systems: techniques, datasets and challenges. Cybersecur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansam</forename><surname>Khraisat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gondal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joarder</forename><surname>Vamplew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kamruzzaman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">WILDS: A benchmark of in-the-wild distribution shifts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sang</forename><forename type="middle">Michael</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berton</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Imran</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><forename type="middle">M</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML</title>
		<meeting>the International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mind the gap: Assessing temporal generalization in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angeliki</forename><surname>Lazaridou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adhiguna</forename><surname>Kuncoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Gribovskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devang</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Liska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tayfun</forename><surname>Terzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mai</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyprien</forename><surname>De Masson D&amp;apos;autume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Kocisk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susannah</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Copod: copula-based outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Botta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezar</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1118" to="1123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ecod: Unsupervised outlier detection using empirical cumulative distribution functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Botta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezar</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Classification and regression by randomforest. R news</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The CLEAR benchmark: Continual learning on real-world imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Datasets and Benchmarks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Isolation-based anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Hua</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generative adversarial active learning for unsupervised outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanchun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1517" to="1528" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">E-graphsage: A graph neural network based intrusion detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Wai Weng Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohanad</forename><surname>Layeghy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><forename type="middle">R</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Portmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Loganomaly: Unsupervised detection of sequential and quantitative anomalies in unstructured logs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weibin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenglin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence, IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Kitsune: An ensemble of autoencoders for online network intrusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisroel</forename><surname>Mirsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Doitshman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Elovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Shabtai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Network and Distributed System Security Symposium, NDSS. The Internet Society</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">UNSW-NB15: a comprehensive data set for network intrusion detection systems (UNSW-NB15 network data set)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nour</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jill</forename><surname>Slay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Military Communications and Information Systems Conference, MilCIS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Longbing Cao, and Anton van den Hengel. Deep learning for anomaly detection: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparison of network intrusion detection performance using feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seraf?n</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">Mor?n</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">A</forename><surname>Prada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Jos?</forename><surname>Fuertes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Dom?nguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Engineering Applications of Neural NetworksEANN</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A survey of network-based intrusion detection data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Wunderlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deniz</forename><surname>Scheuring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Landes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Hotho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Security</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="147" to="167" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>G?rnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Netflow datasets for machine learning-based network intrusion detection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohanad</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Layeghy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nour</forename><surname>Moustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Portmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Data Technologies and Applications Conference, BDTA, and International Conference on Wireless Internet, WiCON</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards a standard feature set for network intrusion detection system datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohanad</forename><surname>Sarhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Layeghy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Portmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mob. Networks Appl</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Toward generating a new intrusion detection dataset and intrusion traffic characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iman</forename><surname>Sharafaldin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Habibi Lashkari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Systems Security and Privacy, ICISSP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Anomaly detection for tabular data with internal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Shenkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Saint: Improved neural networks for tabular data via row attention and contrastive pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gowthami</forename><surname>Somepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Bayan Bruss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01342</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical analysis of honeypot data and building of kyoto 2006+ dataset for NIDS evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungsuk</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Takakura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuo</forename><surname>Okabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Eto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisuke</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Nakao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Building Analysis Datasets and Gathering Experience Returns for Security, BADGERS EuroSys</title>
		<meeting>the First Workshop on Building Analysis Datasets and Gathering Experience Returns for Security, BADGERS EuroSys</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SHIFT: a synthetic driving dataset for continuous multi-task domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Segu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janis</forename><surname>Postels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A detailed analysis of the KDD CUP 99 data set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahbod</forename><surname>Tavallaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebrahim</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">A</forename><surname>Ghorbani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Computational Intelligence for Security and Defense Applications</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards model generalization for intrusion detection: Unsupervised machine learning techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miel</forename><surname>Verkerken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Laurens D&amp;apos;hooge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Wauters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><forename type="middle">De</forename><surname>Volckaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Netw. Syst. Manag</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Tree-based intelligent intrusion detection system in internet of vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Moubayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Hamieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdallah</forename><surname>Shami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Global Communications Conference, GLOBECOM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pyod: A python toolbox for scalable outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zain</forename><surname>Nasrullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v20/19-011.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">96</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<idno>+15.25 +12.69 +27.77</idno>
		<title level="m">Difference (best sup, BERT-unsup)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pr-Auc</forename><surname>Inliers</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<idno>+25.03 +39.95 +57.05</idno>
		<title level="m">Difference (best sup, BERT-unsup)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pr-Auc</forename><surname>Outliers</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<idno>+9.98 +3.48 +13.13</idno>
		<title level="m">Difference (best sup, BERT-unsup)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Table 6: BERT for anomalies ROC-AUC evaluation on the full sets in comparison with the subsets ? ROC-AUC Split</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">We wanted it to come from a stream-like data (as opposed to the less natural, existing benchmarks on images or text [26, 22, 44, 6, 20] and we for two characteristics that we consider essential for a distribution shift benchmark: ? It spreads over a large enough time-span, such that the distribution shift will naturally occur</title>
		<imprint/>
	</monogr>
	<note>rather than being synthetically injected, exhibiting sudden changes</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<title level="m">? It is not solved already (existing methods do not report almost perfect scores on it</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Network traffic datasets We first looked over a wide range of known network traffic datasets for intrusion detection (see Tab. 7), and after analysing them we concluded that most are artificially</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
