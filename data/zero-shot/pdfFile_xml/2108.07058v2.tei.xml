<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FaPN: Feature-aligned Pyramid Network for Dense Image Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihua</forename><surname>Huang</surname></persName>
							<email>shihuahuang95@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
							<email>luzhichaocn@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
							<email>ranchengcn@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology ?</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southern University of Science and Technology ?</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FaPN: Feature-aligned Pyramid Network for Dense Image Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advancements in deep neural networks have made remarkable leap-forwards in dense image prediction. However, the issue of feature alignment remains as neglected by most existing approaches for simplicity. Direct pixel addition between upsampled and local features leads to feature maps with misaligned contexts that, in turn, translate to mis-classifications in prediction, especially on object boundaries. In this paper, we propose a feature alignment module that learns transformation offsets of pixels to contextually align upsampled higher-level features; and another feature selection module to emphasize the lowerlevel features with rich spatial details. We then integrate these two modules in a top-down pyramidal architecture and present the Feature-aligned Pyramid Network (FaPN). Extensive experimental evaluations on four dense prediction tasks and four datasets have demonstrated the efficacy of FaPN, yielding an overall improvement of 1.2 -2.6 points in AP / mIoU over FPN when paired with Faster / Mask R-CNN. In particular, our FaPN achieves the state-of-the-art of 56.7% mIoU on ADE20K when integrated within Mask-Former. The code is available from https://github.com/EMI-Group/FaPN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Dense prediction is a collection of computer vision tasks that aim at labeling every pixel in an image with a predefined class. It plays a fundamental role in scene understanding and is of great importance to real-world applications, such as autonomous driving <ref type="bibr" target="#b6">[7]</ref>, medical imaging <ref type="bibr" target="#b45">[45]</ref>, augmented reality <ref type="bibr" target="#b0">[1]</ref>, etc. The modern solutions for these tasks are built upon Convolutional Neural Networks (CNNs). With the recent advancements in CNN architectures, a steady stream of promising empirical leapforwards was reported across a wide range of dense prediction tasks, including object detection <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref>, semantic segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">29]</ref>, instance segmentation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">26]</ref>, and panoptic segmentation <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>, to name a few. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Small</head><p>Medium Large objects <ref type="bibr">Figure 1</ref>: Comparisons between FPN and FaPN: (Top row) Qualitatively, FaPN significantly improves the performance on object boundaries as opposed to its counterpart, i.e. FPN <ref type="bibr" target="#b24">[24]</ref>. (Bottom row) Quantitatively, FaPN's improvements over FPN are consistent across different tasks, backbones, and object scales. Best view in color.</p><p>Dense prediction requires both rich spatial details for object location and strong semantics for object classification, which most likely reside at different resolution / scale levels <ref type="bibr" target="#b29">[29]</ref>. How to effectively generate a hierarchy of features at different scales becomes one of the key barriers to overcome in handling dense prediction tasks <ref type="bibr" target="#b24">[24]</ref>. Broadly speaking, there are two common practices to address this issue. The first kind uses atrous convolutions with different atrous rates to effectively capture long-range information (i.e. semantic context) without reducing spatial resolution <ref type="bibr" target="#b3">[4]</ref>. The other kind builds a top-down feature pyramid based on the default bottom-top pathway of a ConvNet <ref type="bibr" target="#b1">[2]</ref>. More specifically, the (higher-level) spatially coarser feature maps are upsampled before merging with the corresponding feature maps from the bottom-up path-way. However, there are inaccurate correspondences (i.e. feature misalignment) between the bottom-up and upsampled features owing to the non-learnable nature of the commonly-used upsampling operations (e.g. nearest neighbor) and the re-  <ref type="bibr" target="#b24">[24]</ref> and our FaPN. Both methods are implemented in Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> with ResNet50 <ref type="bibr" target="#b13">[14]</ref> being the backbone and PointRend <ref type="bibr" target="#b21">[21]</ref> as the mask head. Qualitatively, FaPN significantly improves the performance on object boundaries. Images are randomly chosen from <ref type="bibr" target="#b25">[25]</ref> and <ref type="bibr" target="#b8">[9]</ref> for instance (left) and semantic (right) segmentation, respectively. More visualization examples are available in the supplementary materials.</p><p>peated applications of downsampling and upsampling. The misaligned features, in turn, adversely affects the learning in the subsequent layers, resulting in mis-classifications in the final predictions, especially around the object boundaries. To address the aforementioned issue, we propose a feature alignment module that learns to align the upsampled feature maps to a set of reference feature maps by adjusting each sampling location in a convolutional kernel with a learned offset. We further propose a feature selection module to adaptively emphasize the bottom-up feature maps containing excessive spatial details for accurate locating. We then integrate these two modules in a top-down pyramidal architecture and propose the Feature-aligned Pyramid Network (FaPN).</p><p>Conceptually, FaPN can be easily incorporated to existing bottom-up ConvNet backbones <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b34">34]</ref> to generate a pyramid of features at multiple scales <ref type="bibr" target="#b24">[24]</ref>. We implement FaPN in modern dense prediction frameworks (Faster R-CNN <ref type="bibr" target="#b41">[41]</ref>, Mask R-CNN <ref type="bibr" target="#b12">[13]</ref>, PointRend <ref type="bibr" target="#b21">[21]</ref>, Mask-Former <ref type="bibr" target="#b7">[8]</ref>, PanopticFPN <ref type="bibr" target="#b19">[19]</ref>, and PanopticFCN <ref type="bibr" target="#b23">[23]</ref>), and demonstrate its efficacy on object detection, semantic, instance and panoptic segmentation. Extensive evaluations on multiple challenging datasets suggest that FaPN leads to a significant improvement in dense prediction performance, especially for small objects and on object boundaries. Moreover, FaPN can also be easily extended to realtime semantic segmentation by pairing it with a lightweight bottom-up backbone <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33]</ref>. Without bells and whistles, FaPN achieves favorable performance against existing dedicated real-time methods. Our key contributions are: -We first develop (i) a feature alignment module that learns transformation offsets of pixels to contextually align up-sampled (higher-level) features; and (ii) another feature selection module to emphasize (lower-level) features with rich spatial details.</p><p>-With the integration of these two contributions, we present, Feature-aligned Pyramid Network (FaPN), an enhanced drop-in replacement of FPN <ref type="bibr" target="#b24">[24]</ref>, for generating multi-scale features.</p><p>-We present a thorough experimental evaluation demonstrating the efficacy and value of each component of FaPN across four dense prediction tasks, including object detection, semantic, instance, and panoptic segmentation on three benchmark datasets, including MS COCO <ref type="bibr" target="#b25">[25]</ref>, Cityscapes <ref type="bibr" target="#b8">[9]</ref>, COCO-Stuff-10K <ref type="bibr" target="#b2">[3]</ref>.</p><p>-Empirically, we demonstrate that our FaPN leads to a significant improvement of 1.2% -2.6% in performance (AP / mIoU) over the original FPN <ref type="bibr" target="#b24">[24]</ref>. Furthermore, our FaPN achieves the state-of-the-art of 56.7% mIoU on ADE20K when integrated within MaskFormer <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Feature Pyramid Network Backbone: The existing dense image prediction methods can be broadly divided into two groups. The first group utilizes atrous convolutions to enlarge the receptive field of convolutional filters for capturing long-range information without reducing resolutions spatially. DeepLab <ref type="bibr" target="#b3">[4]</ref> is one of the earliest method that adopt atrous convolution for semantic segmentation. It introduced an Atrous Spatial Pyramid Pooling module (ASPP) comprised of atrous convolutions with different atrous rates to aggregate multi-scale context from high-resolution feature maps. Building upon ASPP, a family of methods <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref> were developed. However, the lack of the ability to generate feature maps at multiple scales restricts the application of this type of methods to other dense prediction tasks beyond semantic segmentation. The second group of methods focuses on building an encoder-decoder network, i.e. bottom-up and top-down pathways. The top-down pathway is used to backpropagate the high-level semantic context into the low-level features via a step-by-step upsampling. There is a plethora of encoder-decoder methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref> proposed for different dense image prediction tasks. Decon-vNet <ref type="bibr" target="#b38">[38]</ref> is one of the earliest works that proposed to use upsample operations with learnable parameters, i.e. deconvolution. DSSD <ref type="bibr" target="#b11">[12]</ref> and FPN <ref type="bibr" target="#b24">[24]</ref> are the extensions of SSD <ref type="bibr" target="#b27">[27]</ref> and Faster R-CNN <ref type="bibr" target="#b41">[41]</ref> respectively for object detection. Mask R-CNN <ref type="bibr" target="#b12">[13]</ref> and SOLOs <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b46">46]</ref> are used for real-time instance segmentation. Moreover, Kirillov et al.</p><p>propose the Panoptic FPN <ref type="bibr" target="#b19">[19]</ref> for panoptic segmentation.</p><p>Feature Alignment: In case of the increasing loss of boundary detail with the step-by-step downsampling, Seg-Net <ref type="bibr" target="#b1">[2]</ref> stores the max-pooling indices in its encoder and upsamples feature maps in the decoder with the corresponding stored max-pooling indices. Instead of memorizing the spatial information in the encoder previously as SegNet, GUN <ref type="bibr" target="#b35">[35]</ref> tries to learn the guidance offsets before upsampling in the decoder and then upsamples feature maps following those offsets. To solve the misalignment between extracted features and the RoI caused by the quantizations in RoIPool, RoIAlign <ref type="bibr" target="#b12">[13]</ref> avoids any quantizations and computes the values for each RoI with linear interpolation. To establish accurate correspondences among multiple frames given a large motion for video restoration, TDAN <ref type="bibr" target="#b42">[42]</ref> and EDVR <ref type="bibr" target="#b43">[43]</ref> achieve implicit motion compensation with by deformable convolution <ref type="bibr" target="#b9">[10]</ref> at the feature level. AlignSeg <ref type="bibr" target="#b16">[17]</ref> and SFNet <ref type="bibr" target="#b22">[22]</ref> are two concurrent works that share a similar motivation as ours and both are flow-based alignment methods. In particular, AlignSeg proposes a two-branched bottom-up network and uses two types of alignment modules to alleviate the feature misalignment before feature aggregation. In contrast, we propose to construct a top-down pathway based on the bottomup network and align features from the coarsest resolution (top) to the finest resolution (bottom) in a progressive way. Specifically, we only align 2? upsampled features to their corresponding bottom-up features, while AlignSeg tries to align diversely scaled features (i.e. upsampled from 1/4, 1/8, and even 1/16) directly which are difficult and may not always be feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Feature-aligned Pyramid Network</head><p>In this section, we present the general framework of our method, comprised of a </p><formula xml:id="formula_0">, i.e. C i ? R H 2 i ? W 2 i , where H ? W is the size of the input image. And we de- note ( H 2 i , W 2 i ) by (H i , W i ) for brevity.</formula><p>We use? i to denote the output of a FSM layer given the input of C i . Also, the output after the i-th feature fusion in the top-down pathway is defined as P i , and its upsampled and aligned features to C i?1 as P u i andP u i , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Feature Alignment Module</head><p>Due to the recursive use of downsampling operations, there are foreseeable spatial misalignment between the upsampled feature maps P u i and the corresponding bottomup feature maps C i?1 . Thus, the feature fusion by either element-wise addition or channel-wise concatenation would harm the prediction around object boundaries. Prior to feature aggregation, aligning P u i to its reference? i?1 is essential, i.e. adjusting P u i accordingly to the spatial location information provided by the? i?1 . In this work, the spatial location information is presented by 2D feature maps, where each offset value can be viewed as the shifted distances in 2D space between each point in P u i and its corresponding point in? i?1 . As illustrated by <ref type="figure">Figure 4</ref>, the feature alignment can be mathematically formulated as:</p><formula xml:id="formula_1">P u i = f a P u i , ? i , ? i = f o [? i?1 , P u i ] ,<label>(1)</label></formula><p>where [? i?1 , P u i ] is the concatenation of? i?1 and P u i which provides spatial difference between the upsampled and corresponding bottom-up features. f o (?) and f a (?) denote the functions for learning offsets (? i ) from the spatial differences and aligning feature with the learned offsets, respectively. In this work, f a (?) and f o (?) are implemented using deformable convolutions <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b54">54]</ref>, followed by activation and standard convolutions of the same kernel size. <ref type="figure">Figure 4</ref>: Feature Alignment Module. The offset fields have the same spatial resolution with the input and 2N channels corresponding to N 2D offsets. Specifically, N denotes a convolutional kernel of N sample locations, e.g. N is equal to 9 for a 3 ? 3 conv, and each value in the n-th offset filed is the horizontal or vertical offset for the n-th sample point.</p><formula xml:id="formula_2">upsample C C 2C ? ? ? ? 2N P ! N # !$% C ! &amp; C % ! &amp; ? !</formula><p>Here, we briefly review the deformable convolution <ref type="bibr" target="#b9">[10]</ref>, and then explain why it can be used as our feature alignment function and provide some important implementation details. We first define an input feature map c i ? R Hi?Wi and a k ? k conv layer. Then, the output feature at any positionx p * after the convolutional kernel can be obtained byx</p><formula xml:id="formula_3">p = N n=1 w n ? x p+pn ,<label>(2)</label></formula><p>where N is the size of the k ? k convolutional layer (i.e. N = k ? k), w n and p n ?</p><formula xml:id="formula_4">{(?? k 2 ?, ?? k 2 ?), (?? k 2 ?, 0), . . . , (? k 2 ?, ? k 2 ?)</formula><p>} refer to the weight and the pre-specified offset for the n-th convolutional sample location, respectively. In addition to the pre-specified offsets, the deformable convolution tries to learn additional offsets {?p 1 , ?p 2 , ..., ?p N } adaptively for different sample locations, and Equation <ref type="formula" target="#formula_3">(2)</ref> can be reformulated a?</p><formula xml:id="formula_5">x p = N n=1 w n ? x p+pn+?pn ,<label>(3)</label></formula><p>where each ?p n is a tuple</p><formula xml:id="formula_6">(h, w), with h ? (?H i , H i ) and w ? (?W i , W i ). * where p ? {(0, 0), (1, 0), (0, 1), . . . , (H i ? 1, W i ? 1)}</formula><p>When we apply the deformable convolution over the P u i and take the concatenation of? i?1 and P u i as the reference (i.e. offset fields</p><formula xml:id="formula_7">? i = f o [? i?1 , P u i ] )</formula><p>, the deformable convolution can adjust its convolutional sample locations following the offsets following Equation (1) ? , i.e. aligning P u i according to the spatial distance between? i?1 and P u i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature Selection Module</head><p>Prior to channel reduction for detailed features, it is vital to emphasize the important feature maps that contain excessive spatial details for accurate allocations while suppressing redundant feature maps. Instead of simply using a 1 ? 1 convolution <ref type="bibr" target="#b24">[24]</ref>, we propose a feature selection module (FSM) to explicitly model the importance of feature maps and re-calibrate them accordingly. The general dataflow of the proposed FSM is presented in <ref type="figure">Figure 5</ref>. To begin with, the global information z i of each input feature map c i is extracted by a global average pooling operation, while a feature importance modeling layer f m (?) (i.e. a 1 ? 1 conv layer followed by a sigmoid activation function) learns to use such information for modeling the importance of each feature map and outputs an importance vector u. Next, the original input feature maps are scaled with the importance vector, and then the scaled feature maps are added to the original feature maps, referred as rescaled feature maps. Finally, a feature selection layer f s (?) (i.e. a 1 ? 1 conv layer for efficiency) is introduced over the rescaled feature maps, which is used to selectively maintain important feature maps and drop useless feature maps for channel reduction. Overall, the process of FSM can be formulated a?</p><formula xml:id="formula_8">! ! " ! " Figure 5: Feature Selection Module. C i = [c 1 , c 2 , . . . , c D ] and? i = [? 1 ,? 2 , . . . ,? D ? ]</formula><formula xml:id="formula_9">C i = f s (C i + u * C i ), u = f m (z),<label>(4)</label></formula><p>where z = [z 1 , z 2 , . . . , z D ] and is calculated by</p><formula xml:id="formula_10">z d = 1 H i ? W i Hi h=1 Wi w=1 c d (h, w).<label>(5)</label></formula><p>It is worth mentioning that the design of our FSM is motivated by the squeeze-and-excitation (SE) <ref type="bibr" target="#b15">[16]</ref>. The main difference lies in the additional skip connection introduced between the input and scaled feature maps ( <ref type="figure">Figure 5</ref>). Empirically, we find that lower bounding the scaled feature (through the skip connection) is essential, which avoids any particular channel responses to be over-amplified or -suppressed. Conceptually, both of these two modules learn to adaptively re-calibrate channel-wise responses by channel attention. However, SE is conventionally used in the backbone for enhancing feature extraction, while FSM is used in the neck (i.e. top-down pathway) for enhancing multi-scale feature aggregation. Additionally, the selected/scaled features from FSM are also supplied as references to FAM for learning alignment offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first briefly introduce the benchmark datasets studied in this work, followed by the implementation and training details. We then evaluate the performance of the proposed FaPN on four dense image prediction tasks, including object detection, semantic, instance and panoptic segmentation. Ablation studies demonstrating the effectiveness of each component in FaPN are also provided. Moreover, we incorporate our proposed FaPN with lightweight backbones and evaluate its efficacy under real-time settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets:</head><p>We consider four widely-used benchmark datasets to evaluate our method, including MS COCO <ref type="bibr" target="#b25">[25]</ref> for object detection, instance and panoptic segmentation; Cityscapes <ref type="bibr" target="#b8">[9]</ref>, COCO-Stuff-10K <ref type="bibr" target="#b2">[3]</ref> and ADE20K <ref type="bibr" target="#b53">[53]</ref> for semantic segmentation.</p><p>MS COCO consists of more than 100K images containing diverse objects and annotations, including both bounding boxes and segmentation masks. We use the train2017 set (around 118K images) for training and report results on the val2017 set (5K images) for comparison. For both object detection and instance segmentation tasks, there are 80 categories; and for panoptic segmentation task, there are 80 things and 53 stuff classes annotated.</p><p>Cityscapes is a large-scale dataset for semantic understanding of urban street scenes. It is split into training, validation and test sets, with 2975, 500 and 1525 images, respectively. The annotation includes 30 classes, 19 of which are used for semantic segmentation task. The images in this dataset have a higher and unified resolution of 1024 ? 2048, which poses stiff challenges to the task of real-time semantic segmentation. For the experiments shown in this part, we only use images with fine annotations to train and validate our proposed method.</p><p>COCO-Stuff-10K contains a subset of 10K images from the COCO dataset <ref type="bibr" target="#b25">[25]</ref> with dense stuff annotations. It is a challenging dataset for semantic segmentation as it has 182 categories (91 thing classes plus 91 stuff classes). In this work, we follow the official split -9K images for training and 1K images for test.</p><p>ADE20K is a challenging scene parsing dataset that contains 20k images for training and 2k images for validation. Images in the dataset are densely labeled as hundreds of classes. In this work, only 150 semantic categories are selected to be included in the evaluation. Implementation details: Following the original work of FPN <ref type="bibr" target="#b24">[24]</ref>, we use ResNets <ref type="bibr" target="#b14">[15]</ref> pre-trained on Ima-geNet <ref type="bibr" target="#b10">[11]</ref> as the backbone ConvNets for the bottom-up pathway. We then replace the FPN with our proposed FaPN as the top-down pathway network. Next, we connect the feature pyramid with the Faster R-CNN detector <ref type="bibr" target="#b41">[41]</ref> for object detection, and Mask R-CNN (with PointRend masking head <ref type="bibr" target="#b21">[21]</ref>) for segmentation tasks.</p><p>For performance evaluation, the Average Precision (AP) is used as the primary metric for both object detection and instance segmentation. We evaluate AP on small, medium and large objects, i.e. AP s , AP m , and AP l . Note that AP bb and AP mask denote AP for bounding box and segmentation mask, respectively. The mean Intersection-over-Union (mIoU) and the Panoptic Quality (PQ) are two primary metrics used for semantic and panoptic segmentation, respectively. Additionally, we also use PQ St and PQ T h metrics to evaluate stuff and thing performances separately for panoptic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>We first breakdown the individual impacts of the two components introduced in FaPN, i.e. the feature alignment and selection modules. Using ResNet50 as the bottom-up backbone, we evaluate on Cityscapes for semantic segmentation. <ref type="table" target="#tab_0">Table 1</ref> shows the improvement in accuracy along with the complexity overheads measured in #Params.</p><p>Evidently, with marginal increments in model size, our proposed feature alignment module alone significantly boosts the performance of the original FPN <ref type="bibr" target="#b24">[24]</ref>, yielding an improvement of 2.3 points in mIoU. In particular, our method (80.0@33.1M) is significantly more effective than naively expanding either i) the #Params of FPN by extra 3?3 conv. (77.5@33.4M) or ii) the capacity of the backbone from R50 to R101 (78.9@47.6M). Empirically, we observe that a naive application of SE <ref type="bibr" target="#b15">[16]</ref> (for feature selection) adversely affects the performance, while our proposed FSM provides a further boost in mIoU.</p><p>Recall that the misalignment in this work refers to the spatial misalignment of features induced during the ag- gregation of multi-resolution feature maps (i.e., top-down pathway in FPN), particularly around object boundaries. One plausible cause relates to the non-learnable nature of commonly-used upsampling operations (e.g., bilinear). However, simply swapping it to a learnable operation (e.g., deconvolution) is insufficient, suggesting the need of better engineered methods. This reinforces the motivation of this work. Instead of performing the feature alignment before feature fusion, we place our FAM after feature fusion, in which our FAM learns the offsets from the fused features instead. Although this variation performs better than all other variants, it is still substantially worse than the proposed FaPN, which reiterate the necessity of feature alignment before fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Boundary Prediction Analysis</head><p>We provide the mIoU over the boundary pixels ? in <ref type="table" target="#tab_1">Table 2</ref>. Evidently, our method achieves a substantially better segmentation performance than FPN on boundaries. Moreover, we visualize the input (upsampled features P u 2 ) to and the output (aligned featuresP u 2 ) from the last feature alignment module in FaPN-R50 ( <ref type="figure" target="#fig_5">Figure 6</ref>) to perceive the alignment corrections made by our FAM. In contrast to the raw upsampled features (before FAM) which are noisy and fluctuating, the aligned features are smooth and containing more precise object boundaries. Both the quantitative evaluation and qualitative observation are consistent and suggest that FaPN leads to better predictions on the boundaries. More visualizations are provided in <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>In this section, we present the detailed empirical comparisons to FPN <ref type="bibr" target="#b24">[24]</ref> on four dense prediction tasks, including object detection, semantic, instance and panoptic segmentation in <ref type="table" target="#tab_2">Table 3</ref> -6, respectively. ? we consider n pixels around the outline of each object to be boundary pixels, where n can be one of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12</ref>].  In general, FaPN substantially outperforms FPN on all scenarios of tasks and datasets. There are several detailed observations. First, FaPN improves the primary evaluation metrics by 1.2 -2.6 points over FPN on all four tasks with ResNet50 <ref type="bibr" target="#b14">[15]</ref> as the bottom-up backbone. Second, the improvements brought by FaPN hold for stronger bottomtop backbones (e.g. ResNet101 <ref type="bibr" target="#b14">[15]</ref>) with a longer training schedule of 270K iterations. Third, the improvement from FaPN extends to more sophisticated mask heads, e.g. PointRend <ref type="bibr" target="#b21">[21]</ref>, on instance segmentation, as shown in Table 5 (bottom section).  <ref type="figure">Figure 7</ref>: Example pairs of results comparing FPN <ref type="bibr" target="#b24">[24]</ref> and our FaPN. Images are randomly chosen from <ref type="bibr" target="#b25">[25]</ref>. Best view in color and zoom in for details.  In particular, we notice that the improvement is larger on small objects (e.g. AP bb s , AP mask s ). For instance, FaPN improves the bounding box AP on small objects by 2.1 points and 1.8 points over FPN on MS COCO object detection and instance segmentation, respectively. Conceptually, small objects occupy fewer pixels in an image, and most of pixels are distributed along the object boundaries. Hence, it is vital to be able to correctly classify the boundaries for small objects. However, as features traverse the top-bottom pathway through heuristics-based upsampling operations (e.g. FPN uses nearest neighbor upsampling), shifts in pixels (i.e. misalignment) are foreseeable and the amount of shifts will accumulate as the number of upsampling steps increases. Hereby, the severity of the misalignment will reach its maximum at the finest feature maps in the top-down pathway pyramid, which are typically used for detecting or segmenting small objects, resulting in a significant degradation in performance. On the other hand, FaPN performs feature alignment progressively which in turn alleviates the misalignment at the finest level step by step, and thus achieves significant improvements on small objects compared to the FPN <ref type="bibr" target="#b24">[24]</ref>. Qualitative improvements are also evidenced in <ref type="figure">Figure 7</ref>. Finally, we incorporate FaPN into MaskFormer <ref type="bibr" target="#b7">[8]</ref>, and demonstrate that FaPN leads to state-of-the-art performance on two complex semantic segmentation tasks, i.e. ADE20K and COCO-Stuff-10K, as shown in <ref type="table" target="#tab_6">Table 7</ref>. Overall, extensive comparisons on scenarios comprised of different tasks and datasets have confirmed the effectiveness of our proposed FaPN for dense image prediction. A straightforward replacement of FPN with FaPN achieves substantial improvements without bells and whistles. The generality and flexibility to different bottom-up backbones or mask heads have further strengthened the practical utilities of FaPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Real-time Performance</head><p>Driven by real-world applications (e.g., autonomous driving), there are growing interests in real-time dense prediction, which requires the generation of high-quality predictions with minimal latency. In this section, we aim to investigate the effectiveness of our proposed FaPN under real-time settings, i.e. inference speed ? 30 FPS. The full details are provided in the supplementary materials.</p><p>We compare our FaPN with state-of-the-art real-time semantic segmentation methods on Cityscapes and COCO-Stuff-10K in <ref type="table" target="#tab_7">Table 8</ref>, in terms of accuracy (mIoU) and inference speed (FPS). In general, we observe that a straightforward replacement of FPN with the proposed FaPN results in a competitive baseline against other dedicated real-time semantic segmentation methods.</p><p>In particular, on Cityspaces, FaPN-R18 runs 2? faster than SwiftNet <ref type="bibr" target="#b39">[39]</ref>, while maintaining a similar mIoU performance. In addition, with a larger backbone and input size, FaPN-R34 achieves a competitive mIoU of 78.1 points on the test split, in the same time outputting 30 FPS. On the more challenging COCO-Stuff-10K, our FaPN also outperforms other existing methods by a substantial margin. Specifically, FaPN-R34 outperforms BiSeNetV2 <ref type="bibr" target="#b48">[48]</ref> in both segmentation accuracy measured in mIoU and inference speed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduced Feature-aligned Pyramid Network (FaPN), a simple yet effective top-down pyramidal architecture to generate multi-scale features for dense image prediction. It is comprised of a feature alignment module that learns transformation offsets of pixels to contextually align upsampled higher-level features; and a feature selection module to emphasize the lower-level features with rich spatial details. Empirically, FaPN leads to substantial and consistent improvements over the original FPN on four dense prediction tasks and three datasets. Furthermore, FaPN improves the state-of-the-art segmentation performance when integrated in strong baselines. Additionally, FaPN can be easily extended to real-time segmentation tasks by pairing it with lightweight backbones, where we demonstrate that FaPN performs favorably against dedicated real-time methods. In short, given the promising performance on top of a simple implementation, we believe that FaPN can serve as the new baseline/module for dense image prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Settings</head><p>For all experiments shown in the main paper, we use SGD optimizer with 0.9 momentum and 0.0001 weight decay. The standard data augmentation of horizontal flipping and scaling are also applied. In addition, the weights of the batch normalization <ref type="bibr" target="#b17">[18]</ref> layers derived from the Ima-geNet pre-trained models are kept frozen. To be consistent with prior works, we have not incorporated any testing time augmentation tricks. For semantic segmentation, the model is trained for 65K iterations starting with a learning rate of 0.01 that is reduced by a factor of 10 at 40K and 55K. For the other three dense prediction tasks, the model is trained for 90K or 270K iterations with the initial learning rate of 0.02 that is reduced to 0.002 at 60K/210K and 0.0002 at 80K/250K. Our implementation is based on the Detec-tron2 <ref type="bibr" target="#b47">[47]</ref> with the default configurations, i.e. to maintain a fair comparison with prior works, neither have we tuned any training hyperparameters nor used advanced data augmentations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Real-time Semantic Segmentation Continued</head><p>With a lightweight ResNet (e.g. ResNet18/34) as the bottom-up backbone, we denote the feature maps output by the last three stages, i.e. conv3, conv4, conv5, as {C 3 , C 4 , C 5 }, respectively. At the beginning, we simply attach an FSM layer on C 5 to produce the coarsest resolution feature maps P 5 ? R 128? H 32 ? W 32 (i.e. the output channel of FSM is 128). With a coarser-resolution feature map P i (l ? [4, 5]), we upsample its spatial resolution by a factor of 2 using nearest neighbor upsampling <ref type="bibr" target="#b24">[24]</ref> to obtain</p><formula xml:id="formula_11">P up i ? R 128? H 2 i?1 ? W 2 i?1 .</formula><p>Afterwards, an FAM layer is used to align P up i to its corresponding bottom-up feature map</p><formula xml:id="formula_12">C i?1 ? R 128? H 2 i?1 ? W 2 i?1 deriving from C i?1 by undergo-</formula><p>ing an FSM layer for channel reduction. Instead of elementwise addition, the alignedP up i is then merged with? i?1 by concatenation along with channel, and the merged feature map P i?1 ? R 256? H 2 i?1 ? W 2 i?1 which has the identical spatial size to C i?1 is further input in a Conv 1 ? 1 layer to reduce its channels to 128. This process is iterated until the finest resolution map P 3 ? R 128? H 8 ? W 8 is generated. Finally, we append a prediction layer on P 3 to generate the final semantic mask.</p><p>We train our models using the SGD optimizer with momentum and weight decay set to 0.9 and 0.0005, re-spectively. During training, we apply random horizontal flip and scale to input images, followed by a crop to a fixed size. The scale is randomly selected from {0.75, 1, 1.25, 1.5, 1.75, 2.0}, and the cropped resolutions are 1536?768 and 640?640 for Cityscapes <ref type="bibr" target="#b8">[9]</ref> and COCO-Stuff <ref type="bibr" target="#b2">[3]</ref>, respectively. For all datasets, we set the batch size and the initial learning rate to 16 and 0.01, while the learning rate decays following a "poly" strategy, specifically 0.01?(1? iter maxiters ) 0.9 . Following the prior works <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49]</ref>, we train models for 40K and 20K training iterations on Cityscapes and COCO-Stuff, respectively. In the evaluation process, we compute the inference speed using one Titan RTX GPU without any speed-up trick (e.g., Apex or Ten-sorRT) or optimized depth-wise convolutions, and evaluate the accuracy without any testing augmentation technique (e.g., multi-crop or multi-scale testing). We first demonstrate the effectiveness of each module in our proposed real-time semantic segmentation framework separately and then study different feature fusion methods on the Cityscapes val set. We use ResNet18 pre-trained on ImageNet as our backbone in the following ablation analysis.</p><p>The ablation experimental results are given in <ref type="table" target="#tab_8">Table I</ref>. Basically, the incorporation of FAM into the baseline improves the performance from 68.6% to 73.8%. Furthermore, FSM improves the performance to 74.2% with only 0.4M additional parameters. Besides, when we replace the element-wise sum operation with concatenation for fusing the detailed feature and aligned semantic feature, another 1.4% improvement is achieved with few extra FLOPs. <ref type="figure">Figure</ref>   PointRend head</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Corresponding author. ? Authors are with Department of Computer Science and Engineering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Example pairs of results from FPN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Feature Selection Module (FSM) and a Feature Alignment Module (FAM), as shown in Fig-Overview comparison between FPN and FaPN. Details of the FAM and FSM components are provided in Figure 4 and Figure 5, respectively. ure 3 (right). Specifically, we define the output of the i-th stage of the bottom-up network as C i , which has stride of 2 i pixels with respect to the input image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>refer to the input and output feature maps respectively, where c d and? d ? ? R Hi?Wi , D and D ? denote the input and output channels, respectively. u = [u 1 , u 2 , . . . , u D ] is the feature importance vector, where u d represents the importance of the d-th input feature map. f m and f s represent the feature importance modeling and feature selection layer, respectively. See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of the input (upsampled features) to and the output (aligned features) from our FAM. Zoom in for better details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>I visualizes the semantic segmentation results of FaPN on Cityscapes under real-time settings (FPS ? 30) ? . Noticeably, the proposed Feature Align Module (FAM; third column in Figure I) significantly improves the segmentation quality from the baseline (i.e. FPN; second column in Figure I). With the feature selection module and feature concatenation, our final approach FaPN further improves the performance on real-time semantic segmentation. ? https://paperswithcode.com/sota/ real-time-semantic-segmentation-on-cityscapes Ground truth without FAM with FAM FaPN Figure I: Example visual comparisons among different approaches on the Cityscapes val set. From left to right: ground truths, results from baseline, baseline with FAM and our FaPN, respectively. All models are using ResNet18.C. Additional VisualizationFigure IIandFigure IIIvisualize the dense prediction performance on MS COCO. Evidently, our method achieves a more accurate segmentation on object boundaries and small objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure III :</head><label>III</label><figDesc>Example results from methods with FPN or our FaPN on small objects. In each group, from left to right, they are object detection, instance and panoptic segmentation, the top is achieved by FPN while the bottom by our FaPN. All models are based on ResNet50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablative Analysis: Comparing the performance of our FaPN with other variants on Cityscapes for semantic segmentation. ? denotes placing FAM after feature fusion. "deconv" refers to the deconvolution which is a learnable upsample operation. The relative improvements/overheads are shown in parenthesis.</figDesc><table><row><cell>method</cell><cell cols="2">backbone #Params (M)</cell><cell>mIoU (%)</cell></row><row><cell>FPN</cell><cell>R50</cell><cell cols="2">28.6 (+4.5) 77.4 (+2.6)</cell></row><row><cell>FPN + extra 3?3 conv.</cell><cell>R50</cell><cell cols="2">33.4 (-0.3) 77.5 (+2.5)</cell></row><row><cell>FPN</cell><cell>R101</cell><cell cols="2">47.6 (-14.5) 78.9 (+1.1)</cell></row><row><cell>FPN + FAM</cell><cell>R50</cell><cell cols="2">31.7 (+1.4) 79.7 (+0.3)</cell></row><row><cell>FPN + FAM + SE</cell><cell>R50</cell><cell cols="2">33.1 (+0.0) 78.8 (+1.2)</cell></row><row><cell>FPN + FAM + FSM (FaPN)</cell><cell>R50</cell><cell cols="2">33.1 (+0.0) 80.0 (+0.0)</cell></row><row><cell>FPN + deconv + FSM</cell><cell>R50</cell><cell cols="2">32.7 (+0.4) 76.7 (+3.3)</cell></row><row><cell>FPN + FAM  ? + FSM</cell><cell>R50</cell><cell cols="2">32.7 (+0.4) 79.3 (+0.7)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Segmentation Performance around Boundaries:Comparing the performance of our FaPN with the original FPN<ref type="bibr" target="#b24">[24]</ref> in terms of mIoU over boundary pixels on Cityscape val with different thresholds on boundary pixels.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell></cell><cell>3px</cell><cell>5px</cell><cell>8px</cell><cell>12px</cell><cell>mean</cell></row><row><cell>FPN FaPN improvement</cell><cell cols="2">PointRend [21] R50</cell><cell cols="5">46.9 49.2 (+2.3) (+2.6) (+2.7) (+2.6) (+2.6) 53.6 59.3 63.8 55.9 56.2 62.0 66.4 58.5</cell></row><row><cell>FPN FaPN improvement</cell><cell cols="2">PointRend [21] R101</cell><cell cols="5">47.8 50.1 (+2.3) (+2.5) (+2.4) (+2.3) (+2.3) 54.6 60.5 64.9 57.0 57.1 62.9 67.2 59.3</cell></row><row><cell cols="2">Ground truth</cell><cell cols="3">Before alignment ( ! " )</cell><cell></cell><cell cols="2">After alignment ( " ! " )</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Object Detection: Performance comparisons on MS COCO val set between FPN and FaPN.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>AP bb</cell><cell>AP bb s</cell><cell>AP bb m</cell><cell>AP bb l</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>Faster R-CNN [41] R50</cell><cell cols="4">37.9 39.2 (+1.3) (+2.1) (+2.2) (+0.0) 22.4 41.1 49.1 24.5 43.3 49.1</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>Faster R-CNN [41] R101</cell><cell cols="4">42.0 42.8 (+0.8) (+1.8) (+0.6) (+0.3) 25.2 45.6 54.6 27.0 46.2 54.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Semantic Segmentation: Performance comparisons on Cityscapes val set between FPN and FaPN.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>mIoU</cell><cell>iIoU</cell><cell cols="2">IoU_sup iIoU_sup</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>PointRend [21] R50</cell><cell cols="2">77.4 80.0 (+2.6) (+2.8) 58.5 61.3</cell><cell>89.9 90.6 (+0.7)</cell><cell>76.9 78.5 (+1.6)</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>PointRend [21] R101</cell><cell cols="2">78.9 80.1 (+1.2) (+2.3) 59.9 62.2</cell><cell>90.4 90.8 (+0.4)</cell><cell>77.8 78.6 (+0.8)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Instance Segmentation: Performance comparisons on MS COCO val set between FPN and FaPN.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell cols="2">AP mask AP mask s</cell><cell>AP bb</cell><cell>AP bb s</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>Mask R-CNN [13] R50</cell><cell>35.2 36.4 (+1.2)</cell><cell>17.1 18.1 (+1.0)</cell><cell cols="2">38.6 39.8 (+1.2) (+1.8) 22.5 24.3</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>Mask R-CNN [13] R101</cell><cell>38.6 39.6 (+1.0)</cell><cell>19.5 20.9 (+1.4)</cell><cell cols="2">42.9 43.8 (+0.9) (+1.0) 26.4 27.4</cell></row><row><cell>FPN FaPN + PR (ours) improvement</cell><cell>PointRend [21] R50</cell><cell>36.2 37.6 (+1.4)</cell><cell>17.1 18.6 (+1.5)</cell><cell cols="2">38.3 39.4 (+1.1) (+1.9) 22.3 24.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Panoptic Segmentation: Performance comparisons on MS COCO val set between FPN and FaPN.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>PQ</cell><cell>mIoU PQ St</cell><cell>AP bb</cell><cell>PQ T h</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>PanopticFPN [19] R50</cell><cell cols="4">39.4 41.1 (+1.7) (+2.2) (+3.0) (+0.9) (+1.0) 41.2 29.5 37.6 45.9 43.4 32.5 38.7 46.9</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>PanopticFPN [19] R101</cell><cell cols="4">43.0 44.2 (+1.2) (+1.2) (+2.1) (+0.6) (+0.6) 44.5 32.9 42.4 49.7 45.7 35.0 43.0 50.3</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>PanopticFCN [23] R50</cell><cell cols="4">41.1 41.8 (+0.7) (+0.4) (+0.6) (+0.6) (+0.6) 79.8 49.9 30.2 41.4 80.2 50.5 30.8 42.0</cell></row><row><cell>FPN FaPN (ours) improvement</cell><cell>PanopticFCN [23] R50-600</cell><cell cols="4">42.7 43.5 (+0.8) (+0.5) (+0.7) (+0.7) (+0.6) 80.8 51.4 31.6 43.9 81.3 52.1 32.3 53.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Comparison</figDesc><table><row><cell></cell><cell cols="4">to SOTA on (a) ADE20K val and</cell></row><row><cell cols="5">(b) COCO-Stuff-10K test. We report both single-scale</cell></row><row><cell cols="5">(s.s.) and multi-scale (m.s.) semantic segmentation per-</cell></row><row><cell cols="5">formance. Backbones pre-trained on ImageNet-22K are</cell></row><row><cell cols="4">marked with  ? . Our results are highlighted in shade.</cell><cell></cell></row><row><cell></cell><cell cols="2">(a) ADE20K val</cell><cell></cell><cell></cell></row><row><cell>method</cell><cell>backbone</cell><cell>crop size</cell><cell cols="2">mIoU (s.s.) mIoU (m.s.)</cell></row><row><cell>OCRNet [50]</cell><cell>R101</cell><cell>520 ? 520</cell><cell>-</cell><cell>45.3</cell></row><row><cell>AlignSeg [17]</cell><cell>R101</cell><cell>512 ? 512</cell><cell>-</cell><cell>46.0</cell></row><row><cell>SETR [52]</cell><cell>ViT-L  ?</cell><cell>512 ? 512</cell><cell>-</cell><cell>50.3</cell></row><row><cell>Swin-UperNet [28]</cell><cell>Swin-L  ?</cell><cell>640 ? 640</cell><cell>-</cell><cell>53.5</cell></row><row><cell>MaskFormer [8]</cell><cell>Swin-L  ?</cell><cell>640 ? 640</cell><cell>54.1</cell><cell>55.6</cell></row><row><cell cols="2">MaskFormer + FaPN Swin-L  ?</cell><cell>640 ? 640</cell><cell>55.2</cell><cell>56.7</cell></row><row><cell></cell><cell cols="3">(b) COCO-Stuff-10K test</cell><cell></cell></row><row><cell>method</cell><cell>backbone</cell><cell>crop size</cell><cell cols="2">mIoU (s.s.) mIoU (m.s.)</cell></row><row><cell>OCRNet [50]</cell><cell></cell><cell>520 ? 520</cell><cell>-</cell><cell>39.5</cell></row><row><cell>MaskFormer [8]</cell><cell>R101</cell><cell>640 ? 640</cell><cell>38.1</cell><cell>39.8</cell></row><row><cell>MaskFormer + FaPN</cell><cell></cell><cell>640 ? 640</cell><cell>39.6</cell><cell>40.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Real</figDesc><table><row><cell></cell><cell cols="6">-time semantic segmentation on (a)</cell></row><row><cell cols="7">Cityscapes and (b) COCO-Stuff-10K.  ? denotes a method</cell></row><row><cell cols="7">with a customized backbone. Our results are highlighted in</cell></row><row><cell>shade.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">(a) Cityscapes</cell><cell></cell></row><row><cell>method</cell><cell>backbone</cell><cell cols="2">crop size</cell><cell cols="3">FPS mIoU (val) mIoU (test)</cell></row><row><cell>ESPNet [36]</cell><cell>?</cell><cell cols="2">512 ? 1024</cell><cell>113</cell><cell>-</cell><cell>60.3</cell></row><row><cell>ESPNetV2 [37]</cell><cell>?</cell><cell cols="2">512 ? 1024</cell><cell>-</cell><cell>66.4</cell><cell>66.2</cell></row><row><cell>FaPN</cell><cell>R18</cell><cell cols="2">512 ? 1024</cell><cell>142</cell><cell>69.2</cell><cell>68.8</cell></row><row><cell>BiSeNet [49]</cell><cell>R18</cell><cell cols="3">768 ? 1536 65.6</cell><cell>74.8</cell><cell>74.7</cell></row><row><cell>FaPN</cell><cell>R18</cell><cell cols="3">768 ? 1536 78.1</cell><cell>75.6</cell><cell>75.0</cell></row><row><cell>SwiftNet [39]</cell><cell>R18</cell><cell cols="3">1024 ? 2048 39.9</cell><cell>75.4</cell><cell>75.5</cell></row><row><cell>ICNet [51]</cell><cell>R50</cell><cell cols="3">1024 ? 2048 30.3</cell><cell>-</cell><cell>69.5</cell></row><row><cell>FAPN</cell><cell>R34</cell><cell cols="3">1024 ? 2048 30.2</cell><cell>78.5</cell><cell>78.1</cell></row><row><cell></cell><cell cols="4">(b) COCO-Stuff-10K</cell><cell></cell></row><row><cell>method</cell><cell cols="2">backbone</cell><cell cols="2">crop size</cell><cell cols="2">FPS mIoU (val)</cell></row><row><cell cols="2">BiSeNet [49]</cell><cell>R18</cell><cell></cell><cell></cell><cell>-</cell><cell>28.1</cell></row><row><cell cols="2">BiSeNetV2 [48]</cell><cell>?</cell><cell></cell><cell></cell><cell>42.5</cell><cell>28.7</cell></row><row><cell>ICNet [51]</cell><cell></cell><cell>R50</cell><cell cols="2">640 ? 640</cell><cell>35.7</cell><cell>29.1</cell></row><row><cell>FaPN</cell><cell></cell><cell>R18</cell><cell></cell><cell></cell><cell>154</cell><cell>28.4</cell></row><row><cell>FaPN</cell><cell></cell><cell>R34</cell><cell></cell><cell></cell><cell>110</cell><cell>30.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table I :</head><label>I</label><figDesc>Ablation Study on Real-time Semantic Segmentation: Detailed comparisons of each component in our proposed real-time semantic segmentation FaPN over Citsycapes val set in terms of accuracy, parameters and FLOPs (computational complexity).</figDesc><table><row><cell>method</cell><cell cols="3">mIoU #Params (M) FLOPs (G)</cell></row><row><cell>FPN</cell><cell>68.6</cell><cell>11.4</cell><cell>44.5</cell></row><row><cell>+ FAM</cell><cell>73.8</cell><cell>12.2</cell><cell>51.0</cell></row><row><cell>+ FAM + FSM</cell><cell>74.2</cell><cell>12.6</cell><cell>51.0</cell></row><row><cell>+ FAM + FSM + Concat</cell><cell>75.6</cell><cell>12.6</cell><cell>51.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? Following the convention of the deformable convolution, this study adopts 3 ? 3 as the kernal size for fa(?) and fo(?).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>In this supplementary material we include (1) additional training details in Section A; (2) more details on the realtime semantic segmentation experiment in Section B; and (3) additional qualitative visualizations to demonstrate the effectiveness of the proposed FaPN in Section C.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Augmented reality meets computer vision: Efficient data generation for urban driving scenes. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Hassan Abu Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Karthik Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">SegNet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Per-pixel classification is not all you need for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06278</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananth</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambrish</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">Girshick</forename><surname>Mask R-Cnn</surname></persName>
		</author>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Alignseg: Featurealigned segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Example pairs of instance segmentation results from FPN (Left) and our FaPN (Right) on object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">I</forename><surname>Figure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Both methods are implemented in Mask R-CNN with ResNet-50 being the backbone and PointRend as the mask head</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Ross Girshick. PointRend: Image segmentation as rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic flow for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansheng</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houlong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhai</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">MUX-Conv: Information multiplexing in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evolutionary multi-objective surrogate-assisted neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu Naresh</forename><surname>Boddeti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Sreekumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><forename type="middle">N</forename><surname>Boddeti</surname></persName>
		</author>
		<title level="m">Neural architecture transfer. TPAMI, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">NSGA-Net: Neural architecture search using multi-objective genetic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishnu</forename><surname>Boddeti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GECCO</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Boddeti. Multiobjective evolutionary design of deep convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Whalen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yashesh</forename><surname>Dhebar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyanmoy</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Banzhaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishnu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TEVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Guided upsampling network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Mazzini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.07466</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Espnet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Espnetv2: A light-weight, power efficient, and general purpose convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linda</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">In defense of pre-trained imagenet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Orsic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kreso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Segvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tdan: Temporally-deformable alignment network for video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edvr: Video restoration with enhanced deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Kelvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SOLO: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadhadi</forename><surname>Bagheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 9</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02147</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bisenet: Bilateral segmentation network for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Objectcontextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
