<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNSUPERVISED MULTIMODAL LANGUAGE REPRESENTATIONS USING CONVOLUTIONAL AUTOENCODERS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Koromilas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics &amp; Telecommunications</orgName>
								<orgName type="institution">NCSR -Demokritos</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Giannakopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics &amp; Telecommunications</orgName>
								<orgName type="institution">NCSR -Demokritos</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">UNSUPERVISED MULTIMODAL LANGUAGE REPRESENTATIONS USING CONVOLUTIONAL AUTOENCODERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-multimodal temporal representations</term>
					<term>unsuper- vised multimodal language</term>
					<term>mutlimodal sentiment analysis</term>
					<term>multi- modal emotion recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal Language Analysis is a demanding area of research, since it is associated with two requirements: combining different modalities and capturing temporal information. During the last years, several works have been proposed in the area, mostly centered around supervised learning in downstream tasks. In this paper we propose extracting unsupervised Multimodal Language representations that are universal and can be applied to different tasks. Towards this end, we map the word-level aligned multimodal sequences to 2-D matrices and then use Convolutional Autoencoders to learn embeddings by combining multiple datasets. Extensive experimentation on Sentiment Analysis (MOSEI) and Emotion Recognition (IEMOCAP) indicate that the learned representations can achieve near-state-of-the-art performance with just the use of a Logistic Regression algorithm for downstream classification. It is also shown that our method is extremely lightweight and can be easily generalized to other tasks and unseen data with small performance drop and almost the same number of parameters. The proposed multimodal representation models are open-sourced and will help grow the applicability of Multimodal Language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Multimodal Language Analysis focuses on extracting information from temporal language sequences using acoustic, visual and textual information. The two core application-specific problems of this area are Multimodal Sentiment Analysis, where the goal is to determine whether a utterance contains positive or negative sentiment, and Multimodal Emotion Recognition in which the objective is to predict the underlying emotion of the utterance. In the past years, research has been conducted in these problems, resulting in powerful models that achieve great performance on the particular tasks. Different types of fusion approaches, such as early fusion <ref type="bibr" target="#b0">[1]</ref>, memory fusion <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, multistage fusion <ref type="bibr" target="#b3">[4]</ref> and tensor fusion <ref type="bibr" target="#b4">[5]</ref> have been examined. Modifications on LSTM architectures for multiview learning <ref type="bibr" target="#b5">[6]</ref> or context-dependent analysis <ref type="bibr" target="#b6">[7]</ref> have also been proposed, as well as the concept of attention on recurrent networks <ref type="bibr" target="#b7">[8]</ref>, context-aware attention <ref type="bibr" target="#b8">[9]</ref> and some transformer architectures <ref type="bibr" target="#b9">[10]</ref> have all been researched in depth.</p><p>However, most of the works treat the learning process in a supervised fashion. The individual subdomains (i.e. the different applications and datasets) of Multimodal Language have the particularity of a high level of variability in the recording conditions and the experimental setup in general. For example the most widely used dataset for Emotion Recognition, IEMOCAP <ref type="bibr" target="#b10">[11]</ref>, is recorded in a laboratory with organized camera networks, while MOSEI <ref type="bibr" target="#b2">[3]</ref>, the largest dataset for Multimodal Sentiment Analysis, is collected from Youtube videos. Thus, strongly supervised methods are of limited use and cannot generalize to unseen recording set-ups and different tasks.</p><p>One of the few works that includes an unsupervised factor in their hybrid (including supervised and unsupervised factors) loss function is presented in <ref type="bibr" target="#b11">[12]</ref> were the trained representations are sequential which limits the usage of such embeddings to sequential architectures. Another method that learns unsupervised representations is introduced in <ref type="bibr" target="#b12">[13]</ref>, where the proposed architecture extracts text-based embeddings and thus the information of other modalities is only used to enrich the textual information. However, the resulted representations of <ref type="bibr" target="#b11">[12]</ref> and <ref type="bibr" target="#b12">[13]</ref> cannot properly model tri-modal interactions, since there is always a modality that interacts with the bimodal representation of the remaining modalities rather than the actual input sequences. Adding that such approaches make the overall architecture task or data (eg. <ref type="bibr" target="#b13">[14]</ref>) specific and that they do not test the generalization ability of the produced representations to other tasks and data set-ups, we have no proof for their performance on unseen data and tasks.</p><p>In this work, we propose a simple, yet powerful (in terms of both performance and computational efficiency), unsupervised Multimodal Language representation scheme that adopts a convolutional autoencoder architecture to discover multimodal relationships between aligned representations of audio, text and visual aligned feature sequences. The core contributions of the proposed method are the following:</p><p>1. To our knowledge, this is the first method in the field of general Multimodal Language Analysis that is both multimodal in all three modalities and unsupervised 2. The proposed architecture is extremely transferable to other domains without negative impact neither on the performance or in the number of model parameters used. External experimentation proves that the performance is just slightly reduced when transferring knowledge from one dataset to another. And this happens without retraining the representation method itself, just using its embeddings in the target domain and classified by a simple logistic regression classifier. 3. The performance is competitive related to the SotA, with a major advantage with regards to the complexity of the proposed method: the proposed multimodal representation model has around 200K parameters. 4. The proposed method can even be used, with similar advantages (ultralight and accurate), to extract compact and lowdimensional unimodal or bimodal representations from a temporal sequence. 5. This representation is openly available through an easy-to-use model (https://github.com/lobracost/mlr). The ultra-light pretrained Multimodal Language Analysis model (1MB of size)</p><p>can be directly applied without retraining on similar aligned datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">CONVOLUTIONAL AUTOENCODER FOR MULTIMODAL SEQUENCES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">2-D multimodal sequence representation</head><p>In this paper we follow a widely used approach for basic unimodal feature extraction and multimodal alignment, similar to the one in various proposed methods, such as <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. More specifically, after extracting the features on each modality(visual, textual and aural), the procedure of word-level alignment that was firstly used for this task in <ref type="bibr" target="#b16">[17]</ref>, is performed. That is, the aligned video and audio features are obtained by computing the expectation of their modality feature values over each word utterance time interval. The feature extraction procedure results in a time sequence of feature vectors for each modality that can be formalized by a N x M matrix, where N is the number of timestamps and M is the number of features. Due to the performed alignment, sequences of all modalities include the same number of timestamps N and thus they can be combined in an N x (M audio +Mvision+Mtext) multimodal matrix X. This merged matrix is the selected initial multimodal representation in our method. The resulted matrix contains sequential information from different modalities that are represented by low level features with distinct arithmetic properties. For that reason, it is difficult for a learning algorithm to learn multimodal relationships and thus a proper normalization is needed in order to map the different vectors of each modality in an universal arithmetic range. In this work, we have selected to apply a sequence of standard and min-max scaling as a normalization procedure of the multimodal matrix X in order to produce the Xn (ie. normalized X) matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Representation Learning</head><p>Xn is a two-dimensional representation of a multimodal sequence. By applying Convolutional Networks with 2-D kernels on Xn, we extract local information that is able to capture both uni-modal and cross-modal dynamics across time. More specifically, the kernels of the first layers mostly model unimodal dynamics that range in neighboring timestamps, while kernels of deeper layers are expected to be able to model cross-modal interactions across a wider range of time. Of course, for each task, there is an optimal balance between short-term unimodal and long-term multimodal modeling that can be found during the training procedure. Based on this formulation of the initial multimodal language sequences, we then select to train unsupervised Convolutional architectures, in order to extract universal representations for this problem. A common choice in the literature is that of Convolutional Autoencoders which have been proven to be effective in image-associated representation learning tasks. More details on the specifics of the Convolutonal AE we trained can be found in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Downstream Classification</head><p>The Encoder part of the trained Convolutional AE can serve as a feature extractor that maps the N x (M audio + Mvision + Mtext) multimodal matrix to a K sized feature vector of the AE code. If the learned embeddings have high representational power, the application of a basic Machine Learning algorithm would be effective on downstream tasks of Multimodal Language Analysis. In this work, we focus on testing this hypothesis so our proposed methodology is illustrated in <ref type="figure">figure 1</ref>, where the Convolutional AE is firstly fitted (in an unsupervised manner) on Multimodal Language datasets, while Logistic Regression is then used to train a model that maps the resulted embeddings to the class labels of a particular downstream task. Obviously, our test and validation sets are not be part of the AE's training procedure. The MOSEI <ref type="bibr" target="#b2">[3]</ref> and IEMOCAP <ref type="bibr" target="#b10">[11]</ref> datasets have been used for representation learning, as well as Sentiment Analysis and Emotion Recognition respectively. The processed version of IEMOCAP consists of 7318 segments of recorded dyadic dialogues annotated for the presence of the human emotions happiness, sadness, anger and neutral, while MOSEI is a large scale sentiment analysis dataset made up of 22,777 movie review video clips from more than 1000 online Youtube speakers. The data and feature extraction, as well as the train, validation and test splits were obtained from the widely used in the literature CMU-MultimodalSDK repository (https://github.com/A2Zadeh/CMU-MultimodalSDK).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental Setup</head><p>After feature extraction was performed (COVAREP <ref type="bibr" target="#b17">[18]</ref> for audio, GloVe <ref type="bibr" target="#b18">[19]</ref> for text and Facet [20] for visual) each segment was represented by a 74-d acoustic, a 35-d visual and 300-d textual feature vector. With the use of the aforementioned word-level alignment and concatenation, a matrix, X, of 20 x 409 dimensions is obtained for each utterance. After performing a sequence of standard and min-max scaling for each of the 409 features across all 20 timestamps and all dataset instances, the 2-dimensional inputs are properly prepossessed.</p><p>In order to train the multimodal representations in an unsupervised manner, we initialized a Convolutional AE with a 4-layer Encoder and its corresponding 4-layer Decoder. The kernel size of the first 2 layers of the Encoder was 3x3, while for the last two layers two 5x5 kernels were used. 2x2 padding along with 1x1 stride and 2x2 max-pooling were used in all layers. The channels of the layers were 32, 64, 128 and 10 respectively. This yields in a (flattened) representation of 250 dimensions in the code part of the AE. For better representation ability we used the Gelu activation function, while batch normalization was included across all layers. We trained the Convolutional AE using the mean-squared error as loss function, while Adam was chosen to be the optimizer with initial learning rate of 0.002 and a reduce-on-plateau learning rate scheduler. We also performed normal weight initialization and early stopping based on the mean-squared error of the validation set. <ref type="figure">Fig. 1</ref>. Word-level alignment is performed among the feature sequence of each modality resulting in a 2-D tensor ("image") for each utterance. After proper normalization, a Convolutional Autoencoder is fitted in order to learn unsupervised representations. A Logistic Regression algorithm then trains a model on the downstream task using the produced embeddings as features. For the unsupervised experimental results, we gathered the train sets of MOSEI and IEMOCAP to form the train set and the corresponding validation sets in order to create a multi-dataset validation set. For different kinds of pre-training as well as the performance of the Convolutional AE in terms of the mean-squared error function, we refer the reader to section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results on Downstream Classification</head><p>Using the Encoder part of the Convolutional AE as a feature extractor, we get a 250-d feature vector for each utterance. This means that the initial 20 x 409 (=8180 feature values) matrix is reduced to a 250-d vector (32 times less elements). These embeddings have been used to train a Logistic Regression model for the two downstream tasks of Sentiment Analysis and Emotion Recognition. Following the existing literature <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>, we report the binary accuracy and weighted averaged f1 metrics on sentiment for MOSEI, and on each of the 4 emotions of IEMOCAP in an one-vsall manner.</p><p>In tables 1 and 2 we compare our results with state-of-the-art and well established architectures in the tasks of Sentiment Analysis and Emotion Recognition. More specifically, we compare to MV-LSTM <ref type="bibr" target="#b5">[6]</ref>, MFN <ref type="bibr" target="#b1">[2]</ref>, Graph-MFN <ref type="bibr" target="#b2">[3]</ref>, RAVEN <ref type="bibr" target="#b14">[15]</ref>, MCTN <ref type="bibr" target="#b11">[12]</ref>, CIA <ref type="bibr" target="#b8">[9]</ref>, MulT <ref type="bibr" target="#b9">[10]</ref>, DF <ref type="bibr" target="#b0">[1]</ref>, BC-LSTM <ref type="bibr" target="#b6">[7]</ref>, MARN <ref type="bibr" target="#b7">[8]</ref>, TFN <ref type="bibr" target="#b4">[5]</ref>, RMFN <ref type="bibr" target="#b3">[4]</ref> and ICCN <ref type="bibr" target="#b12">[13]</ref> which, with the use of the CMU-MultimodalSDK repository, are trained and evaluated on the exact same sets. For each model we reference both the original work and the one that evaluated the method on the MOSEI or IEMOCAP datasets. For detailed explanation and comparison of the aforementioned architectures, we refer the reader to detailed reviews on Multimodal Sentiment Analysis <ref type="bibr" target="#b19">[21]</ref> and Multimodal Emotion Recognition <ref type="bibr" target="#b20">[22]</ref>.</p><p>As seen in 1 and 2 the proposed generic mutlimodal representations achieve competitive performance with the use of a basic Machine Learning algorithm (Logistic Regression). More specifically, our lightweight method achieves an average (calculated on state-of-the art competitive models) performance on sentiment analysis with 2 absolute points above minimum and 4.5 below maximum binary accuracy. It also performs close to average and always above minimum for the task of Emotion Recognition. Therefore, a very simple simple classification algorithm can score the average performance of the SotA models, which clearly indicates that the learned multimodal language embeddings have strong representation power and can be used across different multimodal language tasks, despite the different recording set-up across datasets. The code for all experiments can be found in the mlr repository (https://github.com/lobracost/mlr).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ablation Study</head><p>In this section, we examine the role of the modalities combinations, as well as different datasets used for the unsupervised training of the embeddings. To this end, we trained the Convolutional AE on a range of different modality and training data combinations. For each combination, we report the mean-squared error in table 3. In that way we can gain an insight on the quality of the learnt information compression for each modality combination. It is easily derived that the learnt embeddings of our method are more informative with respect to the original visual modality and less collective for the textual one. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">The role of different modalities</head><p>Using the produced embeddings of each modality combination in order to perform downstream classification, we end up with the results of table 4 for the MOSEI dataset. As it can be clearly seen (i) using all three modalities enriches the representation power, and (ii) the textual modality is the most informative for the task of Multimodal Language Analysis, a fact that is also known in the literature <ref type="bibr" target="#b12">[13]</ref>. Thus our method achieves to effectively express unimodal information and learn multimodal interactions of temporal language sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Cross-domain generalization ability</head><p>In order to examine whether information of one dataset can be used to other tasks, we performed different pretraining to the Convolutional Autoencoder and record the performance on downstream classification for the MOSEI dataset. The two crucial remarks from table 5 that make our work widely useful is that (i) our methodology leads to embeddings that can easily generalize to new data that have been recorded in a different way and for different tasks, since the performance for Sentiment Analysis when using embeddings pretrained on IEMOCAP is 0.5 absolute points (in terms of binary accuracy) below the ones trained in the MOSEI train data and (ii) our embeddings can be enriched from information of different datasets, since the classification using the embeddings trained on MOSEI &amp; IEMO-CAP performs better than the ones trained on just the MOSEI training set. That clearly indicates that the proposed Encoder can serve as feature extractor in a range of Multimodal Language tasks and used for generalization in unseen data formats. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model Complexity</head><p>In <ref type="bibr" target="#b19">[21]</ref> the authors retrained 11 of the most powerful and widely used models for Multimodal Language Analysis and list the number of parameters for some of them. However, this study, due to different pretraining, reports smaller amount of parameters for some models (eg. <ref type="bibr" target="#b21">[23]</ref> reports 1,549,321 parameters for MulT on the MO-SEI dataset with a different pretraining procedure). However, for the sake of a complete comparison, we choose to report the results of <ref type="bibr" target="#b19">[21]</ref>, though this may underestimate the number of parameters of the reported models. In <ref type="table" target="#tab_3">table 6</ref> we report the number of parameters of the proposed methods against the models reported in <ref type="bibr" target="#b19">[21]</ref>. It can be noticed that the Encoder used for feature extraction uses the same number of parameters across both datasets, which is not the case for other architectures where the parameter difference across tasks ranges from 123% to 340% (probably due to the number of classes). That is a direct outcome from the fact that our method does not require extensive fine-tuning techniques in order to be generalized to other tasks. Combining the feature extraction (256,202 parameters for both tasks) with the inference part (251 parameters for MOSEI and 1004 for IEMOCAP) we end up with an end-toend method that includes parameters in the range 256,202-257,206 while the rest architectures are highly greedy in terms of parameter amounts since they range from 415,521 to <ref type="bibr" target="#b21">23,</ref><ref type="bibr">198,</ref><ref type="bibr">398</ref>. Thus the CAE-LR is a lightweight method that can be easily generalized to other tasks, without impact in the number of parameters (shown in <ref type="table" target="#tab_3">Table 6</ref>), nor significant performance loss (shown in section 3.3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>In this work, we have presented a method for extracting unsupervised Multimodal Language representations using 2-D aligned multimodal sequences and Convolutional Autoencoders, in a totally unsupervised learning process. Extensive experimentation in Sentiment Analysis and Emotion Recognition prove that the performance of the proposed method is competitive related to the SotA, though the respective representation model is much smaller (around 200K parameters). Apart from being extremely lightweight, the proposed architecture is transferable to other domains without negative impact on the performance: experiments prove that transferring knowledge from one dataset to another, without retraining -not even tuning-the representation model itself (just training a logistic regression classifier on the extracted embeddings), does not significantly affect the classification performance in the target domain. Finally, the proposed representation method is openly available through an easy-touse model (https://github.com/lobracost/mlr) which can be directly applied to any similar Multimodal Language Modeling downstream task. The proposed approach could be further enriched in a future work, by training multimodal embeddings on a greater amount of datasets and reporting performance on more Multimodal Language tasks, such as Persuasiveness Prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of binary accuracy and f1 weighted average metrics against literature models for sentiment classification on MO-SEI. With ? and ? we denote the distance from the minimum and the maximum performance respectively</figDesc><table><row><cell>Model (Reference, Evaluation)</cell><cell>Acc2</cell><cell>F1-weighted</cell></row><row><cell>MV-LSTM ([6], [3])</cell><cell>76.4</cell><cell>76.4</cell></row><row><cell>MFN ([2], [3])</cell><cell>76</cell><cell>76</cell></row><row><cell>Graph-MFN ([3], [10])</cell><cell>76.9</cell><cell>77.0</cell></row><row><cell>RAVEN ([15], [10])</cell><cell>79.1</cell><cell>79.5</cell></row><row><cell>MCTN ([12], [10])</cell><cell>79.8</cell><cell>80.6</cell></row><row><cell>CIA ([9], [9])</cell><cell>80.4</cell><cell>78.2</cell></row><row><cell>MulT ([10], [10])</cell><cell>82.5</cell><cell>82.3</cell></row><row><cell>Average</cell><cell>77.6</cell><cell>76.9</cell></row><row><cell>CAE-LR (Ours)</cell><cell>78 (2? 4.5?)</cell><cell>76.3 (0.3? 4.3?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of binary accuracy and f1 weighted average metrics for each emotion against literature models for Emotion Recognition on IEMOCAP. With ? and ? we denote the distance from the minimum and the maximum performance respectively</figDesc><table><row><cell></cell><cell></cell><cell>Happy</cell><cell>Sad</cell><cell></cell><cell>Angry</cell><cell></cell><cell>Neutral</cell><cell></cell></row><row><cell>Model (Reference, Evaluation)</cell><cell>Acc2</cell><cell>F1-weighted</cell><cell>Acc2</cell><cell>F1-weighted</cell><cell>Acc2</cell><cell>F1-weighted</cell><cell>Acc2</cell><cell>F1-weighted</cell></row><row><cell>DF ([1], [15])</cell><cell>86</cell><cell>81</cell><cell>81.8</cell><cell>81.2</cell><cell>75.8</cell><cell>65.4</cell><cell>59.1</cell><cell>44</cell></row><row><cell>MV-LSTM ([6], [15])</cell><cell>85.9</cell><cell>81.3</cell><cell>80.4</cell><cell>74</cell><cell>85.1</cell><cell>84.3</cell><cell>67</cell><cell>66.7</cell></row><row><cell>BC-LSTM ([7], [15])</cell><cell>84.9</cell><cell>81.7</cell><cell>83.2</cell><cell>81.7</cell><cell>83.5</cell><cell>84.2</cell><cell>67.5</cell><cell>64.1</cell></row><row><cell>MARN ([8], [15])</cell><cell>86.7</cell><cell>83.6</cell><cell>82</cell><cell>81.2</cell><cell>84.6</cell><cell>84.2</cell><cell>66.8</cell><cell>65.9</cell></row><row><cell>TFN ([5], [5])</cell><cell>-</cell><cell>83.6</cell><cell>-</cell><cell>82.8</cell><cell>-</cell><cell>84.2</cell><cell></cell><cell>65.4</cell></row><row><cell>RMFN ([4], [15])</cell><cell>87.5</cell><cell>85.8</cell><cell>83.8</cell><cell>82.9</cell><cell>85.1</cell><cell>84.6</cell><cell>69.5</cell><cell>69.1</cell></row><row><cell>MFN ([2], [15])</cell><cell>90.2</cell><cell>85.8</cell><cell>88.4</cell><cell>86.1</cell><cell>87.5</cell><cell>86.7</cell><cell>72.1</cell><cell>68.1</cell></row><row><cell>MCTN ([12], [10])</cell><cell>84.9</cell><cell>83.1</cell><cell>80.5</cell><cell>79.6</cell><cell>79.7</cell><cell>80.4</cell><cell>62.3</cell><cell>57</cell></row><row><cell>MulT ([10], [10])</cell><cell>90.7</cell><cell>88.6</cell><cell>86.7</cell><cell>86</cell><cell>87.4</cell><cell>87</cell><cell>72.4</cell><cell>70.7</cell></row><row><cell>ICCN ([13], [13])</cell><cell>87.4</cell><cell>84.7</cell><cell>88.6</cell><cell>88</cell><cell>86.3</cell><cell>85.9</cell><cell>69.7</cell><cell>68.5</cell></row><row><cell>Average</cell><cell>87.13</cell><cell>83.92</cell><cell>83.93</cell><cell>82.35</cell><cell>83.89</cell><cell>82.69</cell><cell>67.38</cell><cell>63.95</cell></row><row><cell>CAE-LR (Ours)</cell><cell>85.8 (0.9? 4.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>9?) 81.60 (0.6? 7?) 82.3 (3.4? 6.3?) 82.13 (8.13? 5.87?) 85.6 (9.8? 1.9?) 85.5 (20.1? 1.5?) 63.85 (4.75? 8.55?) 60.91 (16.91? 9.79?)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Convolutional AE pretraining for different modalities and dataset combinations</figDesc><table><row><cell>Modalities</cell><cell cols="3">Embeddings Training</cell><cell>MSE ( x 10 ?4 )</cell></row><row><cell>Audio</cell><cell cols="3">MOSEI &amp; IEMOCAP</cell><cell>32.34</cell></row><row><cell>Vision</cell><cell cols="3">MOSEI &amp; IEMOCAP</cell><cell>22.67</cell></row><row><cell>Text</cell><cell cols="3">MOSEI &amp; IEMOCAP</cell><cell>44.22</cell></row><row><cell>[Audio, Vision]</cell><cell cols="3">MOSEI &amp; IEMOCAP</cell><cell>30.21</cell></row><row><cell>[Vision, Text]</cell><cell cols="3">MOSEI &amp; IEMOCAP</cell><cell>24.34</cell></row><row><cell>[Audio, Text]</cell><cell cols="3">MOSEI &amp; IEMOCAP</cell><cell>29.94</cell></row><row><cell>[Audio, Vision, Text]</cell><cell cols="3">MOSEI &amp; IEMOCAP</cell><cell>24.11</cell></row><row><cell>[Audio, Vision, Text]</cell><cell></cell><cell cols="2">MOSEI</cell><cell>23.12</cell></row><row><cell>[Audio, Vision, Text]</cell><cell cols="3">IEMOCAP</cell><cell>20.58</cell></row><row><cell cols="4">Table 4. Performance on MOSEI sentiment classification using em-</cell></row><row><cell cols="4">beddings trained on different modality combinations</cell></row><row><cell>Modalities</cell><cell></cell><cell></cell><cell>Acc2</cell><cell>F1-weighted</cell></row><row><cell>Audio</cell><cell></cell><cell cols="2">71.06</cell><cell>59.1</cell></row><row><cell>Visual</cell><cell></cell><cell cols="2">71.06</cell><cell>59.1</cell></row><row><cell>Text</cell><cell></cell><cell></cell><cell>75.4</cell><cell>72.63</cell></row><row><cell cols="2">[Audio, Visual]</cell><cell cols="2">71.06</cell><cell>59.87</cell></row><row><cell cols="2">[Visual, Text]</cell><cell cols="2">71.04</cell><cell>59.01</cell></row><row><cell cols="2">[Audio, Text]</cell><cell cols="2">77.86</cell><cell>76.09</cell></row><row><cell cols="2">[Audio, Visual, Text]</cell><cell></cell><cell>78</cell><cell>76.3</cell></row><row><cell cols="4">Table 5. Performance on MOSEI sentiment classification using em-</cell></row><row><cell cols="4">beddings trained on different dataset combinations</cell></row><row><cell cols="3">Pretrained representations</cell><cell>Acc2</cell><cell>F1-weighted</cell></row><row><cell>MOSEI</cell><cell></cell><cell></cell><cell>77.2</cell><cell>75.4</cell></row><row><cell>IEMOCAP</cell><cell></cell><cell></cell><cell>76.7</cell><cell>74.5</cell></row><row><cell cols="2">MOSEI &amp; IEMOCAP</cell><cell></cell><cell>78</cell><cell>76.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Comparison of model parameters</figDesc><table><row><cell></cell><cell cols="2">Number of Parameters</cell></row><row><cell>Model</cell><cell>MOSEI</cell><cell>IEMOCAP</cell></row><row><cell>TFN</cell><cell>6,804,859</cell><cell>23,198,398</cell></row><row><cell>RMFN</cell><cell>-</cell><cell>1,732,884</cell></row><row><cell>MFN</cell><cell>415,521</cell><cell>1,325,508</cell></row><row><cell>MulT</cell><cell>874,651</cell><cell>1,074,998</cell></row><row><cell>CAE-LR (ours)</cell><cell>256,453</cell><cell>257,206</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion for persuasiveness prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnaz</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayanth</forename><surname>Koushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM International Conference on Multimodal Interaction</title>
		<meeting>the 18th ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="284" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Memory fusion network for multi-view sequential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal language analysis with recurrent multistage fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyin</forename><surname>Paul Pu Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="150" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extending long short-term memory for multi-view structured learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Shyam Sundar Rajagopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goecke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="338" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the association for computational linguistics</title>
		<meeting>the 55th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-attention recurrent network for human communication comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Vij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Context-aware interactive attention for multi-modal sentiment and emotion analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Singh Chauhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><forename type="middle">Shad</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11" />
			<biblScope unit="page" from="5647" to="5657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference</title>
		<meeting>the conference</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics. Meeting. NIH Public Access</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page">6558</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikanth S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language resources and evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Found in translation: Learning robust joint representations by cyclic translations between modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnab?s</forename><surname>P?czos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6892" to="6899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathusha</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="8992" to="8999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A transformer-based joint-encoding for emotion recognition and sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Benoit</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No?</forename><surname>Tits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Brousmiche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Dupont</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15955</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Words can shift: Dynamically adjusting word representations using nonverbal behaviors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7216" to="7223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali</forename><surname>Bagher Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2247" to="2256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning factorized multimodal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Representation Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Covarep-a collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 ieee international conference on acoustics, speech and signal processing (icassp)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="960" to="964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing</title>
		<meeting>the 2014 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">What makes the difference? an empirical comparison of fusion strategies for multimodal language analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Gkoumas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="184" to="197" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep multimodal emotion recognition on human speech: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panagiotis</forename><surname>Koromilas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Giannakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">17</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Analyzing multimodal sentiment via acoustic-and visual-lstm with channelaware temporal convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1424" to="1437" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
