<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DualPoseNet: Category-level 6D Object Pose and Size Estimation Using Dual Pose Network with Refined Learning of Pose Consistency</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehong</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zewei</forename><surname>Wei</surname></persName>
							<email>eeweizewei@mail.scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">DexForce Technology Co. Ltd. 3 Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Huawei Technologies Co. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songcen</forename><surname>Xu</surname></persName>
							<email>xusongcen@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<email>kuijia@scut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DualPoseNet: Category-level 6D Object Pose and Size Estimation Using Dual Pose Network with Refined Learning of Pose Consistency</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Category-level 6D object pose and size estimation is to predict full pose configurations of rotation, translation, and size for object instances observed in single, arbitrary views of cluttered scenes. In this paper, we propose a new method of Dual Pose Network with refined learning of pose consistency for this task, shortened as DualPoseNet. DualPoseNet stacks two parallel pose decoders on top of a shared pose encoder, where the implicit decoder predicts object poses with a working mechanism different from that of the explicit one; they thus impose complementary supervision on the training of pose encoder. We construct the encoder based on spherical convolutions, and design a module of Spherical Fusion wherein for a better embedding of posesensitive features from the appearance and shape observations. Given no testing CAD models, it is the novel introduction of the implicit decoder that enables the refined pose prediction during testing, by enforcing the predicted pose consistency between the two decoders using a self-adaptive loss term. Thorough experiments on benchmarks of both category-and instance-level object pose datasets confirm efficacy of our designs. DualPoseNet outperforms existing methods with a large margin in the regime of high precision. Our code is released publicly at https://github. com/Gorilla-Lab-SCUT/DualPoseNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection in a 3D Euclidean space is demanded in many practical applications, such as augmented reality, robotic manipulation, and self-driving car. The field has been developing rapidly with the availability of benchmark datasets (e.g., KITTI <ref type="bibr" target="#b9">[10]</ref> and SUN RGB-D <ref type="bibr" target="#b24">[25]</ref>), where carefully annotated 3D bounding boxes enclosing object instances of interest are prepared, which specify 7 degrees of * Corresponding author freedom (7DoF) for the objects, including translation, size, and yaw angle around the gravity axis. This 7DoF setting of 3D object detection is aligned with common scenarios where the majority of object instances stand upright in the 3D space. However, 7DoF detection cannot precisely locate objects when the objects lean in the 3D space, where the most compact bounding boxes can only be determined given full pose configurations, i.e., with the additional two angles of rotation. Pose predictions of full configurations are important in safety-critical scenarios, e.g., autonomous driving, where the most precise and compact localization of objects enables better perception and decision making.</p><p>This task of pose prediction of full configurations (i.e., 6D pose and size) is formally introduced in <ref type="bibr" target="#b29">[30]</ref> as category-level 6D object pose and size estimation of novel instances from single, arbitrary views of RGB-D observations. It is closely related to category-level amodal 3D object detection <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref> (i.e., the above 7DoF setting) and instance-level 6D object pose estimation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>. Compared with them, the focused task in the present paper is more challenging due to learning and prediction in the full rotation space of SO(3); more specifically, (1) the task is more involved in terms of both defining the category-level canonical poses (cf. Section 3 for definition of canonical poses) and aligning object instances with large intra-category shape variations <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b3">4]</ref>, <ref type="bibr" target="#b1">(2)</ref> deep learning precise rotations arguably requires learning rotation-equivariant shape features, which is less studied compared with the 2D counterpart of learning translation-invariant image features, and (3) compared with instance-level 6D pose estimation, due to the lack of testing CAD models, the focused task cannot leverage the privileged 3D shapes to directly refine pose predictions, as done in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In this work, we propose a novel method for categorylevel 6D object pose and size estimation, which can partially address the second and third challenges mentioned above.</p><p>Our method constructs two parallel pose decoders on top of a shared pose encoder; the two decoders predict poses with different working mechanisms, and the encoder is designed to learn pose-sensitive shape features. A refined learning that enforces the predicted pose consistency between the two decoders is activated during testing to further improve the prediction. We term our method as Dual Pose Network with refined learning of pose consistency, shortened as Du-alPoseNet. <ref type="figure">Fig. 1</ref> gives an illustration.</p><p>For an observed RGB-D scene, DualPoseNet first employs an off-the-shelf model of instance segmentation (e.g., MaskRCNN <ref type="bibr" target="#b10">[11]</ref>) in images to segment out the objects of interest. It then feeds each masked RGB-D region into the encoder. To learn pose-sensitive shape features, we construct our encoder based on spherical convolutions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref>, which provably learn deep features of object surface shapes with the property of rotation equivariance on SO(3). In this work, we design a novel module of Spherical Fusion to support a better embedding from the appearance and shape features of the input RGB-D region. With the learned posesensitive features, the two parallel decoders either make a pose prediction explicitly, or implicitly do so by reconstructing the input (partial) point cloud in its canonical pose; while the first pose prediction can be directly used as the result of DualPoseNet, the result is further refined during testing by fine-tuning the encoder using a self-adaptive loss term that enforces the pose consistency. The use of implicit decoder in DualPoseNet has two benefits that potentially improve the pose prediction: (1) it provides an auxiliary supervision on the training of pose encoder, and <ref type="bibr" target="#b1">(2)</ref> it is the key to enable the refinement given no testing CAD models. We conduct thorough experiments on the benchmark category-level object pose datasets of CAMERA25 and REAL275 <ref type="bibr" target="#b29">[30]</ref>, and also apply our DualPoseNet to the instance-level ones of YCB-Video <ref type="bibr" target="#b2">[3]</ref> and LineMOD <ref type="bibr" target="#b12">[13]</ref>. Ablation studies confirm the efficacy of our novel designs. DualPoseNet outperforms existing methods in terms of more precise pose. Our technical contributions are summarized as follows:</p><p>? We propose a new method of Dual Pose Network for category-level 6D object pose and size estimation. Du-alPoseNet stacks two parallel pose decoders on top of a shared pose encoder, where the implicit one predicts poses with a working mechanism different from that of the explicit one; the two decoders thus impose complementary supervision on training of the pose encoder.</p><p>? In spite of the lack of testing CAD models, the use of implicit decoder in DualPoseNet enables a refined pose prediction during testing, by enforcing the predicted pose consistency between the two decoders using a self-adaptive loss term. This further improves the results of DualPoseNet.</p><p>? We construct the encoder of DualPoseNet based on spherical convolutions to learn pose-sensitive shape features, and design a module of Spherical Fusion wherein, which is empirically shown to learn a better embedding from the appearance and shape features from the input RGB-D regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Instance-level 6D Object Pose Estimation Traditional methods for instance-level 6D pose estimation include those based on template matching <ref type="bibr" target="#b11">[12]</ref>, and those by voting the matching results of point-pair features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. More recent solutions build on the power of deep networks and can directly estimate object poses from RGB images alone <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b19">20]</ref> or RGB-D ones <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>. This task assumes the availability of object CAD models during both the training and test phases, and thus enables a common practice to refine the predicted pose by matching the CAD model with the (RGB and/or point cloud) observations <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18]</ref>. Category-level 3D Object Detection Methods for category-level 3D object detection are mainly compared on benchmarks such as KITTI <ref type="bibr" target="#b9">[10]</ref> and SUN RGB-D <ref type="bibr" target="#b24">[25]</ref>. Earlier approach <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b30">31]</ref> leverages the mature 2D detectors to first detect objects in RGB images, and learning of 3D detection is facilitated by focusing on point sets inside object frustums. Subsequent research proposes solutions <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref> to predict the 7DoF object bounding boxes directly from the observed scene points. However, the 7DoF configurations impose inherent constrains on the precise rotation prediction, with only one yaw angle predicted around the gravity direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category-level 6D Object Pose and Size Estimation</head><p>More recently, category-level 6D pose and size estimation is formally introduced in <ref type="bibr" target="#b29">[30]</ref>. Notably, Wang et al. <ref type="bibr" target="#b29">[30]</ref> propose a canonical shape representation called normalized object coordinate space (NOCS), and inference is made by first predicting NOCS maps for objects detected in RGB images, and then aligning them with the observed object depths to produce results of 6D pose and size; later, Tian et al. <ref type="bibr" target="#b26">[27]</ref> improve the predictions of canonical object models by deforming categorical shape priors. Instead, Chen et al. <ref type="bibr" target="#b4">[5]</ref> trained a variational auto-encoder (VAE) to capture pose-independent features, along with pose-dependent ones to directly predict the 6D poses. Besides, monocular methods are also explored in recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement</head><p>Studies on category-level 6D object pose and size estimation start from NOCS <ref type="bibr" target="#b29">[30]</ref>. The problem can be formally stated as follows. Assume a training set of cluttered scenes captured in RGB-D images, where ground-truth annotations of 6D pose and size for object instances of certain  <ref type="figure">Figure 1</ref>. An illustration of our proposed DualPoseNet. For an observed RGB-D scene, DualPoseNet employs MaskRCNN <ref type="bibr" target="#b26">[27]</ref> to segment out the object of interest, e.g., a mug, giving the observed points in P and the corresponding RGB values in X , and feeds (X , P) into a Pose Encoder ? to learn a pose-sensitive feature representation f . Specifically, ? is designed to have two parallel streams of spherical convolution layers to process the spherical signals S X and S P separately, which are respectively converted from X and P; the resulting features are intertwined in the intermediate layers via a proposed module of Spherical Fusion; finally f is enriched and obtained by aggregation of multi-scale spherical features. On top of ?, an Explicit Pose Decoder ?exp is constructed to directly predict the pose, while an additional Implicit Pose Decoder ?im is employed in parallel with ?exp to generate a canonical version Q of P.</p><p>categories are provided. For each of the contained object instances, the annotation is in the form of full pose configuration of rotation R ? SO(3), translation t ? R 3 , and size s ? R 3 , which can also be translated as a compact, oriented 3D bounding box enclosing the object (cf. <ref type="figure">Fig. 1</ref>). Note that in a 3D Euclidean space, the 6D pose of R and t is defined relatively with respect to a canonical pose centered at the origin. Category-level learning thus relies on the underlying assumption that all training object instances of a same category are aligned at a pre-defined canonical pose (e.g., handles of instances of a mug category are all pointing towards a same direction); otherwise it makes no sense for any learned models to predict poses during testing. For existing datasets <ref type="bibr" target="#b29">[30]</ref>, additional annotations of object masks in RGB images are usually provided, which eases the problem and enables learning to segment out regions of interest from RGB-D images of cluttered scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Proposed Dual Pose Network with Refined Learning of Pose Consistency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>We first present an overview of our proposed Dual Pose Network with refined Learning of Pose Consistency. The whole pipeline is depicted in <ref type="figure">Fig. 1</ref>. For an observed RGB-D scene, DualPoseNet first employs an off-the-shelf model of instance segmentation in images (e.g., MaskR-CNN <ref type="bibr" target="#b10">[11]</ref>) to segment out the objects of interest. This produces a pair (X , P) for each segmented object, where we</p><formula xml:id="formula_0">use P = {p i ? R 3 } N i=1 to represent the N observed points in the masked RGB-D region and X = {x i ? R 3 } N i=1</formula><p>for the corresponding RGB values. DualPoseNet feeds (X , P) into a Pose Encoder ? (cf. Section 4.2) to learn a pose-sensitive feature representation f , followed by an Explicit Pose Decoder ? exp (cf. Section 4.3) to predict the pose; an additional Implicit Pose Decoder ? im (cf. Section 4.4) is employed in parallel with ? exp , which generates a canonical version Q of the observed point cloud P. The use of ? im is the key in DualPoseNet to both improve the pose prediction from ? exp , and enable a refined learning of pose consistency that further improves the precision of prediction, as verified in our experiments in Section 5.1.1. Given cropped RGB-D regions of interest and the ground-truth pose annotations, training of DualPoseNet can be conducted in an end-to-end manner. During testing, there exist (at least) three ways to obtain the pose predictions from DualPoseNet: (1) the direct prediction from DualPoseNet via a forward pass of ? exp ? ?, (2) comput-ing Q = ? im ? ?(X , P) in its canonical pose and obtaining a pose prediction by solving Umeyama algorithm <ref type="bibr" target="#b27">[28]</ref> together with the observed P, similar to existing methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>, and (3) using the refined learning to update the parameters of the encoder ?, and then computing the prediction via a forward pass of ? exp ? ?. In this work, we use the first and third ways to obtain results of DualPoseNet. We illustrate the training and refinement processes in <ref type="figure" target="#fig_1">Fig.  2</ref>. Individual components of the network are explained as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">The Pose Encoder ?</head><p>Precise prediction of object pose requires that the features learned by f = ?(X , P) are sensitive to the observed pose of the input P, especially to rotation, since translation and size are easier to infer from P (e.g., even simple localization of center point and calculation of 3D extensions give a good prediction of translation and scale). To this end, we implement our ? based on spherical convolutions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref>, which provably learn deep features of object surface shapes with the property of rotation equivariance on SO(3). More specifically, we design ? to have two parallel streams of spherical convolution layers that process the inputs X and P separately; the resulting features are intertwined in the intermediate layers via a proposed module of Spherical Fusion. We also use aggregation of multi-scale spherical features to enrich the pose information in f . <ref type="figure">Fig. 1</ref> gives the illustration.</p><p>Conversion as Spherical Signals Following <ref type="bibr" target="#b8">[9]</ref>, we aim to convert X and P separately as discrete samplings S X ? R W ?H?3 and S P ? R W ?H?1 of spherical signals, where W ? H represents the sampling resolution on the sphere. To do so, we first compute the geometric center c = 1 N N i=1 p i of P, and subtract its individual points from c; this moves P into a space with the origin at c. We then cast W ? H equiangular rays from c, which divide the space into W ? H regions. Consider a region indexed by (w, h), with w ? {1, . . . , W } and h ? {1, . . . , H}; when it contains points of P, we find the one with the largest distance to c, denoted as p max h,w , and define the spherical signal at the present region as S X (w, h) = x max h,w and S P (w, h) = p max h,w ? c , where x max h,w denotes the RGB values corresponding to p max h,w ; otherwise, we define S X (w, h) = 0 and S P (w, h) = 0 when the region contains no points of P.</p><p>Learning with Spherical Fusion As shown in <ref type="figure">Fig. 1</ref>, our encoder ? is constructed based on two parallel streams that process the converted spherical signals S X and S P separately. We term them as X -stream and P-stream for ease of presentation. The two streams share a same network structure (except the channels of the first layers), each of which stacks multiple layers of spherical convolution and weighted average pooling, whose specifics are given in <ref type="figure">Fig.  1</ref>. To enable information communication and feature mixing between the two streams, we design a module of Spherical Fusion that works as follows. Let S X l ? R W ?H?d l and S P l ? R W ?H?d l denote the learned spherical feature maps at the respective l th layers of the two streams (i.e., S X 0 = S X and S P 0 = S P ), we compute the input feature maps of layer l + 1 for P-stream as</p><formula xml:id="formula_1">S P l = S P l , S X ,P l ? R W ?H?2d l (1) with S X ,P l = SCONV S X l , S P l ? R W ?H?d l ,<label>(2)</label></formula><p>where SCONV denotes a trainable layer of spherical convolution <ref type="bibr" target="#b8">[9]</ref>, and [?, ?] concatenates spherical maps along the feature dimension. The same applies to X -stream, and we have S X l ? R W ?H?2d l as its input of layer l + 1. The module of spherical fusion (1) can be used in a plug-and-play manner at any intermediate layers of the two streams; we use three such modules between X -stream and P-stream of 5 spherical convolution layers for all experiments reported in this paper. Empirical analysis in Section 5.1.1 verifies the efficacy of the proposed spherical fusion. Note that a simple alternative exists that fuses the RGB and point features at the very beginning, i.e., a spherical signal S = [S X , S P ] ? R W ?H?4 converted from (X , P). Features in S would be directly fused in subsequent layers. Empirical results in Section 5.1.1 also verify that given the same numbers of spherical convolution layers and feature maps, this alternative is greatly outperformed by our proposed spherical fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregation of Multi-scale Spherical Features</head><p>It is intuitive to enhance pose encoding by using spherical features at multiple scales. Since the representation S X ,P l computed by (2) fuses the appearance and geometry features at an intermediate layer l, we technically aggregate multiple of them from spherical fusion modules respectively inserted at lower, middle, and higher layers of the two parallel streams, as illustrated in <ref type="figure">Fig. 1</ref>. In practice, we aggregate three of such feature representations as follows</p><formula xml:id="formula_2">f = MLP (MaxPool (f l , f l , f l )) (3) s.t. f l = MLP Flatten S X ,P l ,</formula><p>where Flatten(?) denotes a flattening operation that reforms the feature tensor S X ,P l of dimension W ? H ? d l as a feature vector, MLP denotes a subnetwork of Multi-Layer Perceptron (MLP), and MaxPool (f l , f l , f l ) aggregates the three feature vectors by max-pooling over three entries for each feature channel; layer specifics of the two MLPs used in (3) are given in <ref type="figure">Fig. 1</ref>. We use f computed from <ref type="formula">(3)</ref> as the final output of the pose encoder ?, i.e., f = ?(X , P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Explicit Pose Decoder ? exp</head><p>Given f from the encoder ?, we implement the explicit decoder ? exp simply as three parallel MLPs that are trained to directly regress the rotation R, translation t, and size s. <ref type="figure">Fig. 1</ref> gives the illustration, where layer specifics of the three MLPs are also given. This gives a direct way of pose prediction from a cropped RGB-D region as (R, t, s) = ? exp ? ?(X , P).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Implicit Pose Decoder ? im</head><p>For the observed point cloud P, assume that its counterpart Q in the canonical pose is available. An affine transformation (R, t, s) between P and Q can be established, which computes q = 1 ||s|| R T (p ? t) for any corresponding pair of p ? P and q ? Q. This implies an implicit way of obtaining predicted pose by learning to predict a canonical Q from the observed P; upon prediction of Q, the pose (R, t, s) can be obtained by solving the alignment problem via Umeyama algorithm <ref type="bibr" target="#b27">[28]</ref>. Since f = ?(X , P) has learned the pose-sensitive features, we expect the corresponding q can be estimated from p by learning a mapping from the concatenation of f and p. In DualPoseNet, we simply implement the learnable mapping as</p><formula xml:id="formula_3">? im (p, f ) = MLP ([p; f ]) .</formula><p>(4) ? im applies to individual points of P in a point-wise manner. We write collectively as Q = ? im (P, f ).</p><p>We note that an equivalent representation of normalized object coordinate space (NOCS) is learned in <ref type="bibr" target="#b29">[30]</ref> for a subsequent computation of pose prediction. Different from NOCS, we use ? im in an implicit way; it has two benefits that potentially improve the pose prediction: (1) it provides an auxiliary supervision on the training of pose encoder ? (note that the training ground truth of Q can be transformed from P using the annotated pose and size), and (2) it enables a refined pose prediction by enforcing the consistency between the outputs of ? exp and ? im , as explained shortly in Section 4.6. We empirically verify both the benefits in Section 5.1.1, and show that the use of ? im improves pose predictions of ? exp ? ?(X , P) in DualPoseNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training of Dual Pose Network</head><p>Given the ground-truth pose annotation (R * , t * , s * ) 1 for a cropped (X , P), we use the following training objective on top of the explicit decoder ? exp :</p><formula xml:id="formula_4">L ?,?exp = ||?(R)??(R * )|| 2 +||t?t * || 2 +||s?s * || 2 ,<label>(5)</label></formula><p>where ?(R) is the quaternion representation of rotation R.</p><p>Since individual points in the predicted</p><formula xml:id="formula_5">Q = {q i } N i=1</formula><p>from ? im are respectively corresponded to those in the observed P = {p i } N i=1 , we simply use the following loss on <ref type="bibr" target="#b0">1</ref> Following <ref type="bibr" target="#b26">[27]</ref>, we use canonical R * for symmetic objects to handle ambiguities of symmetry.  top of the implicit decoder</p><formula xml:id="formula_6">L ?,?im = 1 N N i=1 q i ? 1 ||s * || R * (p i ? t * ) 2 . (6)</formula><p>The overall training objective combines <ref type="bibr" target="#b4">(5)</ref> and <ref type="formula">(6)</ref>, resulting in the optimization problem min ?,?exp,?im</p><formula xml:id="formula_7">L ?,?exp + ?L ?,?im ,<label>(7)</label></formula><p>where ? is a penalty parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">The Refined Learning of Pose Consistency</head><p>For instance-level 6D pose estimation, it is a common practice to refine an initial or predicted pose by a postregistration <ref type="bibr" target="#b1">[2]</ref> or post-optimization <ref type="bibr" target="#b17">[18]</ref>; such a practice is possible since CAD model of the instance is available, which can guide the refinement by matching the CAD model with the (RGB and/or point cloud) observations. For our focused category-level problem, however, CAD models of testing instances are not provided. This creates a challenge in case that more precise predictions on certain testing instances are demanded.</p><p>Thanks to the dual pose predictions from ? exp and ? im , we are able to make a pose refinement by learning to enforce their pose consistency. More specifically, we freeze the parameters of ? exp and ? im , while fine-tuning those of the encoder ?, by optimizing the following problem <ref type="figure">P)</ref> and (R, t, s) = ? exp ? ?(X , P) are the outputs of the two decoders. Note that during training, the two decoders are consistent in terms of pose prediction, since both of them are trained to match their outputs with the ground truths. During testing, due to an inevitable generalization gap, inconsistency between outputs of the two decoders always exists, and our proposed refinement <ref type="bibr" target="#b7">(8)</ref> is expected to close the gap. An improved prediction relies on a better pose-sensitive encoding f = ?(X , P); the refinement (8) thus updates parameters of ? to achieve the goal. Empirical results in Section 5.1.1 verify that the refined poses are indeed towards more precise ones. In practice, we set a loss tolerance as the stopping criterion when fine-tuning L Ref ine </p><formula xml:id="formula_8">min ? L Ref ine ? = 1 N N i=1 q i ? 1 ||s|| R (p i ? t) 2 ,<label>(8)</label></formula><formula xml:id="formula_9">where Q = {q i } N i=1 = ? im ? ?(X ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Datasets We conduct experiments using the benchmark CAMERA25 and REAL275 datasets <ref type="bibr" target="#b29">[30]</ref> for category-level 6D object pose and size estimation. CAMERA25 is a synthetic dataset generated by a context-aware mixed reality approach from 6 object categories; it includes 300, 000 composite images of 1, 085 object instances, among which 25, 000 images of 184 instances are used for evaluation. REAL275 is a more challenging real-world dataset captured with clutter, occlusion and various lighting conditions; its training set contains 4, 300 images of 7 scenes, and the test set contains 2, 750 images of 6 scenes. Note that CAM-ERA25 and REAL275 share the same object categories, which enables a combined use of the two datasets for model training, as done in <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>We also evaluate the advantages of DualPoseNet on the benchmark instance-level object pose datasets of YCB-Video <ref type="bibr" target="#b2">[3]</ref> and LineMOD <ref type="bibr" target="#b12">[13]</ref>, which consist of 21 and 13 different object instances respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We employ a MaskRCNN <ref type="bibr" target="#b10">[11]</ref> implemented by <ref type="bibr" target="#b0">[1]</ref> to segment out the objects of interest from input scenes. For each segmented object, its RGB-D crop is converted as spherical signals with a sampling resolution 64 ? 64, and is then fed into our DualPoseNet. Configurations of DualPoseNet, including channel numbers of spherical convolutions and MLPs, have been specified in <ref type="figure">Fig. 1</ref>. We use ADAM to train DualPoseNet, with an initial learning rate of 0.0001. The learning rate is halved every 50, 000 iterations until a total number of 300, 000 ones. We set the batch size as 64, and the penalty parameter in Eq. <ref type="bibr" target="#b6">(7)</ref> as ? = 10. For refined learning of pose consistency, we use a learning rate 1 ? 10 ?6 and a loss tolerance = 5 ? 10 ?5 . For the instance-level task, we additionally adopt a similar 2nd-stage iterative refinement of residual pose as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref> did; more details are shown in the supplementary material.</p><p>Evaluation Metrics For category-level pose estimation, we follow <ref type="bibr" target="#b29">[30]</ref> to report mean Average Precision (mAP) at different thresholds of intersection over union (IoU) for object detection, and mAP at n ? m cm for pose estimation. However, those metrics are not precise enough to simultaneously evaluate 6D pose and object size estimation, since IoU alone may fail to characterize precise object poses (a rotated bounding box may give a similar IoU value). To evaluate the problem nature of simultaneous predictions of pose and size, in this work, we also propose a new and more strict metric based on a combination of IoU, error of rotation, and error of relative translation, where for the last one, we use the relative version since absolute translations make less sense for objects of varying sizes. For the three errors, we consider respective thresholds of {50%, 75%} (i.e., IoU 50 and IoU 75 ), {5 ? , 10 ? }, and {5%, 10%, 20%}, whose combinations can evaluate the predictions across a range of precisions. For instance-level pose estimation, we follow <ref type="bibr" target="#b28">[29]</ref> and evaluate the results of YCB-Video and LineMOD datasets by ADD-S and ADD(S) metrics, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Category-level 6D Pose and Size Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Ablation Studies and Analyses</head><p>We first conduct ablation studies to evaluate the efficacy of individual components proposed in DualPoseNet. These studies are conducted on the REAL275 dataset <ref type="bibr" target="#b29">[30]</ref>.</p><p>We use both ? exp and ? im for pose decoding from DualPoseNet; ? exp produces the pose predictions directly, which are also used as the results of DualPoseNet both with and without the refined learning, while ? im is an implicit one whose outputs can translate as the results by solving an alignment problem. To verify the usefulness of ? im , we report the results of DualPoseNet with or without the use of ? im in <ref type="table">Table 1</ref>, in terms of the pose precision from ? exp before the refined learning. We observe that the use of ? im improves the performance of ? exp by large margins under all the metrics; for example, the mAP improvement of (IoU 50 , 10 ? , 10%) reaches 5.8%, and that of (IoU 75 , 5 ? , 10%) reaches 4.1%. These performance gains suggest that ? im not only enables the subsequent refined learning of pose consistency, but also provides an auxiliary supervision on the training of pose encoder ? and results in a better pose-sensitive embedding, implying the key role of ? im in DualPoseNet.</p><p>To evaluate the efficacy of our proposed spherical fusion based encoder ?, we compare with three alternative encoders: (1) a baseline of Densefusion <ref type="bibr" target="#b28">[29]</ref>, a pose encoder that fuses the learned RGB features from CNNs and point features from PointNet <ref type="bibr" target="#b22">[23]</ref> in a point-wise manner;</p><p>(2) SCNN-EarlyFusion, which takes as input the concatenation of S X and S P and feeds it into a multi-scale spherical CNN, followed by an MLP; (3) SCNN-LateFusion, which first feeds S X and S P into two separate multi-scale spherical CNNs and applies an MLP to the concatenation  <ref type="table">Table 1</ref>. Ablation studies on variants of our proposed DualPoseNet on REAL275. Evaluations are based on both our proposed metrics (left) and the metrics (right) proposed in <ref type="bibr" target="#b29">[30]</ref>.  We conduct ablation experiments by replacing ? with the above encoders, while keeping ? exp and ? im as remained. Results (without the refined learning of pose consistency) in <ref type="table">Table 1</ref> show that the three alternative encoders perform worse than our proposed ? with spherical fusion. Compared with the densefusion baseline, those based on spherical convolutions enjoy the property of rotation equivariance on SO(3), and thus achieve higher mAPs. With spherical fusion, our proposed pose encoder ? enables information communication progressively along the hierarchy, outperforming either SCNN-EarlyFusion with feature fusion at the very beginning or SCNN-LateFusion at the end. We finally investigate the benefit from the proposed refined learning of pose consistency. Results in <ref type="table">Table 1</ref> show that with the refined learning, pose precisions improve stably across the full spectrum of evaluation metrics, and the improvements increase when coarser metrics are used; this suggests that the refinement indeed attracts the learning towards more accurate regions in the solution space of pose prediction. Examples in <ref type="figure" target="#fig_3">Fig. 3</ref> give corroborative evidence of the efficacy of the refined learning. In fact, the refinement process is a trade-off between the improved precision and refining efficiency. As mentioned in Section 4.6, the refining efficiency depends on the learning rate and number of iterations for fine-tuning the encoder with the objective <ref type="bibr" target="#b7">(8)</ref>. In <ref type="figure" target="#fig_4">Fig. 4</ref>, we plot curves of mAP of (IoU 50 , 10 ? , 20%) against the number of iterations when using different learning rates. It shows faster convergence when using larger learning rates, which, however, may end with less mature final results, even with overfitting. In practice, one may set a proper tolerance for a balanced efficiency and accuracy. We set the learning rate as 1 ? 10 ?6 and = 5 ? 10 ?5 for results of DualPoseNet reported in the present paper. Under this setting, it costs negligible 0.2 seconds per instance on a server with Intel E5-2683 CPU and GTX 1080ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Comparisons with Existing Methods</head><p>We compare our proposed DualPoseNet with the existing methods, including NOCS <ref type="bibr" target="#b29">[30]</ref>, SPD <ref type="bibr" target="#b26">[27]</ref>, and CASS <ref type="bibr" target="#b4">[5]</ref>, on the CAMERA25 and REAL275 <ref type="bibr" target="#b29">[30]</ref> datasets. Note that NOCS and SPD are designed to first predict canonical versions of the observed point clouds, and the poses are obtained from post-alignment by solving a Umeyama algorithm <ref type="bibr" target="#b27">[28]</ref>. Quantitative results in <ref type="table">Table 2</ref> show the superiority of our proposed DualPoseNet on both datasets, especially for the metrics of high precisions. For completeness, we also present in <ref type="table">Table 2</ref> the comparative results under the original evaluation metrics proposed in <ref type="bibr" target="#b29">[30]</ref>; our results are better than existing ones at all but one rather coarse metric of IoU 50 , which is in fact a metric less sensitive to object pose. Qualitative results of different methods are shown in <ref type="figure">Fig. 5</ref>. Comparative advantages of our method over the existing ones are consistent with those observed in <ref type="table">Table  2</ref>. For example, the bounding boxes of laptops in the figure generated by NOCS and SPD are obviously bigger than the exact extensions of laptops, while our method predicts more compact bounding boxes with precise poses and sizes. More comparative results are shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Instance-level 6D Pose Estimation</head><p>We apply DualPoseNet to YCB-Video <ref type="bibr" target="#b2">[3]</ref> and LineMOD <ref type="bibr" target="#b12">[13]</ref>   <ref type="table">Table 2</ref>. Quantitative comparisons of different methods on CAMERA25 and REAL275. Evaluations are based on both our proposed metrics (left) and the metrics (right) proposed in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NOCS</head><p>GT DualPoseNet SPD CAMERA25 REAL275 <ref type="figure">Figure 5</ref>. Qualitative results of different methods on CAMERA25 and REAL275 <ref type="bibr" target="#b29">[30]</ref>.  <ref type="table">Table 3</ref>. Ablation studies on variants of DualPoseNet on YCB-Video <ref type="bibr" target="#b2">[3]</ref> and LineMOD <ref type="bibr" target="#b12">[13]</ref> datasets for instance-level 6D pose estimation. The evaluation metrics are mean ADD-S AUC and mean ADD(S) AUC, respectively. ble 3 confirm the efficacy of our individual components (the encoder ?, the implicit decoder ? im , and the refined learning of pose consistency); following <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>, we also augment our DualPoseNet with a 2nd-stage module for iterative refinement of residual pose, denoted as Dual-PoseNet(Iterative), to further improve the performance. As shown in <ref type="table">Table 4</ref>, DualPoseNet(Iterative) achieves comparable results against other methods, showing its potential for use in instance-level tasks. More quantitative and qualitative results are shown in the supplementary material.</p><p>Method YCB-Video LineMOD Pointfusion <ref type="bibr" target="#b32">[33]</ref> 83.9 73.7 PoseCNN + ICP <ref type="bibr" target="#b31">[32]</ref> 93.0 ? Densefusion (Per-pixel) <ref type="bibr" target="#b28">[29]</ref> 91.2 86.2 Densefusion (Iterative) <ref type="bibr" target="#b28">[29]</ref> 93.  <ref type="table">Table 4</ref>. Quantitative comparisons of different methods on YCB-Video <ref type="bibr" target="#b2">[3]</ref> and LineMOD <ref type="bibr" target="#b12">[13]</ref> datasets for instance-level 6D pose estimation. The evaluation metrics are mean ADD-S AUC and mean ADD(S) AUC, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustrations on the training and refined learning of Dual-PoseNet. During training, we optimize the objective(7), which is a combination of L?,? exp and L?,? im , in an end-to-end manner. During testing, we freeze the parameters of ?exp and ?im, and fine-tune those of ? to minimize L Ref ine ? for pose consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>?</head><label></label><figDesc>(i.e., the refinement stops whenL Ref ine ? ? ), with fast convergence and negligible cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Qualitative results of DualPoseNet without (red) and with (green) the refined learning of pose consistency on REAL275.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Plottings of prediction accuracy (mAP of (IoU50, 10 ? , 20%)) versus the number of iterations when using different learning rates to fine-tune the loss (8) for the refined learning of pose consistency. Experiments are conducted on REAL275<ref type="bibr" target="#b29">[30]</ref>. of the two output features. The used multi-scale spherical CNN is constructed by 8 spherical convolution layers, with aggregation of multi-scale spherical features similar to ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Pose Encoder Pose Encoder Implicit Pose Decoder Implicit Pose Decoder Explicit Pose Decoder Explicit Pose Decoder Pose Encoder Pose Encoder Implicit Pose Decoder Implicit Pose Decoder Explicit Pose Decoder Explicit Pose Decoder</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>? , 5% 10 ? , 5% 5 ? , 10% 5 ? , 20% 10 ? , 10% 10 ? , 20%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Encoder</cell><cell cols="2">? im Refining</cell><cell cols="10">IoU 75 5 2cm 5cm 2cm 5cm IoU 75 IoU 75 IoU 50 IoU 50 IoU 50 5 ? 5 ? 10 ? 10 ? IoU 50 IoU 75</cell></row><row><cell>Densefusion [29]</cell><cell></cell><cell>?</cell><cell>1.5</cell><cell>3.0</cell><cell>7.9</cell><cell>11.4</cell><cell>17.4</cell><cell>26.1</cell><cell>64.9</cell><cell>35.0</cell><cell>9.1</cell><cell>15.6 19.3 36.2</cell></row><row><cell>SCNN-EarlyFusion</cell><cell></cell><cell>?</cell><cell>7.7</cell><cell>14.4</cell><cell>15.8</cell><cell>20.3</cell><cell>35.5</cell><cell>45.8</cell><cell>76.1</cell><cell>51.9</cell><cell cols="2">17.3 24.5 36.2 56.8</cell></row><row><cell>SCNN-LateFusion</cell><cell></cell><cell>?</cell><cell>8.4</cell><cell>14.7</cell><cell>23.8</cell><cell>28.5</cell><cell>41.7</cell><cell>51.4</cell><cell>77.0</cell><cell>56.6</cell><cell cols="2">25.7 34.3 43.5 62.8</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>8.2</cell><cell>13.6</cell><cell>19.7</cell><cell>26.1</cell><cell>37.3</cell><cell>49.1</cell><cell>76.1</cell><cell>55.2</cell><cell cols="2">21.3 31.3 38.5 60.4</cell></row><row><cell>?</cell><cell></cell><cell>?</cell><cell>10.4</cell><cell>16.1</cell><cell>23.8</cell><cell>28.5</cell><cell>43.1</cell><cell>52.6</cell><cell>79.7</cell><cell>60.1</cell><cell cols="2">28.0 34.3 47.8 64.2</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell>11.2</cell><cell>17.2</cell><cell>24.8</cell><cell>29.8</cell><cell>44.5</cell><cell>55.0</cell><cell>79.8</cell><cell cols="3">62.2 29.3 35.9 50.0 66.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>datasets for the instance-level task. Results in Ta-? , 5% 10 ? , 5% 5 ? , 10% 5 ? , 20% 10 ? , 10% 10 ? , 20%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Method</cell><cell cols="10">IoU 75 5 2cm 5cm 2cm 5cm IoU 75 IoU 75 IoU 50 IoU 50 IoU 50 5 ? 5 ? 10 ? 10 ? IoU 50 IoU 75</cell></row><row><cell></cell><cell>NOCS [30]</cell><cell>22.6</cell><cell>29.5</cell><cell>31.5</cell><cell>34.5</cell><cell>54.5</cell><cell>56.8</cell><cell>83.9</cell><cell>69.5</cell><cell cols="2">32.3 40.9 48.2 64.6</cell></row><row><cell>CAMERA25</cell><cell>SPD [27]</cell><cell>47.5</cell><cell>61.5</cell><cell>52.2</cell><cell>56.6</cell><cell>75.3</cell><cell>78.5</cell><cell>93.2</cell><cell>83.1</cell><cell cols="2">54.3 59.0 73.3 81.5</cell></row><row><cell></cell><cell>DualPoseNet</cell><cell>56.2</cell><cell>65.1</cell><cell>65.1</cell><cell>68.0</cell><cell>78.6</cell><cell>81.5</cell><cell>92.4</cell><cell cols="3">86.4 64.7 70.7 77.2 84.7</cell></row><row><cell></cell><cell>NOCS [30]</cell><cell>2.4</cell><cell>3.5</cell><cell>7.1</cell><cell>9.3</cell><cell>19.7</cell><cell>22.3</cell><cell>78.0</cell><cell>30.1</cell><cell>7.2</cell><cell>10.0 13.8 25.2</cell></row><row><cell>REAL275</cell><cell>SPD [27] CASS [5]</cell><cell>8.6 ?</cell><cell>17.2 ?</cell><cell>15.0 ?</cell><cell>17.4 ?</cell><cell>38.5 ?</cell><cell>42.5 ?</cell><cell>77.3 77.7</cell><cell>53.2 ?</cell><cell cols="2">19.3 21.4 43.2 54.1 ? 23.5 ? 58.0</cell></row><row><cell></cell><cell>DualPoseNet</cell><cell>11.2</cell><cell>17.2</cell><cell>24.8</cell><cell>29.8</cell><cell>44.5</cell><cell>55.0</cell><cell>79.8</cell><cell cols="3">62.2 29.3 35.9 50.0 66.8</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mask r-cnn for object detection and instance segmentation on keras and tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waleed</forename><surname>Abdulla</surname></persName>
		</author>
		<ptr target="https://github.com/matterport/Mask_RCNN,2017.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Method for registration of 3-d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil D</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sensor fusion IV: control paradigms and data structures</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1611</biblScope>
			<biblScope unit="page" from="586" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ycb object and model set: Towards common benchmarks for manipulation research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Berk</forename><surname>Calli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Walsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Srinivasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron M</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 international conference on advanced robotics (ICAR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="510" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning canonical shape space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengsheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Category level object pose estimation via neural analysis-by-synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otmar</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Taco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Spherical cnns. In ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertram</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameesh</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient Response Maps for Real-Time Detection of Texture-Less Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 international conference on computer vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going further with point pair features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naresh</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained semi-supervised labeling of large shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Xing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wadim</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slobodan</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A unified framework for multi-view multi-class object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="254" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepim: Deep iterative matching for 6d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="683" to="698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cps++: Improving class-level 6d pose and shape estimation from monocular images with selfsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Busam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Minciullo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05848</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pvnet: Pixel-wise voting network for 6dof pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgbd data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Zoltan-Csaba Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolph</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Shape prior deformation for categorical 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><forename type="middle">H</forename><surname>Ang</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Least-squares estimation of transformation parameters between two point patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Umeyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Densefusion: 6d object pose estimation by iterative dense fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Mart?n-Mart?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3343" to="3352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Normalized object coordinate space for category-level 6d object pose and size estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">He</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinath</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanner</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatraman</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointfusion: Deep sensor fusion for 3d bounding box estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="244" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui Jia. W-Posenet</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11888</idno>
		<title level="m">Dense correspondence regularized pixel pair pose regression</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
