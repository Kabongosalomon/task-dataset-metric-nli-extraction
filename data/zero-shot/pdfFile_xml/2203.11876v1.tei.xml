<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open-Vocabulary DETR with Conditional Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Zang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ntu.edu.sgchen-huang@apple.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Open-Vocabulary DETR with Conditional Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Open-vocabulary object detection, which is concerned with the problem of detecting novel objects guided by natural language, has gained increasing attention from the community. Ideally, we would like to extend an open-vocabulary detector such that it can produce bounding box predictions based on user inputs in form of either natural language or exemplar image. This offers great flexibility and user experience for human-computer interaction. To this end, we propose a novel openvocabulary detector based on DETR-hence the name OV-DETR-which, once trained, can detect any object given its class name or an exemplar image. The biggest challenge of turning DETR into an open-vocabulary detector is that it is impossible to calculate the classification cost matrix of novel classes without access to their labeled images. To overcome this challenge, we formulate the learning objective as a binary matching one between input queries (class name or exemplar image) and the corresponding objects, which learns useful correspondence to generalize to unseen queries during testing. For training, we choose to condition the Transformer decoder on the input embeddings obtained from a pre-trained visionlanguage model like CLIP, in order to enable matching for both text and image queries. With extensive experiments on LVIS and COCO datasets, we demonstrate that our OV-DETR-the first end-to-end Transformer-based open-vocabulary detector -achieves non-trivial improvements over current state of the arts. Our code and model will be available at here.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection, a fundamental computer vision task aiming to localize objects with tight bounding boxes in images, has been significantly advanced in the last decade thanks to the emergence of deep learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b14">15]</ref>. However, most object detection algorithms are unscalable in terms of the vocabulary size, i.e., they are limited to a fixed set of object categories defined in detection datasets <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. For example, an object detector trained on COCO <ref type="bibr" target="#b20">[21]</ref> can only detect 80 classes and is unable to handle new classes beyond the training ones.</p><p>A straightforward approach to detecting novel classes is to collect and add their training images to the original dataset, and then re-train or fine-tune the detection model. This is, however, both impractical and inefficient due to the large cost of data collection and model training. In the detection literature, generalization from base to novel classes has been studied as a zero-shot detection problem <ref type="bibr" target="#b0">[1]</ref> where zero-shot learning techniques like word embedding projection <ref type="bibr" target="#b8">[9]</ref> are widely used.  <ref type="figure">Figure 1</ref>: Comparison between a RPN-based detector and our Open-Vocabulary Transformer-based detector (OV-DETR) using conditional queries. The RPN trained on closed-set object classes is easy to ignore novel classes (e.g., the "cat" region receives little response). Hence the cats in this example are largely missed with few to no proposals. By contrast, our OV-DETR is trained to perform matching between a conditional query and its corresponding box, which helps to learn correspondence that can generalize to queries from unseen classes. Note we can take input queries in the form of either text (class name) or exemplar images, which offers greater flexibility for open-vocabulary object detection.</p><p>Recently, open-vocabulary detection, a new formulation that leverages large pretrained language models, has gained increasing attention from the community <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13]</ref>. The central idea in existing works is to align detector's features with embedding provided by models pre-trained on large scale image-text pairs like CLIP <ref type="bibr" target="#b25">[26]</ref> (see <ref type="figure">Fig 1 (a)</ref>). This way, we can use an aligned classifier to recognize novel classes only from their descriptive texts.</p><p>A major problem with existing open-vocabulary detectors <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13]</ref> is that they rely on region proposals that are often not reliable to cover all novel classes in an image due to the lack of training data, see <ref type="figure">Fig. 1(a)</ref>. This problem has also been identified by a recent study <ref type="bibr" target="#b16">[17]</ref>, which suggests the binary nature of the region proposal network (RPN) could easily lead to overfitting to seen classes (thus fail to generalize to novel classes).</p><p>In this paper, we propose to train end-to-end an open-vocabulary detector under the Transformer framework, aiming to enhance its novel class generalization without using an intermediate RPN. To this end, we propose a novel open-vocabulary detector based on DETR [2]-hence the name OV-DETR-which is trained to detect any object given its class name or an exemplar image. This would offer greater flexibility than conventional open-vocabulary detection from natural language only.</p><p>Despite the simplicity of end-to-end DETR training, turning it into an open-vocabulary detector is non-trivial. The biggest challenge is the inability to calculate the classification cost for novel classes without their training labels. To overcome the challenge, we re-formulate the learning objective as binary matching between input queries (class name or exemplar image) and the corresponding objects. Such a matching loss over diverse training pairs allows to learn useful correspondence that can generalize to unseen queries during testing. For training, we extend the Transformer decoder of DETR to take conditional input queries. Specifically, we condition the Transformer decoder on the query embeddings obtained from a pre-trained vision-language model CLIP <ref type="bibr" target="#b25">[26]</ref>, in order to perform conditional matching for either text or image queries. <ref type="figure">Fig. 1</ref> shows this high-level idea, which proves better at detecting novel classes than RPN-based closed-set detectors.</p><p>We </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Open-Vocabulary Object Detection leverages the recent advances in large pre-trained language models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b9">10]</ref> to incorporate the open-vocabulary information into object detectors. OVR-CNN <ref type="bibr" target="#b34">[35]</ref> first uses BERT <ref type="bibr" target="#b5">[6]</ref> to pre-train the Faster R-CNN detector <ref type="bibr" target="#b27">[28]</ref> on image-caption pairs and then fine-tunes the model on downstream detection datasets. ViLD <ref type="bibr" target="#b12">[13]</ref> adopts a distillation-based approach that aligns the image feature extractor of Mask R-CNN <ref type="bibr" target="#b14">[15]</ref> with the image and text encoder of CLIP <ref type="bibr" target="#b25">[26]</ref> so the CLIP can be used to synthesize the classification weights for any novel class. Our approach differs from these works in that we train a Transformer-based detector end-to-end, with a novel framework of conditional matching. Zero-Shot Object Detection is also concerned with the problem of detecting novel classes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. However, this setting is less practical due to the harsh constraint of limiting access to resources relevant to unseen classes <ref type="bibr" target="#b34">[35]</ref>. A common approach to zeroshot detection is to employ word embeddings like GloVe <ref type="bibr" target="#b24">[25]</ref> as the classifier weights <ref type="bibr" target="#b0">[1]</ref>. Other works have found that using external resources like textual descriptions can help improve the generalization of classifier embeddings <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b26">27]</ref>. Alternatively, Zhao et al . <ref type="bibr" target="#b29">[30]</ref> used Generative Adversarial Network (GAN) <ref type="bibr" target="#b11">[12]</ref> to generate feature representations of novel classes. While Zhu et al . <ref type="bibr" target="#b37">[38]</ref> synthesized unseen classes using a data augmentation strategy. Visual Grounding is another relevant research area where the problem is to ground a target object in one image using natural language input <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b2">3]</ref>. Different from openvocabulary detection that aims to identify all target objects in an image, the visual grounding methods typically involve a particular single object, hence cannot be directly applied to generic object detection. There is a relevant visual grounding method though, which is called MDETR <ref type="bibr" target="#b15">[16]</ref>. This method similarly trains DETR along with a given language model so as to link the output tokens of DETR with specific words. MDETR also adopts a conditional framework, where the visual and textual features are combined to be fed to the Transformer encoder and decoder. However, the MDETR method is not applicable to open-vocabulary detection because it is unable to calculate the cost matrix for novel classes under the classification framework. Our OV-DETR bypasses this challenge by using a conditional matching framework instead. Object Detection with Transformers. The pioneer DETR approach <ref type="bibr" target="#b1">[2]</ref> greatly simplifies the detection pipeline by casting detection as a set-to-set matching problem.</p><p>Several follow-up methods have been developed to improve performance and training efficiency. Deformable DETR <ref type="bibr" target="#b38">[39]</ref> features a deformable attention module, which samples sparse pixel locations for computing attention, and further mitigates the slow convergence issue with a multi-scale scheme. SMCA <ref type="bibr" target="#b10">[11]</ref> accelerates training convergence with a location-aware co-attention mechanism. Conditional DETR <ref type="bibr" target="#b22">[23]</ref> also addresses the slow convergence issue, but with conditional spatial queries learned from reference points and the decoder embeddings. Our work for the first time extends DETR to the open-vocabulary domain by casting open-vocabulary detection as a conditional matching problem, and achieves non-trivial improvements over current state of the arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Open-Vocabulary DETR</head><p>Our goal is to design a simple yet effective open-vocabulary object detector that can detect objects described by arbitrary text inputs or exemplar images. We build on the success of DETR <ref type="bibr" target="#b1">[2]</ref> that casts object detection as an end-to-end set matching problem (among closed classes), thus eliminating the need of hand-crafted components like anchor generation and non-maximum suppression. This pipeline makes it appealing to act as a suitable framework to build our end-to-end open-vocabulary object detector.</p><p>However, it is non-trivial to retrofit a standard DETR with closed-set matching to an open-vocabulary detector that requires matching against unseen classes. One intuitive approach for such open-set matching is to learn a class-agnostic module (e.g., ViLD <ref type="bibr" target="#b12">[13]</ref>) to handle all classes. This is, however, still unable to match for those open-vocabulary classes that come with no labeled images. Here we provide a new perspective on the matching task in DETR, which leads us to reformulate the fixed set-matching objective into a conditional binary matching one between conditional inputs (text or image queries) and detection outputs.</p><p>An overview of our Open-Vocabulary DETR is shown in <ref type="figure">Fig. 2</ref>. At high level, DETR first takes query embeddings (text or image) as conditional inputs obtained from a pre-trained CLIP <ref type="bibr" target="#b25">[26]</ref> model, and then a binary matching loss is imposed against the detection result to measure their matchability. In the following, we will revisit the closedset matching process in standard DETR in Section 3.1. We then describe how to perform conditional binary matching in our OV-DETR in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Revisiting Closed-Set Matching in DETR</head><p>For input image x, a standard DETR infers N object predictions? where N is determined by the fixed size of object queries q that serve as learnable positional encodings. One single pass of the DETR pipeline consists of two main steps: (i) set prediction, and (ii) optimal bipartite matching. Set Prediction. Given an input image x, the global context representations c is first extracted by a CNN backbone f ? and then a Transformer encoder h ? :</p><formula xml:id="formula_0">c = h ? (f ? (x)),<label>(1)</label></formula><p>where the output c corresponds to a sequence of feature embeddings of q. Taking the context feature c and object queries q as inputs, the Transformer decoder h ? then produce the set prediction? where? contains both bounding box predictionsb and class predictionsp for a closed-set of training classes. Optimal Bipartite Matching is to find the best match between the set of N prediction? y and the set of ground truth objects y = {y i } M i=1 (including no object ?). Specifically, one needs to search a permutation of N elements ? ? S N that has the lowest matching cost:? = argmin</p><formula xml:id="formula_1">= {? i } N i=1 :? = h ? (c, q),<label>(2)</label></formula><formula xml:id="formula_2">??S N N i L cost (y i ,? ?(i) ),<label>(3)</label></formula><p>where L cost (y i ,? ?(i) ) is a pair-wise matching cost between ground truth y i and the prediction? ?(i) with index ?(i). Note L cost is comprised of the losses for both class prediction L cls (p, p) and bounding box localization L box (b, b). The whole bipartite matching process produces one-to-one label assignments, where each prediction? i is assigned to a ground-truth annotation y j or ? (no object). The optimal assignment can be efficiently found by the Hungarian algorithm <ref type="bibr" target="#b17">[18]</ref>.</p><p>Challenge. As mentioned above, the bipartite matching method cannot be directly applied to an open-vocabulary setting that contains both base and novel classes. The reason is that computing the matching cost in Eq. (3) requires access of the label information, which is unavailable for novel classes. As a result, the predictions for the N object queries cannot generalize to novel classes due to the lack of training labels for them. As shown in <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>, bipartite matching can only be performed for base classes with available training labels. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Conditional Matching for Open-Vocabulary Detection</head><p>To enable DETR to go beyond closed-set classification and perform open-vocabulary detection, we equip the Transformer decoder with conditional inputs and reformulate the learning objective as binary matching problem. Conditional Inputs. Given an object detection dataset with standard annotations for all the training (base) classes, we need to convert those annotations to conditional inputs to facilitate our new training paradigm. Specifically, for each ground-truth annotation with bounding box b i and class label name y class i , we use the CLIP model <ref type="bibr" target="#b25">[26]</ref> to generate their corresponding image embedding z image i and text embedding z text i :</p><formula xml:id="formula_3">z image i = CLIP image (x, b i ), z text i = CLIP text (y class i ).<label>(4)</label></formula><p>Such image and text embeddings are already well-aligned by the CLIP model. Therefore, we can choose either of them as input queries to condition the DETR's decoder and train to match the corresponding objects. Once training is done, we can then take arbitrary input queries during testing to perform open-vocabulary detection. To ensure equal training conditioned on image and text queries, we randomly select z text i or z image i with probability ? as conditional inputs. Moreover, we follow previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref> to generate additional object proposals for novel classes to enrich our training data. We only extract image embeddings z image i for such novel-class proposals as conditional inputs, since their class names are unavailable to extract text embeddings. Please refer to supplementary materials for more details. Conditional Matching. Our core training objective is to measure the matchability between the conditional input embeddings and detection results. In order to perform such conditional matching, we start with a fully-connected layer F proj to project the conditional input embeddings (z text i or z image i ) to have the same dimension as q. Then the input to the DETR decoder q is given by:</p><formula xml:id="formula_4">q = q ? F proj (z mod i ), mod ? {text, image},<label>(5)</label></formula><p>where we use a simple addition operation ? to convert the class-agnostic object queries q into class-specific q informed by F proj (z mod i ). In practice, adding the conditional input embeddings z to only one object query will lead to a very limited coverage of the target objects that may appear many times in the image. Indeed, in existing object detection datasets, there are typically multiple object instances in each image from the same or different classes. To enrich the training signal for our conditional matching, we copy the object queries q for R times, and the conditional inputs (z text i or z image i ) for N times before performing the conditioning in Eq. <ref type="bibr" target="#b4">(5)</ref>. As a result, we obtain a total of N ?R queries for matching during each forward pass, as shown in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>. Experiments in Section 4.1 <ref type="table" target="#tab_4">Table 3</ref> will validate the importance of such "feature cloning" and also show how we determine N and R based on the performancememory trade-off. Note for the final conditioning process, we further add an attention mask to ensure the independence between different query copies, as is similarly done in <ref type="bibr" target="#b3">[4]</ref>.</p><p>Given the conditioned query features q , our binary matching loss for label assignment is given as:</p><formula xml:id="formula_5">L cost (y,? ? ) = L match (p,p ? ) + L box b,b ? ,<label>(6)</label></formula><p>where L match (p,p ? ) denotes a new matching loss that replaces the classification loss L cls (p,p ? ) in Eq. (3). Here in our case, p is a 2-dimensional probability vector that characterizes the matchability ('matched' vs. 'not matched'), and L match is simply implemented by a BCE loss L BCE between predictedp ? and groud-truth p. <ref type="figure" target="#fig_1">Fig. 3 (b)</ref> shows that, with the 'bird' query as input, our matching loss should allow us to match all the bird instances in one image, while tagging instances from other classes as 'not matched'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimization</head><p>After optimizing Eq. <ref type="formula" target="#formula_5">(6)</ref>, we obtain the optimized label assignments ? for different object queries. This process produces a set of detected objects with assigned box coordinatesb and 2-dim matching probabilityp that we will use to compute our final loss function for modeling training. We further attach an embedding reconstruction head to the model, which learns to predict embedding e to be able to reconstruct each conditional input embedding z text or z image :</p><formula xml:id="formula_6">L embed (e, z) = e ? z mod 1 , mod ? {text, image}.<label>(7)</label></formula><p>The intuition behind embedding reconstruction is that this makes the model learn from various conditional inputs the different concepts in feature space, which helps to provide discriminative guidance for our conditional training. Supplementary materials validate the effectiveness of L embed .</p><p>Our final loss for model training combines L embed with bounding box losses L match (p,p) and L box (b,b) again:</p><formula xml:id="formula_7">L loss (y,?) = L match (p,p) + L box (b,b) + L embed (e, z) = ? L BCE L BCE + ? L L1 L L1 + ? L GIoU L GIoU + ? L embed L embed ,<label>(8)</label></formula><p>where L box consists of the L1 loss and the generalized IoU (GIoU) <ref type="bibr" target="#b28">[29]</ref> loss for boxes, while ? L BCE , ? L L1 , ? L Giou and ? L embed are the weighting parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>During testing, for each image, we send the text embedding z text of all the base+novel classes to the model and merge the results by selecting the top k predictions with highest prediction scores. We follow the prior work <ref type="bibr" target="#b12">[13]</ref> to use k = 100 for COCO dataset and k = 300 for LVIS dataset. To obtain the context representation c in Eq. (1), we forward the input image through the CNN backbone f ? and Transformer encoder h ? . Note c is computed only once and shared for all conditional inputs for efficiency. Then the conditioned object queries from different classes are sent to the Transformer decoder in parallel. In practice, we copy the object queries for R times as shown in <ref type="figure" target="#fig_2">Fig. 4 (b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We evaluate our approach on two standard open-vocabulary detection benchmarks modified from LVIS <ref type="bibr" target="#b13">[14]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> respectively. LVIS <ref type="bibr" target="#b13">[14]</ref> contains 100K images with 1,203 classes. The classes are divided into three groups, namely frequent, common and rare, based on the number of training images. Following ViLD <ref type="bibr" target="#b12">[13]</ref>, we treat 337 rare classes as novel classes and use only the frequent and common classes for training. The COCO <ref type="bibr" target="#b20">[21]</ref> dataset is a widely-used benchmark for object detection, which consists of 80 classes. Following OVR-CNN <ref type="bibr" target="#b34">[35]</ref>, we divide the classes in COCO into 48 base categories and 17 novel categories, while removing 15 categories without a synset in the WordNet hierarchy. The training set is the same as the full COCO but only images containing at least one base class are used. We refer to these two benchmarks as OV-LVIS and OV-COCO hereafter.    Evaluation Metrics. For OV-LVIS, we report the mask mAP for rare, common and frequent classes, denoted by AP m r , AP m c and AP m f . The rare classes are treated as novel classes (AP m novel ). The symbol AP m denotes to the mAP of all the classes. For OV-COCO, we follow previous work that only reports the AP50 b metric, which means the box mAP at IoU threshold 0.5. Extension for Instance Segmentation. For OV-LVIS, instance segmentation results are needed for the evaluation process. Although DETR <ref type="bibr" target="#b1">[2]</ref> and its follow-ups <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b22">23]</ref> are developed for the object detection task, they can also be extended to the instance segmentation task. We follow DETR <ref type="bibr" target="#b1">[2]</ref> to add an external class-agnostic segmentation head to solve the instance segmentation task. The segmentation head employs the fully convolutional network (FCN <ref type="bibr" target="#b21">[22]</ref>) structure, which takes features extracted from the Transformer decoder as input and produces segmentation masks. Text Prompts. Prompt tuning is a critical step when transferring pre-trained language models to downstream computer vision tasks <ref type="bibr" target="#b35">[36]</ref>. We follow the same process as in ViLD <ref type="bibr" target="#b12">[13]</ref> to construct the text prompts. Specifically, for each class we feed the textual name wrapped in 63 different prompt templates (e.g., 'there is a {class name} in the photo') to CLIP's text encoder, and then average the 85 text embeddings, which is known as prompt ensembling <ref type="bibr" target="#b25">[26]</ref>. Implementation Details. Our model is based on Deformable DETR <ref type="bibr" target="#b38">[39]</ref>. All models are trained on 8 Tesla V100 GPUs. We use the ResNet50-C4 backbone as our default choice. Following ViLD <ref type="bibr" target="#b12">[13]</ref>, we also use the open-source CLIP model <ref type="bibr" target="#b25">[26]</ref> based on ViT-B/32 for extracting text and image embeddings. Please refer to our supplementary material for more training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Studies</head><p>We conduct ablation study on OV-LVIS to evaluate the main components in our approach. The Architecture Difference. Previous works such as ViLD <ref type="bibr" target="#b12">[13]</ref> are based on the RPN-based Mask R-CNN <ref type="bibr" target="#b14">[15]</ref>, while our work is based on the Transformer-based detector Deformable DETR <ref type="bibr" target="#b38">[39]</ref>. We first study the difference of these two detectors on the openvocabulary setting trained with base classes only. As shown in <ref type="table" target="#tab_2">Table 1</ref> row(1-2), we observe that Mask R-CNN performs a slightly better than Deformable DETR <ref type="bibr" target="#b38">[39]</ref>. This gap is small, indicating that we have a fair starting point compared to ViLD <ref type="bibr" target="#b12">[13]</ref>. Object Proposals. We then replace Deformable DETR's classifier layer as text embedding provided by CLIP and trained with base classes only. This step is similar to the novel classes), we report mask mAP and a breakdown on novel (rare), common, and frequent classes. For OV-COCO (w/ 48 base classes and 17 novel classes), we report bounding box mAP at IoU threshold 0.5. ?: zero-shot methods that do not use captions or image-text pairs. ?: ensemble model. previous ViLD-text <ref type="bibr" target="#b12">[13]</ref> method. Results is presented in <ref type="table" target="#tab_3">Table 2</ref> row 1. We observe that the AP m novel metric improved from 0.0 to 9.5. To further improve the AP m novel metric, we add the object proposals that may contain the region of novel classes into the training stage. Because we do not know the category id of these object proposals, we observe that the label assignment of these object proposals is inaccurate and will decrease the AP m novel performance from 9.5 to 6.3. Conditional Binary Matching. Now we replace DETR's default close-set labeling assignment as our proposed conditional binary matching. The comparison results between <ref type="table" target="#tab_3">Table 2</ref> row 2-3 shows that our binary matching strategy can better leverage the knowledge from object proposals and improve the AP m novel from 9.5 to 17.4. Such a large improvement shows that the proposed conditional matching is essential when applying the DETR-series detector for the open-vocabulary setting. Importance of Multiple Queries for Training. Recap that we propose to "clone" query features in Section 3.2 (also see <ref type="figure" target="#fig_2">Fig 4)</ref>. We examine different choices of two hyperparameters N and R, and show results in <ref type="table" target="#tab_4">Table 3</ref>. When N = 100, we find that coping queries (from R = 1 to 3) improves the AP m novel from 10.6 to 13.6, and a slight degradation when R = 9 partially due to the limited optimization capacity. When N = 300, we observe that coping queries (from R = 1 to 3) is also beneficial. However, we will face the out-of-memory issue on GPU when N = 300 and R &gt; 3. Overall, we find the combination of N = 300 queries and repetition of R = 3 times serves as the optimal solution.  <ref type="table" target="#tab_5">Table 4</ref> row 4). It pre-trains the detector's projecting layer on image-caption pairs using contrastive loss and then fine-tunes on the object detection task; (2) Variants of ViLD <ref type="bibr" target="#b12">[13]</ref> such as ViLD-text and ViLD-ensemble (see <ref type="table" target="#tab_5">Table 4</ref> rows 5-7). ViLD is the first study that uses CLIP embeddings <ref type="bibr" target="#b25">[26]</ref> for open-vocabulary detection. Compared with ViLD-text, ViLD uses knowledge distillation from the CLIP visual backbone, improves AP novel at the cost of hurting AP base . ViLD-ens. combines the two models and shows improvements for both metrics. Such an ensemble-based method also brings extra time and memory cost. <ref type="table">Table 5</ref>: Generalization to other datasets. We evaluate OV-DETR trained on LVIS when transferred to other datasets such as PASCAL VOC 2007 test set and COCO validation set by simply replacing the text embeddings. The experimental setting is the same as that of ViLD <ref type="bibr" target="#b12">[13]</ref>. We observe that OV-DETR achieves better generalization performance than ViLD <ref type="bibr" target="#b12">[13]</ref>.</p><formula xml:id="formula_8"># Method OV-LVIS OV-COCO AP m AP m novel AP m c AP m f AP50 b AP50 b novel AP50 b base 1 SB [1] ? - - - - 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on Open-vocabulary Benchmarks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head># Method</head><p>Pascal VOC COCO For completeness, we also list the results of some previous zero-shot methods such as SB <ref type="bibr" target="#b0">[1]</ref>, DELO <ref type="bibr" target="#b37">[38]</ref> and PL <ref type="bibr" target="#b26">[27]</ref> in <ref type="table" target="#tab_5">Table 4</ref>   <ref type="bibr" target="#b34">[35]</ref> by a large margin, notably, the 6.6 mAP improvements on novel classes. Compared with ViLD <ref type="bibr" target="#b12">[13]</ref>, OV-DETR still achieves 1.4 mAP gains on all the classs and 1.8 mAP gains on novel classes. In summary, it is observed that that OV-DETR achieves superior performance across different datasets compared with different methods.</p><formula xml:id="formula_9">AP b 50 AP b 75 AP b AP b 50</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Generalization Ability of OV-DETR.</head><p>We follow ViLD <ref type="bibr" target="#b12">[13]</ref> to test the generalization ability of OV-DETR by training the model on LVIS <ref type="bibr" target="#b13">[14]</ref> dataset and evaluated on PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref>. We keep the same implementation details with ViLD <ref type="bibr" target="#b12">[13]</ref>. When transferring our model to other datasets, we switch conditional text queries z text to the new datasets. As shown in <ref type="table">Table 5</ref>, we observe that OV-DETR achieves better transfer performance than ViLD. The experimental results show that the model trained by our conditional-based mechanism has transferability to other domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>We visualize OV-DETR's detection and segmentation results in <ref type="figure" target="#fig_5">Fig. 5</ref>. The results based on conditional text queries, conditional image queries, and a mixture of conditional text and image queries are shown in the top, middle and bottom row, respectively. Overall, our OV-DETR can accurately localize and precisely segment out the target objects from novel classes despite no annotations of these classes during training. It is worth noting that the conditional image queries, such as "crape" in (d) and "fork" in (h), appear drastically different from those in the target images but OV-DETR can still robustly detect them.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Inference Time Analysis</head><p>OV-DETR exhibits great potential in open-vocabulary detection but is by no means a perfect detector. The biggest limitation of OV-DETR is that the inference speed is slow when the number of classes to detect is huge like 1,203 on LVIS <ref type="bibr" target="#b12">[13]</ref>. This problem is caused by the conditional design that requires multiple forward passes in the Transformer decoder (depending on the number of classes). We show a detailed comparison on the inference time between Deformable DETR and OV-DETR in <ref type="table" target="#tab_10">Table 6</ref>. Without using any tricks, the vanilla OV-DETR (#2), i.e., using a single forward pass for each class, is about 2? slower than Deformable DETR (#1) on COCO (w/ 80 classes) while 16? slower on LVIS (w/ 1,203 classes). As discussed in Sec. 3.2 and shown in <ref type="figure" target="#fig_2">Fig. 4(b)</ref>, we optimize the speed by forwarding multiple conditional queries to the Transformer decoder in parallel, which reduces the inference time by 12.5% on COCO and nearly 60% on LVIS (see #3 in <ref type="table" target="#tab_10">Table 6</ref>). Still, there is much room for improvement.</p><p>It is worth noting that such a slow inference problem is not unique to our approachmost instance-conditional models would have the same issue <ref type="bibr" target="#b18">[19]</ref>, which is the common price to pay in exchange for better performance. The computation bottleneck of our method lies in the computation of the Transformer decoder in Eq. <ref type="bibr" target="#b1">(2)</ref>. A potential solution is to design more efficient attention modules <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>, which we leave as future work. In human-computer interaction where users already have target object(s) in mind, e.g., a missing luggage or a specific type of logo, the conditional input is fixed and low in number, thus the inference time is negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Open-vocabulary detection is known to be a challenging problem due to the lack of training data for unseen classes. Recent advances in large language models have offered a new perspective for designing open-vocabulary detectors. In this work, we show how an end-to-end Transformer-based detector can be turned into an open-vocabulary detector based on conditional matching and with the help of pre-trained vision-language models. The results show that, despite having a simplified training pipeline, our open-vocabulary detector based on Transformer significantly outperforms current state of the arts that are all based on two-stage detectors. We hope our approach and the findings presented in the paper can inspire more future work on the design of efficient open-vocabulary detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A More Qualitative Results of OV-DETR Open-Vocabulary COCO. We provide more qualitative results of OV-DETR on Open-Vocabulary COCO setting <ref type="figure">(Fig. 6)</ref>. We visualize the detection results on novel classes and the activation maps of OV-DETR and Region Proposal Network (RPN) <ref type="bibr" target="#b27">[28]</ref> used by ViLD <ref type="bibr" target="#b12">[13]</ref>, which further validate the motivation from the main paper: OV-DETR has higher activation values on objects of novel classes than RPN. Web Images. To verify the generalization ability, we also provide the qualitative results of anime characters in <ref type="figure">Fig. 7</ref>. Although these characters are not provided during training, OV-DETR can successfully detect the regions matched with the conditional image queries. Failure Cases. <ref type="figure">Fig. 8</ref> shows some failure cases of OV-DETR. We notice that detecting small or occluded objects with conditional image query is hard. Our method is not robust to the unrelated out-of-distribution text queries. We will address these shortcomings in further research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Discussion of Object Proposals</head><p>In previous work ViLD <ref type="bibr" target="#b12">[13]</ref>, class-agnostic object proposals are leveraged to transfer the knowledge from CLIP image encoder to the detector. As shown in <ref type="figure">Fig. 9 (a)</ref>, ViLD first train a RPN network on base classes to get M pre-computed proposals. These object proposals may contain objects of novel classes and are essential for training ViLD model. For these M proposals, M predicted region embeddings and the corresponding ground-truth embeddings are computed by a mask R-CNN detector and a CLIP image encoder respectively. Then, a knowledge distillation loss is applied for the predicted region embeddings and the ground-truth embeddings. <ref type="figure">Fig. 9</ref> (b) shows that compared with ViLD, we leverage the object proposals in a different way. Since these object proposals are class-agnostic, they cannot be applied for DETR's matching algorithm. In OV-DETR, we treat the image embeddings extracted by the object proposals as the conditional image query. The expected predictions of OV-DETR are the 'matched' regions of an input image given a conditional image query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hyper-parameters Details</head><p>We keep most of the hyper-parameters the same with previous works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b38">39]</ref>. For loss functions, we set the weighting parameters L BCE = 3.0, L L1 = 5.0, L GIoU = 2.0 and L embed = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Importance of L embed</head><p>In OV-DETR, we introduce an embedding reconstruction head to predict the conditional input embedding z text or z image , and this reconstruction head is optimized by the loss L embed . The results in <ref type="table">Table 7</ref> shows the efficacy of L embed .  <ref type="table">Table 7</ref>:</p><p>Importance of L embed on LVIS.</p><p>Query "sink" Output (ours) Activation map (ours) Activation map (RPN) "scissors"</p><p>"tie" <ref type="figure">Figure 6</ref>: Qualitative results on Open-Vocabulary COCO setting. We visualize the prediction results of OV-DETR on novel classes. We also provide the comparison of activation maps between ours and the RPN network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query:</head><p>Output: <ref type="figure">Figure 7</ref>: Qualitative results on anime characters. These images are collected from web. We use the model trained on LVIS dataset to check the matchability with the given conditional image queries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparing the label assignment mechanisms of DETR and our OV-DETR. (a) For DETR, the classification cost can only be computed for labeled base classes. (b) On the contrary, our OV-DETR performs conditional matching for any object specified by a conditional input, which is class-agnostic and can generalize to novel classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>DETR decoder with (a) single conditional input or (b) multiple conditional inputs in parallel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>rows 1-3. On OV-LVIS benchmark, OV-DETR improves the previous SOTA ViLD by 4.1 on AP m and 1.3 on AP m novel . Compared with ViLD, our method will not affect the performance of base classes when improve the novel classes. Even compared with the ensemble result of ViLD-ensemble, OV-DETR still boosts the performance by 1.5, 0.8, 1.0 and 2.2, respectively (%). Noted that OV-DETR only uses a single model and does not leverage any ensemble-based technique. On OV-COCO benchmark, OV-DETR improves the baseline and outperforms OVR-CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on LVIS. OV-DETR can precisely detect and segment novel objects (e.g., 'crape', 'fishbowl', 'softball') given the conditional text query (top) or conditional image query (middle) or a mixture of them (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>conduct comprehensive experiments on two challenging open-vocabulary object detection datasets, and show consistent improvements in performance. Concretely, our OV-DETR method achieves 17.4 mask mAP of novel classes on the open-vocabulary LVIS dataset [13] and 29.4 box mAP of novel classes on open-vocabulary COCO dataset [35], surpassing SOTA methods by 1.3 and 1.8 mAP, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Mask R-CNN and Def DETRon OV-LVIS, both trained on base classes. ?: copied from ViLD<ref type="bibr" target="#b12">[13]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on using object pro-</figDesc><table><row><cell cols="4">posals (P) and our conditional binary matching</cell></row><row><cell>mechanism (M).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4"># P M AP m AP m novel AP m c AP m f</cell></row><row><cell>1</cell><cell>24.2</cell><cell>9.5</cell><cell>23.2 31.7</cell></row><row><cell>2</cell><cell>19.9</cell><cell>6.3</cell><cell>17.4 28.6</cell></row><row><cell cols="4">3 26.6 17.4 25.0 32.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on N (the number of object queries) and R (the number of copies).</figDesc><table><row><cell cols="3"># N R AP m AP m novel AP m c AP m f</cell></row><row><cell>1 100 1 22.0</cell><cell>10.6</cell><cell>20.9 28.2</cell></row><row><cell>2 100 3 25.7</cell><cell cols="2">13.6 25.0 31.9</cell></row><row><cell>3 100 9 24.3</cell><cell>11.9</cell><cell>22.9 31.3</cell></row><row><cell>4 300 1 24.2</cell><cell>12.3</cell><cell>22.8 30.9</cell></row><row><cell cols="3">5 300 3 26.6 17.4 25.0 32.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Main results on OV-LVIS and OV-COCO. For OV-LVIS (w/ 886 base classes and 317</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 summarizes</head><label>4</label><figDesc>our results. We compare our method with SOTA open-vocabulary detection methods including: (1) OVR-CNN [35] (see</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Comparison of the in-</figDesc><table><row><cell>ference time (second per iteration)</cell></row><row><cell>between Deformable DETR [39] and</cell></row><row><cell>our OV-DETR before/after opti-</cell></row><row><cell>mization on LVIS and COCO.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc># L embed AP m AP m novel AP m c AP m f 1 24.9 14.4 23.2 31.3 2 26.6 17.4 25.0 32.5</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Query:</head><p>Output:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a)</head><p>"bicycle"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b)</head><p>"philosophy" (c) <ref type="figure">Figure 8</ref>: Failure cases of OV-DETR. These images are collected from the web. We use the model trained on LVIS dataset to check the matchability with the given conditional image or text queries. (a): OV-DETR fails to detect these small and occluded objects with the conditional image query ("bicycle"). But as shown in (b), this issue can be solved to some extent by using text query. (c): Given the unrelated text queries (e.g., "philosophy"), OV-DETR will predict wrong false-positive detection results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>pre-computed proposals</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Zero-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="384" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<editor>ECCV. Springer. 2020</editor>
		<imprint>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<idno>CVPR. 2021</idno>
		<imprint>
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Visual grounding via accumulated attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaorui</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="7746" to="7755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<idno>IJCV 88.2</idno>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="303" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>CVPR. Ieee</publisher>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Towards Open Vocabulary Object Detection without Humanprovided Bounding Boxes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fast convergence of detr with spatially modulated co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<idno>ICCV. 2021</idno>
		<imprint>
			<biblScope unit="page" from="3621" to="3630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Open-vocabulary Object Detection via Vision and Language Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuye</forename><surname>Gu</surname></persName>
		</author>
		<idno>ICLR. 2022</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">LVIS: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">MDETR-modulated detection for end-to-end multimodal understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<idno>ICCV. 2021</idno>
		<imprint>
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning Open-World Object Proposals without Learning to Classify</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<editor>RA-L</editor>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1970" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Zero-shot object detection with textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihui</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="8690" to="8697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Conditional detr for fast training convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Depu</forename><surname>Meng</surname></persName>
		</author>
		<idno>ICCV. 2021</idno>
		<imprint>
			<biblScope unit="page" from="3651" to="3660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">A trainable system for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Papageorgiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno>IJCV 38.1</idno>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="15" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An Improved Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanzila</forename><surname>Rahman</surname></persName>
		</author>
		<idno>CVPR. 2021</idno>
		<imprint>
			<biblScope unit="page" from="1653" to="1662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">GTNet: Generative Transfer Network for Zero-Shot Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Shizhen</surname></persName>
		</author>
		<idno>AAAI. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. PMLR. 2020</title>
		<imprint>
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">ZSD-YOLO: Zero-Shot YOLO Detection using Vision-Language KnowledgeDistillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnathan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.12066</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Open-vocabulary object detection using captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<idno>CVPR. 2021</idno>
		<imprint>
			<biblScope unit="page" from="14393" to="14402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning to Prompt for Vision-Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.01134</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Detecting Twenty-thousand Classes using Image-level Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02605</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Don&apos;t Even Look Once: Synthesizing Features for Zero-Shot Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengkai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<idno>CVPR. 2020</idno>
		<imprint>
			<biblScope unit="page" from="11693" to="11702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable Transformers for End-to-End Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<idno>ICLR. 2020</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
