<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">INTEGRATION OF PRE-TRAINED NETWORKS WITH CONTINUOUS TOKEN INTERFACE FOR END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Seo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kwak</surname></persName>
							<email>donghyun.kwak@navercorp.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowon</forename><surname>Lee</surname></persName>
							<email>bowon.lee@inha.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">INTEGRATION OF PRE-TRAINED NETWORKS WITH CONTINUOUS TOKEN INTERFACE FOR END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most End-to-End (E2E) Spoken Language Understanding (SLU) networks leverage the pre-trained Automatic Speech Recognition (ASR) networks but still lack the capability to understand the semantics of utterances, crucial for the SLU task. To solve this, recently proposed studies use pre-trained Natural Language Understanding (NLU) networks. However, it is not trivial to fully utilize both pre-trained networks; many solutions were proposed, such as Knowledge Distillation (KD), cross-modal shared embedding and network integration with Interface. We propose a simple and robust integration method for the E2E SLU network with a novel Interface, Continuous Token Interface (CTI). CTI is a junctional representation of the ASR and NLU networks when both networks are pre-trained with the same vocabulary. Thus, we can train our SLU network in an E2E manner without additional modules, such as Gumbel-Softmax. We evaluate our model using SLURP, a challenging SLU dataset and achieve state-of-the-art scores on intent classification and slot filling tasks. We also verify that the NLU network, pre-trained with Masked Language Model (MLM), can utilize a noisy textual representation of CTI. Moreover, we train our model with extra data, SLURP-Synth, and get better results.</p><p>Index Termsend-to-end spoken language understanding, interface of networks, intent classification, slot filling</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Spoken Language Understanding (SLU), a front-end of many spoken dialogue systems, is a task of extracting semantic information such as intents, slots, or emotions from speech. Conventional SLU pipelines consist of the Automatic Speech Recognition (ASR) model and the Natural Language Understanding (NLU) model, where the ASR model converts speech to text, and the NLU model predicts task-specific information from the text <ref type="bibr" target="#b0">[1]</ref>. However, these pipelines propagate errors from the ASR model to the NLU model, degrading the system's overall performance. To address this problem, recent studies focused on End-to-End (E2E) SLU networks that directly extract semantic information from * Equal contribution. speech <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Most E2E SLU networks leverage the encoder part of pre-trained ASR networks but cannot fully understand the semantics of utterances, which is crucial for the SLU task.</p><p>To solve this, recently proposed studies use pre-trained NLU networks to leverage the powerful language representation of the Pre-trained Language Model (PLM), such as BERT <ref type="bibr" target="#b3">[4]</ref>. Some approaches use Knowledge Distillation (KD) from PLM to SLU network <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> or introduce crossmodal shared embedding space between acoustic and textual features <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, and others try to integrate the ASR and NLU networks with appropriate Interface <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>However, these methods may lose the powerful language representation of BERT in the process of KD or lose important features while embedding cross-modal inputs in a shared space. In the case of network integration with Interface, it depends on the gap between the two representations. The Interface is a junctional representation of two pre-trained networks to be combined <ref type="bibr" target="#b10">[11]</ref>. To make the gradient flow via Interface, it is usually necessary to employ additional modules to match the representations.</p><p>For example, If we integrate the networks with Discrete Token Interface (DTI), one-hot encoding of ASR hypothesis, the model needs additional modules, such as the Gumbel-Max trick and Gumbel-Softmax distribution for forward and backward passes to match the distribution <ref type="bibr" target="#b11">[12]</ref>. This learning process is complicated, as the pre-trained networks could miss their acoustic or linguistic representations.</p><p>In this paper, we propose a novel Interface, Continuous Token Interface (CTI), for a simple and robust network integration. The CTI is naturally derived by pre-training both ASR and NLU networks with the same vocabulary. With our interface, there is a small representation gap between the networks, and no additional modules are needed. Because the networks have the same vocabulary, the softmax probability distribution of the ASR network can be considered as a noisy textual representation from the point of view of the NLU network. We assume it only differs in the noise pattern; our Interface has a neglectable gap of representations, unlike other Interfaces. Therefore we directly feed the ASR network's output to the NLU network, which makes our SLU network trainable in an E2E manner without additional modules, such as Gumbel-Softmax.  <ref type="figure">Fig. 1</ref>: Various methods for leveraging the capacity of the pre-trained NLU network. From left to right, they are methodologies in the order described in Related Work. Note that the blocks with dotted lines are only used in the training procedure.</p><p>Finally, we evaluate our model on the SLURP dataset <ref type="bibr" target="#b12">[13]</ref>, a recently proposed challenging SLU dataset, and achieve state-of-the-art performance in both Intent Classification (IC) and Slot Filling (SF). We also conduct ablation studies to verify that the NLU network, pre-trained with Masked Language Modeling (MLM), can utilize a noisy textual representation of CTI. Moreover, we train our model with extra data, SLURP-Synth, and get better results.</p><p>Our main contributions can be summarized as follows: 1. We integrate the pre-trained networks by a novel Interface, CTI, in an E2E manner without any modules. 2. Our CTI integration allows each part of the model to be trained independently based on data type. 3. We evaluate our model on the SLURP dataset and achieve state-of-the-art scores in both IC and SF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Recently, the E2E SLU approaches are designed to understand contextual semantic information of speech by utilizing PLM <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. These works were studied in various ways, including KD, learning cross-modal latent space, and network integration via Interface. <ref type="figure">Fig. 1</ref> shows these works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Knowledge Distillation</head><p>Inspired by <ref type="bibr" target="#b15">[16]</ref>, KD-based E2E SLU models were proposed to leverage the representation power of PLM <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The SLU network can learn high-level semantic representations from the NLU network by distilling the knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Cross-Modal Shared Embedding</head><p>These approaches were designed to learn a cross-modal latent space by inducing the paired speech and transcript to become closer to each other in the common embedding space <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. The SLU network then has a cross-modal shared embedding space derived from both representations of ASR and NLU networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Network Integration with Interface</head><p>These approaches were proposed to combine ASR and NLU networks and train the models in an end-to-end manner. They extracted acoustic information from the ASR network, including logits, hidden features, or text, and feed them into the NLU network via Interface <ref type="bibr" target="#b16">[17]</ref>. If the Interface is not differentiable, additional modules such as Gumbel-softmax or attention gate are needed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Integration Method and Model Components</head><p>The SLU network we proposed consists of two main components: ASR and NLU networks. For simple and robust integration via CTI, both networks are pre-trained with the same vocabulary. Given speech input S, the ASR network produces tokens with a continuous probability distribution by sof tmax function and delivers the output vectors, Z to the NLU network directly as in Eq. (1). The NLU network finally outputs Y (intent, slot key, and slot value) as in Eq. <ref type="formula" target="#formula_0">(2)</ref>.</p><formula xml:id="formula_0">Z = sof tmax(ASR Dec (ASR Enc (S))) (1) Y = N LU (Z)<label>(2)</label></formula><p>We call this Z (the output of the ASR network) as CTI, which are considered few-hot encoded vectors from the point of view of the NLU network. We hypothesize that it is noisy but more informative than one-hot encoded vectors generated by the conventionally used argmax function.</p><p>If there is a confusing case between two tokens, the conventional ASR network makes only true/false binary predictions. In our proposed model, however, both pieces of information can be processed at the same time even with some noise. We may think of this as a richer feature that contains information about how the token sounds, rather than just noise.</p><p>As a result, the entire SLU network can be trained in an end-to-end manner, with the intermediate noisy text representation. <ref type="figure">Fig. 2</ref> shows our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">ASR Network : Wav2Vec 2.0 based Seq2Seq Model</head><p>Our ASR network is a Sequence-to-Sequence (Seq2Seq) model consisting of Wav2Vec 2.0 Encoder <ref type="bibr" target="#b18">[19]</ref> and Transformer Decoder <ref type="bibr" target="#b19">[20]</ref>, where the Wav2Vec 2.0 Encoder is known to have remarkable representation power by contrastive learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">NLU Network : RoBERTa</head><p>We use RoBERTa <ref type="bibr" target="#b20">[21]</ref>, one of the popular PLMs as our NLU network. We added an attention pooling layer, softmax classi-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>Our proposed model architecture. We integrate the ASR network and the NLU network with CTI. We can train the entire network in an end-to-end manner or each component independently.</p><p>fiers, and a Conditional Random Field (CRF) at the end of the NLU network to predict intent, slot key, and slot value each. We assume that our NLU network, trained with MLM, is suitable for understanding the noisy output of the ASR network. Because this NLU network has the ability to reconstruct randomly masked noisy sentences, it can utilize noisy text representations without additional adaptation steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Task Learning Losses</head><p>First, our proposed SLU network can be trained in an endto-end manner with SLU loss (L SLU ) from speech-to-label data because it is directly integrated with the CTI. With this Interface, we can train each component of the SLU network independently, even after integration. Thus, the ASR network can be trained with usual ASR loss (L ASR ) on speech-to-text data, and the NLU network can be trained with NLU loss (L N LU ) on text-to-label data. And we can use all the losses to train the entire SLU network.</p><formula xml:id="formula_1">L T otal = L SLU + L N LU + L ASR<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">SLU Loss</head><p>The SLU loss (L SLU ) consists of Speech-to-Intent Loss (L S2I ), Speech-to-Slot Key Loss (L S2K ) and Speech-to-Slot Value Loss (L S2V ). L S2I and L S2K are all Cross Entropy (CE) losses for intent, slot key classification each. For L S2K , we adopt in/out/begin (IOB) formats and pre-process the data to solve the misalignment between token and labels by the tokenizer <ref type="bibr" target="#b21">[22]</ref>. And L S2V loss is a summation of CE loss for each token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">NLU Loss</head><p>The NLU loss (L N LU ) consists of Text-to-Intent Loss (L T 2I ), Text-to-Slot Key Loss (L T 2K ) and Text-to-Slot Value Loss (L T 2V ). The NLU loss is similar to the SLU loss, but the input modality is text. Like the MLM task, we randomly mask some tokens of the gold transcript and put this masked sentence as input since it is too easy for the NLU network to predict the slot value from the pure sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">ASR Loss</head><p>The ASR loss (L ASR ) is a token classification loss that is commonly used for Seq2Seq ASR network <ref type="bibr" target="#b22">[23]</ref>. Without this, the quality of the ASR hypothesis slowly degrades in the process of E2E SLU training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS 4.1. Dataset</head><p>Even though there are some SLU benchmark datasets, such as Fluent Speech Commands (FSC) <ref type="bibr" target="#b2">[3]</ref>, their semantic complexity is insufficient to evaluate the capability of the NLU network <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. The E2E SLU baseline without the LM, Wav2Vec2.0-Classifier, outperforms the other models with state-of-the-art performance, as shown in <ref type="table">Table.</ref> 1. Therefore, we evaluate our proposed model on a more challenging dataset, SLURP <ref type="bibr" target="#b12">[13]</ref>, recently proposed for developing an in-home personal robot assistant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training</head><p>Firstly, we pre-train our ASR network on the LibriSpeech dataset <ref type="bibr" target="#b24">[25]</ref>. Here we use the pre-trained parameters of Wav2Vec 2.0 Encoder and Transformer Decoder with randomly initialized parameters. As with the SLURP paper, we fine-tune the ASR network on the SLURP dataset because it requires domain adaptation to far-range speech data. The Word Error Rate (WER) of our ASR network is 16.67, which is slightly lower than the 16.20 of the ASR network in the SLURP paper. We use pre-trained RoBERTa as the NLU network, and finally, we connect two components via our proposed method (CTI), then fine-tune the model on the SLURP dataset with multi-task losses. Note that, unlike other E2E SLU methods, each part of our model can behave like a stand-alone model. Therefore, we can train the NLU network with text-to-label data and the ASR network with speech-to-text data at will. We leave 46 audio files out of 50,628 training audio files during the fine-tuning because there are mismatches between speech and labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results and Analysis</head><p>We evaluate the performance of SLU models with the metric of IC accuracy and SLU-F1 score, where SLU-F1 is a new metric for slot filling tasks proposed with the SLURP dataset <ref type="bibr" target="#b12">[13]</ref>. The scores are shown in <ref type="table">Table.</ref> 2. There are three types of models: NLU, SLU (Inference Only), and SLU (E2E Train). The score of our NLU network is 87.73, which is higher than 84.84 of the NLU network from the SLURP paper in IC. To experiment with SLU (Inference Only) case, these stand-alone NLU networks need to be finetuned on the SLURP dataset. On the other hand, in SLU (E2E Train), the NLU networks are not fine-tuned independently on the SLURP dataset but fine-tuned in an end-to-end manner. Wav2Vec2.0-Classifier only shows 76.6 IC accuracy because it does not explicitly understand the language information. Among all SLU experiments, our model with all multi-task losses achieves state-of-the-art scores, 82.93 (IC) and 71.12 (SLU-F1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Effectiveness of CTI</head><p>We hypothesize that CTI makes NLU networks take richer information about the words that the ASR network confuses; the confusing words are represented as an interpolation of embedding vectors in the NLU network. We compare SLU models with DTI, Gumbel-Interface <ref type="bibr" target="#b10">[11]</ref>, and CTI. For SLU (Inference Only), CTI shows meaningful improvement compared to DTI by 0.8 (IC), and for SLU (E2E Train), CTI outperforms Gumbel-Interface by 0.83 (IC) and 0.57 (SLU-F1) under the same conditions except for the Interface. We assume that the comparison between Gumbel-Interface and CTI could be different from <ref type="bibr" target="#b16">[17]</ref> according to the domain of dataset. Moreover, we introduce more multi-task losses, such as L N LU compared to <ref type="bibr" target="#b16">[17]</ref> for training with each interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">NLU Loss</head><p>We train our model with and without L N LU because CTI allows NLU networks to be trained independently on text-only data. Adding L N LU increases the model score by 0.54 (IC) and 0.51 (SLU-F1). This shows that CTI can preserve the NLU network's powerful text representation, which is learned in the pre-training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Extra Data : SLURP-Synth</head><p>We conduct an additional experiment to check if using extra data improves the model performance as studied in <ref type="bibr" target="#b26">[27]</ref>. The extra data is SLURP-Synth where it consist of text, labels and synthesized speech by Google's Text-to-Speech system <ref type="bibr" target="#b12">[13]</ref>.</p><p>Here we leave 24,365 files that have out-of-distribution labels among the 69,253 files. We first train the entire network with additional Text-only training data. To do this, we extract the Transcript and Intent pairs from SLURP-synth. The results show that using additional Text-only data increases the IC by 2.5. Second, we train the entire network with SLURP-Synth data containing all text, labels, and synthesized speech. This increase the score by 2.58 (IC) and 3.58 (SLU-F1) than Extra data (Text-only) case, and we finally achieved 86.92 (IC) and 74.66 (SLU-F1) which is the state-of-the-art score. This means that critical information still remains in speech-to-label training that has not been fully discovered in text-to-label training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS AND FUTURE WORK</head><p>We propose a simple and robust method for integrating two pre-trained networks via novel Interface, CTI. We achieve state-of-the-art scores of 82.93 (IC), 71.12 (SLU-F1) on the SLURP dataset, and 86.92 (IC), 74.66 (SLU-F1) when we add the SLURP-Synth dataset. In the future, we plan to investigate a new pre-training strategy in which the NLU recovers some tokens corrupted by acoustic noise, such as phonemelevel ASR error distributions. Then the noise pattern of the NLU network inputs will match the ASR network outputs; the E2E SLU model will be less susceptible to the problem of representation gap.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Knowledge Distillation (b) Cross-Modal Shared Embedding (c) Network Integration with Interface</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The comparison of the IC accuracy between our baseline and other models on the FSC dataset.</figDesc><table><row><cell>Model (E2E SLU)</cell><cell>Input</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>Lugosh et al. [3]</cell><cell>Speech</cell><cell>-</cell><cell>98.8</cell></row><row><cell>Kim et al. [6]</cell><cell>Speech</cell><cell>97.8</cell><cell>99.7</cell></row><row><cell>Qian et al. [26]</cell><cell>Speech</cell><cell>-</cell><cell>99.7</cell></row><row><cell>Wav2Vec2.0-Classifier (Ours)</cell><cell>Speech</cell><cell>98.9</cell><cell>99.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The comparison of IC accuracy and SLU-F1 score between proposed models and baselines. ASR?NLU means inference with DTI and ASR?NLU means inference with CTI. In SLU (E2E Train), A, S, and N means ASR loss, SLU loss, and NLU loss for each.</figDesc><table><row><cell>Model Type</cell><cell>Model</cell><cell>Intent</cell><cell>SLU-F1</cell></row><row><cell>NLU</cell><cell>NLU [13] NLU (Ours)</cell><cell>84.84 87.73</cell><cell>-84.34</cell></row><row><cell>SLU (Inference Only)</cell><cell>ASR?NLU [13] ASR?NLU (Ours) ASR?NLU (Ours)</cell><cell>78.33 80.37 81.17</cell><cell>70.84 70.23 70.20</cell></row><row><cell></cell><cell>Wav2Vec2.0-Classifier (Ours)</cell><cell>76.6</cell><cell>-</cell></row><row><cell></cell><cell>Gumbel-Interface (A+S+N) [11]</cell><cell>82.10</cell><cell>70.55</cell></row><row><cell>SLU</cell><cell>CTI (A+S)</cell><cell>82.39</cell><cell>70.61</cell></row><row><cell>(E2E Train)</cell><cell>CTI (A+S+N)</cell><cell>82.93</cell><cell>71.12</cell></row><row><cell></cell><cell>CTI (A+S+N) + Extra data (Text-only)</cell><cell>84.34</cell><cell>71.08</cell></row><row><cell></cell><cell>CTI (A+S+N) + Extra data (All)</cell><cell>86.92</cell><cell>74.66</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Spoken language understanding: Systems for extracting semantic information from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R. De</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5754" to="5758" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Speech Model Pre-Training for End-to-End Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="814" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speech to Text Adaptation: Towards an Efficient Cross-Modal Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="896" to="900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Two-stage textual knowledge distillation to speech encoder for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7463" to="7467" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Tie your embeddings down: Cross-modal latent spaces for end-toend spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kunzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09044</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging unpaired text data for training end-to-end speech-to-intent systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-K</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hoory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7984" to="7988" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">St-bert: Cross-modal language model pre-training for end-toend spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7478" to="7482" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards semi-supervised semantics understanding from speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bodapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Self-Supervised Learning for Speech and Audio Processing</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do as i mean, not as i say: Sequence loss training for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dheram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7473" to="7477" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Categorical reparametrization with gumble-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Slurp: A spoken language understanding resource package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bastianelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vanzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="page" from="7252" to="7262" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semi-supervised spoken language understanding via self-supervised speech and language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Splat: Speechlanguage joint pre-training for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1897" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning and Representation Learning</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end spoken language understanding for generalized voice assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saxon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Top-down attention in end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mottini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6199" to="6203" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Using recurrent neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TASLP</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="4960" to="4964" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic complexity in end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saxon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Strimel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Speech-language pre-training for end-toend spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bianv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7458" to="7462" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using speech synthesis to train end-to-end spoken language understanding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="8499" to="8503" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
