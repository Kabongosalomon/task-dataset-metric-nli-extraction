<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qunliang</forename><surname>Xing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Guan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Mai</forename><forename type="middle">Xu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Tie Liu</roleName><forename type="first">Ren</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zulin</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">MFQE 2.0: A New Approach for Multi-frame Quality Enhancement on Compressed Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2019.2944806</idno>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Quality enhancement</term>
					<term>compressed video</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The past few years have witnessed great success in applying deep learning to enhance the quality of compressed image/video. The existing approaches mainly focus on enhancing the quality of a single frame, not considering the similarity between consecutive frames. Since heavy fluctuation exists across compressed video frames as investigated in this paper, frame similarity can be utilized for quality enhancement of low-quality frames given their neighboring high-quality frames. This task is Multi-Frame Quality Enhancement (MFQE). Accordingly, this paper proposes an MFQE approach for compressed video, as the first attempt in this direction. In our approach, we firstly develop a Bidirectional Long Short-Term Memory (BiLSTM) based detector to locate Peak Quality Frames (PQFs) in compressed video. Then, a novel Multi-Frame Convolutional Neural Network (MF-CNN) is designed to enhance the quality of compressed video, in which the non-PQF and its nearest two PQFs are the input. In MF-CNN, motion between the non-PQF and PQFs is compensated by a motion compensation subnet. Subsequently, a quality enhancement subnet fuses the non-PQF and compensated PQFs, and then reduces the compression artifacts of the non-PQF. Also, PQF quality is enhanced in the same way. Finally, experiments validate the effectiveness and generalization ability of our MFQE approach in advancing the state-of-the-art quality enhancement of compressed video. The code is available at https://github.com/RyanXingQL/MFQEv2.0.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. An example for quality fluctuation (top) and quality enhancement performance (bottom). enhancement approaches do not take any advantage of the information provided by neighboring frames, and thus their performance is severely limited. As <ref type="figure">Fig. 1</ref> shows, the quality of compressed video dramatically fluctuates across frames. Therefore, it is possible to use the high-quality frames (i.e., Peak Quality Frames, called PQFs 1 ) to enhance the quality of their neighboring low-quality frames (non-PQFs). This can be seen as Multi-Frame Quality Enhancement (MFQE), <ref type="bibr" target="#b0">1</ref>. PQF is defined as the frame whose quality is higher than both its previous frame and subsequent frame. similar to multi-frame super-resolution <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>, <ref type="bibr" target="#b29">[29]</ref>. This paper proposes an MFQE approach for compressed video. Specifically, we investigate that there exists large quality fluctuation in consecutive frames, for video sequences compressed by almost all compression standards. Thus, it is possible to improve the quality of a non-PQF with the help of its neighboring PQFs. To this end, we first train a Bidirectional Long Short-Term Memory (BiLSTM) based model as a no-reference method to detect PQFs. Then, a novel Multi-Frame CNN (MF-CNN) architecture is proposed for non-PQF quality enhancement, which takes both the current non-PQF and its adjacent PQFs as input. Our MF-CNN includes two components, i.e., Motion Compensation subnet (MC-subnet) and Quality Enhancement subnet (QE-subnet). The MC-subnet is developed to compensate motion between current non-PQF and its adjacent PQFs. The QE-subnet, with a spatio-temporal architecture, is designed to extract and merge the features of current non-PQF and compensated PQFs. Finally, the quality of the current non-PQF can be enhanced by QE-subnet which takes advantage of higher quality information provided by its adjacent PQFs. For example, as shown in <ref type="figure">Fig. 1</ref>, the current non-PQF (frame 95) and its nearest two PQFs (frames 92 and 96) are both fed into MF-CNN in our MFQE approach. As a result, the lowquality content (basketball) in non-PQF (frame 95) can be enhanced upon essentially the same but qualitatively better content in neighboring PQFs (frames 92 and 96). Moreover, <ref type="figure">Fig. 1</ref> shows that our MFQE approach also mitigates the quality fluctuation, due to the considerable quality improvement of non-PQFs. Note that our MFQE approach is also used for reducing compression artifacts of PQFs by using neighboring PQFs to enhance the quality of the currently processed PQF.</p><p>This work is an extended version of our conference paper <ref type="bibr" target="#b30">[30]</ref> (called MFQE 1.0 in this paper) with additional works and substantial improvements, thus called MFQE 2.0 (called MFQE in this paper for simplicity). The extension is as follows. <ref type="bibr" target="#b0">(1)</ref> We enlarge our database in <ref type="bibr" target="#b30">[30]</ref> from 70 to 160 uncompressed videos. On this basis, more thorough analyses of the compressed video are conducted. <ref type="bibr" target="#b1">(2)</ref> We develop a new PQF-detector, which is based on BiLSTM instead of the support vector machine (SVM) in <ref type="bibr" target="#b30">[30]</ref>. Our new detector is capable of extracting both spatial and temporal information of PQFs, leading to a boost in F 1 -score of PQF detection from 91.1% to 98.2%. (3) We advance our QE-subnet by introducing the multi-scale strategy, batch normalization <ref type="bibr" target="#b31">[31]</ref> and dense connection <ref type="bibr" target="#b32">[32]</ref>, rather than the conventional design of CNN in <ref type="bibr" target="#b30">[30]</ref>. Besides, we develop a lightweight structure for the QE-subnet to accelerate the speed of video quality enhancement. Experiments show that the average Peak Signal-to-Noise Ratio (PSNR) improvement on 18 sequences selected by <ref type="bibr" target="#b33">[33]</ref> largely increases from 0.455 dB to 0.562 dB (i.e., 23.5% improvement), while the number of parameters substantially reduces from 1,787,547 to 255,422 (i.e., 85.7% saving), resulting in at least 2 times acceleration of quality enhancement. <ref type="bibr" target="#b3">(4)</ref> More extensive experiments are provided to validate the performance and generalization ability of our MFQE approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Related works on quality enhancement</head><p>Recently, extensive works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref> have focused on enhancing the visual quality of compressed image. Specifically, Foi et al. <ref type="bibr" target="#b11">[12]</ref> applied point-wise Shape-Adaptive DCT (SA-DCT) to reduce the blocking and ringing effects caused by JPEG compression. Later, Jancsary et al. <ref type="bibr" target="#b13">[14]</ref> proposed reducing JPEG image blocking effects by adopting Regression Tree Fields (RTF). Moreover, sparse coding was utilized to remove the JPEG artifacts, such as <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b15">[16]</ref>. Recently, deep learning has also been successfully applied to improve the visual quality of compressed images. Particularly, Dong et al. <ref type="bibr" target="#b16">[17]</ref> proposed a four-layer AR-CNN to reduce the JPEG artifacts of images. Afterward, D <ref type="bibr" target="#b2">3</ref>  <ref type="bibr" target="#b18">[19]</ref> and Deep Dual-domain Convolutional Network (DDCN) <ref type="bibr" target="#b17">[18]</ref> were proposed as advanced deep networks for the quality enhancement of JPEG image, utilizing the prior knowledge of JPEG compression. Later, DnCNN was proposed in <ref type="bibr" target="#b19">[20]</ref> for several tasks of image restoration, including quality enhancement. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a 20-layer CNN for enhancing image quality. Most recently, the memory network (MemNet) <ref type="bibr" target="#b23">[23]</ref> has been proposed for image restoration tasks, including quality enhancement. In the MemNet, the memory block was introduced to generate the long-term memory across CNN layers, which successfully compensates the middleand high-frequency signals distorted during compression. It achieves the state-of-the-art quality enhancement performance for compressed images.</p><p>There are also some other works <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> proposed for the quality enhancement of compressed video. For example, the Variable-filter-size Residue-learning CNN (VRCNN) <ref type="bibr" target="#b34">[34]</ref> was proposed to replace the in-loop filters for HEVC intra-coding. However, the CNN in <ref type="bibr" target="#b34">[34]</ref> was designed as a component of the video encoder, so that it is not practical for already compressed video. Most recently, a Deep CNN-based Auto Decoder (DCAD), which contains 10 CNN layers, was proposed in <ref type="bibr" target="#b35">[35]</ref> to reduce the distortion of compressed video. Moreover, Yang et al. <ref type="bibr" target="#b24">[24]</ref> proposed the DS-CNN approach for video quality enhancement. In <ref type="bibr" target="#b24">[24]</ref>, DS-CNN-I and DS-CNN-B, as two subnetworks of DS-CNN, are used to reduce the artifacts of intra-and intercoding, respectively. All the above approaches can be seen as single-frame quality enhancement approaches, as they do not take any advantage of neighboring frames with high similarity. Consequently, their performance on video quality enhancement is severely limited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related works on multi-frame super-resolution</head><p>To our best knowledge, there exists no MFQE work for compressed video. The closest area is multi-frame video superresolution. In the early years, Brandi et al. <ref type="bibr" target="#b36">[36]</ref> and Song et al. <ref type="bibr" target="#b37">[37]</ref> proposed to enlarge video resolution by taking advantage of high-resolution key frames. Recently, many multi-frame super-resolution approaches have employed deep neural networks. For example, Huang et al. <ref type="bibr" target="#b38">[38]</ref>   in which the neighboring frames are warped according to the estimated motion, and then both the current and warped neighboring frames are fed into a super-resolution CNN to enlarge the resolution of the current frame. Later, Li et al. <ref type="bibr" target="#b28">[28]</ref> proposed replacing VSRnet by a deeper network with residual learning strategy. All these multi-frame methods exceed the limitation of single-frame approaches (e.g., SR-CNN <ref type="bibr" target="#b39">[39]</ref>) for super-resolution, which only utilize the spatial information within one single frame.</p><p>Recently, the CNN-based FlowNet <ref type="bibr" target="#b40">[40]</ref>, <ref type="bibr" target="#b41">[41]</ref> has been applied in <ref type="bibr" target="#b42">[42]</ref> to estimate the motion across frames for superresolution, which jointly trains the networks of FlowNet and super-resolution. Then, Caballero et al. <ref type="bibr" target="#b29">[29]</ref> designed a spatial transformer motion compensation network to detect the optical flow for warping neighboring frames. The current and warped neighboring frames were then fed into the Efficient Sub-Pixel Convolution Network (ESPCN) <ref type="bibr" target="#b43">[43]</ref> for super-resolution. Most recently, the Sub-Pixel Motion Compensation (SPMC) layer has been proposed in <ref type="bibr" target="#b44">[44]</ref> for video super-resolution. Besides, <ref type="bibr" target="#b44">[44]</ref> utilized Convolutional Long Short-Term Memory (ConvLSTM) to achieve the stateof-the-art performance on video super-resolution.</p><p>The aforementioned multi-frame super-resolution approaches are motivated by the fact that different observations of a same object or scene are highly likely to exist in consecutive frames of video. As a result, the neighboring frames may contain the content missed when down-sampling the current frame. Similarly, for compressed video, the low-quality frames can be enhanced by taking advantage of their adjacent frames with higher quality, because heavy quality fluctuation exists across compressed frames. Consequently, the quality of compressed videos may be effectively improved by leveraging the multi-frame information. To the best of our knowledge, our MFQE approach proposed in this paper is the first attempt in this direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYSIS OF COMPRESSED VIDEO</head><p>In this section, we first establish a large-scale database of raw and compressed video sequences (Section 3.1) for training the deep neural networks in our MFQE approach. We further analyze our database to investigate the frame-level quality fluctuation (Section 3.2) and the similarity between consecutive compressed frames (Section 3.3). The analysis results can be seen as the motivation of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Database</head><p>First, we establish a database including 160 uncompressed video sequences. These sequences are selected from the datasets of Xiph.org <ref type="bibr" target="#b45">[45]</ref>, VQEG <ref type="bibr" target="#b46">[46]</ref> and Joint Collaborative Team on Video Coding (JCT-VC) <ref type="bibr" target="#b47">[47]</ref>. The video sequences contained in our database are at large range of resolutions: SIF (352?240), CIF (352?288), NTSC (720?486), 4CIF (704?576), 240p (416?240), 360p (640?360), 480p (832?480), 720p (1280?720), 1080p (1920?1080), and WQXGA (2560?1600). Moreover, <ref type="figure">Fig. 2</ref> shows some typical examples of the sequences in our database, demonstrating the diversity of video content. Then, all video sequences are compressed by MPEG-1 <ref type="bibr" target="#b48">[48]</ref>, MPEG-2 <ref type="bibr" target="#b49">[49]</ref>, MPEG-4 <ref type="bibr" target="#b50">[50]</ref>, H.264/AVC <ref type="bibr" target="#b51">[51]</ref> and HEVC <ref type="bibr" target="#b52">[52]</ref> at different quantization parameters (QPs) 2 , to generate the corresponding video streams in our database. In addition, <ref type="figure">Fig. 4</ref> visualizes the subjective results of some frames in one video sequence, which is compressed by the latest HEVC standard. We can see that visual quality varies across compressed frames, also implying the frame-level quality fluctuation. Moreover, we measure the Standard Deviation (SD) of frame-level PSNR and Structural Similarity (SSIM) for each compressed video sequence, to quality fluctuation throughout the frames. Besides, the Peak-Valley Difference (PVD), which calculates the average difference between peak values and their nearest valley values, is also measured for both PSNR and SSIM curves of each compressed sequence. Note that the PVD reflects the quality difference between frames within a short period. The results of SD and PVD 2. FFmpeg is used for MPEG-1, MPEG-2, MPEG-4 and H.264/AVC compression, and HM16.5 is used for HEVC compression. are reported in <ref type="table" target="#tab_1">Table 1</ref>, which are averaged over all 160 video sequences in our database. <ref type="table" target="#tab_1">Table 1</ref> shows that the average SD values of PSNR are above 0.87 dB for all five compression standards. This implies that compressed video sequences exist heavy fluctuation along with frames. In addition, we can see from <ref type="table" target="#tab_1">Table 1</ref> that the average PVD results of PSNR are above 1 dB for MPEG-1, MPEG-2, MPEG-4 and HEVC, except that of H.264 (0.4732 dB). Therefore, the visual quality is dramatically different between PQFs and Valley Quality Frames (VQFs), such that it is possible to significantly improve the visual quality of VQFs given their neighboring PQFs. Note that similar results can be found for SSIM as shown in <ref type="table" target="#tab_1">Table 1</ref>. In summary, we can conclude that the significant frame-level quality fluctuation exists for various video compression standards in terms of both PSNR and SSIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Frame-level quality fluctuation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Similarity between neighboring frames</head><p>It is intuitive that the frames within a short time period are with high similarity. We thus evaluate the Correlation Coefficient (CC) values between each compressed frame and its previous/subsequent 10 frames, for all 160 sequences in our database. The mean and SD of the CC values are shown in <ref type="figure">Fig. 5</ref>, which are obtained from all sequences compressed by HEVC. We can see that the average CC values are larger than 0.75 and the SD values of CC are less than 0.20, when the period of two frames is within 10. Similar results can be found for other four video compression standards. This validates the high correlation of neighboring video frames. In addition, it is necessary to investigate the number of non-PQFs between these two neighboring PQFs, denoted by the Peak Separation (PS), since the quality enhancement of each non-PQF is based on two neighboring PQFs. <ref type="table" target="#tab_1">Table 1</ref> also reports the results of PS, which are averaged over all 160 video sequences in our database. We can see from this table <ref type="bibr" target="#b2">3</ref> that the PS values are considerably smaller than 10 frames, especially for the latest H.264 (PS = 2.0529) and HEVC (PS = 2.6641) standards. Such a short distance, together with the similarity results in <ref type="figure">Fig. 5</ref>, indicates the high similarity between two neighboring PQFs. Therefore, the PQFs probably contain some useful content that is distorted in their neighboring non-PQFs. Motivated by this, our MFQE approach is proposed to enhance the quality of non-PQFs through the advantageous information of the nearest PQFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED MFQE APPROACH</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Framework</head><p>The framework of our MFQE approach is shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. As seen in this figure, our MFQE approach first detects PQFs that are used for quality enhancement of non-PQFs. In practical application, raw sequences are not available in video quality enhancement, and thus PQFs and non-PQFs cannot be distinguished through comparison with raw sequences. Therefore, we develop a no-reference PQF detector for our MFQE approach, which is detailed in Section 4.2. Then, we propose a novel MF-CNN architecture to enhance the quality of non-PQFs, which takes advantage of the nearest PQFs, i.e., both previous and subsequent PQFs. As shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, the MF-CNN architecture is composed of the MCsubnet and the QE-subnet. The MC-subnet (introduced in Section 4.3) is developed to compensate the temporal motion between neighboring frames. To be specific, the MC-subnet firstly predicts the temporal motion between the current non-PQF and its nearest PQFs. Then, the two nearest PQFs are warped with the spatial transformer according to the estimated motion. As such, the temporal motion between non-PQF and PQFs can be compensated. Finally, the QE-subnet (introduced in Section 4.4), which has a spatio-temporal architecture, is proposed for quality enhancement. In the QE-subnet, both the current non-PQF and compensated PQFs are the inputs, and then the quality of the non-PQF can be enhanced with the help of the adjacent compensated 3. Note that this paper only defines PS according to PSNR rather than SSIM, but similar results can be found for SSIM. PQFs. Note that, in the proposed MF-CNN, the MC-subnet and QE-subnet are trained jointly in an end-to-end manner. Similarly, each PQF is also enhanced by MF-CNN with the help of its nearest PQFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BiLSTM-based PQF detector</head><p>In our MFQE approach, the no-reference PQF detector is based on a BiLSTM network. Recall that a PQF is the frame with higher quality than its adjacent frames. Thus, the features of the current and neighboring frames in both forward and backward directions are used together to detect PQFs. As revealed in Section 3.2, the PQF frequently appears in compressed video, leading to the quality fluctuation. Due to this, we apply the BiLSTM network <ref type="bibr" target="#b53">[53]</ref> as the PQF detector, in which the long-and short-term correlation between PQF and non-PQF can be extracted and modeled.</p><p>Notations. We first introduce the notations for our PQF detector. The consecutive frames in a compressed video are denoted by {f n } N n=1 , where n indicates the frame order and N is the total number of frames. Then, the corresponding output from BiLSTM is denoted by</p><formula xml:id="formula_0">{p n } N n=1 , in which p n is PQF F p NonPQF F n p Concatenate warp M ? 4 F ? ? 4 p warp M ? 2 F ? ? 2 p</formula><p>downscaling motion estimation the probability of f n being a PQF. Given {p n } N n=1 , the labels of PQFs for each frame can be determined and denoted by {l n } N n=1 . If f n is a PQF, then we have l n = 1; otherwise, we have l n = 0.</p><p>Feature Extraction. Before training, we extract 38 features for each f n . Specifically, 2 compressed domain features, i.e., the number of assigned bits and quantization parameters, are extracted at each frame for detecting the PQF, since they are strongly related to visual quality and can be directly obtained from bitstream. In addition, we follow the noreference quality assessment method <ref type="bibr" target="#b1">[2]</ref> to extract 36 features at pixel domain. Finally, the extracted features are in form of a 38-dimension vector as the input to BiLSTM.</p><p>Architecture. The architecture of the BiLSTM is shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. As seen in this figure, the LSTM is bidirectional, in order to extract and model the dependencies from both forward and backward directions. First, the input 38dimension feature vector is fed into 2 LSTM cells, corresponding to either forward or backward direction. Each of LSTM cells is composed of 128 units at one time step (corresponding to one video frame). Then, the outputs of the bi-directional LSTM cells are fused and sent to the fully connected layer with a sigmoid activation. Consequently, the fully connected layer outputs p n , as the probability of being the PQF frame. Finally, the PQF label l n can be yielded upon p n .</p><p>Postprocessing. In our PQF detector, we further refine the results from BiLSTM according to the prior knowledge of PQF. Specifically, the following two strategies are developed to refine the labels {l n } N n=1 of the PQF detector, where N is the total number of frames.</p><p>Strategy I: Remove the consecutive PQFs. According to the definition of PQF, it is impossible that the PQFs appear consecutively. Hence, if the consecutive PQFs exist:</p><formula xml:id="formula_1">{l n+i } j i=0 = 1 and l n?1 = l n+j+1 = 0, j ? 1,<label>(1)</label></formula><p>we refine the PQF labels according to their probabilities:</p><formula xml:id="formula_2">l n+i = 0, where i = arg max 0?k?j (p n+k ),<label>(2)</label></formula><p>so that only one PQF is left. Strategy II: Break the continuity of non-PQFs. According to the analysis in Section 3, PQFs frequently appear within a limited separation. For example, the average value of PS is 2.66 frames for HEVC compressed sequences. Here, we assume that D is the maximal separation between two PQFs. Given this assumption, if the results of {l n } N n=1 yield more than D consecutive zeros (non-PQFs):</p><formula xml:id="formula_3">{l n+i } d i=0 = 0 and l n?1 = l n+d+1 = 1, d &gt; D,<label>(3)</label></formula><p>then one of their corresponding frames {f n+i } d i=0 need to act as a PQF. Accordingly, we set:</p><formula xml:id="formula_4">l n+i = 1, where i = arg max 0&lt;k&lt;d (p n+k ).<label>(4)</label></formula><p>After refining {l n } N n=1 as discussed above, our PQF detector can locate PQFs and non-PQFs in the compressed video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MC-subnet</head><p>After detecting PQFs, our MFQE approach can enhance the quality of non-PQFs by taking advantage of their neighboring PQFs. Unfortunately, there exists considerable temporal motion between PQFs and non-PQFs. Hence, we develop the MC-subnet to compensate the temporal motion across frames, which is based on the CNN method of Spatial Transformer Motion Compensation <ref type="bibr" target="#b29">[29]</ref>.</p><p>Architecture. The architecture of STMC is shown in <ref type="figure">Fig. 8</ref>. Additionally, the convolutional layers of pixel-wise motion estimation are described in <ref type="table" target="#tab_3">Table 2</ref>. The same as <ref type="bibr" target="#b29">[29]</ref>, our MC-subnet adopts the convolutional layers to estimate the ?4 and ?2 down-scaling Motion Vector (MV) maps, denoted by M ?4 and M ?2 . Down-scaling motion estimation is effective to handle large scale motion. However, because of down-scaling, the accuracy of MV estimation is reduced. Therefore, in addition to STMC, we further develop some additional convolutional layers for pixel-wise motion estimation in our MC-subnet, which does not contain any down-scaling process. Then, the output of STMC includes the ?2 down-scaling MV map M ?2 and the corresponding compensated PQF F ?2 p . They are concatenated with the original PQF and non-PQF, as the input to the convolutional layers of the pixel-wise motion estimation. Consequently, the pixel-wise MV map can be generated, which is denoted by M. Note that the MV map M contains two channels, i.e., horizontal MV map M x and vertical MV map M y . Here, x and y are the horizontal and vertical index of each pixel. Given M x and M y , the PQF is warped to compensate the temporal motion. Let the compressed PQF and non-PQF be F p and F np , respectively. The compensated PQF F p can be expressed as </p><formula xml:id="formula_5">F p (x, y) = I{F p (x + M x (x, y), y + M y (x, y))},<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation of compensated PQFs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Short connection</head><p>Dense connection <ref type="figure">Fig. 9</ref>. The architecture of our QE-subnet. In the multi-scale feature extraction component (denoted by C1-C9), the filter sizes of C1/4/7, C2/5/8 and C3/6/9 are 3 ? 3, 5 ? 5 and 7 ? 7, respectively, and the filter number is set to 32 for each layer. Note that C1-C9 are directly applied to frames F p1 , Fnp or F p2 . In the densely connected mapping construction (denoted by C10-C14), the filter size and number are set to 3 ? 3 and 32, respectively. The last layer C15 has only one filter with the size of 3 ? 3. In addition, the PReLU activation is applied to C1-C14, while BN is applied to C10-C15.</p><p>where I{?} denotes bilinear interpolation. The reason for interpolation is that M x (x, y) and M y (x, y) may be noninteger values.</p><p>Training strategy. Since it is hard to obtain the ground truth of MV, the parameters of the convolutional layers for motion estimation cannot be trained directly. Instead, we can train the parameters by minimizing the MSE between the compensated adjacent frame and the current frame. Note that the similar training strategy is adopted in <ref type="bibr" target="#b29">[29]</ref> for motion compensation in video super-resolution tasks. However, in our MC-subnet, both the input F p and F np are compressed frames with quality distortion. Hence, when minimizing the MSE between F p and the F np , the MC-subnet learns to estimate the distorted MV, resulting in inaccurate motion estimation. Therefore, the MC-subnet is trained under the supervision of the raw frames. That is, we warp the raw frame of the PQF (denoted by F R p ) using the MV map output from the convolutional layers of motion estimation, and minimize the MSE between the compensated raw PQF (denoted by F R p ) and the raw non-PQF (denoted by F R np ). Mathematically, the loss function of the MC-subnet can be written by</p><formula xml:id="formula_6">L MC (? mc ) = ||F R p (? mc ) ? F R np || 2 2 ,<label>(6)</label></formula><p>where ? mc represents the trainable parameters of our MCsubnet. Note that the raw frames F R p and F R np are not required when compensating motion in test and practical use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">QE-subnet</head><p>Given the compensated PQFs, the quality of non-PQFs can be enhanced through the QE-subnet. To be specific, the non-PQF F np , together with the compensated previous and subsequent PQFs (F p1 and F p2 ), are fed into the QE-subnet. This way, both the spatial and temporal features of these three frames are extracted and fused, such that the advantageous information in the adjacent PQFs can be used to enhance the quality of the non-PQF. It differs from the conventional CNN-based single-frame quality enhancement approaches, which can only handle the spatial information within one single frame.</p><p>Architecture. The architecture of QE-subnet is shown in <ref type="figure">Fig. 9</ref>. The QE-subnet consists of two key lightweight components: multi-scale feature extraction (denoted by C1-9) and densely connected mapping construction (denoted by C10-14).</p><p>? Multi-scale feature extraction. The input to the QEsubnet is non-PQF F np and its neighboring compensated PQFs F p1 and F p2 . Then, the spatial features of F np , F p1 and F p2 are extracted by multi-scale convolutional filters, denoted by C1-9. Specifically, the filter size of C1,4,7 is 3 ? 3, while the filter sizes of C2,5,8 and C3,6,9 are 5 ? 5 and 7 ? 7, respectively. The filter numbers of C1-9 are all 32. After feature extraction, 288 feature maps filtered at different scales are obtained. Subsequently, all feature maps from F np , F p1 and F p2 are concatenated, and then flow into the dense connection component. ? Densely connected mapping construction. After obtaining the feature maps from F np , F p1 and F p2 , a densely connected architecture is applied to construct the nonlinear mapping from feature maps to enhancement residual. Note that enhancement residual refers to the difference between original and enhanced frames. To be specific, there are 5 convolutional layers in the nonlinear mapping of the densely connected architecture.</p><p>Each of them has 32 convolutional filters with size of 3 ? 3. In addition, dense connection <ref type="bibr" target="#b32">[32]</ref> is adopted to encourage feature reuse, strengthen feature propagation and mitigate the vanishing-gradient problem. Moreover, Batch Normalization (BN) <ref type="bibr" target="#b31">[31]</ref> is applied to all 5 layers after PReLU activation to reduce internal covariate shift, thus accelerating the training process. We denote the composite non-linear mapping as H l (?), including Convolution (Conv), PReLU and BN. We further denote the output of the l-th layer as x l , such </p><p>where [x 10 , x 11 , ..., x 14 ] refers to the concatenation of the feature maps produced in layers C10-C14. Finally, the enhanced non-PQF F en is generated by the pixel-wise summation of learned enhancement residual R np (? qe ) and input non-PQF F np</p><formula xml:id="formula_8">F en = F np + R np (? qe ),<label>(8)</label></formula><p>where ? qe is defined as the trainable parameters of the QE-subnet.</p><p>Training strategy. The MC-subnet and QE-subnet in our MF-CNN are trained jointly in an end-to-end manner. Recall that F R p1 and F R p2 are defined as the raw frames of the previous and incoming PQFs, respectively. The loss function of our MF-CNN can be formulated as</p><formula xml:id="formula_9">L MF (? mc , ? qe ) = a ? 2 i=1 ||F R pi (? mc ) ? F R np || 2 2 LMC: loss of MC-subnet +b ? F np + R np (? qe ) ? F R np 2 2 LQE: loss of QE-subnet .<label>(9)</label></formula><p>As <ref type="formula" target="#formula_9">(9)</ref> indicates, the loss function of the MF-CNN is the weighted sum of L MC and L QE , which are the 2 -norm training losses of MC-subnet and QE-subnet, respectively. We divide the training into 2 steps. In the first step, we set a b, considering that F p1 and F p2 generated by MCsubnet are the basis of the following QE-subnet, and thus the convergence of MC-subnet is the primary target. After the convergence of L MC is observed, we set a b to minimize the MSE between F np + R np and F R np . Finally, the MF-CNN model can be trained for video quality enhancement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Settings</head><p>In this section, the experimental results are presented to validate the effectiveness of our MFQE 2.0 approach. Note that our MFQE 2.0 approach is called MFQE in this paper, while the MFQE approach of our conference paper <ref type="bibr" target="#b30">[30]</ref> is named as MFQE 1.0 for comparison. In our database, except For the BiLSTM-based PQF detector, the hyperparameter D of (3) is set to 3 in post-processing 4 , because the average value of PS is 2.66 frames for HEVC compressed sequences. In addition, the LSTM length is set to 8. Before training the MF-CNN, the raw and compressed sequences are segmented into 64 ? 64 patches as the training samples. The batch size is set to be 128. We apply the Adam algorithm <ref type="bibr" target="#b54">[54]</ref> with the initial learning rate as 10 ?4 to minimize the loss function <ref type="bibr" target="#b8">(9)</ref>. It is worth mentioning that the MC-subnet may be unable to converge, if the initial learning rate is oversize, e.g., 10 ?3 . For QE subnet, we set a = 1 and b = 0.01 in (9) at first to make the MC-subnet convergent. After the convergence, we set a = 0.01 and b = 1, so that the QE-subnet can converge faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance of the PQF detector</head><p>The performance of PQF detection is critical, since it is the first process of our MFQE approach. Thus, we evaluate the performance of our BiLSTM-based approach in PQF detection. For evaluation, we measure precision, recall and F 1score of PQF detection over all 18 test sequences compressed at five QPs (= 22, 27, 32, 37 and 42). The average results are shown in <ref type="table" target="#tab_5">Table 3</ref>. In this table, we also list the results of PQF detection by the SVM-based approach of MFQE 1.0 as reported in <ref type="bibr" target="#b30">[30]</ref>. Note that the results of only two QPs (= 37 and 42) are reported in <ref type="bibr" target="#b30">[30]</ref>.</p><p>We can see from <ref type="table" target="#tab_5">Table 3</ref> that the proposed BiLSTMbased PQF detector in MFQE 2.0 performs well in terms of precision, recall and F 1 -score. For example, at QP = 37, the average precision, recall and F 1 -score of our BiLSTM-based PQF detector are 100.0%, 96.5% and 98.2%, considerably higher than those of the SVM-based approach in MFQE 1.0. More importantly, the PQF detection of our approach is robust to all 5 QPs, since the average values of F 1score are all above 90%. In addition, <ref type="table" target="#tab_6">Table 4</ref> shows the performance of our BiLSTM-based PQF detector over each of 18 test sequences compressed at QP = 37. As seen in this table, the high performance is achieved by our PQF detector for almost all sequences, as only the recall of sequence BQSquare is below 90%. In conclusion, the effectiveness of our BiLSTM-based PQF detector is validated, laying a firm foundation for our MFQE approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Performance of our MFQE approach</head><p>In this section, we evaluate the quality enhancement performance of our MFQE approach in terms of ?PSNR, which measures the PSNR gap between the enhanced and original compressed sequences. In addition, the structural similarity (SSIM) index is also evaluated. Then, the performance of our MFQE approach is compared with those of AR-CNN <ref type="bibr" target="#b16">[17]</ref>, DnCNN <ref type="bibr" target="#b19">[20]</ref>, Li et al. <ref type="bibr" target="#b20">[21]</ref>, DCAD <ref type="bibr" target="#b35">[35]</ref> and DS-CNN <ref type="bibr" target="#b25">[25]</ref>. Quality enhancement on non-PQFs. Our MFQE approach mainly focuses on enhancing the quality of non-PQFs using the neighboring multi-frame information. Therefore, we first assess the quality enhancement of non-PQFs. <ref type="figure" target="#fig_5">Fig. 10</ref> shows the ?PSNR and ?SSIM results averaged over PQFs and non-PQFs of all 18 test sequences compressed at 4 different QPs. As shown, our MFQE approach significantly outperforms other approaches on non-PQF enhancement. The average improvement of non-PQF quality is 0.614 dB and 0.012 in SSIM, while that of the second-best approach is 0.317 dB in PSNR and 0.007 in SSIM. We can further see from <ref type="figure" target="#fig_5">Fig. 10</ref> that our MFQE approach has a considerably larger PSNR improvement for non-PQFs, compared to that for PQFs. By contrast, for compared approaches, the PSNR improvement of non-PQFs is similar to or even less than that of PQFs. In a word, the above results validate the outstanding effectiveness of our MFQE approach in enhancing the quality of non-PQFs.</p><p>Overall quality enhancement.    <ref type="table" target="#tab_7">Table 5</ref>. This demonstrates the robustness of our MFQE approach in enhancing video quality. This is mainly attributed to the significant improvement on the quality of non-PQFs, which is the majority of compressed video frames.</p><p>Rate-distortion performance. We further evaluate the ratedistortion performance of our MFQE approach by comparing with other approaches. First, <ref type="figure" target="#fig_7">Fig. 11</ref> shows the rate-distortion curves of our and other state-of-the-art approaches over four selected sequences. Note that the results of the DCAD and DS-CNN approaches are plotted in this figure, since they perform better than other compared ap- proaches. We can see from <ref type="figure" target="#fig_7">Fig. 11</ref> that our MFQE approach performs better than other approaches in rate-distortion performance. Then, we quantify the rate-distortion performance by evaluating the BD-bitrate (BD-BR) reduction, which is calculated over the PSNR results of five QPs (= 22, 27, 32, 37 and 42). The results are presented in <ref type="table" target="#tab_9">Table 6</ref>. As can be seen, the BD-BR reduction of our MFQE approach is 14.06% on average, while that of the second-best approach DCAD is only 8.89% on average. In general, the quality enhancement of our MFQE approach is equivalent to improving rate-distortion performance.</p><p>Quality fluctuation. Apart from the compression artifacts, the quality fluctuation in compressed videos may also lead to degradation of QoE <ref type="bibr" target="#b55">[55]</ref>, <ref type="bibr" target="#b56">[56]</ref>, <ref type="bibr" target="#b57">[57]</ref>. Fortunately, our MFQE approach is beneficial to mitigate the quality fluctuation, because of its significant quality improvement on non-PQFs as found in <ref type="figure" target="#fig_5">Fig. 10</ref>. We evaluate the fluctuation of video quality in terms of the SD and PVD results of PSNR curves, which are introduced in Section 3. <ref type="figure" target="#fig_8">Fig. 12</ref> shows the SD and PVD values averaged over all 18 test sequences, which are obtained from the quality enhancement approaches and the HEVC baseline. As shown in this figure, our MFQE approach succeeds in reducing the SD and PVD, while other five compared approaches enlarge the SD and PVD values over the HEVC baseline. The reason is that our MFQE approach has considerably larger PSNR improvement for non-PQFs than that for PQFs, thus reducing the quality gap between PQFs and non-PQFs. In addition, <ref type="figure" target="#fig_0">Fig. 13</ref> shows the PSNR curves of two selected test sequences, for our MFQE approach and the HEVC baseline. It can be seen that the PSNR fluctuation of our MFQE approach is significantly smaller than the HEVC baseline. In summary, our approach is also capable of reducing the quality fluctuation of video compression.</p><p>Subjective quality performance. <ref type="figure" target="#fig_10">Fig. 14</ref> shows the subjective quality performance on the sequences Fourpeople at QP = 37, BasketballPass at QP = 37 and RaceHorses at QP = 42. It can be observed that our MFQE approach reduces the compression artifacts much more effectively than other five  compared approaches. Specifically, the severely distorted content, e.g., the cheek in Fourpeople, the ball in BasketballPass and the horse's feet in RaceHorses, can be finely restored by our MFQE approach with multi-frame strategy. By contrast, such compression distortion can hardly be restored by the compared approaches, as they only use the single lowquality frame. Therefore, our MFQE approach also performs well in subjective quality enhancement.</p><p>Test speed. We evaluate the test speed of quality enhancement using a computer equipped with a CPU of Intel i7-8700 3.20GHz and a GPU of GeForce GTX 1080 Ti. Specifically, we measure the average frame per second (fps), when testing video sequences at different resolutions. Note that the test set has been divided into 5 classes at different resolutions in <ref type="bibr" target="#b33">[33]</ref>. The results averaged over sequences at different resolutions are reported in <ref type="table" target="#tab_10">Table 7</ref>. As shown in this table, when enhancing non-PQFs, MFQE 2.0 can achieve at least 2 times acceleration compared to MFQE 1.0. For PQFs, MFQE 2.0 is also considerably faster than MFQE 1.0. The reason is that the parameters of the MF-CNN architecture in MFQE 2.0 are significantly fewer than those in MFQE 1.0. In a word, MFQE 2.0 is efficient in video quality enhancement, and its efficiency is mainly due to its lightweight structure. Furthermore, we calculate the number of operations for the MFQE approach. For MFQE 1.0, there are 99,561 additions and 215,150,624 multiplications needed for enhancing a 64?64 patch, while those for MFQE 2.0 are 150,276 and 5,942,640. The reason for the dramatic reduction of operations is that we decrease the number of filters in the mapping structure of MF-CNN from 64 to 32, and relieve the burden of feature extraction by cutting the number of output feature maps from 128 to 32. At the same time, we deepen the mapping structure and introduce the dense strategy, batch normalization and residual learning. This way, the nonlinearity of MF-CNN is largely improved, while the number of parameters is effectively saved. In a word, MFQE 2.0 is efficient in video quality enhancement, and its efficiency is mainly due to the lightweight structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation study</head><p>PQF detector. In this section, we validate the necessity and effectiveness of utilizing PQFs to enhance the quality of non-PQFs. To this end, we retrain the MF-CNN model of our MFQE approach to enhance non-PQFs with the help of adjacent frames, instead of PQFs. The MF-CNN network and experiment settings are all consistent with those in Sections 4.3 and 5.1. The retrained model is represented by MFQE NF (i.e., MFQE with neighboring Frames), and the experimental results are shown in <ref type="figure" target="#fig_11">Fig. 15</ref>, which are obtained by averaging over all 18 test sequences compressed at QP = 37. We can see that our approach without considering PQFs can only result in 0.274 dB for ?PSNR gain. By contrast, as aforementioned, our approach with PQFs can achieve 0.562 dB enhancement in ?PSNR. Moreover, as validated in Section 5.3, our MFQE approach obtains considerably higher enhancement on non-PQFs, when compared to the single-frame approaches. In a word, the above ablation study demonstrates the necessity and effectiveness of utilizing PQFs in the video quality enhancement task. Besides, we test the MF-CNN model with ground truth PQFs. Specifically, the ground truth PQF labels are obtained according to the PSNR curves and the definition of PQFs. The experimental results (denoted by MFQE GT, i.e., MFQE with Ground Truth PQFs) are shown in <ref type="figure" target="#fig_11">Fig. 15</ref>. As we can see, the average ?PSNR is 0.563 dB. This indicates an upper bound on the performance with respect to PQF estimation.</p><p>Also, we test the impact of post-processing of the PQF detector, i.e., removing the neighboring PQFs and inserting PQFs between two PQFs with long distance. Specifically, we test the F 1 -score of the PQF detector without postprocessing, and further evaluate its performance on quality enhancement (denoted by MFQE NP, i.e., MFQE with No Post-processing) in terms of ?PSNR. The average F 1score with post-processing slightly increases from 98.15% to 98.21% compared to the detector without post-processing. Additionally, the average ?PSNR decreases by 0.001 dB  after removing post-processing. Although the ?PSNR improvement by taking post-processing is minor, the postprocessing is still necessary in some extreme cases, where post-processing can prevent MFQE approach from inaccuate motion compensation and inferior quality enhancement. Take sequence KristenAndSara as an example. The non-PQF labels of frames 273 and 277 are corrected to PQFs. Consequently, the average ?PSNR of frames 270 to 280 can increase from 0.659 dB to 0.724 dB after using post-processed labels. Finally, we conduct an experiment to validate the improvement of quality enhancement after replacing SVM with BiLSTM both in training and evaluation, which is an advancement of MFQE 2.0 over MFQE 1.0. Specifically, we first replace the BiLSTM detector of MFQE 2.0 with SVM in the detection stage. Then, we retrain and test the model (denoted by MFQE SVM) which consists of the SVM based detector and MF-CNN. The average ?PSNR decreases from 0.562 dB to 0.528 dB (i.e., 6.0% degradation). This validates the contribution of the improved PQF detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MFQE_GT</head><p>Multi-scale and dense connection strategy. We further validate the effectiveness of the multi-scale feature extraction strategy and the densely connected structure in enhancing video quality. First, we ablate all dense connections in the QE-subnet of our MFQE approach. In addition, we increase the filter number of C11 from 32 to 50, so that the number of trainable parameters can be maintained for fair comparison. The corresponding retrained model is denoted by MFQE ND (i.e., MFQE with No Dense connection). Second, we ablate the multi-scale structure in the QE-subnet. Based on the dense-ablated network above, we fix all kernel sizes of the feature extraction component to 5 ? 5. Other parts of the MFQE approach and experiment settings are all the same as those in Sections 4 and 5.1. Accordingly, the retrained model is represented as MFQE GC (i.e., MFQE with General CNN). <ref type="figure" target="#fig_11">Fig. 15</ref> shows the ablation results, which are also averaged over all 18 test sequences at QP = 37. As seen in this table, the PSNR improvement decreases from 0.562 dB to 0.299 dB (i.e., 46.8% degradation) when disabling the dense connections, and then it reduces to 0.278 dB (i.e, 50.5% degradation) when further ablating the multi-scale structure. This indicates the effectiveness of our multi-scale strategy and the densely connected structure.</p><p>Enlarged database. One of the contributions in this paper is that we enlarge our database from 70 to 160 uncompressed video sequences. Here, we verify the effectiveness of the enlarged database over our previous database <ref type="bibr" target="#b30">[30]</ref>. Specifically, we test the performance of our MFQE approach trained over the database in <ref type="bibr" target="#b30">[30]</ref>. Then, the performance is evaluated on all 18 test sequences at QP = 37. The retrained model with its corresponding test result is represented by MFQE PD (i.e., MFQE with the Previous Database) in <ref type="figure" target="#fig_11">Fig. 15</ref>. We can see that MFQE 2.0 achieves substantial improvement on quality enhancement compared with MFQE-A7. In particular, the performance of MFQE 2.0 improves ?PSNR from 0.533 dB to 0.562 dB on average. Hence, our enlarged database is effective in improving video quality enhancement performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Generalization ability of our MFQE approach</head><p>Transfer to H.264. We verify the generalization ability of our MFQE approach for video sequences compressed by another standard. To this end, we test our MFQE approach on the 18 test sequences compressed by H.264 at QP = 37. Note that the test model is the same as that in Section 5.3, which is trained over the training set compressed by HEVC at QP = 37. Consequently, the average PSNR improvement is 0.422 dB. Also, we test the performance of MFQE model retrained over H.264 dataset. The average PSNR improvement is 0.464 dB. In a word, the MFQE model trained over HEVC dataset performs well on H.264 videos, and the MFQE model retrained on H.264 can slightly improve the performance of quality enhancement. This implies the high generalization ability of our MFQE approach across different compression standards.</p><p>Performance on other sequences. It is worth mentioning that the test set in <ref type="bibr" target="#b30">[30]</ref> is different from that in this paper. In our previous work <ref type="bibr" target="#b30">[30]</ref>, 10 test sequences are randomly  <ref type="table" target="#tab_12">Table 8</ref>. Note that 4 test sequences among the 10 test sequences overlap with the 18 test sequences of the above experiments. We can see from <ref type="table" target="#tab_12">Table 8</ref> that our approach has 0.680 dB improvement in ?PSNR and again outperforms other approaches. In this table, the results of compared approaches are also better than those reported in <ref type="bibr" target="#b30">[30]</ref> and their papers. It is because of retraining over the enlarged database. In conclusion, our MFQE approach has high generalization ability over different test sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we have proposed a CNN-based MFQE approach to enhance the quality of compressed video by reducing compression artifacts. Differing from the conventional single-frame quality enhancement approaches, our MFQE approach improves the quality of one frame by utilizing its nearest PQFs that have higher quality. To this end, we developed a BiLSTM-based PQF detector to classify PQFs and non-PQFs in compressed video. Then, we proposed a novel CNN framework, called MF-CNN, to enhance the quality of non-PQFs. Specifically, our MF-CNN framework consists of two subnets, i.e., the MC-subnet and QE-subnet. First, the MC-subnet compensates motion between PQFs and non-PQFs. Subsequently, the QE-subnet enhances the quality of each non-PQF by feeding the current non-PQF and the nearest compensated PQFs. In addition, PQF quality is enhanced in the same way. Finally, extensive experimental results showed that our MFQE approach significantly improves the quality of compressed video, superior to other state-of-the-art approaches. Consequently, the overall quality can be significantly enhanced, with considerably higher quality and less quality fluctuation than other approaches. There may exist two research directions for future work. (1) Our work in this paper only takes PSNR and SSIM as the objective metrics to be enhanced. The potential future work may further embrace perceptual quality metrics in our approach to improve the Quality of Experience (QoE) in video quality enhancement. (2) Our work mainly focuses on the quality enhancement at the decoder side. To further improve the performance of quality enhancement, information from the encoder, such as the partition of coding units, can be utilized. This is a promising future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>PSNR (dB) curves of compressed video by various compression standards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>28 Fig. 4 .Fig. 5 .</head><label>2845</label><figDesc>An example of frame-level quality fluctuation in video Football compressed by HEVC. The average CC value of each pair of adjacent frames in HEVC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>shows the PSNR curves of 6 video sequences, which are compressed by different compression standards. It can be seen that PSNR significantly fluctuates along with the compressed frames. This indicates that there exists considerable quality fluctuation in compressed video sequences for MPEG-1, MPEG-2, MPEG-4, H.264/AVC and HEVC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>The framework of our proposed MFQE approach. Both non-PQFs and PQFs are enhanced by MF-CNN with the help of their nearest previous and subsequent PQFs. Note that the networks of enhancing PQFs and non-PQFs are trained, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>The architecture of our BiLSTM based PQF detector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 10 .</head><label>10</label><figDesc>Average results of ?PSNR (dB) and ?SSIM for PQFs and non-PQFs in all test sequences at different QPs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Among them, AR-CNN, DnCNN and Li et al. are the latest quality enhancement approaches for compressed images, while DCAD and DS-CNN are the state-of-the-art video quality enhancement approaches. For fair comparison, all compared approaches are retrained over our training set, the same as our MFQE approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 11 .</head><label>11</label><figDesc>Rate-distortion curves of four test sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Averaged SD and PVD of test sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 13 .</head><label>13</label><figDesc>PSNR curves of HEVC baseline and our MFQE approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 14 .</head><label>14</label><figDesc>Subjective quality performance on Fourpeople at QP = 37, BasketballPass at QP = 37 and RaceHorses at QP = 42.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 15 .</head><label>15</label><figDesc>Overall ?PSNR (dB) of test sequences in ablation study. The explanations of abbreviations are as follows: (1) GT: Ground Truth PQFs. (2) NP: No Post-processing. (3) PD: Previous Database. (4) SVM: SVMbased detector. (5) ND: No Dense connection. (6) GC: General CNN. (7) NF: neighboring Frames serving as "PQFs".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 2. Examples of video sequences in our enlarged database.</figDesc><table><row><cell></cell><cell>HEVC</cell><cell>H.264</cell><cell>MPEG-4</cell><cell></cell><cell>MPEG-2</cell><cell>MPEG-1</cell><cell>HEVC</cell><cell>H.264</cell><cell></cell><cell>MPEG-4</cell><cell>MPEG-2</cell><cell>MPEG-1</cell><cell></cell><cell>HEVC</cell><cell>H.264</cell><cell>MPEG-4</cell><cell></cell><cell>MPEG-2</cell><cell>MPEG-1</cell></row><row><cell>32</cell><cell cols="2">PartyScene</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>37</cell><cell></cell><cell></cell><cell cols="3">BasketballPass</cell><cell>33</cell><cell></cell><cell></cell><cell cols="4">BlowingBubbles</cell></row><row><cell>28 30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>31 33 35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>29 31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames</cell><cell>27 29</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames</cell><cell>25 27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames</cell></row><row><cell>200</cell><cell>245</cell><cell>290</cell><cell>335</cell><cell>380</cell><cell>425</cell><cell>470</cell><cell>0</cell><cell>45</cell><cell>90</cell><cell>135</cell><cell>180</cell><cell>225</cell><cell>0</cell><cell>45</cell><cell>90</cell><cell>135</cell><cell></cell><cell>180</cell><cell>225</cell></row><row><cell></cell><cell>HEVC</cell><cell>H.264</cell><cell>MPEG-4</cell><cell></cell><cell>MPEG-2</cell><cell>MPEG-1</cell><cell>HEVC</cell><cell>H.264</cell><cell></cell><cell>MPEG-4</cell><cell>MPEG-2</cell><cell>MPEG-1</cell><cell></cell><cell>HEVC</cell><cell>H.264</cell><cell>MPEG-4</cell><cell></cell><cell>MPEG-2</cell><cell>MPEG-1</cell></row><row><cell>32 34</cell><cell cols="2">BQSquare</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell cols="2">RaceHorses</cell><cell></cell><cell></cell><cell></cell><cell>36</cell><cell>BQMall</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>34</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames</cell><cell>26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames</cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frames</cell></row><row><cell>200</cell><cell>245</cell><cell>290</cell><cell>335</cell><cell>380</cell><cell>425</cell><cell>470</cell><cell>80</cell><cell cols="2">125</cell><cell>170</cell><cell></cell><cell>215</cell><cell>200</cell><cell>245</cell><cell>290</cell><cell>335</cell><cell>380</cell><cell>425</cell><cell>470</cell></row></table><note>de- veloped a Bidirectional Recurrent Convolutional Network (BRCN), which improves the super-resolution performance over traditional single-frame approaches. Kappeler et al. proposed a Video Super-Resolution network (VSRnet) [27],</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Averaged SD, PVD and PS values of our database.</figDesc><table><row><cell>Metrics</cell><cell>MPEG-1</cell><cell>MPEG-2</cell><cell>MPEG-4</cell><cell>H.264</cell><cell>HEVC</cell></row><row><cell></cell><cell></cell><cell cols="2">PSNR (dB)</cell><cell></cell><cell></cell></row><row><cell>SD</cell><cell>2.2175</cell><cell>2.2273</cell><cell>2.1261</cell><cell>1.6899</cell><cell>0.8788</cell></row><row><cell>PVD</cell><cell>1.1553</cell><cell>1.1665</cell><cell>1.0842</cell><cell>0.4732</cell><cell>1.1734</cell></row><row><cell></cell><cell></cell><cell>SSIM</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SD</cell><cell>0.0717</cell><cell>0.0726</cell><cell>0.0735</cell><cell>0.0552</cell><cell>0.0105</cell></row><row><cell>PVD</cell><cell>0.0387</cell><cell>0.0391</cell><cell>0.0298</cell><cell>0.0102</cell><cell>0.0132</cell></row><row><cell></cell><cell></cell><cell cols="2">Separation (frames)</cell><cell></cell><cell></cell></row><row><cell>PS</cell><cell>5.3646</cell><cell>5.4713</cell><cell>5.4123</cell><cell>2.0529</cell><cell>2.6641</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2</head><label>2</label><figDesc>Convolutional layers for pixel-wise motion estimation.</figDesc><table><row><cell>Layers</cell><cell cols="5">Conv 1 Conv 2 Conv 3 Conv 4 Conv 5</cell></row><row><cell>Filter size</cell><cell>3 ? 3</cell><cell>3 ? 3</cell><cell>3 ? 3</cell><cell>3 ? 3</cell><cell>3 ? 3</cell></row><row><cell>Filter number</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>24</cell><cell>2</cell></row><row><cell>Stride</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell></row><row><cell>Function</cell><cell>PReLU</cell><cell>PReLU</cell><cell>PReLU</cell><cell>PReLU</cell><cell>Tanh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3</head><label>3</label><figDesc>Performance of our PQF detector on test sequences.</figDesc><table><row><cell>Approach</cell><cell>QP</cell><cell cols="3">Precision Recall F 1 -score (%) (%) (%)</cell></row><row><cell></cell><cell>22</cell><cell>100.0</cell><cell>95.9</cell><cell>97.8</cell></row><row><cell></cell><cell>27</cell><cell>98.2</cell><cell>94.1</cell><cell>96.1</cell></row><row><cell>MFQE 2.0</cell><cell>32</cell><cell>100.0</cell><cell>84.3</cell><cell>90.7</cell></row><row><cell></cell><cell>37</cell><cell>100.0</cell><cell>96.5</cell><cell>98.2</cell></row><row><cell></cell><cell>42</cell><cell>100.0</cell><cell>97.3</cell><cell>98.6</cell></row><row><cell>MFQE 1.0</cell><cell>37 42</cell><cell>90.7 94.0</cell><cell>92.1 90.9</cell><cell>91.1 92.2</cell></row><row><cell cols="5">that each layer can be formulated as follows,</cell></row></table><note>x 11 = H 11 ([x 10 ]) x 12 = H 12 ([x 10 , x 11 ]) x 13 = H 13 ([x 10 , x 11 , x 12 ]) x 14 = H 14 ([x 10 , x 11 , x 12 , x 13 ]),</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Performance of our PQF detector on test sequences at QP = 37.</figDesc><table><row><cell cols="2">Sequence</cell><cell cols="3">Precision Recall F 1 -score (%) (%) (%)</cell></row><row><cell>A</cell><cell>Traffic PeopleOnStreet</cell><cell>100.0 100.0</cell><cell>97.4 97.4</cell><cell>98.7 98.7</cell></row><row><cell></cell><cell>Kimono</cell><cell>100.0</cell><cell>98.4</cell><cell>99.2</cell></row><row><cell></cell><cell>ParkScene</cell><cell>100.0</cell><cell>98.4</cell><cell>99.2</cell></row><row><cell>B</cell><cell>Cactus</cell><cell>100.0</cell><cell>99.2</cell><cell>99.6</cell></row><row><cell></cell><cell>BQTerrace</cell><cell>100.0</cell><cell>96.2</cell><cell>98.0</cell></row><row><cell></cell><cell>BasketballDrive</cell><cell>100.0</cell><cell>97.4</cell><cell>98.7</cell></row><row><cell></cell><cell>RaceHorses</cell><cell>100.0</cell><cell>93.8</cell><cell>96.8</cell></row><row><cell>C</cell><cell>BQMall PartyScene</cell><cell>100.0 100.0</cell><cell>98.7 98.4</cell><cell>99.3 99.2</cell></row><row><cell></cell><cell>BasketballDrill</cell><cell>100.0</cell><cell>91.9</cell><cell>95.8</cell></row><row><cell></cell><cell>RaceHorses</cell><cell>100.0</cell><cell>94.9</cell><cell>97.4</cell></row><row><cell>D</cell><cell>BQSquare BlowingBubbles</cell><cell>100.0 100.0</cell><cell>86.2 98.4</cell><cell>92.6 99.2</cell></row><row><cell></cell><cell>BasketballPass</cell><cell>100.0</cell><cell>94.0</cell><cell>96.9</cell></row><row><cell></cell><cell>FourPeople</cell><cell>100.0</cell><cell>99.3</cell><cell>99.7</cell></row><row><cell>E</cell><cell>Johnny</cell><cell>100.0</cell><cell>98.0</cell><cell>99.0</cell></row><row><cell></cell><cell>KristenAndSara</cell><cell>100.0</cell><cell>99.3</cell><cell>99.7</cell></row><row><cell></cell><cell>Average</cell><cell>100.0</cell><cell>96.5</cell><cell>98.2</cell></row></table><note>for 18 standard test sequences of Joint Collaborative Team on Video Coding (JCT-VC) [33], other 142 sequences are randomly divided into non-overlapping training set (106 se- quences) and validation set (36 sequences). We compress all 160 sequences by HM16.5 under Low-Delay configuration, setting the Quantization Parameters (QPs) to 22, 27, 32, 37 and 42, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>presents the results of ?PSNR and ?SSIM, averaged over all frames of each test sequence. As shown in this table, our MFQE approach consistently outperforms all compared approaches. To be</figDesc><table /><note>specific, at QP = 37, the highest ?PSNR of our MFQE approach reaches 0.920 dB, i.e., for sequence PeopleOnStreet. The averaged ?PSNR of our MFQE approach is 0.562 dB, which is 23.5% higher than that of MFQE 1.0 (0.455 dB), 88.0% higher than that of Li et al. (0.299 dB), 74.5% higher than that of DCAD (0.322 dB), and 87.3% higher than that of DS-CNN (0.300 dB). Even higher ?PSNR improvement can</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5</head><label>5</label><figDesc>Overall comparison for ?PSNR (dB) and ?SSIM (?10 ?4 ) over test sequences at five QPs.</figDesc><table><row><cell>QP</cell><cell></cell><cell>Approach</cell><cell cols="2">AR-CNN [17] *</cell><cell cols="2">DnCNN [20]</cell><cell cols="2">Li et al. [21]</cell><cell cols="2">DCAD [35]</cell><cell cols="2">DS-CNN [25]</cell><cell cols="2">MFQE 1.0</cell><cell cols="2">MFQE 2.0</cell></row><row><cell></cell><cell></cell><cell>Metrics</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell><cell>PSNR</cell><cell>SSIM</cell></row><row><cell></cell><cell>A</cell><cell>Traffic PeopleOnStreet</cell><cell>0.239 0.346</cell><cell>47 75</cell><cell>0.238 0.414</cell><cell>57 82</cell><cell>0.293 0.481</cell><cell>60 92</cell><cell>0.308 0.500</cell><cell>67 95</cell><cell>0.286 0.416</cell><cell>60 85</cell><cell>0.497 0.802</cell><cell>90 137</cell><cell>0.585 0.920</cell><cell>102 157</cell></row><row><cell></cell><cell></cell><cell>Kimono</cell><cell>0.219</cell><cell>65</cell><cell>0.244</cell><cell>75</cell><cell>0.279</cell><cell>78</cell><cell>0.276</cell><cell>78</cell><cell>0.249</cell><cell>75</cell><cell>0.495</cell><cell>113</cell><cell>0.550</cell><cell>118</cell></row><row><cell></cell><cell></cell><cell>ParkScene</cell><cell>0.136</cell><cell>38</cell><cell>0.141</cell><cell>50</cell><cell>0.150</cell><cell>48</cell><cell>0.160</cell><cell>50</cell><cell>0.153</cell><cell>50</cell><cell>0.391</cell><cell>103</cell><cell>0.457</cell><cell>123</cell></row><row><cell></cell><cell>B</cell><cell>Cactus</cell><cell>0.190</cell><cell>38</cell><cell>0.195</cell><cell>48</cell><cell>0.232</cell><cell>58</cell><cell>0.263</cell><cell>58</cell><cell>0.239</cell><cell>58</cell><cell>0.439</cell><cell>88</cell><cell>0.501</cell><cell>100</cell></row><row><cell></cell><cell></cell><cell>BQTerrace</cell><cell>0.195</cell><cell>28</cell><cell>0.201</cell><cell>38</cell><cell>0.249</cell><cell>48</cell><cell>0.279</cell><cell>50</cell><cell>0.257</cell><cell>48</cell><cell>0.270</cell><cell>48</cell><cell>0.403</cell><cell>67</cell></row><row><cell></cell><cell></cell><cell>BasketballDrive</cell><cell>0.229</cell><cell>55</cell><cell>0.251</cell><cell>58</cell><cell>0.296</cell><cell>68</cell><cell>0.305</cell><cell>68</cell><cell>0.282</cell><cell>65</cell><cell>0.406</cell><cell>80</cell><cell>0.465</cell><cell>83</cell></row><row><cell></cell><cell></cell><cell>RaceHorses</cell><cell>0.219</cell><cell>43</cell><cell>0.253</cell><cell>65</cell><cell>0.276</cell><cell>65</cell><cell>0.282</cell><cell>65</cell><cell>0.267</cell><cell>63</cell><cell>0.340</cell><cell>55</cell><cell>0.394</cell><cell>80</cell></row><row><cell>37</cell><cell>C</cell><cell>BQMall PartyScene</cell><cell>0.275 0.107</cell><cell>68 38</cell><cell>0.281 0.131</cell><cell>68 48</cell><cell>0.325 0.131</cell><cell>88 45</cell><cell>0.340 0.164</cell><cell>88 48</cell><cell>0.330 0.174</cell><cell>80 58</cell><cell>0.507 0.217</cell><cell>103 73</cell><cell>0.618 0.363</cell><cell>120 118</cell></row><row><cell></cell><cell></cell><cell>BasketballDrill</cell><cell>0.247</cell><cell>58</cell><cell>0.331</cell><cell>68</cell><cell>0.376</cell><cell>88</cell><cell>0.386</cell><cell>78</cell><cell>0.352</cell><cell>68</cell><cell>0.477</cell><cell>90</cell><cell>0.579</cell><cell>120</cell></row><row><cell></cell><cell></cell><cell>RaceHorses</cell><cell>0.268</cell><cell>55</cell><cell>0.311</cell><cell>73</cell><cell>0.328</cell><cell>83</cell><cell>0.338</cell><cell>83</cell><cell>0.318</cell><cell>75</cell><cell>0.507</cell><cell>113</cell><cell>0.594</cell><cell>143</cell></row><row><cell></cell><cell>D</cell><cell>BQSquare BlowingBubbles</cell><cell>0.080 0.164</cell><cell>8 35</cell><cell>0.129 0.184</cell><cell>18 58</cell><cell>0.086 0.207</cell><cell>25 68</cell><cell>0.197 0.215</cell><cell>38 65</cell><cell>0.201 0.228</cell><cell>38 68</cell><cell>-0.010 0.386</cell><cell>15 120</cell><cell>0.337 0.533</cell><cell>65 170</cell></row><row><cell></cell><cell></cell><cell>BasketballPass</cell><cell>0.259</cell><cell>58</cell><cell>0.307</cell><cell>75</cell><cell>0.343</cell><cell>85</cell><cell>0.352</cell><cell>85</cell><cell>0.335</cell><cell>78</cell><cell>0.628</cell><cell>138</cell><cell>0.728</cell><cell>155</cell></row><row><cell></cell><cell></cell><cell>FourPeople</cell><cell>0.373</cell><cell>50</cell><cell>0.388</cell><cell>60</cell><cell>0.449</cell><cell>70</cell><cell>0.506</cell><cell>78</cell><cell>0.459</cell><cell>70</cell><cell>0.664</cell><cell>85</cell><cell>0.734</cell><cell>95</cell></row><row><cell></cell><cell>E</cell><cell>Johnny</cell><cell>0.247</cell><cell>10</cell><cell>0.315</cell><cell>40</cell><cell>0.398</cell><cell>60</cell><cell>0.410</cell><cell>50</cell><cell>0.378</cell><cell>40</cell><cell>0.548</cell><cell>55</cell><cell>0.604</cell><cell>68</cell></row><row><cell></cell><cell></cell><cell>KristenAndSara</cell><cell>0.409</cell><cell>50</cell><cell>0.421</cell><cell>60</cell><cell>0.485</cell><cell>68</cell><cell>0.524</cell><cell>70</cell><cell>0.481</cell><cell>60</cell><cell>0.655</cell><cell>75</cell><cell>0.754</cell><cell>85</cell></row><row><cell></cell><cell></cell><cell>Average</cell><cell>0.233</cell><cell>45</cell><cell>0.263</cell><cell>58</cell><cell>0.299</cell><cell>66</cell><cell>0.322</cell><cell>67</cell><cell>0.300</cell><cell>63</cell><cell>0.455</cell><cell>88</cell><cell>0.562</cell><cell>109</cell></row><row><cell>42</cell><cell></cell><cell>Average</cell><cell>0.285</cell><cell>96</cell><cell>0.221</cell><cell>77</cell><cell>0.318</cell><cell>105</cell><cell>0.324</cell><cell>109</cell><cell>0.310</cell><cell>101</cell><cell>0.444</cell><cell>130</cell><cell>0.589</cell><cell>165</cell></row><row><cell>32</cell><cell></cell><cell>Average</cell><cell>0.176</cell><cell>19</cell><cell>0.256</cell><cell>35</cell><cell>0.275</cell><cell>37</cell><cell>0.316</cell><cell>44</cell><cell>0.273</cell><cell>38</cell><cell>0.431</cell><cell>58</cell><cell>0.516</cell><cell>68</cell></row><row><cell>27</cell><cell></cell><cell>Average</cell><cell>0.177</cell><cell>14</cell><cell>0.272</cell><cell>24</cell><cell>0.295</cell><cell>28</cell><cell>0.316</cell><cell>30</cell><cell>0.267</cell><cell>23</cell><cell>0.399</cell><cell>34</cell><cell>0.486</cell><cell>42</cell></row><row><cell>22</cell><cell></cell><cell>Average</cell><cell>0.142</cell><cell>8</cell><cell>0.287</cell><cell>18</cell><cell>0.300</cell><cell>19</cell><cell>0.313</cell><cell>19</cell><cell>0.254</cell><cell>15</cell><cell>0.307</cell><cell>19</cell><cell>0.458</cell><cell>27</cell></row></table><note>* All compared approaches in this paper are retrained over our training set, the same as MFQE 2.0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6</head><label>6</label><figDesc>Overall BD-BR reduction (%) of test sequences with the HEVC baseline as an anchor. Calculated at QP = 22, 27, 32, 37 and 42.</figDesc><table><row><cell></cell><cell>Sequence</cell><cell>AR-CNN</cell><cell>DnCNN</cell><cell>Li et al.</cell><cell>DCAD</cell><cell>DS-CNN</cell><cell>MFQE 1.0</cell><cell>MFQE 2.0</cell></row><row><cell>A</cell><cell>Traffic PeopleOnStreet</cell><cell>7.40 6.99</cell><cell>8.54 8.28</cell><cell>10.08 9.64</cell><cell>9.97 9.68</cell><cell>9.18 8.67</cell><cell>14.56 13.71</cell><cell>16.98 15.08</cell></row><row><cell></cell><cell>Kimono</cell><cell>6.07</cell><cell>7.33</cell><cell>8.51</cell><cell>8.44</cell><cell>7.81</cell><cell>12.60</cell><cell>13.34</cell></row><row><cell></cell><cell>ParkScene</cell><cell>4.47</cell><cell>5.04</cell><cell>5.35</cell><cell>5.68</cell><cell>5.42</cell><cell>12.04</cell><cell>13.66</cell></row><row><cell>B</cell><cell>Cactus</cell><cell>6.16</cell><cell>6.80</cell><cell>8.23</cell><cell>8.69</cell><cell>8.78</cell><cell>12.78</cell><cell>14.84</cell></row><row><cell></cell><cell>BQTerrace</cell><cell>6.86</cell><cell>7.62</cell><cell>8.79</cell><cell>9.98</cell><cell>8.67</cell><cell>10.95</cell><cell>14.72</cell></row><row><cell></cell><cell>BasketballDrive</cell><cell>5.83</cell><cell>7.33</cell><cell>8.61</cell><cell>8.94</cell><cell>7.89</cell><cell>10.54</cell><cell>11.85</cell></row><row><cell></cell><cell>RaceHorses</cell><cell>5.07</cell><cell>6.77</cell><cell>7.10</cell><cell>7.62</cell><cell>7.48</cell><cell>8.83</cell><cell>9.61</cell></row><row><cell>C</cell><cell>BQMall PartyScene</cell><cell>5.60 1.88</cell><cell>7.01 4.02</cell><cell>7.79 3.78</cell><cell>8.65 4.88</cell><cell>7.64 4.08</cell><cell>11.11 6.67</cell><cell>13.50 11.28</cell></row><row><cell></cell><cell>BasketballDrill</cell><cell>4.67</cell><cell>8.02</cell><cell>8.66</cell><cell>9.80</cell><cell>8.22</cell><cell>10.47</cell><cell>12.63</cell></row><row><cell></cell><cell>RaceHorses</cell><cell>5.61</cell><cell>7.22</cell><cell>7.68</cell><cell>8.16</cell><cell>7.35</cell><cell>10.41</cell><cell>11.55</cell></row><row><cell>D</cell><cell>BQSquare BlowingBubbles</cell><cell>0.68 3.19</cell><cell>4.59 5.10</cell><cell>3.59 5.41</cell><cell>6.11 6.13</cell><cell>3.94 5.55</cell><cell>2.72 10.73</cell><cell>11.00 15.20</cell></row><row><cell></cell><cell>BasketballPass</cell><cell>5.11</cell><cell>7.03</cell><cell>7.78</cell><cell>8.35</cell><cell>7.49</cell><cell>11.70</cell><cell>13.43</cell></row><row><cell></cell><cell>FourPeople</cell><cell>8.42</cell><cell>10.12</cell><cell>11.46</cell><cell>12.21</cell><cell>11.13</cell><cell>14.89</cell><cell>17.50</cell></row><row><cell>E</cell><cell>Johnny</cell><cell>7.66</cell><cell>10.91</cell><cell>13.05</cell><cell>13.71</cell><cell>12.19</cell><cell>15.94</cell><cell>18.57</cell></row><row><cell></cell><cell>KristenAndSara</cell><cell>8.94</cell><cell>10.65</cell><cell>12.04</cell><cell>12.93</cell><cell>11.49</cell><cell>15.06</cell><cell>18.34</cell></row><row><cell></cell><cell>Average</cell><cell>5.59</cell><cell>7.36</cell><cell>8.20</cell><cell>8.89</cell><cell>7.85</cell><cell>11.41</cell><cell>14.06</cell></row><row><cell cols="5">be observed, when compared with AR-CNN and DnCNN.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">At other QPs (= 22, 27, 32 and 42), our MFQE approach</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">consistently outperforms other state-of-the-art video qual-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ity enhancement approaches. Similar improvement can be</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>found for SSIM in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7</head><label>7</label><figDesc>Test speed (fps) and parameters.</figDesc><table><row><cell cols="2">MFQE</cell><cell cols="4">Test speed WQXGA 1080p 480p 240p 720p</cell><cell>Parameters</cell></row><row><cell>1.0</cell><cell>DS-CNN 1 MF-CNN 2</cell><cell>0.57 0.36</cell><cell>1.12 0.73</cell><cell>5.92 3.83</cell><cell>19.38 2.54 12.55 1.63</cell><cell>1,344,449 1,787,547</cell></row><row><cell>2.0</cell><cell>MF-CNN 3</cell><cell>0.79</cell><cell>1.61</cell><cell>8.35</cell><cell>25.29 3.66</cell><cell>255,422</cell></row></table><note>1 for PQF enhancement.2 for non-PQF enhancement.3 for both PQF and non-PQF enhancement.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8 Overall</head><label>8</label><figDesc>?PSNR (dB) of 10 test sequences at QP = 37. PeopleOnStreet 8: Kimono 9: RaceHorses 10: BasketballPass selected from the previous database including 70 videos. In this paper, our 18 test sequences are selected by Joint Collaborative Team on Video Coding (JCT-VC)<ref type="bibr" target="#b33">[33]</ref>, which is a standard test set for video compression. For fair comparison, we test the performance of our MFQE 2.0 and all compared approaches over the previous test set. The experimental results are presented in</figDesc><table><row><cell>Seq.</cell><cell>AR-CNN</cell><cell>DnCNN</cell><cell>Li et al.</cell><cell>DCAD</cell><cell>DS-CNN</cell><cell>MFQE 1.0</cell><cell>MFQE 2.0</cell></row><row><cell>1</cell><cell>0.280</cell><cell>0.359</cell><cell>0.459</cell><cell>0.510</cell><cell>0.415</cell><cell>0.655</cell><cell>0.775</cell></row><row><cell>2</cell><cell>0.266</cell><cell>0.303</cell><cell>0.387</cell><cell>0.399</cell><cell>0.339</cell><cell>0.492</cell><cell>0.579</cell></row><row><cell>3</cell><cell>0.315</cell><cell>0.365</cell><cell>0.422</cell><cell>0.439</cell><cell>0.394</cell><cell>0.629</cell><cell>0.735</cell></row><row><cell>4</cell><cell>0.321</cell><cell>0.312</cell><cell>0.401</cell><cell>0.421</cell><cell>0.388</cell><cell>0.599</cell><cell>0.719</cell></row><row><cell>5</cell><cell>0.237</cell><cell>0.229</cell><cell>0.287</cell><cell>0.311</cell><cell>0.290</cell><cell>0.414</cell><cell>0.476</cell></row><row><cell>6</cell><cell>0.261</cell><cell>0.312</cell><cell>0.392</cell><cell>0.373</cell><cell>0.343</cell><cell>0.659</cell><cell>0.723</cell></row><row><cell>7</cell><cell>0.346</cell><cell>0.414</cell><cell>0.482</cell><cell>0.481</cell><cell>0.465</cell><cell>0.772</cell><cell>0.920</cell></row><row><cell>8</cell><cell>0.219</cell><cell>0.244</cell><cell>0.187</cell><cell>0.279</cell><cell>0.280</cell><cell>0.472</cell><cell>0.550</cell></row><row><cell>9</cell><cell>0.267</cell><cell>0.311</cell><cell>0.328</cell><cell>0.317</cell><cell>0.358</cell><cell>0.394</cell><cell>0.594</cell></row><row><cell>10</cell><cell>0.259</cell><cell>0.307</cell><cell>0.343</cell><cell>0.332</cell><cell>0.375</cell><cell>0.484</cell><cell>0.728</cell></row><row><cell cols="2">Ave. 0.277</cell><cell>0.316</cell><cell>0.369</cell><cell>0.386</cell><cell>0.365</cell><cell>0.557</cell><cell>0.680</cell></row><row><cell></cell><cell cols="7">1: TunnelFlag 2: BarScene 3: Vidyo1 4: Vidyo3 5: Vidyo4 6: MaD</cell></row><row><cell></cell><cell>7:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. D should be adjusted according to the compression standard and configuration.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENT</head><p>This work was supported by the NSFC projects 61876013, 61922009 and 61573037.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cisco visual networking index: Global mobile data traffic forecast update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cisco</forename><surname>Systems</surname></persName>
		</author>
		<ptr target="https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/mobile-white-paper-c11-520862.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Study of subjective and objective quality assessment of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seshadrinathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1427" to="1441" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weight-based r-? rate control for perceptual hevc coding on conversational videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="127" to="140" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video quality evaluation methodology and verification testing of hevc compression performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weerakkody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mrak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ramzan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Baroncini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="90" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Study of temporal effects on subjective video quality of experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Bampis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katsavounidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5217" to="5231" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Saliency-guided complexity control for hevc decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Broadcasting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Restoration and recognition in a loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Petrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="638" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous super-resolution and feature extraction for recognition of low-resolution faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Hennings-Yeomans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Facial deblur inference to improve recognition of blurred faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Takeshima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kozakaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Yamaguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1115" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Close the loop: Joint blind image restoration and recognition with sparse representation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="770" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blocking artifacts suppression in blockcoded images using overcomplete wavelet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-C</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="450" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pointwise shapeadaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1395" to="1411" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adaptive non-local means filter for image deblocking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="522" to="530" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Loss-specific training of non-parametric image restoration models: A new state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image deblocking via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Communication</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="663" to="677" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reducing artifacts in JPEG decompression via a learned dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="718" to="728" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="576" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">D3: Deep dual-domain based fast restoration of JPEG-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2764" to="2772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3142" to="3155" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient deep convolutional neural networks model for compressed image deblocking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting>the IEEE International Conference on Multimedia and Expo (ICME)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1320" to="1325" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CAS-CNN: A deep convolutional neural network for image compression artifact suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cavigelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Benini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="752" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4539" to="4547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoder-side HEVC quality enhancement with scalable convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="817" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhancing quality for hevc compressed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video superresolution with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="122" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video super-resolution via motion compensation and deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time video super-resolution with spatio-temporal networks and motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-frame quality enhancement for compressed video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6664" to="6673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Comparison of the coding efficiency of video coding standards-including high efficiency video coding (hevc)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on circuits and systems for video technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1669" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A convolutional neural network approach for post-processing in hevc intra coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Multimedia Modeling (MMM)</title>
		<meeting>the International Conference on Multimedia Modeling (MMM)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A novel deep learning-based method of improving coding efficiency from the decoder-end for HEVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Data Compression Conference (DCC)</title>
		<meeting>the Data Compression Conference (DCC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Super resolution of video using key frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>De Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<meeting>the IEEE International Symposium on Circuits and Systems (ISCAS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1608" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Video super-resolution algorithm using bi-directional overlapped block motion compensation and on-the-fly dictionary training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="274" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video super-resolution via bidirectional recurrent convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1015" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="295" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FlowNet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V D</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">FlowNet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">End-to-end learning of video super-resolution with motion compensation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the German Conference on Pattern Recognition</title>
		<meeting>the German Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="203" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Detail-revealing deep video super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4472" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Xiph.org video test media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Org</surname></persName>
		</author>
		<ptr target="https://media.xiph.org/video/derf/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">VQEG video datasets and organizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqeg</surname></persName>
		</author>
		<ptr target="https://www.its.bldrdoc.gov/vqeg/video-datasets-and-organizations.aspx" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Common test conditions and software reference configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bossen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Collaborative Team on Video Coding (JCT-VC) of ITU-T SG16 WP3 and ISO/IEC JTC1/SC29/WG11, 5th meeting</title>
		<imprint>
			<date type="published" when="2011-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The mpeg video compression algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="140" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Digital video coding standards and their role in video communications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="907" to="924" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The MPEG-4 video standard verification model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Overview of the H. 264/AVC video coding standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bjontegaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Luthra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="560" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Overview of the high efficiency video coding (HEVC) standard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ohm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiegand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1649" to="1668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Mozer</surname></persName>
		</author>
		<title level="m">A Discrete Probabilistic Memory Model for Discovering Dependencies in Time. Springer Berlin Heidelberg</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Low-delay rate control for DCT video coding via ?-domain source modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="928" to="940" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Psnr control for GOP-level constant quality in H.264 video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Vito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C D</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Symposium on Signal Processing and Information Technology</title>
		<meeting>the IEEE International Symposium on Signal Processing and Information Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="612" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adaptive quantization-parameter clip scheme for smooth quality in H.264/AVC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1911" to="1919" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
