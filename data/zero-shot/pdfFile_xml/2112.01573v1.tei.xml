<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
							<email>cygong@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lemeng</forename><surname>Wu</surname></persName>
							<email>lmwu@cs.utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujian</forename><surname>Zhang</surname></persName>
							<email>szhang19@utexas.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
							<email>haosu@eng.ucsd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FuseDream: Training-Free Text-to-Image Generation with Improved CLIP+GAN Space Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1. Generated images for three query texts about dogs with different methods, including naive optimization with CLIP+GAN (top left of each panel), BigSleep [20] (top right of each panel), and FuseDream (the bottom row) with BigGAN-256 (left) and 512 (right).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generating images from natural language instructions is an intriguing yet highly challenging task. We approach textto-image generation with a CLIP+GAN approach, which optimizes in the latent space of an off-the-shelf GAN to find images that achieve maximum semantic relevance score with the given input text as measured by the CLIP model. Compared to traditional methods that train generative models mapping from text to image starting from scratch, the CLIP+GAN approach is training-free, zero-shot and can be easily customized with different generators.</p><p>However, optimizing CLIP score in the GAN space casts a highly challenging optimization problem and off-the-shelf optimizers such as Adam fail to yield satisfying results. In this work, we propose a FuseDream pipeline, which improves the CLIP+GAN approach with three key techniques: 1) a AugCLIP score which robustifies the standard CLIP score by introducing random augmentation on image. 2) a novel initialization and over-parameterization strategy for optimization which allows us to efficiently navigate the nonconvex landscape in GAN space. 3) a composed generation technique which, by leveraging a novel bi-level optimization formulation, can compose multiple images to extend the GAN space and overcome the data-bias.</p><p>When promoted by different input text, FuseDream can generate high-quality images with varying objects, backgrounds, artistic styles, and novel counterfactual concepts that do not appear in the training data of the GAN that we use. Quantitatively, the images generated by Fuse-Dream yield top-level Inception score and FID score on MS COCO dataset, without additional architecture design or training. Our code is publicly available at https: //github.com/gnobitab/FuseDream.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A landmark task in multi-modal machine learning is textto-image generation, generating realistic images that are semantically related to a given text input <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35]</ref>. This is a highly challenging task because the generative model needs to understand the text, image, and how they should be associated semantically. Recently, significant progresses have been achieved by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">25]</ref> which generate high quality and semantically relevant images using models trained with self-supervised loss on large-scale datasets.</p><p>The traditional approach to text-to-image generation is to train a conditional generative model from scratch with a dataset of (text, image) pairs <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35]</ref>. This procedure, however, requires to collect a large training dataset, casts a high training cost, and can not be easily customized. Recently, a more flexible text-to-image generation approach is enabled with the availability of powerful joint text-image encoders (notably the CLIP model <ref type="bibr" target="#b24">[24]</ref>) that provide faithful semantic relevance score of text-image pairs. Together with powerful pre-trained GANs (such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">40]</ref>), it is made possible to do text-to-image generation by optimizing in the latent space of a GAN to create images that have high semantic relevance with the input text. Notable examples include BigSleep <ref type="bibr" target="#b20">[20]</ref> and VQ-GAN+CLIP <ref type="bibr" target="#b5">[6]</ref>, which can generate intriguing and artistic images from text by maximizing the CLIP score in the latent space of BigGAN and VQGAN, respectively. Compared with the traditional benchmarks, methods that combine GAN and CLIP are training-free and zero-shot, requiring no dedicated training dataset and training cost. It is also much more flexible and modular: a user can easily replace the generator (GAN) or the encoder model (CLIP) with more powerful or customized ones that fit best for their own problems and computational budget.</p><p>On the other hand, the results from the existing CLIP+GAN methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">20]</ref> are still not satisfying in many cases. For example, although BigSleep can generate images in different styles and create interesting visual arts, it finds difficulty in generating clear and realistic images and the resulting images can be only weakly related to the query text. As shown in <ref type="figure">Figure 1</ref> (top right of each panel), BigSleep can not generate a clearly recognizable image for the simple concept of 'blue dog'. For counterfactual concepts like 'flaming dog', the image given by BigSleep tends to entangle the flame and dog concepts in an unnatural way. In <ref type="figure">Figure 1</ref> (top left of each panel), we implement another baseline that maximizes the CLIP score in the input space of BigGAN <ref type="bibr" target="#b3">[4]</ref> with the off-the-shelf Adam <ref type="bibr" target="#b16">[17]</ref> optimizer, which yields even worse results than BigSleep.</p><p>In this work, we analyze the problems in the existing CLIP+GAN procedures. We identity three key bottlenecks of the exiting approach and address them with a number of techniques to significantly improve the pipeline.</p><p>1) Robust Score We observe that the original CLIP score does not serve as a good objective function for optimizing in the GAN space, as it tends to yield semantically unrelated images that "adversarially" maximize the CLIP score. We propose an AugCLIP score, which robustifies the CLIP score by averaging it on multiple perturbation or augmentation of the input images.</p><p>2) Improved Optimization Strategy Maximizing the CLIP score in the GAN space yields a highly non-convex, multi-modal optimization problem and off-the-shelf optimization methods tend to be stuck at sub-optimal local maxima. We address this problem with a novel initialization and over-parameterization strategy which allow us to traverse in the non-convex loss landscape more efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Composed Generation</head><p>The image space of the CLIP+GAN approach is limited by the pre-trained GAN that we use. This makes it difficult to generate images with novel combinations of objects that did not appear in the training data of the GAN. We address this problem by proposing a composed generation technique, which cooptimizes two images so that they can be seamless composed together to yield a natural and semantically relevant image. We formulate composed generation into a novel bi-level optimization problem, which maximizes AugCLIP score while incorporating a perceptual consistency score as the secondary objective, and solve it efficiently with a recent dynamic barrier gradient descent algorithm <ref type="bibr" target="#b10">[11]</ref>.</p><p>Our pipeline, which we call FuseDream 1 , can generate not only clear objects from complex text description, but also complicate scenes as these in MS COCO <ref type="bibr" target="#b18">[19]</ref>. Thanks to the representation power of CLIP, FuseDream can create images with different backgrounds, textures, locations, artistic styles, and even counterfactual objects. With the composed generation techniques, FuseDream can create images with novel combinations of objects that do not appear in original training data of the GAN that we use. Comparing to directly training large-scale text-to-image generative models, our method is much more computationfriendly while achieving comparable or even better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Text-to-Image Generation with CLIP+GAN</head><p>We first introduce the general idea of text-to-image generation by combining pre-trained image generators (in particular GANs) and joint image+text encoders (in particular CLIP). We then analyze a key limitation of the naive realization of this approach. GAN An image generator g : R D ? R H?W ?3 is a neural network that takes an D-dimensional latent code ? and output a colored image I of size H ? W . Formally,</p><formula xml:id="formula_0">I = g(?).</formula><p>One can generate and manipulate different images by controlling the input ?. In this work, we use BigGAN <ref type="bibr" target="#b3">[4]</ref> unless otherwise specified, which is a class-conditional GAN whose latent vector ? = {z, y} includes both a Gaussian noise vector z ? R Z and a class embedding vector y ? R Y . It was trained on the large-scale ImageNet dataset <ref type="bibr" target="#b27">[27]</ref> with objects from 1, 000 different categories. CLIP A joint image-text encoder, notably Contrastive Language-Image Pretraining (CLIP) <ref type="bibr" target="#b24">[24]</ref>, consists of a pair of language encoder f text and image encoder f image , which map a text T and an image I into a common latent space on which their relevance can be evaluated by cosine similarity</p><formula xml:id="formula_1">s CLIP (T , I) = f text (T ), f image (I) ||f text (T )|| ? ||f image (I)|| .<label>(1)</label></formula><p>The CLIP model was trained such that semantically related pairs of T and I have high similarity scores. CLIP+GAN One can synthesis a text-to-image generator by combining a pre-trained GAN g and CLIP {f text , f image }. Given an input text T , we can generate a realistic image I that is semantically related to T by optimizing the latent code ? such that the generated image I = g(?) has maximum CLIP score s CLIP (T , I). Formally,</p><formula xml:id="formula_2">max ? s CLIP (T , g(?)) .<label>(2)</label></formula><p>This confines the output image within the space of natural images while maximizing the semantic relevance to the input text. The optimization is solved with Adam <ref type="bibr" target="#b16">[17]</ref> in <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b39">39]</ref>. We truncate z to [?2, 2] as a typical practice when using BigGAN <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">CLIP Can be Easily Attacked and Stuck</head><p>Naively solving (2) does not yield satisfying images as shown in the top-left images of <ref type="figure">Figure 1</ref>. We observe that the unsatisfying results can be attributed to two interconnected reasons: 1) CLIP scores can be easily "attacked", in that it is easy to maximize CLIP within a small neighborhood of any image, indicating the existence of "adversarial" images with high CLIP score but low semantic relevance with the input text.</p><p>2) The optimization in (2) can indeed effectively act as an adversarial optimization on s CLIP , yielding images that are similar to the initialization but spuriously high CLIP score. Case Study 1: Attacking CLIP As shown in <ref type="figure">Figure 2</ref>, we apply an adversarial attacker, Fast Gradient Sign Method (FGSM) <ref type="bibr" target="#b12">[13]</ref> on s CLIP on a natural image I, that is, we solve max ? s CLIP (I + ?) s.t. ? ? with a small perturbation magnitude &gt; 0. We find that FGSM can easily find an image that is almost identical with the original image, yet with much higher CLIP score. This indicates a danger of "overfitting" when we directly maximize the CLIP score. Case Study 2: Dog?Cat In <ref type="figure" target="#fig_0">Figure 3</ref>, we show an example of optimizing (2) with an input text T ='A photo of a cat', initialized from an ? 0 whose image I = g(? 0 ) is a dog. We can see that although s CLIP is successfully maximized, the image remains similar to the initialization and does not transfer from a dog to a cat as expected. In this case, the optimization in (2) exhibit an adversarial attacking like behavior: it is stuck nearby the initialization while spuriously increasing the CLIP score.</p><p>In both cases above, the problem can be resolved by using our AugCLIP score, which we introduce in sequel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Method -FuseDream</head><p>We now introduce our main techniques for improving the CLIP+GAN pipeline. Section 3.1 introduces the AugCLIP <ref type="figure">Figure 2</ref>. Results of adversarial attack on CLIP and AugCLIP score. (1) By applying FGSM on CLIP score, we get an image that has high sCLIP but visually identical to the original image. (2) Our AugCLIP score is robust against the FGSM attacking. score which robustifies the CLIP score to avoid the adversarial attacking phenomenon. Section 3.2 introduces an initialization and over-parameterization technique to better solve the non-convex optimization. Section 3.3 introduces a composed generation to generate out-of-distribution images with novel combination of objects and backgrounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">AugCLIP: Avoiding Adversarial Generation</head><p>To address the adversarial attacking issue of the CLIP score, we propose the following AugCLIP score,</p><formula xml:id="formula_3">s AugCLIP (T , I) = E I ??(?|I) [s CLIP (T , I )] ,<label>(3)</label></formula><p>where I is a random perturbation of the input image I drawn from a distribution ?(?|I) of candidate data augmentations. In our work, we adopt the various data augmentation techniques considered in DiffAugment <ref type="bibr" target="#b38">[38]</ref>, including random colorization, random translation, random resize, and random cutout.</p><p>AugCLIP is more robust against adversarial attacks, because a successful adversarial perturbation against AugCLIP must simultaneously attack s CLIP on most of the randomly augmented images, which is must harder than attacking a single image. The averaging over random aug-mentation also makes the landscape smoother and hence harder to attack, as shown theoretically and empirically in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">28]</ref>. Meanwhile, adding the augmentation does not hurt the semantic relation encoded by CLIP, because the CLIP model was originally trained on images with different colorizations, views and translations, and is hence compatible with our augmentation strategy. Case Study 1 &amp; 2 As shown in <ref type="figure" target="#fig_0">Figure 3</ref>, the AugCLIP score is significantly more robust against adversarial attacking. <ref type="figure">Figure 4</ref> shows that simply replacing s CLIP with s AugCLIP allows us to escape the adversarial generation regime and yield more semantically relevant images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Improving Optimization</head><p>The optimization of s AugCLIP can still suffer from suboptimal local maxima due to the high non-convexity of loss landscape. We introduce an initialization and overparameterization strategy to improve the optimization.</p><p>Unlike traditional methods that start from a single initialization, we start with sampling a large number M of copies</p><formula xml:id="formula_4">of initialization {? 0 i } M i=1 . We then select the top k initial- ization, say {? 0 (i) } k i=1</formula><p>, that has the highest AugCLIP score, and use them as the initial basis vector for the subsequent optimization, that is, we reparameterize the solution into ? = k i=1 w (i) ? (i) and jointly optimize the basis vectors</p><formula xml:id="formula_5">{? (i) } k i=1 and the coefficients {w (i) } k i=1 with w (i) ? R: max {? (i) ,w (i) } k i=1 s AugCLIP T , g k i=1 w (i) ? (i) ,<label>(4)</label></formula><p>with {? (i) } initialized from the k selected {? 0 (i) }, and w (i) initialized from 1/k. We set M = 10, 000 (which can be parallel evaluated) and a relatively small k (e.g., k ? 15).</p><p>Although the optimization in (4) is equivalent to that of (2), it is equipped with an over-parameterized and more natural coordinate and better initialization, and hence tends to yield better results when solved with gradient-based optimization methods. In particular, the update of the combination weights {w (i) } corresponds to fast and global move in the linear span of the basis vectors {? 0 (i) } k i=1 , making it easier to escape local optima.</p><p>In practice, as we use BigGAN, the latent code ? = (z, y) is initialized with z ? N (0, I) and y randomly selected from the latent representations for the 1,000 classes of ImageNet (which is better than initializing y ? N (0, I) as we show in Appendix). Gradient-based or Gradient-free Optimizer? In this work, we adopt the widely-used Adam <ref type="bibr" target="#b16">[17]</ref> optimizer. Some recent works recommended to use gradient-free optimizers, like BasinCMA <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b32">32]</ref>, for optimizing in GAN spaces due to the high non-convexity. However, our study shows that BasinCMA tends to cast a higher computation cost than Adam, because BasinCMA requires a Algorithm 1 FuseDream (with single image generation)</p><formula xml:id="formula_6">1: Input: Query text T ; a GAN g; CLIP {f text , f image }. 2: Generate M initialization {? 0 i } M i=1 randomly, and select the top-k {? 0 (i) } k i=1</formula><p>with the highest s AugCLIP . 3: Solve the optimization problem (4) using Adam, with</p><formula xml:id="formula_7">{? (i) } k i=1 initialized from {? 0 (i) } k i=1 . 4: return image I = g k i=1 w (i) ? (i) . Algorithm 2 FuseDream-Composition 1: Input: Query text T ; a GAN g; CLIP {f text , f image }. A finite candidate set ? of the composition parameter?. 2: for? in ? do 3:</formula><p>Solve the optimization problem <ref type="bibr" target="#b4">(5)</ref> with <ref type="formula" target="#formula_12">(6)</ref>, yielding a composed image I?. 4: end for 5: return image I?,? ? ? with the highest s AugCLIP .</p><p>large number of forward passes on the objective at each iteration, while Adam only requires a single forward and backward pass. Empirically, we found that Adam is ?20 faster than BasinCMA. Although gradient-based methods are more prune to local optima than gradient-free methods, it becomes less an issue with our AugCLIP loss and the proposed initialization and over-parameterization technique. We include more discussion and comparison with BasinCMA in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Composed generation</head><p>The space of images of the CLIP+GAN approach is limited by the representation power of the GAN we use. This makes the method difficult to generate out-of-distribution images and prune to inherit the data biases e.g., center, spatial and color bias <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref>, from the original training set of the GAN. We propose composed generation, which expands the image space and reduce the data bias by composing two images generated by GAN to gain highly flexibility.</p><p>Our method co-optimizes a foreground image I f g = g(? f g ) and a background image I bg = g(? bg ), where? f g and? bg are two over-parameterized latent codes as Eq 4. The two images are used to generate a fused image</p><formula xml:id="formula_8">I Fuse = Fuse(I f g , I bg , {?, t})</formula><p>by first scaling I f g in size with a factor ? ? (0, 1), and then pasting it on top of I bg at one of 9 locations t ? {left, center, right} 2 . We hope to choose? := {? f g ,? bg }, and? := {?, t} to maximize the AugCLIP score of I:</p><formula xml:id="formula_9">s Fuse (?,?) := s AugCLIP (T , I Fuse ).</formula><p>On the other hand, because the two images I f g and I bg are generated independently, the composed image may have <ref type="figure">Figure 4</ref>. Ablation of different initialization and optimization strategies (Init.=Initialization). The numbers below the images are the sAugCLIP score. z is randomly initialized from the standard Gaussian distribution and y is initialized from the latent representations from the 1,000 classes in ImageNet. The left top query text is from the CUB dataset <ref type="bibr" target="#b34">[34]</ref>. We can see that: (1) images with higher sAugCLIP tend to interpret the input text better, indicating the effectiveness of sAugCLIP; (2) FuseDream (right three columns of each panel) with M = 10, 000 and large k generates high sAugCLIP and high-quality images with multiple mixed concepts and nonexistent objects. unnatural and artificial discontinuity on the boundary. To address this, we introduce an additional loss that enforces the perceptual consistency between I f g and I bg ,</p><formula xml:id="formula_10">Fuse (?,?) := per (I f g , Crop(I bg , {?, t})),</formula><p>where per denotes the LPIPS metric <ref type="bibr" target="#b37">[37]</ref>, a measure that approximates human perception of image similarity.</p><p>Hence, we want to both maximize the AugCLIP score and minimize the perceptual loss Fuse . A naive approach is to optimize their linear combination. However, this would require a careful and case-by-case tuning of the combination coefficient when generating each image. Bi-level Optimization We propose a tuning-free approach for combining two losses via a simple bi-level (or lexicographic) optimization problem (see e.g., <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b11">12]</ref>),</p><formula xml:id="formula_11">min ?,? Fuse (?,?) s.t. (?,?) ? arg max s Fuse ,<label>(5)</label></formula><p>where arg max s Fuse denotes the set of (local) maxima of s Fuse . This formulation seeks in the optimum set of s Fuse the points that minimize Fuse . It prioritizes the optimization of s Fuse while incorporating Fuse as a secondary loss.</p><p>We optimize? = {?, t} by brute-force search in the discrete set ? ? {0.65, 0.5} and t ? {left, center, right} 2 . For each fixed?, we optimize the continuous vector? using the dynamic barrier gradient descent algorithm from <ref type="bibr" target="#b11">[12]</ref>, which yields the following simple update rule at iteration t,</p><formula xml:id="formula_12">? t+1 ?? t ? t v t , v t = ? t Fuse ? ? t ?s t Fuse , ? t = max ?||?s t Fuse || 2 + ? t Fuse , ?s t Fuse ||?s t Fuse || 2 , 0 ,<label>(6)</label></formula><p>where t &gt; 0 is the step size; ? is a hyperparameter (? = 1 by default); we wrote that ?s t Fuse = ??s Fuse (? t ) and</p><formula xml:id="formula_13">? t Fuse = ?? Fuse (? t ).</formula><p>Intuitively, one may view this algorithm as iteratively minimizing the linearly combined loss Fuse ?? t s Fuse , with the coefficient ? t dynamically decided by the angle between gradient ? Fuse and ?s Fuse , in a way that removes the component of ?? Fuse that is conflict with ?s Fuse , so that s Fuse is ensured to decrease monotonically given that it is the primary loss. See Appendix and <ref type="bibr" target="#b10">[11]</ref> for more details.</p><p>In practice, we combine <ref type="formula" target="#formula_12">(6)</ref> with Adam by treating v t as the gradient direction. In addition, we obtain the final composed image by applying Poisson blending on I f g and I bg to yield even smoother image I following <ref type="bibr" target="#b15">[16]</ref>. Our algorithm is summarized in Alg. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Works</head><p>The general idea of optimizing in the latent space of GAN has been widely used as a powerful framework for generating, editing and recovering images; see, e.g., <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">39]</ref>, to name only a few. For example, <ref type="bibr" target="#b39">[39]</ref> proposes to project real images to the latent space of GAN to edit images. <ref type="bibr" target="#b13">[14]</ref> applied principal component analysis to the GAN space to create interpretable controls for image synthesis. <ref type="bibr" target="#b15">[16]</ref> optimizes the latent code to embed a given image into the BigGAN <ref type="bibr" target="#b3">[4]</ref> by a gradient-free optimizer, Bas-inCMA <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">32]</ref>, to allow flexible image editing in the GAN space. A recent work <ref type="bibr" target="#b6">[7]</ref> uses a layer-wise optimization to improve the performance of solving inverse problems, such as super-resolution and inpainting, with GAN space. Most of these methods solely focus on a single task on the image domain, while our method aims to connect images with text by leveraging the power of CLIP.</p><p>On the other direction, the idea of using CLIP score <ref type="bibr" target="#b24">[24]</ref> has been explored in various directions, including video retrieval <ref type="bibr" target="#b21">[21]</ref>, visual question answering <ref type="bibr" target="#b30">[30]</ref>, and languageguided image manipulation/generation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b23">23]</ref>. In particular, <ref type="bibr" target="#b23">[23]</ref> adopts CLIP and StyleGAN to guide the style of a simple image, typically a photo of face, pet or car. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">20]</ref> are open-source repositories that implemented the vanilla GAN+CLIP procedure, which we improves substantially with the new techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We compare FuseDream equipped with BigGAN-256 with a number of baselines including DM-GAN <ref type="bibr" target="#b40">[40]</ref>, Obj-GAN <ref type="bibr" target="#b17">[18]</ref>, CogView <ref type="bibr" target="#b8">[9]</ref>, and etc. We test the methods on the popular MS COCO dataset <ref type="bibr" target="#b18">[19]</ref>, and find that Fuse-Dream clearly outperforms baselines even though BigGAN was pretrained on ImageNet. Thanks to the rich representation power brought by CLIP, FuseDream can generate images with varying aspects, including artistic style, weather, background, texture, and etc., and is capable of creating nonexistent, counterfactual yet plausible objects. In addition, with the composed generation technique, we can gen-  <ref type="table">Table 1</ref>. Comparison with state-of-the-art text-to-image generators on the test set of MS COCO. Our method gets the highest Inception Score and the lowest FID score. Note that the text-toimage GANs in the second block are directly trained on COCO to maximize the retrieval performance of the model in <ref type="bibr" target="#b35">[35]</ref> during training, and hence have much higher R-precision even than the real images. For fair comparison, we also report R-precision calculated by the CLIP model. erate better images with multiple objects. See Appendix for high resolution copies of the images shown in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Evaluation on MS COCO Test Set</head><p>To compare with other text-to-image generation methods, we evaluate our methods on a subset of 30,000 captions sampled from the COCO dataset. We follow the same standard evaluation protocol in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b40">40]</ref>, with the official code provided by <ref type="bibr" target="#b17">[18]</ref> 2 . We use Fr?chet inception distance (FID), Inception score (IS) and R-precision to evaluate the performance. For R-precision, following <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b40">40]</ref>, we compute the cosine similarity between a global image vector and 100 candidate sentence vectors, extracted by a pre-trained CNN-RNN retrieval model <ref type="bibr" target="#b35">[35]</ref>. The candidate text descriptions include one ground-truth caption and 99 randomly selected unrelated sentences. R-precision is computed as the retrieval accuracy of all the 30,000 generated images. We randomly repeat the process for 3 times and report the mean and the standard deviation of R-precision. Note that baseline GANs are usually trained to maximize this score. For fair comparison, we replace the retrieval model used in <ref type="bibr" target="#b35">[35]</ref> with the CLIP text and image encoder, and report an additional CLIP R-precision score.</p><p>The results are shown in <ref type="table">Table 1</ref>. FuseDream achieves a comparable IS score to that of real images (34.26 versus 34.88). Compared with DALL-E <ref type="bibr" target="#b25">[25]</ref> and CogView <ref type="bibr" target="#b8">[9]</ref>, which are trained on billions of internet images with huge computation cost, we significantly improve the IS score from around 18 to 34, FID from 27 to 21 (e.g. FID 21. <ref type="bibr" target="#b15">16</ref> for FuseDream with BigGAN-256, k = 5). Note that the BigGAN that we use was trained on ImageNet although the evaluation is on COCO images; we can expect to achieve  better results by using a stronger generative model trained on COCO dataset. Images Generated From COCO Captions We show a number of generated images given input captions from COCO dataset in <ref type="figure">Figure 7</ref>. FuseDream generates images with more details and objectives. For example, given 'The traffic and people on a commercial street corner at night' , FuseDream can generates people, cars and a prosperous street with many lights.</p><p>Varying Artistic Styles Although BigGAN is trained on the ImageNet whose images are mostly realistic, with CLIP, FuseDream is capable of producing meaningful images with different artistic styles, as displayed in the first row in <ref type="figure">Figure 6</ref>. The images are with six different styles, e.g. photo, monochrome, printmaking, painting, abstract painting and ink and wash painting. We can generate meaningful fake images with many granularities even if the input sentence is complicated. Given the sentence ('old palaces and towers quivering within the wave's intenser day') from Percy Shelley's Ode to the West Wind, FuseDream successfully generates palaces, towers, wave, and day light. Varying Textures, Backgrounds and More As shown in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b29">29]</ref>, it is hard to control texture and background in standard GANs. However, FuseDream can nicely control the texture and background of images through the input sentences. As shown in the second and third rows of <ref type="figure">Figure 6</ref>, FuseDream easily put a car in different backgrounds (e.g. underwater, night, sky) and with different textures (e.g. colorful glow, starry, ghost). Changing the object to robot, we can also generate meaningful but fake robots under different weather (e.g. rain, sunshine, snow). Moreover, FuseDream seems to show understanding about cultural difference by generating obviously different meals for the U.S., Russia and China: meal of the U.S. contains <ref type="figure">Figure 9</ref>. Images from FuseDream (with single image generation) and FuseDream-Composition. We find that: (1) FuseDream-Composition can generate seamlessly fused images with no unnatural discontinuous on the fusing boundary; (2) Prompted with the simple query texts in the top row, FuseDream (single) tends to mix two objects together or ignore some concepts in the query, while FuseDream-Composition can generate images with clear and disentangled objects; (3) For the more complex text in the second row, Fuse-Dream-Composition can generate images with fine-grained details (e.g., 'dark forest', 'walk', 'moonlight', 'flying', 'flaming mountain'). <ref type="figure">Figure 10</ref>. More mages generated by FuseDream-Composition for 'A colorful robot walks on an iced lake under moonlight', verifying that our method can generate high-quality images with different random seeds. The 36 images are selected out of 50 random seeds according to the AugCLIP score, and are ordered with descending AugCLIP score (from the left to right, the top to down).</p><p>corn, potato mesh and fried chicken; meal of Russia contains black bread and Russian Borscht; meal of China contains egg dumplings and spring rolls. Generating Counterfactual Contents In previous examples, we have shown some counterfactual examples, e.g. flaming dogs in <ref type="figure">Figure 1</ref>, car in the sky in <ref type="figure">Figure 6</ref>. Here, we use FuseDream to generate more high-quality counterfactual images with different objects, background and style. <ref type="figure" target="#fig_3">Figure 8</ref> demonstrates that we can generate 'glow and light dog', 'castle in the sky', 'cube butterfly' and 'underwater forest' . These images have different objects, backgrounds and styles, and do not appear in real world, nor in BigGAN's ImageNet training data. It is surprising that FuseDream successfully generates these out-of-domain images with high quality, especially given that we never change the parameters of the BigGAN. Multiple Concepts with FuseDream-Composition: We verify the performance of composed generation technique by generating images that contain two objects. These two objects do not typically co-appear in normal images, e.g. cat and butterfly, dog and church, etc. As shown in <ref type="figure">Figure 9</ref>, FuseDream (with single image generation) may entangle the two objects together or miss one of the objects.</p><p>For example, 'a dog near a boat' gives a boat with a doglike sail. 'A butterfly near a boat' generates only a butterfly while missing the boat. However, by using composed generation, we can generate images with both objects. Even for more complicated sentences, we can generate meaningful and high-quality images (see the second row in <ref type="figure">Figure 9</ref>).</p><p>To verify the robustness of our method to random seed, we generate more images in <ref type="figure">Figure 10</ref> for 'A colorful robot walks on an iced lake under moonlight'; we obtain a diverse set of images that are well related to the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We propose FuseDream, which enables high quality, state-of-the-art text-to-image generation with CLIPguided GAN. Compared with traditional training-based approaches, our method is training-free, zero-shot, easily customizable, and is hence easily accessible to users with limited computational resource or special demands. Our novel techniques of AugCLIP score, over-parameterized optimization and composed generation are of independent interest and useful in other latent space optimization problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We use the official pre-trained BigGAN model in PyTorch <ref type="bibr" target="#b2">3</ref> . For initialization, we use M = 10, 000, where the batch size is 10 and the initialization runs for 1, 000 steps. For optimization, we use Adam optimizer with a learning rate of 5 ? 10 ?3 with no weight decay, and optimize for 1, 000 iterations. On a GTX 3090 GPU, using BigGAN-256, FuseDream approximately requires 100 seconds for initialization and 80 seconds for optimization, resulting in 180 seconds (?3 minutes) in total. Changing the pre-trained model to BigGAN-512 increases the initialization time to 220 seconds and the optimization time to 120 seconds (yielding ?7 minutes in total).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Studies on Composed Generation</head><p>We discuss the design choices for the composed generation in Sec. 3.3. To demonstrate the effectiveness of the bi-level optimization, formulation (5), we consider two alternative formulations for trading off the AugCLIP score s Fuse and the perceptual loss Fuse of the fused image:</p><p>Linear Combination A standard way to trade-off two loss functions is to optimize their linear combination:</p><formula xml:id="formula_14">min ?,? (1 ? ?) Fuse (?,?) ? ?s Fuse (?,?),<label>(7)</label></formula><p>where ? ? (0, 1) is a linear combination coefficient used to balance the two objectives. However, as shown in <ref type="figure">Figure 11</ref>, the key disadvantage of this approach is that the optimal choice of ? depends on the query text, and hence needs to be tuned by the user case by case. This makes the overall procedure computationally expensive and difficult to automatize. In comparison, the bi-level optimization approach does not require to tune ? case by case, and provides high quality fused image with only a single run. Inverse Bi-level Optimization In the bi-level optimization in <ref type="formula" target="#formula_11">(5)</ref>, we prioritize the optimization of the AugCLIP score s Fuse while adding the perceptual loss Fuse as the secondary loss. An alternative approach is to switch the roles of the two loss functions, prioritizing Fuse and treating s Fuse as the seondary loss: </p><p>As shown in <ref type="figure">Figure.</ref> 11, this approach does not work as well as <ref type="formula" target="#formula_11">(5)</ref>, because it tends to generate images with poor AugCLIP score. Intuitively, (5) is better because s Fuse is difficult to optimize and Fuse is much easier to optimize, and hence it makes more sense to prioritize the optimization of s Fuse .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experimental Results</head><p>Choice of Initialization As we discussed in Section 3.2, it is recommended to initialize the class label y by randomly selecting from the latent representations for the 1,000 classes of ImageNet . An alternative approach is to initialize y from the standard Gaussian distribution, which, however, leads to highly noisy images as we show in <ref type="figure">Figure.</ref> 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear Interpolation</head><p>We linearly interpolate between two generated images to examine the intermediate images in the latent space. Specifically, given two query text T 1 and T 2 , and let ? 1 and ? 2 be the latent code provided by FuseDream (without composed generation), we generate a sequence of images via</p><formula xml:id="formula_16">I ? = g(?? 1 + (1 ? ?)? 2 ),</formula><p>where ? ? [0, 1]. As shown in <ref type="figure">Figure.</ref> 14, the intermediate images provides a smooth interpolation between the images of the two queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of the Data Augmentation Techniques</head><p>We perform an ablation study on choice of data augmentation techniques (random colorization, random translation, random resize, and random cutout) in the AugCLIP score. Results are shown in <ref type="figure">Figure.</ref> 15.  <ref type="figure">Figure 11</ref>. We show two examples to demonstrate the benefit of using our bi-level optimization formulation in composed generation. We compare our formulation (5) with the inverse bi-level optimization <ref type="bibr" target="#b7">(8)</ref> and linear combination method <ref type="bibr" target="#b6">(7)</ref>. We search the linear combination coefficient ? from 0.1 to 0.9 uniformly. Observations: (1) For the linear combination method, For two text queries (a) and (b), the effect of similar ? is different. For instance, when ? = 0.5, the generated image for (a) has acceptable visual quality, but the generated image for (b) failed to generate recognizable 'dog'; (2) Instead, our bi-level optimization formulation (5) with dynamic-barrier gradient descent relieves the user from tuning ? for each image. It yields low perceptual loss without sacrificing too much of the AugCLIP score, and finally generates natural fused images; (3) The inverse bi-level optimization (8) formulation cannot get good results.  <ref type="figure" target="#fig_0">Figure 13</ref>. Three examples of linearly interpolating between the latent codes of two generated images. We observe a smooth transition and the intermediate results are still natural and realistic. <ref type="figure">Figure 14</ref>. The initial basis images when k = 5 for the query text 'Sunshine in a forest' (top row), 'A blue toy rabbit' (middle row) and 'A rainbow dog' (bottom row). <ref type="figure" target="#fig_1">Figure 15</ref>. Images generated by FuseDream when we incorporate different data augmentation techniques in AugCLIP. The numbers indicate their corresponding AugCLIP score computed with all data augmentation. We observe that each data augmentation technique has their unique influence on the generated image, but the best visual result is obtained by applying all the augmentation techniques. <ref type="figure">Figure 16</ref>. More examples for demonstrating the ability of FuseDream to generate images that are outside the training domain (ImageNet) of BigGAN. The 'real' images are adopted from the Internet for comparison. <ref type="figure">Figure 17</ref>. Direct optimization of the AugCLIP score with BasinCMA <ref type="bibr" target="#b32">[32]</ref> without our proposed initialization and over-parameterization strategy. 'Mega-step' refers to the number of iterations in the outer loop when using BasinCMA. We use the same computer with GTX 3090 to test the computational time. The base GAN is BigGAN-256. The objective is sAugCLIP. In <ref type="figure">Figure.</ref>(A), with similar running time (?5 minutes), BasinCMA failed to generate realistic images for 'pineapple' and 'motorbike helmet', while FuseDream successes (See <ref type="figure" target="#fig_1">Figure. 15</ref>). In <ref type="figure">Figure.</ref>(B), with longer optimization time (?30 minutes), BasinCMA can generate semantically related images. <ref type="figure" target="#fig_3">Figure 18</ref>. Original images and their super-resolution version (? 4 ) generated by <ref type="bibr" target="#b33">[33]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Optimizing sCLIP and sAugCLIP in the GAN space as<ref type="bibr" target="#b1">(2)</ref>. Initialized from a dog image, maximizing sCLIP only slightly changes in color of image while yielding spurious increase of sCLIP (top row). But maximizing sAugCLIP quickly turns the dog into a white cat (bottom row). Note that maximizing sAugCLIP yields a similar amount of increase on sCLIP, but not the other way around.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Red box: Image generated by FuseDream-Composition on 'An elephant on top of a roof'. Blue box: Image given by composed generation that only optimizes the AugCLIP score sFuse without considering the perceptual loss Fuse, which shows a clear discontinuity between I bg and I f g . Yellow box: Image given by FuseDream without composition, which has a less clear elephant without nose and unrecognizable 'roof', because BigGAN cannot handle composed concepts well. Green Box: The AugCLIP score sFuse and perceptual loss Fuse vs. iteration using the algorithm in<ref type="bibr" target="#b5">(6)</ref>. Note that the generated images in blue and red boxes have similar AugCLIP, indicating that the bi-level optimization can improve Fuse for free without scarifying sFuse. The white dashed box on I bg indicates the position and size of I f g . Notice how the style and the color of I f g and P bg slowly unify across the iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>First row: images with different styles generated by our method. Second row: images with different backgrounds, textures, weather, and etc. Third row: more complicated examples combining multiple objects, textures, styles, and other information together. The three images in the third row are selected out of 5 random seeds according to the AugCLIP score. Comparison between FuseDream and DF-GAN<ref type="bibr" target="#b31">[31]</ref>, DM-GAN<ref type="bibr" target="#b40">[40]</ref>, BigSleep<ref type="bibr" target="#b20">[20]</ref>, CogView<ref type="bibr" target="#b8">[9]</ref> on MS COCO test set. FuseDream can handle the complicated captions from MS COCO and generate meaningful images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 8 .</head><label>8</label><figDesc>Counterfactual images generated by our method. The three images in each panel are selected out of 5 random seeds according to AugCLIP value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>s</head><label></label><figDesc>Fuse (?,?) s.t. (?,?) ? arg min Fuse (?,?).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Query Text: 'An elephant on top of a roof.' (b) Query Text: 'A dog near a boat'</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 .</head><label>12</label><figDesc>This figures show the other initialization choice (initialize z and y from the standard Gaussian distribution N (0, I)) for generating the images in Figure. 4, as mentioned in the main text. After optimization, the generated images are still noises. In comparison, initializing from the latent codes of ImageNet classes gives more natural images. The corresponding query texts are: (A) This small bird has a pink breast and crown, and black primaries and secondaries. (B) A church near forest under moonlight. (C) A photo of an ice cube melting under the sun. (D) An armchair in the shape of an avocado.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The "fuse" in the name refers to both the idea of 1) fusing GAN and CLIP and 2) our composed generation technique.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/jamesli1618/Obj-GAN</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/ajbrock/BigGAN-PyTorch</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Im-age2stylegan: How to embed images into the stylegan latent space?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameen</forename><surname>Abdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yipeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mimicgan: Robust projection onto image manifolds with corruption mimicking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rushil</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavya</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Kailkhura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bremer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.07748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Certified adversarial robustness via randomized smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<idno>PMLR, 2019. 4</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1310" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Crowson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">+</forename><surname>Vqgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clip</surname></persName>
		</author>
		<ptr target="https://github.com/nerdyrodent/VQGAN-CLIP.2" />
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Intermediate layer optimization for inverse problems using deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.07364</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bilevel optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Dempe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Zemkoho</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyi</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13290</idno>
		<title level="m">Mastering text-to-image generation via transformers</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Generating images from caption and vice versa via clip-guided generative latent space search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Federico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Galatolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gca</forename><surname>Mario</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gigliola</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vaglini</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01645</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic and harmless regularization with constrained and lexicographic optimization: A dynamic barrier approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bi-objective trade-off with dynamic barrier gradient descent. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>H?rk?nen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganspace</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02546</idno>
		<title level="m">Discovering interpretable gan controls</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Semantic object accuracy for generative text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transforming and projecting images into class-conditional generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyoung</forename><surname>Huh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="12174" to="12182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucidrains</surname></persName>
		</author>
		<ptr target="https://github.com/lucidrains/big-sleep.1" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02793</idno>
		<title level="m">Generating images from captions with attention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Styleclip: Text-driven manipulation of stylegan imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongze</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2085" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Provably robust deep learning via adversarially trained smoothed classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Hadi Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bubeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04584</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Sauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06046</idno>
		<title level="m">Counterfactual generative networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">How much can clip benefit vision-and-language tasks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian</forename><forename type="middle">Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06383</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingkun</forename><surname>Bao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.05865</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimal gait and form for animal locomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Wampler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Popovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">17</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real-esrgan: Training real-world blind super-resolution with pure synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangbin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Caltech-ucsd birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Differentiable augmentation for data-efficient gan training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10738,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Out-of-domain Generation We use query texts including famous landmarks, arts, animation figures, etc., to examine the ability of FuseDream to generate images outside the training domain of ImageNet. As shown in Figure. 15, FuseDream can generate famous landmarks, masterpieces, emojis, cartoon characters that are not included in the ImageNet training data. Optimization with Gradient-free Optimizer We replace Adam and optimize AugCLIP score with BasinCMA optimizer</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>which is a gradient-free optimizer used in previous works [3, 16] for optimizing in the GAN latent space. Results are shown in Figure. 17</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">High-resolution images We use [33] to get high-resolution version of sampled generated images in Figure 18</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
