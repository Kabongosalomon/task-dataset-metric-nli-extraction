<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin-Bin</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<postCode>518075</postCode>
									<settlement>Shenzhen</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin-Xin</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">YouTu Lab</orgName>
								<address>
									<postCode>518075</postCode>
									<settlement>Shenzhen</settlement>
									<region>Tencent</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Key Laboratory for Novel Software Technology</orgName>
								<orgName type="institution">Nanjing University</orgName>
								<address>
									<postCode>210023</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Geng</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">MOE Key Laboratory of Computer Network and Information Integration</orgName>
								<orgName type="institution">Southeast University</orgName>
								<address>
									<postCode>211189</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Expectation of Label Distribution for Facial Age and Attractiveness Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>label distribution</term>
					<term>deep learning</term>
					<term>convolutional neural network</term>
					<term>age estimation</term>
					<term>attractiveness estimation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Facial attributes (e.g., age and attractiveness) estimation performance has been greatly improved by using convolutional neural networks. However, existing methods have an inconsistency between the training objectives and the evaluation metric, so they may be suboptimal. In addition, these methods always adopt image classification or face recognition models with a large amount of parameters, which carry expensive computation cost and storage overhead. In this paper, we firstly analyze the essential relationship between two state-of-the-art methods (Ranking-CNN and DLDL) and show that the Ranking method is in fact learning label distribution implicitly. This result thus firstly unifies two existing popular state-of-the-art methods into the DLDL framework. Second, in order to alleviate the inconsistency and reduce resource consumption, we design a lightweight network architecture and propose a unified framework which can jointly learn facial attribute distribution and regress attribute value. The effectiveness of our approach has been demonstrated on both facial age and attractiveness estimation tasks. Our method achieves new state-of-the-art results using the single model with 36? fewer parameters and 3? faster inference speed on facial age/attractiveness estimation. Moreover, our method can achieve comparable results as the state-of-the-art even though the number of parameters is further reduced to 0.9M (3.8MB disk storage).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The human face contains a lot of important information related to individual characteristics, such as identity, expression, age, attractiveness and gender. Such information has been widely applied in real-world applications such as video surveillance, customer profiling, human-computer interaction and person identification. Among these tasks, developing automatic age and attractiveness estimation methods has become an attractive yet challenging topic in recent years.</p><p>Why is it a challenging task to find age/attractiveness from facial images? First, compared with image classification <ref type="bibr" target="#b0">[1]</ref> or face recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, the existing facial attribute datasets are always limited because it is very hard to gather a completely and sufficiently labeled dataset. In <ref type="table" target="#tab_0">Table 1</ref>, we list detailed information of various facial attribute datasets. For example, there are only 2476 training images in the ChaLearn15 apparent age estimation challenge <ref type="bibr" target="#b4">[5]</ref>. Second, the number of images is very imbalanced in different label groups. What is more, distributions of different datasets are also very different. As <ref type="figure" target="#fig_0">Fig. 1</ref> depicts, there are two peaks on ChaLearn16 (the early one at around 2 years old and the latter one at 26 years old) and Morph (the early one at around 20 years old and the latter one at 40 years old), while ChaLearn15 has only one peak. Similar phenomenon also appears in facial attractiveness datasets. These imbalances bring a serious challenge for developing an unbiased estimation system. Third, compared to other facial attributes, such as gender or expression, age/attractiveness estimation is a very fine-grained recognition task, e.g., we humans very hardly sense the change of one person's facial characteristics when he/she grew from 25 to 26 years old.</p><p>Can we use a unified framework to effectively estimate age and attractiveness from facial images? First, these two tasks are both based on facial images thus it is possible to use a unified base model. Second, the labels are ordinal in either the age estimation or the facial attractiveness one. Generally, they are both integrated into ordinal regression or ordinal classification topics whose main character is that the categories are related in a natural or implied order. Common examples of such tasks are movie and facial attractiveness ratings (e.g., from 1 star to 5 stars) or facial age (e.g., from baby to child to adults) or im-age aesthetics (e.g., from "unacceptable" to "professional" to "exceptional"). Third, there is uncertainty information between any two adjacent labels for facial age and attractiveness tasks and their evaluation metric are also the same commonly using MAE. Therefore, we try to integrate facial age and attractiveness estimation into a unified framework in this paper.</p><p>The common evaluation metric of age/attractiveness estimation is the Mean Absolute Error (MAE) between the predicted value and ground-truth. Thus, it is very natural to treat facial attributes estimation as a metric regression problem <ref type="bibr" target="#b5">[6]</ref> which minimizes the MAE. However, such methods usually cannot achieve satisfactory performance because some outliers may cause a large error term, which leads to an unstable training procedure. Later, Rothe et al. <ref type="bibr" target="#b6">[7]</ref> trained deep convolutional neural network (CNN) for age estimation as multi-class classification, which maximizes the probability of ground-truth class without considering other classes. This method easily falls into over-fitting because of the imbalance problem among classes and limited training images <ref type="bibr" target="#b7">[8]</ref>.</p><p>Recently, ranking CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> and deep label distribution learning (DLDL) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13]</ref> techniques achieved stateof-the-art performance on facial age estimation. Both methods use the correlation information among adjacent ages at different levels. The ranking method transforms single-value estimation to a series of binary classification problems at the training stage. Then, the output of the rankers are aggregated directly from these binary outputs at predication stage. DLDL firstly converts a real-value to a discrete label distribution. Then, the aim of the training is to fit the entire distribution. At inference stage, like <ref type="bibr" target="#b6">[7]</ref>, an expected value over the predicted distribution is taken as the final output. We can easily find that there is an inconsistency between the training objectives and the evaluation metric in all these methods. Thus, they may be suboptimal. We expect to improve their performance if this inconsistency is removed.</p><p>In addition, we observe that almost all state-of-the-art facial attributes estimation methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15]</ref> are initialized by a pre-trained model which is trained on large-scale image classification (e.g., ImageNet <ref type="bibr" target="#b0">[1]</ref>) or face recognition (e.g., VG-GFace <ref type="bibr" target="#b1">[2]</ref>) datasets, and fine-tuned on the target dataset. These pre-trained models adopt some popular and powerful architectures (e.g., VGGNet <ref type="bibr" target="#b15">[16]</ref>). Unfortunately, these models often have huge computational cost and storage overhead. Taking VGG-16 for example, it has 138.34 million parameters, taking up more than 500MB storage space. Therefore, it is hard to be deployed on resource-constrained devices, e.g., mobile phones. Recently, some researchers devoted to compressing these pretrained models so that both reducing the number of parameters and keeping accuracy are possible <ref type="bibr" target="#b16">[17]</ref>. Unlike these compression methods, we directly design a thin and deep network architecture and train it from scratch.</p><p>In this paper, we integrate label distribution learning <ref type="bibr" target="#b22">[23]</ref> and expectation regression into a unified framework to alleviate the inconsistency between training and evaluation stages with a simple and lightweight CNN architecture. The proposed approach effectively and efficiently improves the performance of the previous DLDL on both prediction error and inference speed for facial attributes estimation, so we call it DLDL-v2. Our contributions are summarized as follows.</p><p>? We provide, to the best of our knowledge, the first analysis and show that the ranking method is in fact learning label distribution implicitly. This result thus unifies existing state-of-the-art facial attributes estimation methods into the DLDL framework; ? We propose an end-to-end learning framework which jointly learns label distribution with the correlation information among neighboring labels and regresses single label ground-truth in both feature learning and classifier learning; ? We create new state-of-the-art results on facial age and attractiveness estimation tasks using single and small model without external age/attractiveness labeled data or multimodel ensemble; ? Our proposed framework is partly interpretable. We find the network employ different patterns to estimate age for people at different age stage. Meanwhile, we also quantitatively analyze the sensitivity of our approach to different face regions. We organize the rest of this paper as follows. Related works on facial attributes (e.g., age and attractiveness) estimation are introduced in Section 2. Then, Section 3 presents the proposed DLDL-v2 approach including the problem definition, the relationship between existing methods, and our joint learning framework and its model architecture. After that, the experiments are reported in Section 4. In Section 5, we discuss how DLDL-v2 makes the final determination for an input facial image and analyze why it can work well. Finally, the conclusion is given in Section 6. Some preliminary results have been published in a conference presentation <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In the past two decades, many researchers have worked on facial attributes estimation. Earlier researches are two stage solutions, including feature extraction and model learning. Recently, deep learning methods are proposed, which integrate both stages into an end-to-end framework. In this section, we briefly review these two types of frameworks.</p><p>Two stage methods. The task of the first stage is how to extract discriminative features from facial images. Active appearance model (AAM) <ref type="bibr" target="#b24">[25]</ref> is the earliest method through extracting shape and appearance features of face images. Later, the Bio-inspired feature (BIF) <ref type="bibr" target="#b25">[26]</ref>, as the most successful age feature, is widely used in age estimation. But, in face attractiveness analysis, geometric features <ref type="bibr" target="#b26">[27]</ref> and texture features <ref type="bibr" target="#b27">[28]</ref> depended on facial landmark positions are widely used, since the BIF feature may be suboptimal for facial attractiveness prediction. Obviously, the drawback of hand-designed features is that one needs to re-design a feature extraction method when facing a new task, which usually requires domain knowledge and a lot of efforts. The second stage is how to exactly estimate facial attributes using these designed features. Classification and regression models are often used to estimate facial attributes. The former includes k-nearest neighbors (KNN), multilayer perceptron (MLP) and support vector machine (SVM), and the latter contains quadratic regression, support vector regression (SVR) and soft-margin mixture regression <ref type="bibr" target="#b28">[29]</ref>. Instead of classification and regression, ranking techniques <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> utilize the ordinal information of age to learn a model for facial age estimation.</p><p>In addition, Geng et al. proposed a label distribution learning (LDL) approach to utilize the correlation among adjacent labels, which improved performance on age estimation <ref type="bibr" target="#b34">[35]</ref> and beauty sensing <ref type="bibr" target="#b35">[36]</ref>. Recently, some improvements of LDL <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> have been proposed. Xing et al. <ref type="bibr" target="#b36">[37]</ref> used logistic boosting regression instead of the maximum entropy model in LDL. Meanwhile, He et al. <ref type="bibr" target="#b37">[38]</ref> generated age label distributions through weighted linear combination of the label of input image and that of its context-neighboring images. These methods only learn a classifier, but not the visual representations.</p><p>Single stage methods. Deep CNNs have achieved impressive performance on various visual recognition tasks. The greatest success is learning feature representations instead of using hand-crafted features via the single stage learning strategy. Existing facial attribute estimation techniques fall into four categories: metric regression (MR) <ref type="bibr" target="#b5">[6]</ref>, multi-class classification (DEX) <ref type="bibr" target="#b6">[7]</ref>, Ranking <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> and DLDL <ref type="bibr" target="#b7">[8]</ref>.</p><p>MR treats age estimation as a real-valued regression problem. The training procedure usually minimizes the squared difference between the estimated value and the ground-truth.</p><p>DEX adopts a general image classification framework which maximizes the probability of the ground-truth class during training. In the inference stage, Rothe et al. <ref type="bibr" target="#b6">[7]</ref> empirically showed that the expected value over the softmax-normalized output probabilities can achieve better performance than the class prediction of maximum probabilities. However, both MR and DEX easily lead to an unstable training <ref type="bibr" target="#b7">[8]</ref>.</p><p>Ranking methods transform facial attribute regression as a series of binary classification problems. Niu et al. <ref type="bibr" target="#b8">[9]</ref> proposed a multi-out CNN via integrating multiple binary classification problems to a CNN. Then, Chen et al. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> trained a series of binary classification CNNs to get better performance. Given a testing image, the output of the rankers are aggregated directly from these binary outputs.</p><p>DLDL converts a single value to a label distribution and learns it in an end-to-end fashion. Recently, Shen et al. <ref type="bibr" target="#b38">[39]</ref> proposed LDLFs via combining DLDL and differentiable decision trees. Hu et al. <ref type="bibr" target="#b39">[40]</ref> exploited age difference information to improve the age estimation accuracy. These approaches have achieved state-of-the-art performance on age estimation. In addition, Yang et al. <ref type="bibr" target="#b40">[41]</ref> proposed a multi-task deep frame-work via jointly optimizing image classification and distribution learning for emotion recognition. However, these methods may be suboptimal, because there is an inconsistency between the training objectives and evaluation metric.</p><p>In this paper, we focus on how to alleviate or remove this inconsistency in a deep CNN with fewer parameters. Age and attractiveness estimation from still face images are suitable applications of the proposed research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>In this section, we firstly give the definition of the joint learning problem. Next, we show that ranking is implicitly learning label distribution. Finally, we present our framework and network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Joint Learning Problem</head><p>Notation. We use boldface lowercase letters like p to denote vectors, and the i-th element of p is denoted as p i . 1 denotes a vector of ones. Boldface uppercase letters like W are used to denote matrices, and the element in the i-th row and j-th column is denoted as W i j . The circle operator ? is used to denote element-wise multiplication.</p><p>The input space is X = R h?w?c , where h, w and c are height, width and the number of channels of an input image, respec-</p><formula xml:id="formula_0">tively. Label space Y = R is real-valued. A training set with N instances is denoted as D = {(x n , y n )} N n=1</formula><p>, where x n ? X denotes the n-th input image and y n ? Y its corresponding label. We may omit the image index n for clarity. The joint learning aims to learn a mapping function F : X ? Y such that the error between prediction? and ground-truth y be as small as possible on a given input image x.</p><p>However, metric regression often cannot achieve satisfactory performance. We observe that people usually predict another person's apparent age in a way like "around 25 years old" in real life, which indicates using not only 25 but also neighboring ages (e.g., 24 and 26) to describe the face. Similar case also happens in facial attractiveness assessment. Based on the observation, label distribution learning methods can utilize the information via transforming the single value regression problem to a label distribution learning problem.</p><p>To fulfill this goal, instead of outputting a single value y ? R for an input x, we quantize the range of possible y values into several labels. For example, it is reasonable to assume that y ? [0, 100] in age estimation. Thus, we can define l = [0 : l : 100] (MATLAB notation) as the ordered label vector, where l is a fixed real number. A label distribution p is then (p 1 , p 2 , . . . , p K ), where p i is the probability that y = l i (i.e., Pr(y = l i ) for 1 ? i ? K) <ref type="bibr" target="#b7">[8]</ref>. Since we use equal step size l in quantizing y, the probability density function (p.d.f.) of normal distribution is a natural choice to generate the ground-truth p from y and ?:  where ? is a hyper-parameter. The goal of label distribution learning is to maximize the similarity between p and the CNN generated distributionp at training stage. In the prediction stage, predicted distributionp is reversed to a single value by a special inference function. It is suboptimal because there exists inconsistence between training objective and evaluation metric. We are interested to not only learn the label distribution p but also regress a real value y in one framework in an end-to-end manner.</p><formula xml:id="formula_1">p k = 1 ? 2?? exp ? (l k ? y) 2 2? 2 ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ranking is Learning Label Distribution</head><p>The ranking-based <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> and DLDL-based <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b12">13]</ref> methods have achieved state-of-the-art performance in facial age/attractiveness estimation problems. In this section, we analyze the essential relationship between them.</p><p>We explore their relationship from the perspective of label encoding. In DLDL-based approaches, for a face image x with true label y and hyper-parameter ?, the target vector p ld (i.e., label distribution) is generated by a normal p.d.f. (Eq. (1)). For example, the target vector of a 50 years old face is shown in <ref type="figure" target="#fig_2">Fig. 2a</ref>, where l = [0, 1, . . . , 100]. In ranking CNN, K ? 1 binary classifiers are required for K ranks because the k-th binary classifier focuses on determining whether the age rank of an image is greater than l k or not. For a face image x with true label y ? (l k?1 , l k ], the target vector with length K ? 1 is encoded as p rank = [1, . . . , 1, 0, . . . , 0], where the first k ? 1 values are 1 and the rest being 0. The target ranking vector of a 50 years old face is shown in <ref type="figure" target="#fig_2">Fig. 2c</ref> as the dark line.</p><p>As we all know, for a generic normal distribution with p.d.f. p, mean y and deviation ?, the cumulative distribution function (c.d.f.) is</p><formula xml:id="formula_2">c k = 1 2 1 + erf l k ? y ? ? 2 ,<label>(2)</label></formula><p>where erf(x) = 2 ? ? x 0 e ?t 2 dt. <ref type="figure" target="#fig_2">Fig. 2b</ref> shows the c.d.f. corresponding to the p.d.f. in <ref type="figure" target="#fig_2">Fig. 2a</ref>. From Eq. (2), we know</p><formula xml:id="formula_3">? ? ? ? ? ? ? 1 ? c k &gt; 0.5, if l k &lt; y 1 ? c k ? 0.5, if l k ? y ,<label>(3)</label></formula><p>As shown in <ref type="figure" target="#fig_2">Fig. 2c</ref>, the curve of 1 ? c is very close to that of p rank when ? is a small positive real number. Thus,</p><formula xml:id="formula_4">p rank k ? 1 ? c k ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Softmax Expectation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HPooling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Convolutional Neural Network Distribution Learning Expectation Regression</head><p>Input KL Loss L1 Loss <ref type="figure">Figure 3</ref>: The framework of our DLDL-v2. Given an input image and its ground-truth, we firstly generate a label distribution and then jointly optimize label distribution learning and expectation regression in one unified framework in an end-to-end manner.</p><p>where k = 1, 2, . . . , K ? 1.</p><p>Eq. (4) shows p rank is a specific case of label distribution learning, where the distribution is the cumulative one with ? ? 0. That is to say, Ranking is to learn a c.d.f. essentially, while DLDL aims at learning a p.d.f. More generally, we have</p><formula xml:id="formula_5">c = T p ld ,<label>(5)</label></formula><p>where T is a transformation matrix with T i j = 1 for all i ? j and T i j = 0 when i &gt; j. Substituting (5) in to (4), we have</p><formula xml:id="formula_6">p rank ? 1 ? Tp ld .<label>(6)</label></formula><p>Therefore, there is a linear relationship between Ranking encoding and label distribution. The label distribution encoding p ld can represent more meaningful age/attractiveness information with different ?, but ranking encoding p rank does not. Furthermore, DLDL is more efficient, because only one network has to be trained. However, as discussed earlier, all these methods may be suboptimal because there exists inconsistency between training objective and evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Joint Learning Framework</head><p>In order to jointly learn label distribution and output the expectation, in this section we propose the DLDL-v2 framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">The Label Distribution Learning Module</head><p>In order to utilize the good properties of label distribution learning, we integrate it into our framework to formulate a label distribution learning module. As shown in <ref type="figure">Fig. 3</ref>, this module includes a fully connected layer, a softmax layer and a loss layer. This module follows the DLDL method in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Specifically, given an input image x and the corresponding label distribution p, we assume f = F (x; ?) is the activation of the last layer of CNN, where ? denotes the parameters of the CNN. A fully connected layer transfers f to x ? R K by</p><formula xml:id="formula_7">x = W T f + b .<label>(7)</label></formula><p>Then, we use a softmax function to turn x into a probability distribution, that is,p</p><formula xml:id="formula_8">k = exp(x k ) t exp(x t ) .<label>(8)</label></formula><p>Given an input image, the goal of the label distribution learning module is to find ?, W, and b to generatep that is similar to p.</p><p>We employ the Kullback-Leibler divergence as the measurement of the dissimilarity between ground-truth label distribution and prediction distribution. Thus, we can define a loss function on one training sample as follows <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_9">L ld = k p k ln p k p k .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">The Expectation Regression Module</head><p>Note that the label distribution learning module only learns a label distribution but cannot regress a precise value. In order to reduce the inconsistency between training and evaluation stages, we propose an expectation regression module to further refine the predicted value. As shown in <ref type="figure">Fig. 3</ref>, this module includes an expectation layer and a loss layer.</p><p>The expectation layer takes the predicted distribution and label set as input and emits its expectation</p><formula xml:id="formula_10">y = kp k l k ,<label>(10)</label></formula><p>wherep k denotes the prediction probability that the input image belongs to label l k . Given an input image, the expectation regression module minimizes the error between the expected value? and ground-truth y. We use the 1 loss as the error measurement as follows:</p><formula xml:id="formula_11">L er = |? ? y| ,<label>(11)</label></formula><p>where | ? | denotes absolute value. Note that this module does not introduce any new parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Learning</head><p>Given a training data set D, the learning goal of our framework is to find ?, W and b via jointly learning label distribution and expectation regression. Thus, our final loss function is a weighted combination of the label distribution loss L ld and the expectation regression loss L er :</p><formula xml:id="formula_12">L = L ld + ?L er ,<label>(12)</label></formula><p>where ? is a weight which balances the importance between two types of losses. Substituting Eq. (9), Eq. (10) and Eq. <ref type="formula" target="#formula_1">(11)</ref> into Eq. <ref type="formula" target="#formula_1">(12)</ref>, we have</p><formula xml:id="formula_13">L = ? k p k lnp k + ? kp k l k ? y .<label>(13)</label></formula><p>We adopt stochastic gradient descent to optimize parameters of our model. The derivative of L with respect top k is</p><formula xml:id="formula_14">?L ?p k = ? p k p k + ?l k sign(? ? y) .<label>(14)</label></formula><p>For any k and j, the derivative of softmax (Eq. <ref type="formula" target="#formula_8">(8)</ref>) is well known, as</p><formula xml:id="formula_15">?p k ?x j =p k (? (k= j) ?p j ) ,<label>(15)</label></formula><p>where ? (k= j) is 1 if k = j, and 0 otherwise. According to the chain rule, we have</p><formula xml:id="formula_16">?L ?x j = (p j ? p j ) + ?sign(? ? y)p j (l j ??) ,<label>(16)</label></formula><p>i.e.,</p><formula xml:id="formula_17">?L ?x =p ? p + ? sign(? ? y)p ? (l ??1) .<label>(17)</label></formula><p>Applying the chain rule for Eq. <ref type="formula" target="#formula_7">(7)</ref> again, the derivative of L with respect to W, b and ? are easily obtained, as</p><formula xml:id="formula_18">?L ?W = ?L ?x f , ?L ?b = ?L ?x , ?L ?? = ?L ?x W T ?F ?? .<label>(18)</label></formula><p>Once W, b and ? are learned, the prediction value? of any new instance x is generated by Eq. (10) in a forward network computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Network Architecture</head><p>Considering both model size and efficiency, we modify VGG-16 <ref type="bibr" target="#b15">[16]</ref> from four aspects as follows. VGG-16 consists of 13 convolution (Conv) layers, five max-pooling (MP) layers and three fully connected (FC) layers, and each Conv layer and FC layer is followed by a ReLU layer.</p><p>First, we observe that the three FC layers roughly contain 90% parameters of the whole model. We remove all FC layers and add a hybrid-pooling (HP) layer which is constructed by an MP layer and a global avg-pooling (GAP) layer. We find that the HP strategy is more effective than single GAP. Second, to further reduce model size, we reduce the number of the filters in each Conv layer to make it thinner. Third, batch normalization (BN) <ref type="bibr" target="#b42">[43]</ref> has been widely used in the latest architecture such as ResNet <ref type="bibr" target="#b43">[44]</ref>. Thus, we add a BN layer after each Conv layer to accelerate network training. Last but not least, we add the label distribution learning module and the expectation regression module after the HP layer, as shown in <ref type="figure">Fig. 3</ref>.</p><p>Since we design the network for age/attractiveness estimation and its architecture is thinner than the original VGG-16, we call our model ThinAgeNet or ThinAttNet which employs the compression rate of 0.5 and has 3.7M parameters. <ref type="bibr" target="#b0">1</ref> We also train a very small model with the compression rate of 0.25, and we call it TinyAgeNet or TinyAttNet which only has 0.9M parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct experiments to validate the effectiveness of the proposed DLDL-v2 approach on facial age and attractiveness datasets, based on the open source framework Torch7. All experiments are conducted on an NVIDIA M40 GPU. In order to re-produce all results in this paper, we will release source code and pre-trained models upon paper acceptance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Pre-preprocessing. We use multi-task cascaded CNN <ref type="bibr" target="#b44">[45]</ref> to conduct face detection and facial points detection for all images. Then, based on these facial points, we align faces to the upright pose. Finally, all faces are cropped and resized to 224?224. Before feeding to the network, all resized images are to subtract mean and divide standard deviation for each color channel.</p><p>Data Augmentation. There are many non-controlled environmental factors such as face position, illumination, diverse backgrounds, image color (i.e., gray and color) and image quality, especially in the ChaLearn datasets. To handle these issues, we apply data augmentation techniques to every training image, so that the network can take a different variation of the original image as input at each epoch of training. Specifically, we mainly employ five types of augmentation methods for a cropped and resized training image, including random horizontal flipping, random scaling, random color/gray changing, random rotation and standard color jittering.</p><p>Training Details. We pre-train a deep CNN model with softmax loss for face recognition on a subset of the MS-Celeb-1M dataset <ref type="bibr" target="#b2">[3]</ref>. One issue is that a small part of identities have a large number of images and others have only a few in this dataset. To avoid the imbalance problem among identities, we cut those identities whose number of images is lower than a threshold. In our experiments, we use about 5M images of 54K identities as training data.</p><p>After pre-training is finished, we remove the classification layer of the network and add the label distribution learning and expectation regression modules. Then, fine-tuning is conducted on target datasets. We set ? = 1 in Eq. <ref type="bibr" target="#b13">(14)</ref>. The ordered label vector is defined as l = [l min : l : l max ] (MATLAB notation). For age estimation, we set l min = 0, l = 1, and l max = 100. For attractiveness estimation, we set l min = 1 and l = 0.1. Because there are different scoring rules on SCUT-FBP and CFD dataset, l max is set to 5 and 7, respectively. The label distribution of each image is generated using Eq. (1). The groundtruth (age or attractiveness score) is provided in all datasets. The standard deviation, however, is provided in ChaLearn15, ChaLearn16 and SCUT-FBP, but not Morph and CFD. Generally, a standard deviation ? that is close to the interval between neighboring labels is a good choice that has been discussed and analyzed in the DLDL <ref type="bibr" target="#b7">[8]</ref>. Following this principle, we simply set ? = 2 in Morph and ? = 0.5 in CFD, respectively. All networks are optimized by Adam, with ? 1 = 0.9, ? 2 = 0.999 and = 10 ?8 . The initial learning rate is 0.001 for all models, and it is decreased by a factor of 10 every 30 epochs. Each model is trained 60 epochs using mini-batches of 128.</p><p>Inference Details. At the inference stage, we feed a testing image and its horizontally flipping copy into the network and average their predictions as the final estimation for the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>MAE is used to evaluate the performance of facial age or attractiveness estimation,</p><formula xml:id="formula_19">MAE = 1 N N n=1 |? n ? y n |,<label>(19)</label></formula><p>where? n and y n are the estimated and the ground-truth of the n-th testing image, respectively. In addition, a special measurement ( -error) is defined by the ChaLearn competition, as</p><formula xml:id="formula_20">-error = 1 N N n=1 1 ? exp ? (? n ? y n ) 2 2(? n ) 2 ,<label>(20)</label></formula><p>where ? n is the standard deviation of the n-th testing image. We also follow <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b12">13]</ref> to compute Root Mean Squared Error (RMSE) and Pearson Correlation (PC), which can be computed as:</p><formula xml:id="formula_21">RMSE = 1 N N n=1 |? n ? y n | 2 ,<label>(21)</label></formula><formula xml:id="formula_22">PC = N n=1 (y n ? y)(? n ??) N n=1 |y n ? y| 2 N n=1 |? n ??| 2 ,<label>(22)</label></formula><p>where y = 1 N N n=1 y n , and? = 1 N N n=1? n are the mean values of the ground-truth and predicted scores over all testing images. These two evaluation metrics are only utilized to evaluate the performance of facial attractiveness estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experiments on Age Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Age Estimation Datasets</head><p>Three types of datasets are used in our experiments. The first type contains two small-scale apparent age datasets (ChaLearn15 <ref type="bibr" target="#b4">[5]</ref> and ChaLearn16 <ref type="bibr" target="#b17">[18]</ref>) which are collected in the wild. The second type is a large-scale real age dataset (Morph) <ref type="bibr" target="#b19">[20]</ref>. We follow the experimental setting in <ref type="bibr" target="#b23">[24]</ref> for evaluation. The third one is UTKFace <ref type="bibr" target="#b18">[19]</ref> which the ground truth of age are estimated through the DEX algorithm and double checked by a human annotator. We utilize the split employed in <ref type="bibr" target="#b45">[46]</ref>, with 20% test images and 80% images for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Age Estimation Results</head><p>We compare our approach with the state-of-the-art in both prediction performance and inference time.</p><p>Low Error. Table2 2, 3, 4 and 5 report the comparisons of the MAE and -error performance of our method and previous state-of-the-art methods on four age estimation datasets.</p><p>In the ChaLearn15 challenge, the best result came from DEX. DEX method's success relies on a lot of external age labeled training images (260282 additional photos). Under the same setting (without external data), our method outperforms DEX by a large margin in <ref type="table" target="#tab_0">Table 11</ref>. On ChaLearn16, the -error 0.267 of our approach is closest to the best competition result 0.241 <ref type="bibr" target="#b14">[15]</ref> on the testing set. Note that our result is only based on a single model without external age labeled data. In <ref type="bibr" target="#b14">[15]</ref>, they not only used external age labeled data but also employed multi-model ensemble. On Morph, our method creates a new state-of-the-art 1.969 MAE. To our best knowledge, this is the first time to achieve below two years in MAE on the Morph dataset.  In short, our DLDL-v2 (ThinAgeNet) outperforms the stateof-the art methods without external age labeled data and multimodel ensemble on ChaLearn15, ChaLearn16, Morph and UTKFace.</p><p>High Efficiency. We measure the speed on one P40 GPU accelerated by cuDNN v7.6. The number of parameters, forward/backward memory size, computational complexity (GFlops) and actual inference time of our approach and some previous methods are reported in <ref type="table" target="#tab_6">Table 6</ref>. Since <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref> do not release pre-trained models, we cannot test the running time and report the number of parameters of these models. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57]</ref> all used similar network architecture (i.e., VGG-16 or VGGFace). Since <ref type="bibr" target="#b14">[15]</ref> employed 14 models, it's model size and running time is 14 times of <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b7">[8]</ref>. Compared to the state-of-the-art, DLDL-v2 (ThinAgeNet) achieves the best performance using single model with 36? fewer parameters and 3? reduction in inference time. Furthermore, we also report DLDL-v2's TinyA-geNet results which achieve a better result (150? fewer parameters and 6? speed improvement) than the original DLDL <ref type="bibr" target="#b7">[8]</ref>.</p><p>In addition, we compare ThinAgeNet and TinyAgeNet with   <ref type="bibr" target="#b55">[56]</ref>, and MobileNet-v2 <ref type="bibr" target="#b60">[61]</ref>. Note that we compute inference time with two different batch sizes (n=128 and n=1). It has been observed in practice that when using a larger batch there is a significant speed up because of parallel computing characteristics of GPU. We can see that the TinyA-geNet is the best one among all networks both memory storage overhead and actual inference time. It is worth noting that although MobileNet-v2 has a lower theoretical computational complexity, it is still slower than our TinyAgeNet in terms of actual inference time. The main reason is that the depth-wise separable convolution in MobileNet-v2 requires efficient convolution computation, which may be still not well supported. <ref type="figure">Fig. 4</ref> shows some examples on ChaLearn16 testing images using our DLDL-v2 ThinAgeNet. In many cases, our solution is able to predict the age of faces accurately. Failures may come from some special cases such as occlusion, low resolution, heavy makeup and extreme pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Visual Assessment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Experiments on Attractiveness Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Attractiveness Estimation Datasets</head><p>To further demonstrate the effectiveness of the proposed DLDL-v2, we perform extensive experiments on two facial attractiveness datasets: SCUT-FBP <ref type="bibr" target="#b20">[21]</ref> and CFD <ref type="bibr" target="#b21">[22]</ref>.</p><p>SCUT-FBP <ref type="bibr" target="#b20">[21]</ref> is a widely used facial beauty assessment dataset. It contains 500 Asian female faces with neutral expressions, simple backgrounds, no accessories, and minimal occlusion. Each face is scored by 75 workers with a 5-point scale, in which 1 means strong agreement about the face being the  <ref type="figure">Figure 4</ref>: Examples of apparent age estimation using DLDL-v2 (ThinAgeNet) on ChaLearn16 testing images. The left ten columns show good age estimations and the right five columns are poor cases.  least attractive and 5 means strong agreement about the face being the most attractive. For each face, its mean score and the corresponding standard deviation are given. We follow the setting in <ref type="bibr" target="#b12">[13]</ref> and <ref type="bibr" target="#b20">[21]</ref>, 80% images are randomly selected as the training set, and the remain 20% as the testing set.</p><p>CFD <ref type="bibr" target="#b21">[22]</ref> provides high-resolution and standardized photographs with meaningful annotations (e.g., attractiveness, babyfacedness and expression etc.). Unlike SCUT-FBP, this dataset includes male and female faces of multiple ethnicity (Asian, Black, Latino, and White) between the ages of 17-65. Similar to SCUT-FBP, each faces is scored by some participants with diverse background in a 7-point scale (1 = Not at all, 7 = Extremely). In this study, we employ all 597 faces with natural expression and the corresponding attractiveness scores for experiments. We use 80% images for training and the remain 20% for testing. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Attractiveness Estimation Results</head><p>In <ref type="table" target="#tab_7">Table 7</ref> and 8 , we report the performance on SCUT-FBP and CFD and compare with the state-of-the-art methods in the literature.</p><p>Comparing with those methods using hand-crafted features, such as Regression <ref type="bibr" target="#b20">[21]</ref> and SLDL <ref type="bibr" target="#b35">[36]</ref>, the proposed DLDL-v2 (ThinAttNet) achieves 0.930 PC and 0.212 MAE on SCUT-FBP. It outperforms Regression <ref type="bibr" target="#b20">[21]</ref> by 0.282 in PC, and improves SLDL <ref type="bibr" target="#b35">[36]</ref> by 0.135 in RMSE. What is more, for those methods using deep label distribution, such as LDL (ResNet50) <ref type="bibr" target="#b12">[13]</ref> as one of the state-of-the-art methods, our DLDL-v2 still outperforms it. Furthermore, our method is comparable to the fusional solution of deep features and geometric features in <ref type="bibr" target="#b12">[13]</ref>. There are two major reasons. First, our pre-trained model is trained on a face recognition dataset which is closer to facial attractiveness than those object classification datasets (ResNet50 is trained by ImageNet) in <ref type="bibr" target="#b12">[13]</ref>. Second, we jointly learn label distribution and regress the facial attractiveness score in DLDL-v2, which can effectively erase the inconsistency between training objective and evaluation metric (MAE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Visual Assessment</head><p>In order to intuitively visualize the prediction performance of our DLDL-v2 on facial attractiveness task, we show the top eight and bottom eight test images based on the prediction scores of DLDL-v2 with ThinAttNet in <ref type="figure">Fig. 5</ref>. On selected 16 testing images, prediction scores of 12 images highly match with those of human raters. This result qualitatively demonstrates that our DLDL-v2 is able to generate human-like results. In addition, some possible facial attractiveness cues may be observed via comparing between the top and bottom faces with attractiveness score. Generally speaking, faces with higher attractive scores have smoother and lighter skin, oval face with larger eyes, narrower nose with a pointed tip, and better harmony in facial organs than those with lower scores.  <ref type="figure">Figure 5</ref>: Examples of facial attractiveness estimation using DLDL-v2 (ThinAttNet) on SCUT-FBP testing images. These images are ranked by our DLDL-v2 prediction scores. The top eight faces are showed in the left side, and the bottom eight faces are showed in the right side. The red numbers denote good estimation (MAE is less than 0.3), and the blue numbers denote poor estimation (MAE is greater than 0.3). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study and Diagnostic Experiments</head><p>DLDL-v2 (ThinAgeNet) is employed for ablation study on facial age datasets in this section. We firstly investigate the efficacy of the proposed data augmentation and the pooling strategy. For fair comparison, we fix l = 1 and ? = 1. Then, to investigate the effectiveness of the proposed joint learning mechanism, we compare it with two stage and single stage methods under the same setting. At last, we also explore the sensitivity of hyper-parameters in our DLDL-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1.">Influence of Data Augmentation</head><p>Data augmentation techniques increase the amount of training data using information only in training set, which is an effective way to reduce the over-fitting of deep models. From <ref type="table" target="#tab_10">Table 9</ref>, we can observe 0.26?0.27 MAE improvements on apparent age datasets and 0.38 MAE improvement on Morph using data augmentation. This indicates that data augmentation can greatly improve the performance of age estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2.">Effectiveness of Pooling Strategy</head><p>GAP is one of the most popular and simple method for aggregating the spatial information of feature maps in state-ofthe-art network architecture such as ResNet <ref type="bibr" target="#b43">[44]</ref>. It outputs the spatial average of each feature map of the last convolution layer. Max-pooling takes the maximal value of each small region (e.g., 2?2) in a feature map as its output. HP is constructed by a max-pooling and a GAP layer. HP firstly encourages the network to learn a discriminative feature in a small region via max-pooling, then all discriminative features are aggregated by GAP. Thus, the feature of HP is more discriminative than that of GAP. If we directly use global max-pooling instead of HP, the training of network easily fall into over-fitting. To explore the effect of the pooling strategy, we further use the HP to replace the traditional GAP when combining data augmentation. It can be seen in <ref type="table" target="#tab_10">Table 9</ref> that the proposed HP can consistently reduce the prediction error on all datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3.">Comparisons with Two Stage Methods</head><p>We compare the proposed approach with two stage methods considering two types of features. The first one is the BIF <ref type="bibr" target="#b25">[26]</ref>, as the most successful hand-crafted age feature, which was widely used in age estimation. The second one is CNN features which are extracted from our pre-trained face recognition model. For BIF, we adopt 6 bands and 8 orientations <ref type="bibr" target="#b25">[26]</ref>, which produces 4616-dimensional features. The CNN features are extracted from the hybrid pooling layer of the pre-trained model and their dimension is 256. These features are normalized by 2 without using any dimensionality reduction technique.</p><p>We choose three classical age estimation algorithms, including SVR <ref type="bibr" target="#b25">[26]</ref>, OHRank <ref type="bibr" target="#b29">[30]</ref> and BFGS-LDL <ref type="bibr" target="#b34">[35]</ref>. For SVR and OHRank, the Liblinear software is used to train regression or classification models. <ref type="bibr" target="#b1">2</ref> For BFGS-LDL, we use the open source LDL package. <ref type="bibr" target="#b2">3</ref> Instead of age prediction with the maximal probability in <ref type="bibr" target="#b34">[35]</ref>, we use the expected value over prediction distribution because it has better performance.</p><p>The experimental results are shown in <ref type="table" target="#tab_0">Table 10</ref>. First, OHRank and BFGS-LDL using BIF and CNN features have similar performances on all datasets. This further validates our previous analysis that ranking is learning label distribution. Second, our proposed approach significantly outperforms all baseline methods. The major reason is that two stage methods cannot learn visual representations. This suggests that it is crucially important to jointly learn visual features and recognition model using an end-to-end manner. At last, OHRank and BFGS-LDL are much better than SVR, which indicates learn-ing label distribution can really help us to improve estimation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.4.">Comparisons with Single Stage Methods</head><p>We employ six very strong methods under the same setting as baselines:</p><p>? MR: In MR, the ground-truth label y is projected to [?1, 1] by a linear transform. For MR, we need to make a little modification to DLDL-v2. Specifically, we add an FC layer with single output after HP, and follow a hyperbolic tangent activation function f (x) = tanh(x) for speedup convergence. The 2 and 1 loss function is used to train MR. ? DEX: In DEX, true label y is quantized to different label group, which is treated as a class. To train DEX, we only need remove the expectation module and modify loss function to cross-entropy loss in DLDL-v2. In inference time, an expected value over prediction probabilities is used for final estimation. ? Ranking: In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, multiple binary classification networks are independently trained, which lead to timeconsuming of training and storage overhead of model. We propose a new multiple output CNN and jointly train these binary classifiers. Specifically, we firstly remove the label distribution and expectation module in DLDL-v2. Then, we add an FC layer with K ? 1 output units and follow a sigmoid layer. For training Ranking CNN, we employ K ? 1 binary cross-entropy loss. In inference stage, the prediction is computed by? = l i * , where i * = 1</p><formula xml:id="formula_23">+ K?1 k=1 [?(x) &gt; 0.5].</formula><p>[?] denotes the truth-test operator, which is 1 if the inner condition is true, and 0 otherwise. Our experiments showed that this new setup has lower MAE than that in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>? ER ( 1 ): We only employ the expectation regression (ER) loss L er to optimize DLDL-v2's parameters via removing label distribution loss L ld in Eq. (12). ? DLDL: We set ? = 0 in Eq. (12) to learn DLDL-v2. <ref type="table" target="#tab_0">Table 11</ref> reports the results of all single stage methods. We can see that the MAE and -error of Ranking, ER and DLDL methods are significantly lower than that of MR and DEX on all datasets. This indicates that utilizing label distribution is helpful to reduce age estimation error. Meanwhile, we also find that the prediction error of Ranking is close to that of DLDL, which conforms to the analysis in Section 3.2. Furthermore, the performance of DLDL is better than that of Ranking, which suggests that learning p.d.f. is more effective than learning c.d.f. It is noteworthy that ER ( 1 ) and DLDL are two extreme cases of our DLDL-v2. DLDL-v2 consistently outperforms ER ( 1 ) and DLDL on all datasets, which indicates the joint learning can ease the difficult of network optimization. In <ref type="table" target="#tab_0">Table 11</ref>, we can see that the proposed joint learning achieves the best performance among all methods. It means that erasing the inconsistency between training and evaluation stages can help us make a better prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.5.">Sensitivity of Hyper-parameters</head><p>We explore the influence of hyper-parameters ? and l, where ? is a weight which balances the importance between  label distribution and expectation regression loss, and l refers to the number of discrete labels (K). In <ref type="table" target="#tab_0">Table 12</ref>, we report results on all three age datasets with different value of ? and l. We can see that our method is not sensitive to ? and l with 0.01 ? ? ? 10 and 0.25 ? l ? 4. Note that, too many discrete labels lead to little training samples for per class in DEX <ref type="bibr" target="#b6">[7]</ref>, which may make prediction less precise. However, our method can ease the problem, because the training samples associated with each class label is significantly increased without actually increase the number of the total training examples. Surprising, there is also a good enough performance when the number output neurons (i.e., K) is 26. In our experiment, we fixed hyperparameters l = 1 and ? = 1 without carefully tuning them. In practice, it is convenient to find optimal hyper-parameters using a hold-out set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Understanding DLDL-v2</head><p>We have demonstrated that DLDL-v2 has excellent performance for facial age and attractiveness estimation. A natural problem is how DLDL-v2 makes the final decision for an input facial image. In this section, we try to answer this question. Then, we analyze why it can work well when compared with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">How Does DLDL-v2 Estimate Facial Attributes?</head><p>In order to understand how DLDL-v2 makes the final decision for an input facial image, we visualize a score map that can intuitively show which regions of face image are related to the network decision. To obtain the score map, we firstly employ a class-discriminative localization technique <ref type="bibr" target="#b65">[66]</ref> that can <ref type="figure">Figure 6</ref>: Examples of the score map visualizations on ChaLearn16 testing images. The first row is for infants ([0, 3]), the second row is for adults ( <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b34">35]</ref>) and the third row is for senior people ([65, 100]) (Best viewed in color). generate class activation maps. Then, these activation maps are aggregated by predicted probabilities.</p><p>Let us briefly review our framework. The last convolution block produce activation maps F j ? R h ?w . These activations are then spatially pooled by a hybrid pooling and linearly transformed (i.e., Eq. <ref type="formula" target="#formula_7">(7)</ref>) to produce probabilitiesp ? R K with a label distribution module. To produce class activation maps, we apply linearly transform layer to F j as follows</p><formula xml:id="formula_24">A k = j w k j F j + b k .<label>(23)</label></formula><p>Then, the score map can be derived by</p><formula xml:id="formula_25">S = kp k A k .<label>(24)</label></formula><p>In Eq. (24), the value of S i j represents the contribution of the network's decision at position of i-th row and j-th column. Bigger values mean more contributions and vice versa. For comparing the correspondence between highlighted regions in S and an input image, we scale S to the size of an input image. In <ref type="figure">Fig. 6</ref>, we visualize the score maps of testing images (ChaLearn16) coming from different age group. we can see that the highlighted regions (i.e., red regions) are significantly different for different age group faces. For infants, the highlighted region locates in the center of two eyes. For adults, the strong areas include two eyes, nose and mouth. For senior people, the highlighted regions consist of the forehead, brow, two eyes and nose. In short, the network uses different patterns to estimate different age.</p><p>We also show some examples coming from SCUT-FBP testing images in <ref type="figure" target="#fig_3">Fig. 7</ref>. We can observe that it is not significant for the highlighted regions between these faces with higher attractiveness score and that of lower score. An explanation is that DLDL-v2 may be able to estimate facial attractiveness through simply comparing the difference of the common facial traits such as eyebrows, eyes, nose, mouth etc.. In fact, the SCUT-FBP dataset indeed has the lower complexity (female faces with simple backgrounds, no accessories, and minimal occlusion) than age estimation on ChaLearn16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sensitivity to Different Face Regions</head><p>To further quantitatively analyze the sensitivity of DLDL-v2 to different face regions. We occlude different portions of the input image by setting it to mean values of all training images.    Specifically, we use two type of occlusions, small square region (size of 32?32) and horizontal stripe (size of 32?224), as in <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b6">7]</ref>. We occlude the input images (size of 224?224) using this two type of occlusions in a sliding window fashion. In all, we obtain 49+7 occluded inputs for each input image. For each occluded input, we record prediction performance (i.e., MAE) on all testing images. Finally, we compute the relative performance loss between with and without occlusions to measure the sensitivity of occlusion region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCUT-FBP</head><p>In <ref type="figure" target="#fig_5">Fig. 8</ref>, we show the quantitative results under different occlusions. First, we observe that larger values usually appear in some specific regions such as forehead, two eyes, nose, mouth, and chin. This indicates the decision of DLDL-v2 heavily depends on these crucial regions. Second, these values are significantly different in the different regions. For example, on ChaLearn16 testing images, the largest and second largest value appear around nose and eyes, which suggests nose and eyes are the most important facial traits for age estimation. Third, although SCUT-FBP and CFD both are used to evaluate facial attractiveness datasets, the distributions of the largest value are greatly different. The largest value of the former appears the region of eyes, and that of the latter appears the region of mouth and chin. In fact, the faces of SCUT-FBP come from Asia female, which is scored by Chinese, while CFD dataset consists multi-race faces and is scored by diverse background annotators. Therefore, this difference may be due to the phenomenon that different races may have an inconsistent understanding of facial attractiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Why does DLDL-v2 make good estimation?</head><p>Compared to MR, the training procedure of our DLDL-v2 is more stable because it not only regresses the single value with expectation module but also learns a label distribution. Compared to DEX, through introducing label distribution learning module to DLDL-v2, the training instances associated with each class label is significantly increased without actually increasing the number of the total training images, which effectively alleviate the risk of over-fitting.</p><p>For Ranking and DLDL-based methods, we have proved that they are both learning a label distribution from different levels. Therefore, they both share the advantages of label distribution learning. However, there are three major differences in the network architectural between these methods and our DLDL-v2. First, these methods depend heavily on a pre-trained model such as VGGNet or VGGFace with more parameters while DLDL-v2 has a thinner architecture with fewer parameters. Therefore, DLDL-v2 has higher efficiency in inference time and lower storage overhead. Second, DLDL-v2 effectively avoids the inconsistency between training objective and evaluation metric via introducing the expectation regression module. Third, DLDL-v2 is a fully convolutional network, which removes all but the final fully connected layer. It is very helpful to understand that how DLDL-v2 makes the final decision because of the parameter sharing mechanism between Eq. 7 and 23. In a word, these differences make DLDL-v2 have good performance on accuracy, speed, model size and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we proposed a solution for facial age and attractiveness estimation problems. We firstly analyze that Ranking-based methods are implicitly learning label distribution as DLDL-based methods. This result unifies existing stateof-the-art facial age and attractiveness estimation methods into the DLDL framework. Second, our proposed DLDL-v2 framework can effectively erase the inconsistency between training and evaluation stages via jointly learning label distribution and regressing single value with a thin and deep network architecture. It creates new state-of-the-art results on facial age and attractive estimation tasks with fewer parameters and faster speed, which indicates it is easy to be deployed on resourceconstrained devices. In addition, our DLDL-v2 is also a partly interpretable deep framework which employs different patterns to estimate facial attributes.</p><p>It is noteworthy that our approach is easily scalable to others label uncertainty tasks, such as skeletal maturity assessment on pediatric hand radiographs <ref type="bibr" target="#b67">[68]</ref>, head pose estimation <ref type="bibr" target="#b68">[69]</ref>, popularity of selfie <ref type="bibr" target="#b69">[70]</ref>, image aesthetic assessment <ref type="bibr" target="#b70">[71]</ref> etc. In addition, a further theoretical study between the ranking-CNN and DLDL will also be our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of facial attributes' distributions. (a) illustrates distributions of three facial age datasets, and (b) illustrates distributions of two facial attractiveness datasets (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>one minus c.d.f.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>(a) and (b) show p.d.f. and c.d.f. curves with the same mean and different standard deviation. (c) shows the curves of one minus c.d.f. and ranking encoding (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Examples of the score map visualizations on SCUT-FBP testing images. The first and second row shows Top-10 and Bottom-10 faces rated by prediction scores (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>0%</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Sensitivity to different face regions on facial age and attractiveness estimation datasets. The first row shows average image of all testing images on four datasets. The second and third row show heatmaps of the relative performance loss under two occlusions, respectively (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Details of facial age and attractiveness datasets.</figDesc><table><row><cell></cell><cell>Dataset</cell><cell cols="2">#Images Label range</cell></row><row><cell></cell><cell cols="2">ChaLearn15 [5] 2476+1136</cell><cell>3-85</cell></row><row><cell>Age</cell><cell cols="2">ChaLearn16 [18] 5613+1978 UTKFace [19] 24108</cell><cell>0-96 0-116</cell></row><row><cell></cell><cell>Morph [20]</cell><cell>55134</cell><cell>16-70</cell></row><row><cell>Attractiveness</cell><cell>SCUT-FBP [21] CFD [22]</cell><cell>500 597</cell><cell>1-5 1-7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons with state-of-the-art methods for real age estimation on Morph dataset.</figDesc><table><row><cell>Methods</cell><cell>Venue Year</cell><cell cols="2">External Data Single?</cell><cell>Model Network</cell><cell>MAE?</cell></row><row><cell>Human [47]</cell><cell>TPAMI 2015</cell><cell>?</cell><cell>Yes</cell><cell>human workers</cell><cell>6.30</cell></row><row><cell>OR-CNN [9]</cell><cell>CVPR 2016</cell><cell>?</cell><cell>Yes</cell><cell>MR-CNN</cell><cell>3.27</cell></row><row><cell>DEX [7]</cell><cell>IJCV 2016</cell><cell>?</cell><cell>Yes</cell><cell>VGG-16</cell><cell>3.25</cell></row><row><cell>DEX [7]</cell><cell>IJCV 2016</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell>2.68</cell></row><row><cell>LDAE [48]</cell><cell>PR 2017</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell>2.35</cell></row><row><cell>DLDL [8]</cell><cell>TIP 2017</cell><cell>?</cell><cell>Yes</cell><cell>VGG-Face</cell><cell>2.42</cell></row><row><cell cols="2">Rank-CNN [10, 11] CVPR 2017</cell><cell>?</cell><cell>No</cell><cell>Binary CNNs</cell><cell>2.96</cell></row><row><cell>DLDLF [39]</cell><cell>NIPS 2017</cell><cell>?</cell><cell>Yes</cell><cell>VGG-Face</cell><cell>2.24</cell></row><row><cell>DRFS [42]</cell><cell>CVPR 2018</cell><cell>?</cell><cell>Yes</cell><cell>VGG-16</cell><cell>2.17</cell></row><row><cell>MV [49]</cell><cell>CVPR 2018</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell>2.16</cell></row><row><cell>AgeEn [50]</cell><cell>TPAMI 2018</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell>2.52</cell></row><row><cell>C3AE [51]</cell><cell>CVPR 2019</cell><cell></cell><cell>Yes</cell><cell>C3AE</cell><cell>2.75</cell></row><row><cell>BridgeNet [52]</cell><cell>CVPR 2019</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell>2.38</cell></row><row><cell>SADAL [53]</cell><cell>TMM 2020</cell><cell>?</cell><cell>Yes</cell><cell>VGG-Face</cell><cell>2.59</cell></row><row><cell>VDAL [53]</cell><cell>TMM 2020</cell><cell>?</cell><cell>Yes</cell><cell>VGG-Face</cell><cell>2.57</cell></row><row><cell>1CH [54]</cell><cell>ICLR 2020</cell><cell></cell><cell cols="3">Yes VGG-16 (w/o FCs) 2.22</cell></row><row><cell>POE [55]</cell><cell>CVPR 2021</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell>2.35</cell></row><row><cell>PML [56]</cell><cell>CVPR 2021</cell><cell>?</cell><cell>Yes</cell><cell>ResNet-34</cell><cell>2.31</cell></row><row><cell>PML [56]</cell><cell>CVPR 2021</cell><cell></cell><cell>Yes</cell><cell>ResNet-34</cell><cell>2.15</cell></row><row><cell cols="2">DLDL-v2 (TinyAgeNet)</cell><cell>?</cell><cell>Yes</cell><cell>Thin VGG-16</cell><cell>2.29</cell></row><row><cell cols="2">DLDL-v2 (ThinAgeNet)</cell><cell>?</cell><cell>Yes</cell><cell>Tiny VGG-16</cell><cell>1.97</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>Venue Year</cell><cell cols="2">External Data Single?</cell><cell cols="3">Model Network MAE? -error? ValSet</cell></row><row><cell cols="2">Human [47] TPAMI 2015</cell><cell>?</cell><cell>Yes</cell><cell>ResNet-50</cell><cell>-</cell><cell>0.340</cell></row><row><cell>DEX [7]</cell><cell>IJCV 2016</cell><cell>?</cell><cell>Yes</cell><cell>VGG-16</cell><cell cols="2">5.369 0.456</cell></row><row><cell>DEX [7]</cell><cell>IJCV 2016</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell cols="2">3.252 0.282</cell></row><row><cell>DLDL [8]</cell><cell>TIP 2017</cell><cell>?</cell><cell>Yes</cell><cell cols="3">VGG-Face 3.510 0.310</cell></row><row><cell>AGEn [50]</cell><cell>TPAMI 2018</cell><cell></cell><cell>Yes</cell><cell>VGG-16</cell><cell cols="2">3.210 0.280</cell></row><row><cell>ODL [57]</cell><cell>TCSVT 2019</cell><cell>?</cell><cell>Yes</cell><cell cols="3">VGG-Face 3.950 0.312</cell></row><row><cell cols="2">SADAL [53] TMM 2020</cell><cell>?</cell><cell>Yes</cell><cell cols="3">VGG-Face 3.780 0.309</cell></row><row><cell>VDAL [53]</cell><cell>TMM 2020</cell><cell>?</cell><cell>Yes</cell><cell cols="3">VGG-Face 3.580 0.285</cell></row><row><cell>PML [56]</cell><cell>CVPR 2021</cell><cell>?</cell><cell>Yes</cell><cell cols="3">ResNet-34 3.455 0.293</cell></row><row><cell>PML [56]</cell><cell>CVPR 2021</cell><cell></cell><cell>Yes</cell><cell cols="3">ResNet-34 2.915 0.243</cell></row><row><cell cols="2">DLDL-v2 (TinyAgeNet)</cell><cell>?</cell><cell cols="4">Yes Tiny VGG-16 3.427 0.301</cell></row><row><cell cols="2">DLDL-v2 (ThinAgeNet)</cell><cell>?</cell><cell cols="4">Yes Thin VGG-16 3.135 0.272</cell></row></table><note>Comparisons with state-of-the-art methods for apparent age estimation on ChaLearn 2015 dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons with state-of-the-art methods for apparent age estimation on ChaLearn 2016 dataset.</figDesc><table><row><cell>Methods</cell><cell>Venue Year</cell><cell cols="5">External Data Num. Nets? Network MAE? -error? Model TestSet</cell></row><row><cell cols="2">OrangeLabs [15] CVPRW 2016</cell><cell></cell><cell>14</cell><cell>VGG-16</cell><cell>-</cell><cell>0.241</cell></row><row><cell>palm-seu [58]</cell><cell>CVPRW 2016</cell><cell></cell><cell>4</cell><cell>VGG-Face</cell><cell>-</cell><cell>0.321</cell></row><row><cell cols="2">cmp+ETH [59] CVPRW 2016</cell><cell></cell><cell>10</cell><cell>VGG-16</cell><cell>-</cell><cell>0.336</cell></row><row><cell>AGEn [50]</cell><cell>TPAMI 2018</cell><cell></cell><cell>1</cell><cell cols="3">VGG-16 3.820 0.310</cell></row><row><cell>MV [49]</cell><cell>CVPR 2018</cell><cell></cell><cell>1</cell><cell>VGG-16</cell><cell>-</cell><cell>0.287</cell></row><row><cell cols="2">DLDL-v2 (TinyAgeNet)</cell><cell>?</cell><cell>1</cell><cell cols="3">Tiny-VGG 3.765 0.291</cell></row><row><cell cols="2">DLDL-v2 (ThinAgeNet)</cell><cell>?</cell><cell>1</cell><cell cols="3">Thin-VGG 3.452 0.267</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">Comparisons with state-of-the-art methods for age estimation on UTK-</cell></row><row><cell cols="6">Face. The results of OR-CNN [9] and CORAL [46] are from Gustafsson et</cell></row><row><cell cols="6">al. [60].  ? indicates methods using entire UTKFace dataset (0-116 years old)</cell></row><row><cell cols="6">and  ? indicates methods using a subset of UTKFace covering faces between 21</cell></row><row><cell>and 60 years old.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Venue Year</cell><cell cols="3">External Data Single? Network Model</cell><cell>MAE?</cell></row><row><cell>OR-CNN [9]  ?</cell><cell>CVPR 2016</cell><cell></cell><cell>Yes</cell><cell>ResNet-50</cell><cell>5.74</cell></row><row><cell cols="2">Andrey Savchenko [61]  ? PeerJ 2019</cell><cell></cell><cell cols="3">Yes MobileNet-v2 5.44</cell></row><row><cell>Axel Berg et al.[62]  ?</cell><cell>ICPR 2019</cell><cell>?</cell><cell>Yes</cell><cell>ResNet-50</cell><cell>4.55</cell></row><row><cell>CORAL [46]  ?</cell><cell>PRL 2020</cell><cell>?</cell><cell>Yes</cell><cell>ResNet-50</cell><cell>5.47</cell></row><row><cell>Gustafsson et al. [60]  ?</cell><cell>ECCV 2020</cell><cell>?</cell><cell>Yes</cell><cell>ResNet-50</cell><cell>4.65</cell></row><row><cell cols="2">DLDL-v2 (TinyAgeNet)  ?</cell><cell>?</cell><cell>Yes</cell><cell>Tiny-VGG</cell><cell>4.26</cell></row><row><cell cols="2">DLDL-v2 (ThinAgeNet)  ?</cell><cell>?</cell><cell>Yes</cell><cell>Thin-VGG</cell><cell>4.24</cell></row><row><cell cols="2">DLDL-v2 (TinyAgeNet)  ?</cell><cell>?</cell><cell>Yes</cell><cell>Tiny-VGG</cell><cell>4.27</cell></row><row><cell cols="2">DLDL-v2 (ThinAgeNet)  ?</cell><cell>?</cell><cell>Yes</cell><cell>Thin-VGG</cell><cell>4.15</cell></row></table><note>some recent network architectures such as ResNet-50 [60, 62, 47], ResNet-34</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparisons of model parameters and forward times with state-ofthe-art methods. M means million (10 6 ), G means billion (10 9 ), MB means megabyte and Time denotes the average forward times of n images in milliseconds on one P40 GPU.</figDesc><table><row><cell>Methods</cell><cell>Model artitecture</cell><cell>Param Memory GFlops Time (ms)? (M)? (MB)? (G)? n=1 n=128</cell></row><row><cell>[8, 7, 48, 39, 42, 49]</cell><cell></cell><cell></cell></row><row><cell>[50, 52, 53, 55, 57]</cell><cell>VGG-16</cell><cell>134.7 322.13 15.53 7.09 3.36</cell></row><row><cell>[15, 58, 59]</cell><cell></cell><cell></cell></row><row><cell>[54]</cell><cell cols="2">VGG-16(w/o FCs) 14.8 321.76 15.41 4.93 3.26</cell></row><row><cell>[60, 62, 47]</cell><cell>ResNet-50</cell><cell>23.7 286.55 4.12 8.15 1.92</cell></row><row><cell>[56]</cell><cell>ResNet-34</cell><cell>21.3 96.28 3.67 6.00 0.88</cell></row><row><cell>[61]</cell><cell>MobileNet-v2</cell><cell>2.4 152.86 0.32 6.48 0.87</cell></row><row><cell>ThinAgeNet</cell><cell>Thin-VGG</cell><cell>3.7 160.88 3.88 2.35 1.17</cell></row><row><cell>TinyAgeNet</cell><cell>Tiny-VGG</cell><cell>0.9 80.44 0.98 2.20 0.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparisons with state-of-the-art methods for facial attractiveness estimation on SCUT-FBP dataset. ? indicates methods using ten-fold cross validation, 90% of images for training and 10% for evaluation and ? indicates methods using multi-features fusion.</figDesc><table><row><cell>Methods</cell><cell>Venue Year</cell><cell>Model</cell><cell cols="3">MAE? RMSE? PC?</cell></row><row><cell>Regression [21]</cell><cell>SMC 2015</cell><cell>G+Tfeats</cell><cell cols="3">0.393 0.515 0.648</cell></row><row><cell>SLDL [36]  ?</cell><cell cols="4">IJCAI 2017 LBP+Hog+Gabor 0.302 0.408</cell><cell>-</cell></row><row><cell>CNN [21]</cell><cell cols="2">SMC 2015 Six-layer CNN</cell><cell>-</cell><cell>-</cell><cell>0.819</cell></row><row><cell>LDL [13]</cell><cell>TMM 2017</cell><cell>ResNet-50</cell><cell cols="3">0.217 0.300 0.917</cell></row><row><cell>LDL [13]  ?</cell><cell cols="5">TMM 2017 ResNet-50+GFeats 0.213 0.278 0.930</cell></row><row><cell cols="2">R2-ResNeXt [63] ICPR 2018</cell><cell>ResNeXt</cell><cell cols="3">0.242 0.305 0.896</cell></row><row><cell>DALDL [64]</cell><cell>CCBR 2019</cell><cell>VGG-16</cell><cell cols="3">0.227 0.312 0.903</cell></row><row><cell>MT-ResNet [65]</cell><cell>CDS 2021</cell><cell>ResNet-50</cell><cell cols="3">0.246 0.321 0.891</cell></row><row><cell cols="2">DLDL-v2 (TinyAttNet)</cell><cell>Tiny VGG-16</cell><cell cols="3">0.221 0.294 0.915</cell></row><row><cell cols="2">DLDL-v2 (ThinAttNet)</cell><cell>Thin VGG-16</cell><cell cols="3">0.212 0.273 0.930</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Comparisons with state-of-the-art methods for facial attractiveness estimation on CFD dataset.</figDesc><table><row><cell>Methods</cell><cell>MAE? RMSE? PC?</cell></row><row><cell cols="2">DLDL-v2 (TinyAttNet) 0.400 0.521 0.716</cell></row><row><cell cols="2">DLDL-v2 (ThinAttNet) 0.364 0.472 0.766</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison of different methods for age estimation.</figDesc><table><row><cell></cell><cell>Factors</cell><cell>ChaLearn15 ChaLearn16 Morph</cell></row><row><cell cols="3">Aug Pooling MAE -error MAE -error MAE</cell></row><row><cell>? ? ?</cell><cell cols="2">HP 3.399 0.303 3.717 0.290 2.346 GAP 3.210 0.282 3.539 0.274 2.039 HP 3.135 0.272 3.452 0.267 1.969</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Comparisons with two stage methods for age estimation (lower is better).</figDesc><table><row><cell cols="2">Feats Methods</cell><cell>ChaLearn15 ChaLearn16 Morph MAE -error MAE -error MAE</cell></row><row><cell></cell><cell>SVR</cell><cell>6.832 0.545 9.225 0.595 4.303</cell></row><row><cell>BIF</cell><cell cols="2">OHRank 6.403 0.525 7.680 0.533 3.841</cell></row><row><cell></cell><cell cols="2">BFGS-LDL 6.441 0.505 7.626 0.515 3.883</cell></row><row><cell></cell><cell>SVR</cell><cell>5.333 0.471 6.348 0.495 4.370</cell></row><row><cell>CNN</cell><cell cols="2">OHRank 4.202 0.383 4.668 0.380 3.919</cell></row><row><cell></cell><cell cols="2">BFGS-LDL 4.037 0.359 4.457 0.345 3.865</cell></row><row><cell cols="2">DLDL-v2</cell><cell>3.135 0.272 3.452 0.267 1.969</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Comparisons with single stage methods for age estimation (lower is better).</figDesc><table><row><cell>Methods</cell><cell>ChaLearn15 ChaLearn16 Morph MAE -error MAE -error MAE</cell></row><row><cell cols="2">MR ( 2 ) 3.665 0.337 3.696 0.294 2.282</cell></row><row><cell cols="2">MR ( 1 ) 3.655 0.334 3.722 0.301 2.347</cell></row><row><cell>DEX</cell><cell>3.558 0.306 4.163 0.332 2.311</cell></row><row><cell cols="2">Ranking 3.365 0.298 3.645 0.290 2.164</cell></row><row><cell>ER ( 1 )</cell><cell>3.287 0.291 3.641 0.282 2.214</cell></row><row><cell>DLDL</cell><cell>3.228 0.285 3.509 0.272 2.132</cell></row><row><cell cols="2">DLDL-v2 3.135 0.272 3.452 0.267 1.969</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>The influences of hyper-parameters for our DLDL-v2.</figDesc><table><row><cell cols="2">Hyper-param</cell><cell>ChaLearn15 ChaLearn16 Morph</cell></row><row><cell>?</cell><cell cols="2">l (K) MAE -error MAE -error MAE</cell></row><row><cell cols="3">0.01 1 (101) 3.223 0.282 3.493 0.270 1.960</cell></row><row><cell cols="3">0.10 1 (101) 3.188 0.278 3.455 0.268 1.972</cell></row><row><cell cols="3">1.00 1 (101) 3.135 0.272 3.452 0.267 1.969</cell></row><row><cell cols="3">10.00 1 (101) 3.144 0.273 3.487 0.270 1.977</cell></row><row><cell>1.00</cell><cell cols="2">4 (26) 3.182 0.276 3.473 0.270 1.963</cell></row><row><cell>1.00</cell><cell cols="2">2 (51) 3.184 0.274 3.484 0.271 1.963</cell></row><row><cell cols="3">1.00 0.50 (201) 3.184 0.278 3.484 0.269 1.992</cell></row><row><cell cols="3">1.00 0.25 (401) 3.167 0.274 3.459 0.265 2.028</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">0.5 compression rate means every Conv layer has only 50% channels as that in VGG-16.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.csie.ntu.edu.tw/~cjlin/liblinear/ 3 http://ldl.herokuapp.com/download</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. British Machine Vision Conference</title>
		<meeting>British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep-feature encoding-based discriminative model for age-invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Shakeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="442" to="457" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ChaLearn looking at people 2015: Apparent age and cultural event recognition datasets and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision Workshop</title>
		<meeting>IEEE International Conference on Computer Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An allin-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep expectation of real and apparent age from a single image without facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep label distribution learning with label ambiguity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ordinal regression with multiple output CNN for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4920" to="4928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using Ranking-CNN for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5183" to="5192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep age estimation: From classification to ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2209" to="2222" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">D2c: Deep cumulatively and comparatively learning for human age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="95" to="105" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Label distribution-based facial attractiveness computation by deep residual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Samal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2196" to="2208" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Age and gender recognition in the wild with deep attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="563" to="571" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Apparent age estimation from face images combining general and children-specialized deep learning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Berrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ThiNet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5058" to="5066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ChaLearn looking at people and faces of the world: Face analysis workshop and challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Torres</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Corneou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Ali</forename><surname>Bagheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshop</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5810" to="5818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Morph: A longitudinal image database of normal adult age-progression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ricanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tesafaye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Automatic Face and Gesture Recognition</title>
		<meeting>IEEE Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<title level="m">IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1821" to="1826" />
		</imprint>
	</monogr>
	<note>SCUT-FBP: A benchmark dataset for facial beauty perception</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Chicago face database: A free stimulus set of faces and norming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Correll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wittenbrink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1122" to="1135" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1734" to="1748" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Age estimation using expectation of label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="712" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="681" to="685" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human age estimation using bioinspired features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="112" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human facial beauty using geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="940" to="950" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A humanlike predictor of facial attractiveness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kagian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leyvand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Soft-Margin mixture of regressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6532" to="6540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ordinal hyperplanes ranker with cost sensitivities for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cumulative attribute space for age and crowd density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2467" to="2474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relative attribute SVM+ learning for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="827" to="839" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human age estimation based on locality and ordinal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2522" to="2534" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Auxiliary demographic information assisted age estimation with cascaded structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2531" to="2541" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facial age estimation by learning from label distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2401" to="2412" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sense beauty by label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2648" to="2654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Logistic boosting regression for label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data-dependent label distribution learning for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3846" to="3858" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Label distribution learning forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Conference on Neural Information Processing Systems</title>
		<meeting>31st Conference on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="834" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Facial age estimation with age difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3087" to="3097" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint image emotion classification and distribution learning via deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Joint Conference on Artificial Intelligence</title>
		<meeting>International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep regression forests for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multi-task cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rank consistent ordinal regression for neural networks with application to age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mirjalili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Raschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="325" to="331" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Demographic estimation from face images: Human vs. machine performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1148" to="1161" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Effective training of convolutional neural networks for face-based gender and age prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Berrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dugelay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="page" from="15" to="26" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Mean-variance loss for deep age estimation from a face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5285" to="5294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Efficient group-n encoding and decoding for facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2610" to="2623" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Exploring the limits of compact model for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="12587" to="12596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bridgenet: A continuityaware probabilistic network for age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1145" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Similarity-aware and variational deep adversarial learning for robust facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1808" to="1822" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Order learning and its application to age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning probabilistic ordinal embeddings for uncertainty-aware regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13896" to="13905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pml: Progressive margin loss for long-tailed age classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10503" to="10512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Ordinal deep learning for facial age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="486" to="501" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep age distribution learning for apparent age estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Structured output svm prediction of apparent age, gender and smile from deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uric?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Energy-based models for deep probabilistic regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">K</forename><surname>Gustafsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Sch?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="325" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Efficient facial representations for age, gender and identity recognition in organizing photo albums using multi-output convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Savchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep ordinal regression with label diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oskarsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 25th International Conference on Pattern Recognition</title>
		<meeting>25th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2740" to="2747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">R2-ResNeXt: a resnext-based regression model with relative ranking for facial beauty prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 24th International Conference on Pattern Recognition</title>
		<meeting>24th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Facial attractiveness prediction by deep adaptive label distribution learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Chinese Conference on Biometric Recognition</title>
		<meeting>Chinese Conference on Biometric Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="198" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Mt-resnet: a multi-task deep network for facial attractiveness prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2nd International Conference on Computing and Data Science</title>
		<meeting>2nd International Conference on Computing and Data Science</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="44" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European conference on computer vision</title>
		<meeting>European conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Performance of a deep-learning neural network model in assessing skeletal maturity on pediatric hand radiographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Lungren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Halabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">V</forename><surname>Stence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="313" to="322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">DriveAHead: A large-scale driver head pose dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haurilet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1165" to="1174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">How to take a good selfie?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seifu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lalanne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 23rd Annual ACM Conference on Multimedia Conference</title>
		<meeting>the 23rd Annual ACM Conference on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="923" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Image aesthetic assessment: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="80" to="106" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
