<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
							<email>2zhuxinge123@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<settlement>Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
							<email>3mayuexin@shanghaitech.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>4ccloy@ntu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="laboratory">S-Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
							<email>liyikang@pjlab.org.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point-to-Voxel Knowledge Distillation for LiDAR Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This article addresses the problem of distilling knowledge from a large teacher model to a slim student network for LiDAR semantic segmentation. Directly employing previous distillation approaches yields inferior results due to the intrinsic challenges of point cloud, i.e., sparsity, randomness and varying density. To tackle the aforementioned problems, we propose the Point-to-Voxel Knowledge Distillation (PVD), which transfers the hidden knowledge from both point level and voxel level. Specifically, we first leverage both the pointwise and voxelwise output distillation to complement the sparse supervision signals. Then, to better exploit the structural information, we divide the whole point cloud into several supervoxels and design a difficulty-aware sampling strategy to more frequently sample supervoxels containing less-frequent classes and faraway objects. On these supervoxels, we propose inter-point and inter-voxel affinity distillation, where the similarity information between points and voxels can help the student model better capture the structural information of the surrounding environment. We conduct extensive experiments on two popular LiDAR segmentation benchmarks, i.e., nuScenes <ref type="bibr" target="#b2">[3]</ref> and SemanticKITTI [1]. On both benchmarks, our PVDconsistently outperforms previous distillation approaches by a large margin on three representative backbones, i.e., Cylinder3D <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, SPVNAS <ref type="bibr" target="#b24">[25]</ref> and MinkowskiNet <ref type="bibr" target="#b4">[5]</ref>. Notably, on the challenging nuScenes and SemanticKITTI datasets, our method can achieve roughly 75% MACs reduction and 2? speedup on the competitive Cylinder3D model and rank 1st on the SemanticKITTI leaderboard among all published algorithms 1 . Our code is available at https://github.com/cardwing/Codes-for-PVKD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>LiDAR semantic segmentation plays a vital role in the perception of autonomous driving as it provides per-point semantic information of the surrounding environment. With the advent of deep learning, plenty of LiDAR segmentation models have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b37">37]</ref> and have dominated the leaderboard of many benchmarks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3]</ref>. However, the impressive performance comes at the expense of heavy computation and storage, which impedes them from being deployed in resource-constrained devices.</p><p>To enable the deployment of these powerful LiDAR segmentation models on autonomous vehicles, knowledge distillation <ref type="bibr" target="#b9">[10]</ref> is a prevailing technique to transfer the dark knowledge from the overparameterized teacher model to the slim student network to achieve model compression. However, directly applying previous distillation algorithms <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">28]</ref> to LiDAR semantic segmentation brings marginal gains due to the intrinsic difficulty of point cloud, i.e., sparsity, randomness and varying density.</p><p>To address the aforementioned challenges, we propose the Point-to-Voxel Knowledge Distillation (PVD). As the name implies, we propose to distil the knowledge from both point-level and voxel-level. Specifically, to combat against the sparse supervision signals, we first propose to distil the pointwise and voxelwise probabilistic outputs from the teacher, respectively. The pointwise output contains finegrained perceptual information while the voxelwise prediction embraces coarse but richer clues about the surrounding environment.</p><p>To effectively distil the valuable structural knowledge from the unordered point sequences, we propose to exploit the point-level and voxel-level affinity knowledge. The affinity knowledge is obtained via measuring the pairwise semantic similarity of the point features and voxel features. However, straightforwardly mimicking the affinity knowledge of the whole point cloud is intractable since there are tens of thousands of points and the affinity matrix of these point features have over ten billion elements. Consequently, <ref type="bibr">Figure 1</ref>. Comparison between the prediction of our method (PVD) and the competitive channel distillation (CD) algorithm <ref type="bibr" target="#b23">[24]</ref> on SemanticKITTI validation set. Points that are mistakenly classified are painted red for better visualization. It is obvious that PVD can make the student model predict more accurately for those minority classes (person) and faraway objects (bicycles, highlighted with green rectangles) than the baseline distillation approach. Here, KD denotes knowledge distillation.</p><p>we put forward the supervoxel partition to divide the whole point cloud into a fixed number of supervoxels. At each distillation step, we only sample K supervoxels and distil the affinity knowledge computed from point features and voxel features in these supervoxels, thus significantly enhancing the learning efficiency. Considering that uneven numberof-points distribution exists among different classes and objects at distinct distances, we further introduce a difficultyaware sampling strategy to more frequently sample supervoxels that contain minority classes and faraway objects, emphasizing the learning on hard cases.</p><p>We summarize our contributions as follow. To our knowledge, we are the first to study how to apply knowledge distillation to LiDAR semantic segmentation for model compression. To address the difficulty of distilling knowledge from point cloud, namely sparsity, randomness and varying density, we propose the point-to-voxel knowledge distillation. In addition, we put forward the supervoxel partition to make the affinity distillation tractable. A difficultyaware sampling strategy is also employed to more frequently sample supervoxels that contain minority classes and distant objects, thus remarkably enhancing the distillation efficacy on these hard cases. As can be seen from <ref type="figure">Fig. 1</ref>, our method produces much more accurate predictions than the baseline distillation approaches especially for those minority classes and faraway objects.</p><p>We conduct extensive experiments on the nuScenes <ref type="bibr" target="#b2">[3]</ref> and SemanticKITTI <ref type="bibr" target="#b0">[1]</ref> datasets and the results demonstrate that our algorithm consistently outperforms previous distillation approaches by a large margin on three contemporary models, i.e., Cylinder3D <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b37">37]</ref>, SPV-NAS <ref type="bibr" target="#b24">[25]</ref> and MinkowskiNet <ref type="bibr" target="#b4">[5]</ref>. Notably, on the nuScenes and SemanticKITTI benchmarks, PVD achieves approximately 75% MACs reduction and 2? speedup on the top-performing Cylinder3D model with very minor performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>LiDAR semantic segmentation: LiDAR semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b29">[29]</ref><ref type="bibr" target="#b30">[30]</ref><ref type="bibr" target="#b31">[31]</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37]</ref> is crucial for the navigation of autonomous vehicles. PointNet <ref type="bibr" target="#b20">[21]</ref> is one of the pioneering work that uses Multi-Layer Perception (MLP) to process point cloud directly. Although effective in processing small-scale point cloud, PointNet and its variants <ref type="bibr" target="#b21">[22]</ref> are extremely slow to handle large-scale outdoor point cloud. To cope with large-scale outdoor point cloud, Hu et al. <ref type="bibr" target="#b13">[14]</ref> exploit random sampling for point selection and a local feature aggregation module is designed to further preserve the key features. Xu et al. <ref type="bibr" target="#b31">[31]</ref> put forward the range-point-voxel fusion network to make use of the advantages of different views. Zhu et al. <ref type="bibr" target="#b37">[37]</ref> propose the Cylinder3D method that adopts the cylindrical partition and asymmetrical convolution to better employ the valuable information in point cloud. Tang et al. <ref type="bibr" target="#b24">[25]</ref> leverage neural architecture search to automatically find the optimal structure for the task at hand. Although these models have shown impressive performance on various benchmarks, a common drawback is that these networks are too cumbersome to be deployed on resource-constrained devices. To enable the deployment of these powerful yet lumbersome LiDAR semantic segmentation models on real-world applications, we propose the point-to-voxel knowledge distillation to achieve model compression. Knowledge distillation: Knowledge distillation (KD) stems from the seminal work of G. Hinton et al. <ref type="bibr" target="#b9">[10]</ref>. The primary objective of KD is to transfer the rich dark knowledge from a cumbersome teacher model to a compact student model to mitigate the performance gap between these two models. The majority of the KD approaches concentrate on image classification tasks and they take various forms of knowledge as the distillation targets, e.g., intermediate outputs <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>, visual attention maps <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b33">33]</ref>, inter-layer similarity maps <ref type="bibr" target="#b32">[32]</ref>, sample-level similarity maps <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">27]</ref>, etc. Recently, some researchers have adapted conventional KD techniques to distil knowledge for semantic segmentation tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">28]</ref>. For instance, Liu et al. <ref type="bibr" target="#b16">[17]</ref> propose to distil three levels of knowledge simultaneously, namely the pixel-level knowledge, the pairwise similarity knowledge and the holistic knowledge. He et al. <ref type="bibr" target="#b8">[9]</ref> make the student mimic the compressed knowledge as well as the affinity information of the teacher. Although previous distillation algorithms have shown excellent performance on 2D segmentation, straightforwardly deploying them on LiDAR segmentation tasks brings marginal gains owing to the inherent sparsity, randomness and varying density of point cloud. And to our best knowledge, we are the first to apply knowledge distillation to LiDAR semantic segmentation. The proposed PVD can effectively transfer both point-level and voxel-level knowledge to students and is suitable for distilling various LiDAR semantic segmentation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given an input point cloud X ? R N ?3 , the objective of LiDAR semantic segmentation is to assign a class label l ? {0, 1, ..., C ? 1} to each point, where N is the number of points and C is the number of classes. Contemporary algorithms use CNNs for end-to-end prediction.</p><p>Considering that autonomous vehicles typically have limited computation and storage resources and call for realtime performance, efficient models are employed to fulfill the preceding requirement. Knowledge distillation <ref type="bibr" target="#b9">[10]</ref> is widely adopted to achieve model compression via conveying the rich dark knowledge from the large teacher model to the compact student network. However, previous distillation methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">28]</ref> are tailored for 2D semantic segmentation tasks. Directly applying these algorithms to distill knowledge for 3D segmentation tasks produces unsatisfactory results owing to the intrinsic difficulty of point cloud, i.e., sparsity, randomness and varying density. To address the aforementioned challenges, we propose the Point-to-Voxel Knowledge Distillation (PVD) to transfer the knowledge from both the point level and voxel level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Framework overview of Cylinder3D</head><p>We first have a brief review of the Cylinder3D model <ref type="bibr" target="#b37">[37]</ref> and then introduce the point-to-voxel distillation algorithm based on it. Taking the point cloud as input, Cylinder3D first generates the corresponding feature for each point using a stack of MLPs and then reassigns the point features F p ? R N ?C f based on the cylindrical partition where C f is the dimension of point features. Point features that belong to the same voxel are aggregated together through the maxpooling operation to obtain the voxel features F v ? R M ?C f where M is the number of non-empty voxels. Next, these voxel features are fed into the asymmetrical 3D convolution networks to produce the voxel-wise output O v ? R R?A?H?C . A point-wise refinement module is further employed to produce the refined point-wise prediction O p ? R N ?C . Here, N , C, R, A and H denote the number of points, number of classes, radius, angle and height, respectively. Eventually, we will use the argmax operation to process the pointwise prediction to obtain the classification result of each point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Point-to-Voxel Output Distillation</head><p>The primary difference between 2D and 3D semantic segmentation lies in the input. Compared with images, point cloud is sparse and it is difficult to train the efficient student model using the sparse supervision signal. Previous distillation approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref> typically resort to distilling the ultimate output of the teacher network, i.e., the pointwise output of the teacher network for LiDAR semantic segmentation. Although the pointwise output contains finegrained perceptual information of the environment, such knowledge is inefficient to learn as there are hundreds of thousands of points. To improve the learning efficiency, in addition to the pointwise output, we propose to distil the voxelwise output as the number of voxels is smaller and is easier to learn. The combination of both pointwise output distillation and voxelwise output distillation naturally form the coarse-to-fine learning process. The pointwise and voxelwise output distillation loss is given below:</p><formula xml:id="formula_0">L p out (O p S , O p T ) = 1 N C N n=1 C c=1 KL(O p S (n, c)?O p T (n, c)),<label>(1)</label></formula><formula xml:id="formula_1">L v out (O v S , O v T ) = 1 RAHC R r=1 A a=1 H h=1 C c=1 KL(O v S (r, a, h, c)?O v T (r, a, h, c)),<label>(2)</label></formula><p>where KL(.) denotes the Kullback-Leibler divergence loss. Labels for the voxelwise output: Since a voxel may contain points from different classes, how to assign the proper label to the voxel is also crucial to the performance. Following <ref type="bibr" target="#b37">[37]</ref>, we adopt the majority encoding strategy that uses the class label having the maximum number of points inside a voxel as the voxel label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Point-to-Voxel Affinity Distillation</head><p>Distilling the knowledge of the pointwise and voxelwise outputs is insufficient as it merely considers the knowledge of each element and fails to capture the structural information of the surrounding environment. Such structural knowledge is vital to the LiDAR-based semantic segmentation model as the input points are unordered. A natural remedy is to adopt the relational knowledge distillation <ref type="bibr" target="#b16">[17]</ref> which calculates the pairwise similarity of all point features. However, there exist two shortcomings in this learn- We take the Cylinder3D model <ref type="bibr" target="#b37">[37]</ref> as example. There are two networks in our framework, one is the teacher and the other is the student. The student model is obtained via pruning 50% channels of each layer of the teacher. A teacher model is comprised of five parts, i.e., point feature extraction module, point-to-voxel transformation module (voxelization), an encoderdecoder model (asymmetric 3D convolution network), the Dimension-Decomposition Contextual Modeling (DDCM) module, and the point refinement module. Given the input point cloud, we first divide it into a fixed number of supervoxels and sample K supervoxels according to the difficulty-aware sampling strategy (K=1 in this figure, denoted by the red sector). Then, the student is forced to mimic two levels of knowledge from the teacher. The first-level knowledge is the pointwise output and voxelwise output of the whole point cloud.</p><p>The second-level knowledge is the inter-point affinity matrix and inter-voxel affinity matrix of the sampled supervoxels.</p><p>ing scheme: 1) since there are usually hundreds of thousands of points in an input point cloud, the similarity matrix which has over ten billion elements is computationally expensive to calculate and extremely difficult to learn. 2) There exist significant quantity differences between different classes and objects at different distances. The abovementioned learning strategy ignores such difference and treats all classes and objects equally, thus making the distillation process sub-optimal. Supervoxel partition: In order to more efficiently learn the relational knowledge, we divide the whole point cloud into several supervoxels whose size is R s ? A s ? H s . Each supervoxel is comprised of a fixed number of voxels and the total number of supervoxels is</p><formula xml:id="formula_2">N s = ? R Rs ? ? ? A As ? ? ? H Hs ? where ?.</formula><p>? is the ceiling function. We will sample K supervoxels to perform the affinity distillation. Difficulty-aware sampling: To make supervoxels that contain less frequent classes and faraway objects more likely to be sampled, we present the difficulty-aware sampling strategy. The weight for choosing the i-th supervoxel is:</p><formula xml:id="formula_3">W i = 1 f class ? d i R ? 1 N s ,<label>(3)</label></formula><p>where f class is the class frequency, d i is the distance of the outer arc of the i-th supervoxel to the origin in the X-Y plane. We treat the classes that have more than 1% of all points in the whole dataset as majority classes and the remaining classes are considered as minority classes. We empirically set the class frequency of the supervoxel as: f class = 4 exp(?2N minor ) + 1, where N minor is the number of voxels of minority classes in the supervoxel. If there is no voxel of minority class, the f class will be 5. And as the the number of voxels of minority classes increases, f class will be close to 1 quickly. Then, we normalize the weight and obtain the probability of the i-th supervoxel being sampled is:</p><formula xml:id="formula_4">P i = Wi Ns i=1</formula><p>Wi . Point/voxel features processing: Note that for each point cloud, the number of input points is different and the den-sity is varying, thus making the number of point features and voxel features variable in a supervoxel. As to the calculation of loss function, it is desirable to keep the number of features fixed. Hence, we set the number of retained point features and non-empty voxel features as N p and N v , respectively. If the number of point features is larger than N p , then we will retain N p point features by randomly discarding additional point features of majority class. If the number of point features is smaller than N p , we will append all-zero features to the current features to obtain N p features, as is shown in <ref type="figure" target="#fig_1">Fig. 3 (a)</ref>. The voxel features are processed in a similar way.</p><p>Here we have N p point featuresF p r ? R Np?C f and N v voxel featuresF v r ? R Nv?C f in the r-th supervoxel, respectively. Then, for each supervoxel, we calculate the inter-point affinity matrix according to the following equation:</p><formula xml:id="formula_5">C p (i, j, r) =F p r (i) TF p r (j) ?F p r (i)? 2 ?F p r (j)? 2 , r ? {1, ..., K} (4)</formula><p>The affinity score captures the similarity of each pair of point features and it can be taken as the high-level structural knowledge to be learned by the student. The interpoint affinity distillation loss is given as below:</p><formula xml:id="formula_6">L p af f (C p S , C p T ) = 1 KN 2 p K r=1 Np i=1 Np j=1 ?C p S (i, j, r) ? C p T (i, j, r)? 2 2 .<label>(5)</label></formula><p>The inter-voxel affinity matrix is computed similarly. Eventually, we make the student mimic the generated affinity matrices of the teacher model. The inter-voxel affinity distillation loss is presented as follows:</p><formula xml:id="formula_7">L v af f (C v S , C v T ) = 1 KN 2 v K r=1 Nv i=1 Nv j=1 ?C v S (i, j, r) ? C v T (i, j, r)? 2 2 .<label>(6)</label></formula><p>Visualization of the learned affinity maps: From <ref type="figure" target="#fig_2">Fig. 4</ref>, we can see that PVD causes a closer affinity map between student and teacher. With PVD , features that belong to the same class are pulled closer while those of different classes are pushed apart in the feature space, resulting in a much clear affinity map. Compared with the rival channel distillation approach <ref type="bibr" target="#b23">[24]</ref>, PVD can better transfer the structural knowledge from teacher to student, which strongly validates the superiority of PVD in distilling LiDAR segmentation models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Final objective</head><p>Our final loss function is composed of seven terms, i.e., the weighted cross entropy loss for the pointwise output and voxelwise output, the lovasz-softmax loss <ref type="bibr" target="#b1">[2]</ref>, the point-tovoxel output distillation loss and the point-to-voxel affinity distillation loss:</p><formula xml:id="formula_8">L =L p wce + L v wce + L lovasz + ? 1 L p out (O p S , O p T ) + ? 2 L v out (O v S , O v T ) + ? 1 L p af f (C p S , C p T ) + ? 2 L v af f (C v S , C v T ),<label>(7)</label></formula><p>where ? 1 , ? 2 , ? 1 and ? 2 are the loss coefficients to balance the effect of the distillation losses on the main task loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. Following the practice of Cylinder3D <ref type="bibr" target="#b37">[37]</ref>, we conduct experiments on two popular LiDAR semantic segmentation benchmarks, i.e., nuScenes <ref type="bibr" target="#b2">[3]</ref> and Se-manticKITTI <ref type="bibr" target="#b0">[1]</ref>. For nuScenes, it consists of 1000 driving scenes, in which 850 scenes are selected for training and validation, and the remaining 150 scenes are chosen for testing. 16 classes are utilized for LiDAR semantic segmentation after merging similar classes and eliminating infrequent classes. For SemanticKITTI, it is comprised of 22 point cloud sequences, where sequences 00 to 10, 08 and 11 to 21 are used for training, validation and testing, respectively. A total number of 19 classes are chosen for training and evaluation after merging classes with distinct moving status and discarding classes with very few points. Evaluation metrics.</p><p>Following <ref type="bibr" target="#b37">[37]</ref>, we adopt the intersection-over-union (IoU) of each class and mIoU of all classes as the evaluation metric. The calculation of IoU is:</p><formula xml:id="formula_9">IoU i = T Pi</formula><p>T Pi+F Pi+F Ni , where T P i , F P i and F N i represent the true positive, false positive and false negative of class i. Implementation details. Following <ref type="bibr" target="#b37">[37]</ref>, we leverage Adam <ref type="bibr" target="#b14">[15]</ref> as the optimizer and the initial learning rate is set as 2e-3. Batch size is set as 4 and the number of training epochs is 40. ? 1 , ? 2 , ? 1 and ? 2 are set as 0.1, 0.15, 0.15 and 0.25, respectively. We take the rival and open sourced Cylinder3D 2 <ref type="bibr" target="#b37">[37]</ref> approach as the backbone since the topperforming RPVNet <ref type="bibr" target="#b31">[31]</ref> and AF2S3Net <ref type="bibr" target="#b3">[4]</ref> do not release their codes. Random flipping, rotation, scaling and transformation are taken as the data augmentation strategy. The size of the voxel output is 480?360?32, where the three dimensions denote the radius, angle and height, respectively. The size of the supervoxel is set as 120?60?8. N v and N p are set as 3000 and 6000, respectively. The number of sampled supervoxels K is set as 4. The inter-point affinity distillation is performed on the output of the point feature extraction module and the inter-voxel affinity distillation is conducted on the output of the encoder-decoder backbone. For nuScenes, Cylinder3D 0.5? is produced from the original Cylinder3D model by pruning 50% channels for each layer of the whole network. For SemanticKITTI, Cylinder3D 0.5? is obtained by merely pruning 50% channels for each layer of the asymmetrical 3D convolution network and we keep the point feature extraction module unchanged as it is vital to extract rich information from the input point cloud. We also apply our method to compress SPVNAS 3 <ref type="bibr" target="#b24">[25]</ref> and MinkowskiNet <ref type="bibr" target="#b4">[5]</ref> to verify the scalability of our algorithm. More details are provided in the supplementary material. Baseline distillation algorithms. In addition to the stateof-the-art methods in each benchmark, we also compare our method with classical KD methods and contemporary distillation approaches tailored for 2D semantic segmentation, including vanilla KD <ref type="bibr" target="#b9">[10]</ref>, SKD <ref type="bibr" target="#b16">[17]</ref>, CD <ref type="bibr" target="#b23">[24]</ref>, IFV <ref type="bibr" target="#b28">[28]</ref> and KA <ref type="bibr" target="#b8">[9]</ref>. Here, SKD takes the output probability maps and pairwise similarity maps as mimicking targets. We remove the original holistic distillation loss for SKD as incorporating GANs into current framework will cause severe training instability; CD utilizes the intermediate feature maps and score maps as knowledge; IFV transfers the intra-class feature variation from the teacher to the student; KA makes the student distil the compressed knowledge and the affinity information of the whole output from the teacher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Comparison with state-of-the-art LiDAR segmentation models: We compare our model with contemporary Li-DAR semantic segmentation models, e.g., KPConv <ref type="bibr" target="#b25">[26]</ref>, TORNADONet <ref type="bibr" target="#b6">[7]</ref> and SPVNAS <ref type="bibr" target="#b24">[25]</ref>. From <ref type="table" target="#tab_0">Table 1</ref>, we 2 https://github.com/xinge008/Cylinder3D 3 https://github.com/mit-han-lab/spvnas can see that Cylinder3D 0.5?+PVD (the penultimate row) achieves comparable performance with the original Cylin-der3D model on the SemanticKITTI test set. Compared to KPConv and SPVNAS, our Cylinder3D 0.5?+PVD not only achieves better performance, e.g., 3.8% higher than SPVNAS in mIoU, but also has much lower latency than the SPVNAS method (259 ms v.s. 76 ms). Specifically, on minority classes such as bicycle, motocycle and bicyclist, the IoU of Cylinder3D 0.5?+PVD is at least 10.5% higher than the SPVNAS method. And with some engineering tricks like finetuning and flip &amp; rotation test ensemble, our Cylinder3D 0.5?+PVD (the last row) can obtain 71.2 mIoU, which is 2.3 mIoU higher than the original Cylin-der3D model. Impressive performance is also observed in nuScenes validation set. Our Cylinder3D 0.5?+PVD exhibits similar performance with the original Cylinder3D network in terms of the overall mIoU and the IoU on each class. Comparison with previous distillation methods: From <ref type="table" target="#tab_0">Table 1</ref> and 2, we can see that PVD significantly outperforms baseline distillation algorithms in both benchmarks. The performance gap between PVD and the most competitive KD method is larger than 1.8. For instance, on Se-manticKITTI test set, our PVD is 2.8 mIoU higher than the CD method. And on both majority classes and minority classes, our PVD significantly outperforms traditional distillation algorithms. For instance, on nuScenes dataset, PVD is at least 2 mIoU higher than SKD in classes such as bicycle, bus, car, trailer and sidewalk. The aforementioned results strongly demonstrate the effectiveness of PVD in transferring knowledge for teacher-student learning. Generalization to more architectures: To verify the generalization of our method, we also apply PVD to compress SPVNAS <ref type="bibr" target="#b24">[25]</ref> and MinkowskiNet <ref type="bibr" target="#b4">[5]</ref>. Since SPVNAS does not provide the training code for the NAS-based architecture, we conduct experiments on its manually designed architecture. As can be seen from <ref type="table">Table 3</ref>, our PVD still brings more gains to the student model than baseline distillation algorithms. For instance, our PVD outperforms the SKD algorithm by 2.6 mIoU in terms of mIoU on the SPVNAS backbone. It is noteworthy that PVD can safely achieve 75% MACs reduction without causing severe performance drop. The above results strongly demonstrate the good scalability of our method. Qualitative results: As can be seen from <ref type="figure">Fig. 5</ref>, compared with the SKD approach, our PVD greatly improves the prediction of the student model. The prediction errors of PVD on those minority classes, e.g., person and bicycle, are significantly smaller than those of SKD. Besides, on objects that are faraway from the origin, e.g., the car highlighted by the green rectangle, PVD also yields more accurate predictions than SKD. And PVD has lower inter-class similarity and higher intra-class similarity, which explicitly showcases  the efficacy of PVD in distilling structural knowledge from the teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation studies</head><p>In this section, we perform comprehensive ablation studies to examine the efficacy of each component, supervoxel size as well as the sampling strategy on the final  <ref type="figure">Figure 5</ref>. Visual comparison of different methods on the SemanticKITTI validation set. Here, the ground-truth for the inter-voxel affinity map is the ideal map where the intra-class similarity score is 1 and inter-class similarity score is 0.  <ref type="figure">Figure 6</ref>. Comparison between different sampling strategies.</p><p>Supervoxel size. Note that we perform the inter-point and inter-voxel distillation on the sampled supervoxels. Supervoxel size has a non-negligible effect on the efficacy of PVD since a overly small size will make student learn little from the affinity distillation loss while a large supervoxel size will weaken the learning efficiency of PVD. Here, we keep the number of sampled supervoxels as 4 to remove the effect of this factor. From <ref type="table" target="#tab_3">Table 5</ref>, we can see that setting the supervoxel size to (120, 60, 8) yields the best performance. Remarkably increasing or decreasing the supervoxel size will harm the distillation efficacy. Sampling strategy. We compare four different sampling strategies, i.e., the original difficulty-aware sampling, distance-aware sampling, category-aware sampling and random sampling. Here, distance-aware sampling is to only more frequently sample distant points while category-aware sampling will more likely sample points belonging to rare classes. From <ref type="figure">Fig. 6</ref>, it is apparent that difficulty-aware sampling brings more gains than the other three strategies. Specifically, difficulty-aware sampling outperforms both distance-aware and category-aware sampling, suggesting that both distance and categorical awareness are crucial to the distillation effect. The large gap between difficultyaware sampling and random sampling validates the necessity of difficulty-aware sampling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel point-to-voxel knowledge distillation approach (PVD) tailored for LiDAR semantic segmentation. PVD is comprised of the point-tovoxel output distillation and affinity distillation. The supervoxel partition and difficulty-aware sampling strategy are further proposed to improve the learning efficiency of affinity distillation. We perform experiments on two LiDAR semantic segmentation benchmarks and show that PVD significantly outperforms baseline distillation algorithms on distilling Cylinder3D, SPVNAS and MinkowskiNet. The impressive results indicate that there still exists large redundancy in 3D segmentation models and our approach can serve as a strong baseline to compress these cumbersome models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative results</head><p>We provide the complete quantitative results of different algorithms on SPVNAS <ref type="bibr" target="#b24">[25]</ref> and MinkowskiNet <ref type="bibr" target="#b4">[5]</ref> in <ref type="table" target="#tab_4">Table 6</ref>. Apparently, PVD consistently outperforms previous distillation algorithms by a large margin. For instance, on SPVNAS, PVD can bring 2.6 more points than the SKD method in mIoU. For both models, PVD can almost mitigate the performance gap between the original network and the pruned model. The encouraging results on SPVNAS and MinkowskiNet convincingly demonstrates the good scalability of PVD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation studies</head><p>Loss coefficients. By comparing each row with the last row in <ref type="table">Table 7</ref>, we have the following observations: 1) the loss coefficient of the inter-voxel affinity distillation should be larger than other distillation losses to yield the best distillation effect (row 1 and 2). 2) Exchanging the loss coefficients of the point-based distillation loss and voxel-based distillation loss deteriorates the performance, which means the voxel-based distillation loss guides the point-based loss and is more important (row 5). 3) Slightly increasing the loss coefficients will not significantly affect the overall performance, which demonstrates the robustness of PVD (row 3 and 4).</p><p>Performance sensitiveness to f class . We conduct experiments on examining the effect of f class . We rewrite the f class to be f class = ? exp (?N minor ) + 1. Then, we randomly choose ? from {3, 4, 5, 6} and ? from { -1, -2, -3}, and compare the performance of different combinations. Experimental results reveal that the final performance  <ref type="table">Table 8</ref>. The voxel-based loss term indeed has larger impacts on the final performance. One potential reason is that the voxel representation provides richer structural information of the environment as it aggregates the information of all points within a voxel. Broader impact of PVD. We apply PVD to Se-manticKITTI multi-scan segmentation tasks and observe 4.2% performance improvement on the Cylinder3D 0.5? backbone. The resulting model ranks 3rd on the Se-manticKITTI multi-scan competition <ref type="bibr" target="#b3">4</ref> . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Elaborated implementation details</head><p>For the baseline knowledge distillation approaches, the value of each loss coefficient is provided in <ref type="table" target="#tab_6">Table 9</ref>. Since training a single model from scratch may take one more week, we resort to loading the pre-trained weights to accelerate the training process. In this condition, the overall training duration will be shortened to three days. Note that all methods adopt this strategy to ensure fair comparison. The latency is recorded using a single GPU (NVIDIA Tesla PG503-216 GV100) and the final value of latency is obtained after averaging the latency of 100 samples. The training protocol for SPVNAS and MinkowskiNet is exactly the same as their open-sourced codes <ref type="bibr" target="#b4">5</ref> . Finetuning denotes retraining the trained model for 10 more epochs with the learning rate being 2e-4. MACs calculation: Since sparse convolution merely operates on the non-zero positions and different input point cloud sequence has different non-zero patterns, we first estimate the average kernel map size following <ref type="bibr" target="#b24">[25]</ref> for each layer and then use the following equation to compute the FLOPs of each layer: F LOP s = N ? K s ? C in ? C out , where K s is the size of kernel map, N is the number of points, C in is the number of input channels, C out is the number of output channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative results</head><p>We provide more visual comparisons of PVD with previous distillation algorithms in <ref type="figure">Fig. 7</ref>. Compared with the rival SKD approach, our PVD can significantly improve the prediction of the student model. The prediction errors of PVD on those minority classes, e.g., person and bicycle, are much smaller than those of SKD. Besides, on objects that are faraway from the origin, e.g., the car highlighted by the green rectangle, PVD also yields more accurate predictions than SKD. And PVD has lower inter-class similarity and higher intra-class similarity than SKD. The aforementioned results explicitly demonstrate the efficacy of PVD in distilling structural knowledge from the teacher model to the student.  <ref type="figure">Figure 7</ref>. Visual comparison of different methods on the SemanticKITTI validation set. Here, the ground-truth for the inter-voxel affinity map is the ideal map where the intra-class similarity score is 1 and inter-class similarity score is 0.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Framework overview.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Computation of the inter-point affinity matrix within a supervoxel. Given the sampled supervoxel, we first (a) extract features from each point and then (b) obtain the affinity matrix via calculating the pairwise point features. Finally, the produced affinity matrix of the student is forced to mimic that of the teacher. Note that all-zeros features (the dashed rectangle) will be appended to current features if the number of point features is smaller than Np.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Comparison between inter-voxel affinity maps of different algorithms. The affinity value is obtained via normalizing the cosine similarity score to [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Performance w.r.t the distances of objects. We repartition SemanticKITTI according to the position of the cars. Cars in the training set are relatively close to the origin (? 20 m) while cars in the validation set are relatively far away from the origin (&gt; 20 m). We apply PVD to distill the Cylin-der3D model on the newly divided dataset. Experimental results show that PVD can still bring 4.7% to the Cylin-der3D 0.5? model on cars (91.3% v.s. 95.6%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of our proposed method and state-of-the-art LiDAR semantic segmentation methods as well as previous distillation approaches on SemanticKITTI test set. Cylinder3D 0.5? is abbreviated as C3D 0.5? to save space. * means that finetuning and flip &amp; rotation test ensemble are applied. All results can be found in the online leaderboard.</figDesc><table><row><cell>Methods</cell><cell>mIoU</cell><cell>Latency (ms)</cell><cell>car</cell><cell>bicycle</cell><cell>motorcycle</cell><cell>truck</cell><cell>other-vehicle</cell><cell>person</cell><cell>bicyclist</cell><cell>motorcyclist</cell><cell>road</cell><cell>parking</cell><cell>sidewalk</cell><cell>other-ground</cell><cell>building</cell><cell>fence</cell><cell>vegetation</cell><cell>trunk</cell><cell>terrain</cell><cell>pole</cell><cell>traffic</cell></row><row><cell>Salsanext [6]</cell><cell>59.5</cell><cell>-</cell><cell cols="19">91.9 48.3 38.6 38.9 31.9 60.2 59.0 19.4 91.7 63.7 75.8 29.1 90.2 64.2 81.8 63.6 66.5 54.3 62.1</cell></row><row><cell>KPConv [26]</cell><cell>58.8</cell><cell cols="20">263 96.0 32.0 42.5 33.4 44.3 61.5 61.6 11.8 88.8 61.3 72.7 31.6 95.0 64.2 84.8 69.2 69.1 56.4 47.4</cell></row><row><cell>FusionNet [34] *</cell><cell>61.3</cell><cell>-</cell><cell cols="19">95.3 47.5 37.7 41.8 34.5 59.5 56.8 11.9 91.8 68.8 77.1 30.8 92.5 69.4 84.5 69.8 68.5 60.4 66.5</cell></row><row><cell>KPRNet [16] *</cell><cell>63.1</cell><cell>-</cell><cell cols="19">95.5 54.1 47.9 23.6 42.6 65.9 65.0 16.5 93.2 73.9 80.6 30.2 91.7 68.4 85.7 69.8 71.2 58.7 64.1</cell></row><row><cell>TORNADONet [7] *</cell><cell>63.1</cell><cell>-</cell><cell cols="19">94.2 55.7 48.1 40.0 38.2 63.6 60.1 34.9 89.7 66.3 74.5 28.7 91.3 65.6 85.6 67.0 71.5 58.0 65.9</cell></row><row><cell>SPVNAS [25] *</cell><cell>66.4</cell><cell cols="20">259 97.3 51.5 50.8 59.8 58.8 65.7 65.2 43.7 90.2 67.6 75.2 16.9 91.3 65.9 86.1 73.4 71.0 64.2 66.9</cell></row><row><cell>Cylinder3D [37] *</cell><cell>68.9</cell><cell cols="20">170 97.1 67.6 63.8 50.8 58.5 73.7 69.2 48.0 92.2 65.0 77.0 32.3 90.7 66.5 85.6 72.5 69.8 62.4 66.2</cell></row><row><cell>C3D 0.5?</cell><cell>65.3</cell><cell></cell><cell cols="19">93.4 62.3 59.2 48.3 56.4 72.3 66.3 21.0 91.2 61.3 75.3 30.4 89.8 65.4 84.2 71.4 67.3 60.2 64.2</cell></row><row><cell>C3D 0.5? + KD</cell><cell>65.6</cell><cell></cell><cell cols="19">93.8 62.5 59.4 48.6 55.3 72.9 66.5 21.9 91.8 61.3 75.7 30.5 90.4 65.5 84.3 71.7 67.6 60.3 64.8</cell></row><row><cell>C3D 0.5? + CD</cell><cell>66.1</cell><cell></cell><cell cols="19">94.5 62.7 59.8 49.3 57.2 72.1 67.1 22.7 92.1 61.4 74.9 30.8 90.9 67.3 84.6 72.2 68.3 61.1 65.1</cell></row><row><cell>C3D 0.5? + IFV C3D 0.5? + SKD</cell><cell>65.5 65.8</cell><cell>76</cell><cell cols="19">93.7 62.4 59.1 48.7 56.7 72.4 66.6 21.4 91.5 62.0 75.6 30.4 90.3 66.2 84.7 71.5 67.5 60.6 64.3 93.6 62.7 59.6 48.5 57.4 72.8 66.7 24.3 91.6 61.4 75.9 30.8 90.1 65.6 84.8 71.7 67.5 60.7 65.2</cell></row><row><cell>C3D 0.5? + KA</cell><cell>65.5</cell><cell></cell><cell cols="19">93.4 62.6 58.9 48.5 56.5 72.7 66.5 20.7 91.6 61.5 75.3 30.1 89.9 65.6 84.4 71.3 67.8 60.3 65.8</cell></row><row><cell>C3D 0.5? + PVD</cell><cell>68.9</cell><cell></cell><cell cols="19">96.7 66.4 61.0 60.0 59.3 73.2 72.1 25.0 91.4 66.5 76.2 37.1 93.0 70.5 85.9 72.7 69.8 64.1 67.8</cell></row><row><cell>C3D 0.5? + PVD *</cell><cell>71.2</cell><cell></cell><cell cols="19">97.0 67.9 69.3 53.5 60.2 75.1 73.5 50.5 91.8 70.9 77.5 41.0 92.4 69.4 86.5 73.8 71.9 64.9 65.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of our proposed method and state-of-the-art LiDAR semantic segmentation methods as well as previous distillation approaches on nuScenes validation set.</figDesc><table><row><cell>Methods</cell><cell>mIoU</cell><cell>barrier</cell><cell>bicycle</cell><cell>bus</cell><cell>car</cell><cell>construction</cell><cell>motorcycle</cell><cell>pedestrian</cell><cell>traffic-cone</cell><cell>trailer</cell><cell>truck</cell><cell>driveable</cell><cell>other</cell><cell>sidewalk</cell><cell>terrain</cell><cell>manmade</cell><cell>vegetation</cell></row><row><cell>RangeNet++ [19]</cell><cell>65.5</cell><cell cols="16">66.0 21.3 77.2 80.9 30.2 66.8 69.6 52.1 54.2 72.3 94.1 66.6 63.5 70.1 83.1 79.8</cell></row><row><cell>PolarNet [35]</cell><cell>71.0</cell><cell cols="16">74.7 28.2 85.3 90.9 35.1 77.5 71.3 58.8 57.4 76.1 96.5 71.1 74.7 74.0 87.3 85.7</cell></row><row><cell>Salsanext [6]</cell><cell>72.2</cell><cell cols="16">74.8 34.1 85.9 88.4 42.2 72.4 72.2 63.1 61.3 76.5 96.0 70.8 71.2 71.5 86.7 84.4</cell></row><row><cell>Cylinder3D [37]</cell><cell>76.1</cell><cell cols="16">76.4 40.3 91.2 93.8 51.3 78.0 78.9 64.9 62.1 84.4 96.8 71.6 76.4 75.4 90.5 87.4</cell></row><row><cell>C3D 0.5?</cell><cell>73.6</cell><cell cols="16">74.6 36.2 88.2 87.3 47.9 76.4 77.0 63.4 58.8 82.3 95.1 70.0 73.5 73.6 88.7 85.2</cell></row><row><cell>C3D 0.5? + KD</cell><cell>73.9</cell><cell cols="16">75.2 35.4 88.3 88.2 47.6 76.8 77.2 63.6 57.3 83.1 95.7 70.1 75.2 73.1 89.2 85.3</cell></row><row><cell>C3D 0.5? + CD</cell><cell>74.1</cell><cell cols="16">75.4 36.1 88.4 89.3 46.9 76.1 77.6 62.9 58.0 84.3 96.0 70.3 74.8 74.6 90.1 85.6</cell></row><row><cell>C3D 0.5? + IFV</cell><cell>73.8</cell><cell cols="16">74.7 36.6 88.3 88.6 47.2 76.7 77.1 63.1 58.2 83.5 95.1 70.2 73.4 73.8 88.9 84.3</cell></row><row><cell>C3D 0.5? + SKD</cell><cell>74.2</cell><cell cols="16">74.9 37.3 87.6 89.1 47.5 76.2 77.4 63.2 59.3 83.4 95.9 70.4 73.9 74.3 90.3 87.1</cell></row><row><cell>C3D 0.5? + KA</cell><cell>73.9</cell><cell cols="16">74.2 36.3 88.5 87.6 47.1 76.9 78.3 63.5 57.6 83.4 94.9 70.3 73.8 73.2 88.4 86.3</cell></row><row><cell>C3D 0.5? + PVD</cell><cell>76.0</cell><cell cols="16">76.2 40.0 90.2 94.0 50.9 77.4 78.8 64.7 62.0 84.1 96.6 71.4 76.4 76.3 90.3 86.9</cell></row><row><cell cols="8">Table 3. Performance of different algorithms on compressing SPV-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">NAS and MinkowskiNet on SemanticKITTI validation set.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Algorithm</cell><cell></cell><cell cols="4">mIoU MACs (G)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SPVNAS [25]</cell><cell></cell><cell>63.8</cell><cell></cell><cell>118.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SPVNAS 0.5?</cell><cell></cell><cell>60.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SPVNAS 0.5? + CD SPVNAS 0.5? + SKD</cell><cell>60.9 61.2</cell><cell></cell><cell>29.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">SPVNAS 0.5? + PVD</cell><cell>63.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MinkowskiNet [5]</cell><cell></cell><cell>61.9</cell><cell></cell><cell>114.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MinkowskiNet 0.5?</cell><cell>58.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MinkowskiNet 0.5? + CD MinkowskiNet 0.5? + SKD</cell><cell>59.6 59.4</cell><cell></cell><cell>28.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">MinkowskiNet 0.5? + PVD</cell><cell>61.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Influence of each component on the final performance.</figDesc><table><row><cell cols="5">Lout p Lout v L af f p L af f v mIoU</cell></row><row><cell>? ? ? ?</cell><cell>? ? ?</cell><cell>? ?</cell><cell>?</cell><cell>63.1 63.4 64.1 64.7 66.4</cell></row><row><cell cols="5">performance. The experiments are conducted in the Se-</cell></row><row><cell cols="5">manticKITTI validation set. More ablation studies are put</cell></row><row><cell cols="3">in the supplementary material.</cell><cell></cell><cell></cell></row><row><cell cols="5">Effect of each component. From Table 4, we have the</cell></row><row><cell cols="5">following observations: 1) Combining both point-to-voxel</cell></row><row><cell cols="5">output distillation and affinity distillation brings the most</cell></row><row><cell cols="5">performance gains. 2) The voxel-level distillation brings</cell></row><row><cell cols="5">more gains than the point-level distillation, suggesting the</cell></row><row><cell cols="5">necessity of introducing the voxel-level mimicking loss. 3)</cell></row><row><cell cols="5">The affinity distillation yields more gains than the output</cell></row><row><cell cols="5">distillation, demonstrating the importance of leveraging the</cell></row><row><cell cols="5">relational knowledge to better capture the structural infor-</cell></row><row><cell>mation.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Effect of supervoxel size on the performance.</figDesc><table><row><cell></cell><cell cols="3">supervoxel size mIoU</cell></row><row><cell></cell><cell cols="2">(60, 30, 4)</cell><cell>65.3</cell></row><row><cell></cell><cell cols="2">(90, 45, 6)</cell><cell>65.7</cell></row><row><cell></cell><cell cols="2">(120, 60, 8)</cell><cell>66.4</cell></row><row><cell></cell><cell cols="2">(180, 90, 12)</cell><cell>65.6</cell></row><row><cell></cell><cell cols="2">(240, 180, 16)</cell><cell>65.2</cell></row><row><cell></cell><cell>67.0</cell><cell></cell></row><row><cell></cell><cell>66.5</cell><cell></cell></row><row><cell>mIoU</cell><cell>65.5 66.0</cell><cell></cell></row><row><cell></cell><cell>65.0</cell><cell></cell></row><row><cell></cell><cell>64.5</cell><cell></cell></row><row><cell></cell><cell>Difficulty</cell><cell>Class</cell><cell>Distance Random</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Performance of different algorithms on distilling SPV-NAS and MinkowskiNet on SemanticKITTI validation set.</figDesc><table><row><cell>Algorithm</cell><cell cols="2">mIoU MACs (G)</cell></row><row><cell>SPVNAS [25]</cell><cell>63.8</cell><cell>118.6</cell></row><row><cell>SPVNAS 0.5?</cell><cell>60.4</cell><cell></cell></row><row><cell>SPVNAS 0.5? + KD</cell><cell>60.6</cell><cell></cell></row><row><cell>SPVNAS 0.5? + CD</cell><cell>60.9</cell><cell></cell></row><row><cell>SPVNAS 0.5? + IFV</cell><cell>60.8</cell><cell>29.7</cell></row><row><cell>SPVNAS 0.5? + SKD</cell><cell>61.2</cell><cell></cell></row><row><cell>SPVNAS 0.5? + KA</cell><cell>60.7</cell><cell></cell></row><row><cell>SPVNAS 0.5? + PVD</cell><cell>63.8</cell><cell></cell></row><row><cell>MinkowskiNet [5]</cell><cell>61.9</cell><cell>114.0</cell></row><row><cell>MinkowskiNet 0.5?</cell><cell>58.9</cell><cell></cell></row><row><cell>MinkowskiNet 0.5? + KD</cell><cell>59.2</cell><cell></cell></row><row><cell>MinkowskiNet 0.5? + CD</cell><cell>59.6</cell><cell></cell></row><row><cell>MinkowskiNet 0.5? + IFV</cell><cell>59.1</cell><cell>28.5</cell></row><row><cell>MinkowskiNet 0.5? + SKD</cell><cell>59.4</cell><cell></cell></row><row><cell>MinkowskiNet 0.5? + KA</cell><cell>59.2</cell><cell></cell></row><row><cell>MinkowskiNet 0.5? + PVD</cell><cell>61.8</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Performance of using different loss coefficients for PVD. Influence of each component on the final performance.</figDesc><table><row><cell>?1</cell><cell>?2</cell><cell>?1</cell><cell>?2</cell><cell>mIoU</cell></row><row><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>65.2</cell></row><row><cell cols="4">0.25 0.25 0.25 0.25</cell><cell>65.5</cell></row><row><cell cols="4">0.15 0.15 0.15 0.25</cell><cell>66.2</cell></row><row><cell>0.1</cell><cell>0.2</cell><cell>0.2</cell><cell>0.25</cell><cell>66.3</cell></row><row><cell>0.15</cell><cell>0.1</cell><cell cols="2">0.25 0.15</cell><cell>65.4</cell></row><row><cell>0.1</cell><cell cols="3">0.15 0.15 0.25</cell><cell>66.4</cell></row><row><cell cols="5">Lout p Lout v L af f p L af f v mIoU</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>63.1 63.4 63.7 63.6 64.5</cell></row><row><cell cols="5">of PVD on Cylinder3D 0.5? ranges from 66.2 to 66.4. The</cell></row><row><cell cols="5">small fluctuations in distillation performance indicates that</cell></row><row><cell cols="4">PVD is not very sensitive to f class .</cell><cell></cell></row><row><cell cols="5">The influence of each component of PVD. Detailed per-</cell></row><row><cell cols="5">formance of each component is summarized in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Loss coefficients of different distillation methods.</figDesc><table><row><cell>Model</cell><cell cols="2">KD IFV</cell><cell cols="6">SKD ?pi ?pa ? f ea ?score ? ada ? af f CD KA</cell><cell>?1</cell><cell>?2</cell><cell>PVD ?1</cell><cell>?2</cell></row><row><cell>Cylinder3D</cell><cell>0.2</cell><cell>0.2</cell><cell>0.15</cell><cell>0.2</cell><cell>0.1</cell><cell>0.3</cell><cell>0.2</cell><cell cols="5">0.15 0.1 0.15 0.15 0.25</cell></row><row><cell>SPVNAS</cell><cell>0.1</cell><cell>0.3</cell><cell cols="3">0.15 0.15 0.05</cell><cell>0.1</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.15</cell></row><row><cell cols="2">MinkowskiNet 0.2</cell><cell>0.2</cell><cell>0.1</cell><cell>0.2</cell><cell>0.05</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.15</cell><cell>0.2</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://competitions.codalab.org/competitions/20331#results (singlescan competition) till 2021-11-18 04:00 Pacific Time, and our method is termed Point-Voxel-KD. Our method (PV-KD) ranks 3rd on the multi-scan challenge till 2021-12-1 00:00 Pacific Time.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://competitions.codalab.org/competitions/20331#results (multiscan competition) till 2021-12-1 00:00 Pacific Time, and our method is termed PV-KD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/mit-han-lab/spvnas</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is partially supported under the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Se-manticKITTI: A Dataset for Semantic Scene Understanding of Lidar Sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="9297" to="9307" />
		</imprint>
	</monogr>
	<note>Sven Behnke, Cyrill Stachniss, and Jurgen Gall</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amal</forename><forename type="middle">Rannen</forename><surname>Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="4413" to="4421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">NuScenes: A Multimodal Dataset for Autonomous Driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">(af)2-S3Net: Attentive Feature Fusion with Adaptive Feature Selection for Sparse Semantic Segmentation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12547" to="12556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">4D Spatio-Temporal ConvNets: Minkowski Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">SalsaNext: Fast, Uncertainty-Aware Semantic Segmentation of lidar Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Cortinhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Tzelepis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><forename type="middle">Erdal</forename><surname>Aksoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tornado-net: Multiview Total Variation Semantic Segmentation with Diamond Inception Module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Gerdzhev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Razani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Taghavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Bingbing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge Adaptation for Efficient Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youliang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="578" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inter-Region Affinity Distillation for Road Marking Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12486" to="12495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Lightweight Lane Detection CNNs by Self Attention Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1013" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to Steer by Mimicking Features from Heterogeneous Auxiliary Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8433" to="8440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randla-net: Efficient Semantic Segmentation of Large-Scale Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhai</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="11108" to="11117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">KPRNet: Improving Projection-based Lidar Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyvid</forename><surname>Kochanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Fatemeh Karimi Nejadasl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Booij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12668</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Structured Knowledge Distillation for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03739</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Point-Voxel CNN for Efficient 3D Deep Learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RangeNet++: Fast and Accurate Lidar Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Vizzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4213" to="4220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relational Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">FitNets: Hints for Thin Deep Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Channel-wise Distillation for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyong</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Flexible and Deformable Convolution for Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kpconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6411" to="6420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Similarity-Preserving Knowledge Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fred</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Intra-class Feature Variation Distillation for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="346" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Squeezeseg: Convolutional Neural Nets with Recurrent CRF for Real-Time Road-Object Segmentation from 3D liDAR Point Cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1887" to="1893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Squeeze-segv3: Spatially-Adaptive Convolution for Efficient Point-Cloud Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">RPVnet: A Deep and Efficient Range-Point-Voxel Fusion Network for Lidar Point Cloud Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yushi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep FusionNet for Point Cloud Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="644" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Polarnet: An Improved Grid Representation for Online Lidar Point Clouds Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zerong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Foroosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cylindrical and Asymmetrical 3D Convolution Networks for Lidar-based Perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cylindrical and Asymmetrical 3D Convolution Networks for Lidar Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinge</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangzhou</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuexin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
