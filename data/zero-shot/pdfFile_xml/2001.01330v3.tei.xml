<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><forename type="middle">Tudor</forename><surname>Ionescu</surname></persName>
							<email>raducu.ionescu@gmail.com.</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>AND</roleName><forename type="first">Nicolae</forename><surname>Verga</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carol Davila&quot; University of Medicine and Pharmacy</orgName>
								<address>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Colt , ea Hospital</orgName>
								<address>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000. Convolutional Neural Networks with Intermediate Loss for 3D Super-Resolution of CT and MRI Scans</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2020.2980266</idno>
					<note>Corresponding author: Radu Tudor Ionescu The research leading to these results has received funding from the EEA Grants 2014-2021, under Project contract no.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Computed Tomography (CT) scanners that are commonly-used in hospitals and medical centers nowadays produce low-resolution images, e.g. one voxel in the image corresponds to at most onecubic millimeter of tissue. In order to accurately segment tumors and make treatment plans, radiologists and oncologists need CT scans of higher resolution. The same problem appears in Magnetic Resonance Imaging (MRI). In this paper, we propose an approach for the single-image super-resolution of 3D CT or MRI scans. Our method is based on deep convolutional neural networks (CNNs) composed of 10 convolutional layers and an intermediate upscaling layer that is placed after the first 6 convolutional layers. Our first CNN, which increases the resolution on two axes (width and height), is followed by a second CNN, which increases the resolution on the third axis (depth). Different from other methods, we compute the loss with respect to the ground-truth high-resolution image right after the upscaling layer, in addition to computing the loss after the last convolutional layer. The intermediate loss forces our network to produce a better output, closer to the ground-truth. A widely-used approach to obtain sharp results is to add Gaussian blur using a fixed standard deviation. In order to avoid overfitting to a fixed standard deviation, we apply Gaussian smoothing with various standard deviations, unlike other approaches. We evaluate the proposed method in the context of 2D and 3D super-resolution of CT and MRI scans from two databases, comparing it to related works from the literature and baselines based on various interpolation schemes, using 2? and 4? scaling factors. The empirical study shows that our approach attains superior results to all other methods. Moreover, our subjective image quality assessment by human observers reveals that both doctors and regular annotators chose our method in favor of Lanczos interpolation in 97.55% cases for an upscaling factor of 2? and in 96.69% cases for an upscaling factor of 4?. In order to allow others to reproduce our state-of-the-art results, we provide our code as open source at https://github.com/lilygeorgescu/3d-super-res-cnn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDEX TERMS</head><p>Convolutional neural networks, single-image super-resolution, CT images, MRI images, medical image super-resolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M EDICAL centers and hospitals around the globe are typically equipped with single-energy Computer Tomography (CT) or Magnetic Resonance Imaging (MRI) scanners that produce cross-sectional images (slices) of various body parts. The resulting images are of low-resolution, since one pixel usually corresponds to at most one-millimeter piece of tissue. The thickness of one slice is one millimeter at best, so the 3D CT images are composed of volumetric pixels (voxels) that usually correspond to one cubic millimeter (1 ? 1 ? 1 mm 3 ) of tissue. One of the main benefits of this noninvasive scanning technique is that it allows doctors to see if there are malignant tumors inside the body. Nevertheless, doctors, and even machine learning systems <ref type="bibr" target="#b0">[1]</ref>, are not able to accurately contour (segment) the tumor regions because of the low-resolution of CT or MRI scans. According to a FIGURE 1: Our method for 3D image super-resolution based on two subsequent fully convolutional neural networks. In the first stage, the input volume is resized in two dimensions (width and height). In the second stage, the processed volume is further resized in the third dimension (depth). Using a scale factor of 2?, an input volume of 256 ? 256 ? 64 components is upsampled to 512 ? 512 ? 128 components (on all axes). Best viewed in color. team of radiologists from Colt , ea Hospital in Bucharest, that provided a set of anonymized CT scans for our experiments, the desired resolution is to have one voxel correspond to one cubic micrometer (a thousandth part of a cubic millimeter) of tissue. In other words, the goal is to increase the resolution of 3D CT and MRI scans by a factor of 10? in each direction.</p><p>The main motivation behind our work is to allow radiologists and oncologists to accurately segment tumors and make better treatment plans. In order to achieve the desired goal, we propose a machine learning method that takes as input a 3D image and increases the resolution of the input image by a factor of 2? or 4?, providing as output a high-resolution 3D image. To our knowledge, there are only a few previous works <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b17">[18]</ref> that study the super-resolution of CT or MRI images. Similar to some of these previous works <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>- <ref type="bibr" target="#b17">[18]</ref>, we approach single-image super-resolution (SISR) of CT and MRI scans using deep convolutional neural networks (CNNs). We propose a CNN architecture composed of 10 convolutional layers and an intermediate sub-pixel convolutional (upscaling) layer <ref type="bibr" target="#b18">[19]</ref> that is placed after the first 6 convolutional layers. Different from related works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> that use the sub-pixel convolutional layer of Shi et al. <ref type="bibr" target="#b18">[19]</ref>, we add 4 convolutional layers after the upscaling layer. In order to obtain 3D super-resolution, we employ two CNNs with similar architectures, as illustrated in <ref type="figure">Figure 1</ref>. The first CNN increases the resolution on two axes (width and height), while the second CNN takes the output from the first CNN and further increases the resolution on the third axis (depth). Different from related methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we compute the loss with respect to the groundtruth high-resolution image right after the upscaling layer, in addition to computing the loss after the last convolutional layer. The intermediate loss forces our network to produce a better output, closer to the ground-truth. In order to improve the results and obtain sharper images, a common approach is to apply Gaussian smoothing on top of the input images, using a fixed standard deviation. Different from other medical image super-resolution methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, we use various standard deviations in order to avoid overfitting to a certain standard deviation and improve the generalization capacity of our model.</p><p>We note that our model belongs to a class of deep neural networks known as fully convolutional neural networks. The main advantage of using such models, which do not include dense (fully-connected) layers, is that the input samples do not have to be of the same size. This flexibilty enables a broad range of applications such as image segmentation <ref type="bibr" target="#b19">[20]</ref>, object tracking <ref type="bibr" target="#b20">[21]</ref>, crowd detection <ref type="bibr" target="#b21">[22]</ref>, time series classification <ref type="bibr" target="#b22">[23]</ref> and single-image super-resolution <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>. Different from other fully convolutional neural networks <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>, our network is specifically designed for SISR, having a custom architecture that includes an upscaling layer <ref type="bibr" target="#b18">[19]</ref> useful only for SISR, as well as a novel loss function.</p><p>We conduct super-resolution experiments on two databases of 3D CT and MRI images, one gathered from the Colt , ea Hospital (CH) and one that is publicly available online, known as NAMIC 1 . We compare our method with several interpolation baselines (nearest, bilinear, bicubic, Lanczos) and state-of-the-art methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>, in terms of the peak signal-to-noise ratio (PSNR), the structural similarity index (SSIM) and the information fidelity criterion (IFC). We perform comparative experiments on both 2D and 3D single-image super-resolution under commonly-used upscaling factors, namely 2? and 4?. The empirical results indicate that our approach is able to surpass all the other methods included in the experiments. For example, on the NAMIC data set, we obtain a PSNR of 40.57 and an SSIM of 0.9835 for 3D super-resolution by a factor of 2?, while Pham et al. <ref type="bibr" target="#b12">[13]</ref> reported a PSNR of 38.28 and an SSIM of 0.9781 in the same setting. Furthermore, we conduct a subjective image quality assessment by human observers, asking 6 doctors and 12 regular annotators to choose between the CT images produced by our method and those produced by Lanczos interpolation (the best interpolation method). The annotators opted for our method in favor of Lanczos interpolation in 97.55% cases for an upscaling factor of 2? and in 96.69% cases for an upscaling factor of 4?. These results indicate that our method is significantly better than Lanczos interpolation. In order to allow further developments and results replication, we provide our code as open source in a public repository 2 .</p><p>To summarize, our contribution is threefold:</p><p>? We propose a novel CNN model for 3D super-resolution of CT and MRI scans, which is based on an intermediate loss added to the standard output loss and on smoothing the input using random standard deviations for the Gaussian blur. ? We conduct a subjective image quality assessment by human observers to determine the quality and the utility of our super-resolution results, as in <ref type="bibr" target="#b14">[15]</ref>. ? We provide our code online for download, allowing our results to be easily replicated. We organize the rest of this paper as follows. We present related work in Section II. We describe our method in detail in Section III. We present experiments and results in Section IV. Finally, we draw our conclusions in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The purpose of SISR is to reconstruct a high-resolution (HR) image from its low-resolution (LR) counterpart. Before the deep learning age, researchers have used exemplar-based or sparse coding methods for SISR. Exemplar-based methods learn mapping functions from external LR and HR exemplar pairs <ref type="bibr" target="#b23">[24]</ref>- <ref type="bibr" target="#b25">[26]</ref>. Sparse coding methods <ref type="bibr" target="#b26">[27]</ref> are representative for external exemplar-based SR methods. For example, the method of Yang et al. <ref type="bibr" target="#b26">[27]</ref> builds a dictionary with LR patches and the corresponding HR patches.</p><p>To our knowledge, the first work to present a deep learning approach for SISR is <ref type="bibr" target="#b27">[28]</ref>. Dong et al. <ref type="bibr" target="#b27">[28]</ref> proposed a CNN composed of 8 convolutional layers. The network was trained in an end-to-end fashion, minimizing the reconstruction error between the HR image and the output of the network. They used bicubic interpolation to resize the image, before giving it as input to the network. Hence, the CNN takes a blurred HR image as input and learns how to make it sharper. Since the input is an HR image, this type of CNN is time consuming. Therefore, Shi et al. <ref type="bibr" target="#b18">[19]</ref> introduced a new method for upsampling the image using the CNN activation maps produced by the last layer. Their network is more efficient, because 2 Available at https://github.com/lilygeorgescu/3d-super-res-cnn. it builds the HR image only at the very end. Other works, such as <ref type="bibr" target="#b28">[29]</ref>, proposed deeper architectures, focusing strictly on accuracy. Indeed, Zhang et al. <ref type="bibr" target="#b28">[29]</ref> presented one of the deepest CNNs used for SR, composed of 400 layers. They used a channel attention mechanism and residual blocks to handle the depth of the network.</p><p>For medical SISR, some researchers have focused on sparse representations <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref>, while others on training convolutional neural networks [1]- <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p><p>The authors of <ref type="bibr" target="#b7">[8]</ref> proposed a weakly-supervised joint convolutional sparse coding method to simultaneously solve the problems of super-resolution and cross-modal image synthesis. In <ref type="bibr" target="#b9">[10]</ref>, the authors adopted a method based on compressed sensing and self-similarity constraint, obtaining better results than <ref type="bibr" target="#b16">[17]</ref> in terms of SSIM and PSNR.</p><p>Some works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> focused on 2D upsampling, i.e. on increasing the width and height of CT/MRI slices, while other works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref> focused on 3D upsampling, i.e. on increasing the resolution of full 3D CT/MRI scans on all three axes (width, height and depth).</p><p>For 2D upsampling, some works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b13">[14]</ref> used interpolated low resolution (ILR) images, while other works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> used the efficient sub-pixel convolutional neural network (ESPCN) introduced in <ref type="bibr" target="#b18">[19]</ref>. Similar to the latter approaches <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we employ the sub-pixel convolutional layer of Shi et al. <ref type="bibr" target="#b18">[19]</ref>. Different from these related works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref>, we add a convolutional block after the sub-pixel convolutional layer, in order to enhance the HR output image. Furthermore, we propose a novel loss function for our CNN model. Instead of computing the loss between the output image and the ground-truth highresolution image, we also compute the loss between the intermediate image given by the sub-pixel convolutional layer and the high-resolution image. This forces our neural network to learn a better intermediate representation, increasing its performance.</p><p>There are some works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b14">[15]</ref> that employed generative adversarial networks (GANs) <ref type="bibr" target="#b29">[30]</ref> to upsample medical images. Although our approach based on fully convolutional neural networks is less related to GAN-based SISR methods, we decided to include the approach of You et al. <ref type="bibr" target="#b14">[15]</ref> in our experiments, as a recent and relevant baseline.</p><p>For 3D upsampling, Chen et al. <ref type="bibr" target="#b1">[2]</ref> trained a CNN with 3D convolutions and used a GAN-based loss function to produce sharper and more realistic images. In order to upsample a 3D image, Du et al. <ref type="bibr" target="#b3">[4]</ref> employed a deconvolutional layer composed of 3D filters to upsample the LR image, in an attempt to reduce the computational complexity. As <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we tackle the problem of 3D CT/MRI image superresolution. However, instead of using inefficient 3D filters to upsample the LR images in a single step, we propose a more efficient two-stage approach that uses 2D filters. Our approach employs a CNN to increase the resolution in width and height, and another CNN to further increase the VOLUME 8, 2020 resolution depth-wise.</p><p>Most SISR works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>, apply Gaussian smoothing using a fixed standard deviation on the training images, thus training the models in more difficult conditions. However, we believe that using a fixed standard deviation can harm the performance, as deep models tend to overfit to the training data. Different from the standard methodology, each time we apply smoothing on a training image, we chose a different standard deviation, randomly. This simple change improves the generalization capacity of our model, yielding better performance at test time.</p><p>While many works focus only on the super-resolution task, the work of Sert et al. <ref type="bibr" target="#b0">[1]</ref> is focused on the gain brought by the upsampled images in solving a different task. Indeed, the authors <ref type="bibr" target="#b0">[1]</ref> obtained an improvement of 7.5% in the classification of segmented brain tumors when the upsampled images were used.</p><p>We note that there is also some effort in designing and obtaining CT scan results of higher resolution directly from CT scanners. For example, X-ray microtomography (micro-CT) <ref type="bibr" target="#b30">[31]</ref>, which is based on pixel sizes of the cross-sections in the micrometer range, has applications in medical imaging <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. Another alternative to standard (single-energy) CT is dual-energy or multi-energy CT <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Different from the expensive alternatives such as dual-energy CT and micro-CT, our approach to increasing the resolution of single-energy CT images using a machine learning algorithm represents an economical and accessible mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>Our method for solving the 3D image super-resolution problem relies on deep neural networks. The universal approximation theorem <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> states that neural networks with at least one hidden layer are universal function approximators. Hence, the hypothesis class represented by neural networks is large enough to accommodate any hypothesis explaining the data. This high model capacity seems to help deep neural networks in attaining state-of-the-art results in many domains <ref type="bibr" target="#b37">[38]</ref>, including image super-resolution <ref type="bibr" target="#b27">[28]</ref>. This is the main reason behind our decision to use neural networks. Interestingly, Dong et al. <ref type="bibr" target="#b27">[28]</ref> show that deep convolutional neural networks for super-resolution are equivalent to previouslyused sparse-coding methods <ref type="bibr" target="#b26">[27]</ref>. However, the recent literature indicates that deep neural networks attain better results than handcrafted methods in practice <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b38">[39]</ref>, mainly because the parameters (weights) are learned from data in an end-to-end fashion. Further specific decisions, such as the neural architecture or the loss function, are taken based on empirical observations. Our approach is divided into two stages, as illustrated in <ref type="figure">Figure 1</ref>. In the first stage, we upsample the image on height and width using a deep fully convolutional neural network. Then, in the second stage, we further upsample the resulting image on the depth axis using another fully convolutional neural network. Therefore, our complete method is designed for resizing the 3D input volume on all three axes. While the CNN used in the first stage resizes the image on two axes, the CNN used in the second stage resizes the image on a single axis. Both CNNs share the same architecture, the only difference being in the upsamling layer (the second CNN upsamples in only one direction). At training time, our CNNs operate on patches. However, since the architecture is fully convolutional, the models can operate on entire slices at inference time, for efficiency reasons.</p><p>We hereby note that 3D super-resolution is not equivalent to 2D super-resolution in a slice-by-slice order. Our first CNN performs 2D super-resolution in a slice-by-slice order, increasing an input of size h?w?d to the size r?h?r?w?d, where r is the scale factor. Since we end up with the same number of slices (r), this is not enough. This is why we need the second CNN to further increase the image from r ? h ? r ? w ? d voxels to r ? h ? r ? w ? r ? d voxels. The final output of r ? h ? r ? w ? r ? d voxels could also be obtained by employing a single CNN with 3D convolutions. In most of our convolutional layers, each 2D convolutional filter is formed of 3 ? 3 ? 32 + 1 = 289 weights to be learned. As we employ two networks for 3D super-resolution, we learn 2 ? 289 = 578 weights. For an equivalent model based on 3D convolutions, each 3D convolutional filter would be formed of 3 ? 3 ? 3 ? 32 + 1 = 865 weights. This analysis proves that our two CNNs put together have less weights than a single 3D CNN. We thus conclude that our approach is more efficient. We note that our approach is essentially based on decomposing the 3D convolutional filters in a product of two 2D convolutional filters. We note that the same principle is applied in literature <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> to build more efficient CNN models by decomposing 2D convolutional layers in two subsequent 1D convolutional layers that operate on independent dimensions.</p><p>We further describe in detail the proposed CNN architecture, loss function and data augmentation procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. ARCHITECTURE</head><p>Our architecture, depicted in <ref type="figure" target="#fig_0">Figure 2</ref> and used for both CNNs, is composed of 10 convolutional (conv) layers, each followed by Rectified Liner Units (ReLU) <ref type="bibr" target="#b41">[42]</ref> activations. We decided to use ReLU activations, as this represents the most popular choice of transfer function in current research based on deep learning. All convolutional layers contain filters with a spatial support of 3?3. While older deep models were based on larger filters, e.g. the AlexNet <ref type="bibr" target="#b42">[43]</ref> architecture contains filters of 11 ? 11, the recent trend is towards using smaller filters, e.g. the ResNet <ref type="bibr" target="#b43">[44]</ref> architecture does not contain filters larger than 3 ? 3.</p><p>Our 10 conv layers are divided into two blocks. The first block, formed of the first 6 conv layers, starts with the input of the neural network and ends just before the upscaling layer. Each of the first 5 convolutional layers are formed of 32 filters. For the CNN used in the first stage, the number of filters in the sixth convolutional layer is equal to the square of the scale factor, e.g. for a scale factor of 4? the number of filters is 16. For the CNN used in the second stage, the  number of filters in the sixth convolutional layer is equal to the scale factor, e.g. for a scale factor of 4? the number of filters is 4. The difference is caused by the fact that the first CNN upscales on two axes, while the second CNN upscales on one axis. The first convolutional block contains a shortskip connection, from the first conv layer to the third conv layer, and a long-skip connection, from the first conv layer to the fifth conv layer.</p><p>The first convolutional block is followed by a sub-pixel convolutional (upscaling) layer, which was introduced in <ref type="bibr" target="#b18">[19]</ref>. In the upscaling layer, the activation maps produced by the sixth conv layer are assembled into a single activation map. Throughout the first convolutional block, the FIGURE 4: An example of low-resolution input activation maps and the corresponding high-resolution output activation map given by the sub-pixel convolutional layer for upscaling on one axis. For a scaling factor of r = 2 in one direction, the sub-pixel convolutional layer requires r = 2 activation maps as input. Best viewed in color. spatial size of the low-resolution input is preserved, i.e. the activation maps of the sixth conv layer have h I ? w I components, where h I and w I are the height and the width of the input image I. In order to increase the input r times on both axes, the output of the sixth conv layer must be a tensor of h I ? w I ? r 2 components. The activation map resulting from the sub-pixel convolutional layer is a matrix of (h I ? r) ? (w I ? r) components. For super-resolution on two axes, the pixels are rearranged as shown in <ref type="figure" target="#fig_1">Figure 3</ref>. In a similar fashion, we can increase the input r times on one axis. In this case, the output of the sixth conv layer must be a tensor of h I ? w I ? r components. This time, the activation map resulting from the sub-pixel convolutional layer can be either a matrix of (h I ?r)?w I components or a matrix of h I ?(w I ?r) components, depending on the direction we aim to increase the resolution. For super-resolution on one axis, the pixels are rearranged as shown in <ref type="figure">Figure 4</ref>. To our knowledge, we are the first to propose a sub-pixel convolutional (upscaling) layer for super-resolution in one direction. <ref type="bibr">VOLUME 8, 2020</ref> When Shi et al. <ref type="bibr" target="#b18">[19]</ref> introduced the sub-pixel convolutional layer, they used it as the last layer of their CNN. Hence, the output depth of the upscaling layer is equal to the number of channels in the output image. Different from Shi et al. <ref type="bibr" target="#b18">[19]</ref>, we employ further convolutions after the upscaling layer. Nevertheless, since we are working with CT/MRI (grayscale) images, our final output has a single channel. In our architecture, the upscaling layer is followed by our second convolutional block, which starts with the seventh convolutional layer and ends with the tenth convolutional layer. The first three conv layers in our second block are formed of 32 filters. The tenth conv layer contains a single convolutional filter, since our output is a grayscale image that has a single channel. The second convolutional block contains a short skip connection, from the seventh conv layer to the ninth conv layer. The spatial size of h O ? w O components of the activation maps is preserved throughout the second convolutional block, where h O and w O are the height and the width of the output image O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LOSSES AND OPTIMIZATION</head><p>In order to obtain a CNN model for single-image superresolution, the aim is to minimize the differences between the ground-truth high-resolution image and the output image provided by the CNN. Researchers typically employ the mean absolute difference as the objective function. Given a low-resolution input image I and the corresponding groundtruth high-resolution image O, the loss based on the mean absolute value is formally defined as follows:</p><formula xml:id="formula_0">L(?, I, O) = w O i=1 h O j=1 |f (?, I) ? O|,<label>(1)</label></formula><p>where ? are the CNN parameters (weights), f is the transformation function learned by the CNN, and w O and h O represent the width and the height of the ground-truth image O, respectively. When we train our CNN model, we do not employ the standard approach of minimizing the difference between the output provided by the CNN and the ground-truth HR image. Instead, we propose a novel approach based on an intermediate loss function. Since the conv layers after the upscaling layer are meant to refine the high-resolution image without taking any additional information from the low-resolution input image, we note that the high-resolution image resulting immediately after the upscaling layer should be as similar as possible to the ground-truth high-resolution image. Therefore, we propose a loss function that aims to minimize the difference between the intermediately-obtained HR image and the ground-truth HR image, in addition to minimizing the difference between the final HR output image and the ground-truth HR image. Let f 1 denote the transformation function that corresponds to the first convolutional block and the upscaling layer, and let f 2 denote the transformation function that corresponds to the second convolutional block.</p><p>With these notations, the transformation function f of our full CNN architecture can be written as follows:</p><formula xml:id="formula_1">f (?, I) = f 2 (? 2 , f 1 (? 1 , I)),<label>(2)</label></formula><p>where ? are the parameters of the full CNN, ? 1 are the parameters of the first convolutional block and ? 2 are the parameters of the second convolutional block, i.e. ? is a concatenation of ? 1 and ? 2 . Having defined f 1 and f 2 as above, we can formally write our loss function as follows:</p><formula xml:id="formula_2">L f ull = L standard + ? ? L intermediate ,<label>(3)</label></formula><p>where ? is a parameter that controls the importance of the intermediate loss with respect to the standard loss, L standard is the standard loss defined in Equation <ref type="formula" target="#formula_0">(1)</ref> and L intermediate is defined as follows:</p><formula xml:id="formula_3">L intermediate (? 1 , I, O) = w O i=1 h O j=1 |f 1 (? 1 , I) ? O|. (4)</formula><p>In the experiments, we set ? = 1, since we did not find any strong reason to assign a lower or higher importance to the intermediate loss. By replacing ? with 1 and the loss values from Equation <ref type="formula" target="#formula_0">(1)</ref> and Equation <ref type="formula">(4)</ref>, Equation <ref type="formula" target="#formula_2">(3)</ref> becomes:</p><formula xml:id="formula_4">L f ull (?, I, O) = w O i=1 h O j=1 |f (?, I) ? O| + w O i=1 h O j=1 |f 1 (? 1 , I) ? O|.<label>(5)</label></formula><p>In order to optimize towards the objective defined in Equation (5), we employ the Adam optimizer <ref type="bibr" target="#b44">[45]</ref>, which is known to converge faster than Stochastic Gradient Descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. DATA AUGMENTATION</head><p>A common approach to force the CNN to produce sharper output images is to apply Gaussian smoothing using a fixed standard deviation at training time <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>. By training the CNN on blurred low-resolution images, the superresolution task becomes harder. During inference, when the input images are no longer blurred, the task will be much easier. However, smoothing only the training images with a fixed standard deviation will inherently generate a distribution gap between training and test data. If the CNN fits the training distribution well, it might not produce the desired results at inference time. This is because machine learning models are based on the assumption that training data and test data are sampled from the same distribution. We propose to solve this problem by using a randomlychosen standard deviation for each training image. Although the training data distribution will still be different from the testing data distribution, it will include the distribution of test samples, as illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>. In order to augment the training data, we apply Gaussian blur with a probability of 0.5 (only half of the images are smoothed) using a kernel of 3 ? 3 components and a randomly-chosen standard deviation between 0 and 0.5. In this way, we increase the variance of the training data without introducing any bias. In this case, if the CNN fits the training distribution well, it will produce good super-resolution outputs during inference, since there is no distribution gap between training and test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. DATA SETS</head><p>The first data set used in the experiments consists of 10 anonymized 3D images of brain CT provided by the Medical School at Colt , ea Hospital. We further refer to this data set as the Colt , ea Hospital (CH) data set. In order to fairly train and test our CNN models and baselines, we randomly selected 6 images for training and used the remaining 4 images for testing. The training set has 359 slices (2D images) and the testing set has 238 slices. The height and the width of the slices vary between 192 and 512 pixels, while the depth of the 3D images varies between 3 and 176 slices. The resolution of a voxel is 1 ? 1 ? 1 mm 3 .</p><p>The second data set used in our experiments is the National Alliance for Medical Image Computing (NAMIC) Brain Multimodality data set. The NAMIC data set consists of 20 3D MRI images, each composed of 176 slices of 256 ? 256 pixels. As for the CH data set, the resolution of a voxel is 1 ? 1 ? 1 mm 3 . For our experiments, we used T1-weighted (T1w) and T2-weighted (T2w) images independently. Following <ref type="bibr" target="#b12">[13]</ref>, we split the NAMIC data set into a training set containing 10 3D images and a test set containing the other 10 images. We kept the same split for T1w and T2w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EXPERIMENTAL SETUP 1) Evaluation Metrics</head><p>As most previous works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b17">[18]</ref>, we employed the two most common evaluation metrics, namely the peak signal-to-noise ratio (PSNR) and the structural similarity index (SSIM). The PSNR is the ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Although the PSNR is one of the most used metrics for image reconstruction, some researchers <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref> argued that it is not highly indicative of the perceived similarity. The SSIM <ref type="bibr" target="#b45">[46]</ref> aims to address this shortcoming by taking contrast, luminance and texture into account. The result of the SSIM is a number between -1 and 1, where a value of 1 means the ground-truth image and the reconstructed image are identical. Similarly, a higher PSNR value indicates a better reconstruction, although the PSNR does not have an upper bound.</p><p>Since PSNR and SSIM values cannot guarantee a visually favorable result, we employ an additional metric for the final results, namely the information fidelity criterion (IFC) <ref type="bibr" target="#b47">[48]</ref>. Although IFC is scarcely used in literature <ref type="bibr" target="#b14">[15]</ref>, Yang et al. <ref type="bibr" target="#b48">[49]</ref> pointed out that IFC is correlated well with the human perception of SR images. As for PSNR and SSIM, higher IFC values indicate better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Image Quality Assessment by Human Observers</head><p>Because the above metrics rely only on the pixel values and not on the perceived visual quality, we decided to evaluate our method with the help of human annotators. Although a deep learning method can provide better PSNR, SSIM or IFC values, it might produce artifacts that could be misleading for right diagnostics and treatment. We thus have to make sure that our approach does not produce any unwanted artifacts visible to humans. We conducted the image quality assessment on the CH data set, testing our CNNbased method against Lanczos interpolation. We used CT slices extracted from high-resolution 3D images resulting after applying super-resolution on all three axes. For each upsampling factor, 2? and 4?, we extracted 100 CT slices at random from the test set. Hence, each human evaluator had to annotate 200 image pairs (100 for each upsampling factor). For each evaluation sample, an annotator would have seen the original image in the middle and the two reconstructed images on its sides, one on the left side and the other on the right side. The annotators had a magnifying glass tool that allowed them to look at details and discover artifacts. The locations (left or right) of the images reconstructed by our CNN and by Lanczos interpolation were randomly picked every time. To prevent any form of cheating, the randomly picked locations were unknown to the annotators. For each test sample, we asked each annotator to select the image that best reconstructed the original image. Our experiment was completed by 18 human annotators, 6 of them being doctors specialized in radiotherapy and oncology. In total, we collected 3600 annotations (18 annotators ? 200 samples).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) Baselines</head><p>We compared our method with standard resizing methods based on various interpolation schemes, namely nearest neighbors, bilinear, bicubic and Lanczos. In addition to these VOLUME 8, 2020 baselines, we compared with three methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> that focused on 2D SISR and one method <ref type="bibr" target="#b12">[13]</ref> that focused on 3D SISR. We note that You et al. <ref type="bibr" target="#b14">[15]</ref> did not report results on NAMIC. Nonetheless, You et al. <ref type="bibr" target="#b14">[15]</ref> were kind to provide access to their source code. We thus applied their method on both CH and NAMIC data sets, keeping the same settings and hyperparameters as recommended by the authors. As You et al. <ref type="bibr" target="#b14">[15]</ref>, we employed their method only on 2D superresolution for the 2? upscaling factor. For the other three baselines, we included the NAMIC scores reported in the respective articles <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. PARAMETER TUNING AND PRELIMINARY RESULTS</head><p>We conducted a series of preliminary experiments to determine the optimal patch size as well as the optimal width (number of convolutional filters) for our CNN. In order to find the optimal patch size, we tried out patches of 4 ? 4, 7 ? 7, 14 ? 14 and 17 ? 17 pixels. In term of the number of filters, we tried out values in the set {32, 64, 128} for all conv layers in our network. These parameters were tuned in the context of 2D super-resolution on the CH data set. The corresponding results are presented in <ref type="table" target="#tab_0">Table 1</ref>. First of all, we note that our method produces better SSIM and PSNR values, i.e. 0.9270 and 36.22, for patches of 7 ? 7 pixels. Second of all, we observe that adding more filters on the conv layers slightly increases the SSIM and PSNR values. However, the gains in terms of SSIM and PSNR come with a great cost in terms of time. For example, using 128 filters on each conv layer triples the processing time in comparison with using 32 filters on each conv layer. For the subsequent experiments, we thus opted for patches of 7 ? 7 pixels and conv layers with 32 filters. We believe that it is important to note that, although the number of training CT slices is typically in the range of a few hundreds, the number of training patches is typically in the range of hundreds of thousands. For instance, the number of 7 ? 7 training patches extracted from the CH data set for the 2? upscaling factor is 326,000. We thus stress out that the number of training samples is high enough to train highlyaccurate deep learning models.</p><p>During training, we used mini-batches of 128 images throughout all the experiments. In a set of preliminary experiments, we did not observe any significant differences when using mini-batches of 64 or 256 images. In each experiment, we trained the CNN for 40 epochs, starting with a learning rate (step size) of 10 ?3 and decreasing the learning rate to 10 ?4 after the first 20 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. ABLATION STUDY RESULTS</head><p>We performed an ablation study to emphasize the effect of various components over the overall performance. The ablation results obtained on the CH data set for super-resolution on height and width by a factor of 2? are presented in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>In our first ablation experiment, we have eliminated all the enhancements in order to show the performance level of a baseline CNN on the CH data set. Since there are several SISR works <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> based on the standard ESPCN model <ref type="bibr" target="#b18">[19]</ref>, we have eliminated the second convolutional block in the second ablation experiment, transforming our architecture into a standard ESPCN architecture. The performance drops from 0.9270 to 0.9236 in terms of SSIM and from 36.22 to 35.94 in terms of PSNR. In the subsequent ablation experiments, we have removed, in turns, the intermediate loss, the short-skip connections and the long-skip connection. The results presented in <ref type="table" target="#tab_1">Table 2</ref> indicate that all these components are relevant to our model, bringing significant performance benefits in terms of both SSIM and PSNR. In our last ablation experiment, we used a fixed standard deviation instead of a variable one for the Gaussian blur added on training patches. We notice that our data augmentation approach based on a variable standard deviation brings the highest gains in terms of SSIM (from 0.9236 to 0.9270) and PSNR (from 35.69 to <ref type="bibr">36.22)</ref>, with respect to the other ablated components.</p><p>Since the differences in terms of PSNR or SSIM for the ablated models are hard to quantify as small or large with respect to the complete CNN, we conducted paired McNemar's significance testing <ref type="bibr" target="#b49">[50]</ref> to determine if the differences are statistically significant or not. We considered a p-value threshold of 0.001 for our statistical testing. Every ablated model that is significantly different from the complete model is marked with ? in <ref type="table" target="#tab_1">Table 2</ref>. We note that the complete CNN is significantly better than each ablated version, although the actual differences in terms of PSNR or SSIM might seem small. We thus conclude that all the proposed enhancements provide significant performance gains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. RESULTS ON CH DATA SET</head><p>We first compared our CNN-based model with a series of interpolation baselines and a state-of-the-art method <ref type="bibr" target="#b14">[15]</ref> on the CH data set. We present the results for super-resolution on two axes (width and height) in <ref type="table" target="#tab_2">Table 3</ref>. Among the considered baselines, it seems that the Lanczos interpolation method provides better results than the bicubic, the bilinear or the nearest neighbor methods. Our CNN model is able to surpass all baselines for both upscaling factors, 2? and   4?. Compared to the best interpolation method (Lanczos), our method is 0.0180 better in terms of SSIM, 1.31 better in terms of PSNR and 0.43 better in terms of IFC. Furthermore, our CNN provides superior results to the GAN-based method of You et al. <ref type="bibr" target="#b14">[15]</ref>.</p><p>We note that, in <ref type="table" target="#tab_1">Table 2</ref>, we reported an SSIM of 0.9270 and a PSNR of 36.22 for our method, while in <ref type="table" target="#tab_2">Table 3</ref>, we reported an SSIM of 0.9291 and a PSNR of 36.39. In order to boost the performance of our method in accordance with the observed differences between Tables 2 and 3, we employed the self-ensemble strategy used by Lim et al. <ref type="bibr" target="#b50">[51]</ref>. For each input image, the self-ensemble strategy consists in generating additional images using geometric transformations, e.g. rotations and flips. Following Lim et al. <ref type="bibr" target="#b50">[51]</ref>, we generated 7 augmented images from the LR input image, upsampling all 8 images (the original image and the 7 additional ones) using our CNN. We then applied the inverse transformations to the resulting 8 HR images in order to obtain 8 output images that are aligned with the ground-truth HR images. The final output image is obtained by taking the median of the HR images. In the following experiments on CH and NAMIC data sets, the reported results always include the described self-ensemble strategy.</p><p>We provide the results for super-resolution on all three axes in <ref type="table" target="#tab_3">Table 4</ref>. First of all, we notice that the SSIM, the PSNR and the IFC values are lower for all methods when dealing with 3D super-resolution <ref type="table" target="#tab_3">(Table 4</ref>) instead of 2D super-resolution <ref type="table" target="#tab_2">(Table 3</ref>). This shows that the task of 3D super-resolution is much harder than 2D super-resolution. This is an expected result, considering that the dimensionality of the reconstruction space increases significantly for 3D super-resolution, i.e. there are many more HR outputs corresponding to a single LR input, while the training data is the same. Nevertheless, our method exhibits smaller performance drops when going from 2D super-resolution to 3D super-resolution. As for the 2D super-resolution experiments on CH data set, our CNN model for 3D super-resolution is superior to all baselines for both upscaling factors. We thus VOLUME 8, 2020  <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> and the Lanczos interpolation baseline on the NAMIC data set. For Zeng et al. <ref type="bibr" target="#b16">[17]</ref>, we included results for both single-channel super-resolution (SCSR) and multi-channel super-resolution (MCSR). The PSNR, the SSIM and the IFC values are reported for both T1w and T2w images and for two upscaling factors, 2? and 4?. The best results on each column are highlighted in bold.  conclude that our CNN model is better than all interpolation baselines on the CH data set, for both 2D and 3D superresolution and for all upscaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. RESULTS ON NAMIC DATA SET</head><p>On the NAMIC data set, we compared our method with the best-performing interpolation method on the CH data set, namely Lanczos interpolation, as well as some stateof-the-art 2D <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> and 3D <ref type="bibr" target="#b12">[13]</ref> super-resolution methods. We note that most previous works, including <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref>, used bicubic interpolation as a relevant baseline. Unlike these works, we opted for Lanczos interpolation, which provided better results than bicubic interpolation and other interpolation methods on the CH data set. We first present the 2D super-resolution results in <ref type="table" target="#tab_4">Table 5</ref>. The 2D SR results indicate that the GAN-based method of You et al. <ref type="bibr" target="#b14">[15]</ref> is superior to the CNN baselines <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b16">[17]</ref>. However, none of the state-of-the-art methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> is able to attain better performance than Lanczos interpolation. This proves that Lanczos interpolation is a much stronger baseline. Among the deep learning methods, our CNN is the only one to surpass Lanczos interpolation for 2D SR on NAMIC. We believe that this result is noteworthy.</p><p>We also present the 3D super-resolution results in <ref type="table" target="#tab_6">Table 6</ref>. The 3D SR results show that the approach of Pham et al. <ref type="bibr" target="#b12">[13]</ref> is better than Lanczos interpolation, which is remarkable. Our CNN is even better, surpassing both the Lanczos interpolation and the approach of Pham et al. <ref type="bibr" target="#b12">[13]</ref>.</p><p>As for the CH data set, we observe that the PSNR, the SSIM and the IFC scores for 2D super-resolution are higher than the corresponding scores for 3D super-resolution. The same explanation applies to the NAMIC data set, i.e. the CNNs have to produce likely reconstruction patterns in a much larger space.</p><p>While some of the considered state-of-the-art methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b16">[17]</ref> presented results only for some cases on NAMIC, either 2D super-resolution on T1w images or 3D super-resolution on T2w images, we provide our results for all possible cases. We note that our CNN model surpasses Lanczos interpolation in each and every case. Furthermore, our model provides superior results than all the state-of-theart methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref> considered in our evaluation on the NAMIC data set.</p><p>In addition to the quantitative results shown in <ref type="table" target="#tab_4">Tables 5 and  6</ref>, we present qualitative results in <ref type="figure">Figure 6</ref>. We selected 5 examples of 2D super-resolution results generated by Lanczos interpolation, by the GAN-based method of You et al. <ref type="bibr" target="#b14">[15]</ref> and by our CNN model. A close inspection reveals that our results are generally sharper than those of Lanczos interpolation and those of You et al. <ref type="bibr" target="#b14">[15]</ref>. As also confirmed by the SSIM, the PSNR and the IFC values presented in <ref type="table" target="#tab_4">Tables 5  and 6</ref>, the images generated by our CNN are closer to the ground-truth images. At the scale factor of 2? considered in <ref type="figure">Figure 6</ref>, our CNN does not produce any patterns or artifacts that deviate from the ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. IMAGE QUALITY ASSESSMENT RESULTS</head><p>We provide the outcome of the subjective image quality assessment by human observers in <ref type="table">Table 7</ref>. The study reveals that both doctors and regular annotators opted for our approach in favor of Lanczos interpolation at an overwhelming rate (97.55% at the 2? scale factor and 96.69% at the 4? FIGURE 6: Image super-resolution examples selected from the NAMIC data set. In order to obtain the input images of 128?128 pixels, the original NAMIC images were downsampled by a scale factor of 2?. HR images of 256 ? 256 pixels generated by Lanczos interpolation, by the GAN-based method of You et al. <ref type="bibr" target="#b14">[15]</ref> and by our CNN model are compared with the original (ground-truth) HR images. scale factor). For the 2? scale factor, 10 out of 18 annotators preferred the output of our CNN in all the 100 presented cases. We note that doctors #2 and #4 opted for Lanczos interpolation in 15 and 14 cases (for the 2? scale factor), respectively, which was not typical to the other annotators. Similarly, for the 4? scale factor, there are 3 annotators (doctor #4, doctor #5 and person #6) that seem to prefer Lanczos interpolation at a higher rate than the other annotators. After discussing with the doctors about their choices, we discovered that, in most cases, they prefer the sharper output of our CNN. However, the CNN seems to introduce some reconstruction patterns (learned from training data) that do not correspond exactly to the ground-truth. This phenomenon seems to be more prevalent at the 4? scale factor, although the phenomenon is still rarely observed. This explains why doctors #4 and #5 preferred Lanczos interpolation in more cases than the other doctors, although the majority of their votes are still in favor of our CNN. When they opted for Lanczos interpolation, they considered that it is safer to consider its blurred and less informative output. In trying to find an explanation for these reconstruction patterns, we analyzed the output of the CNN without data augmentation. We observed such reconstruction patterns even when training data augmentation was removed, ruling out this hypothesis. Given a low-resolution input patch, the CNN finds the most likely high-resolution patch corresponding to the input. This 7: Image quality assessment results collected from 6 doctors and 12 regular annotators, for the comparison between our CNN-based method versus Lanczos interpolation. For each upscaling factor, each annotator had to select an option for a number of 100 image pairs. To prevent cheating, the randomly picked locations (left or right) for the generated HR images were unknown to the annotators. <ref type="bibr">Annotator</ref>  likelihood is learned by the CNN when it is trying to minimize the loss over the entire training set. Although producing the most probable output works well in most cases, using a machine learning model, e.g. a CNN, is not the perfect solution. The explanation becomes clear if we consider that multiple HR patches can correspond to the same LR input patch and that choosing the most likely HR patch is not always the right answer. We thus conclude that our CNN suffers from the same problem as any other machine learning model. Furthermore, we stress out that the reconstruction patterns in question are plausible from a biological point of view, i.e. the doctors were able to spot them only by comparing the HR output with the ground-truth HR image. We note that these patterns should not be mistaken with artifacts that could be caused by underfitting or a poor architectural choice. Our CNN does not introduce such artifacts. Based on our subjective image quality assessment, we concluded with the doctors that going beyond the 4? scale factor, solely with a method based on algorithmic super-resolution, is neither safe (a CNN might introduce too many patterns far from the ground-truth) nor helpful (a standard interpolation method is not informative). However, the doctors agree that either super-resolution method is desirable in favor of the input low resolution images. Therefore, in order to reach the scale factor of 10? desired by the doctors, we have to look in other directions in future work. A promising direction is to combine multiple inputs, e.g. by using CT and MRI scans of the same person or by using CT/MRI scans taken at different moments in time (before and after the contrast agent is introduced).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented an approach based on fully convolutional neural networks for the super-resolution of CT/MRI scans. Our method is able to reliably upscale 3D CT/MRI image up to a scale factor of 4?. We have compared our approach with several baseline interpolation and state-of-the-art methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b16">[17]</ref>. The empirical results indicated that our approach provides superior results on both CH and NAMIC data sets. We have also conducted a subjective image quality assessment by human observers, showing that our method is significantly better than Lanczos interpolation. The subjective image quality assessment also revealed the limitations of a pure algorithmic approach. The doctors invited to take part in our study concluded that going to a scale factor higher than 4? requires alternative solutions. In future work, we aim to continue our research by extending the proposed CNN method to multi-channel input. This will likely help us in achieving higher upscaling factors, e.g. 10?, required for the accurate diagnostics and treatment of cancer, an actively studied and extremely important research topic <ref type="bibr" target="#b51">[52]</ref>, <ref type="bibr" target="#b52">[53]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 2 :</head><label>2</label><figDesc>Our convolutional neural network for super-resolution on two axes, height and width. The network is composed of 10 convolutional layers and an upsampling (sub-pixel convolutional) layer. It takes as input low-resolution patches of 7 ? 7 pixels and, for the r = 2 scale factor, it outputs high-resolution patches of 14 ? 14 pixels. The convolutional layers are represented by green arrows. The sub-pixel convolutional layer is represented by the red arrow. The long-skip and short-skip connections are represented by blue arrows. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 3 :</head><label>3</label><figDesc>An example of low-resolution input activation maps and the corresponding high-resolution output activation map given by the sub-pixel convolutional layer for upscaling on two axes. For a scaling factor of r = 2 in both directions, the sub-pixel convolutional layer requires r 2 = 4 activation maps as input. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 5 :</head><label>5</label><figDesc>Distribution of training samples (represented by green triangles) and test samples (represented by red circles), when the training samples are smoothed using a fixed standard deviation (left-hand side) versus using a randomlychosen standard deviation (right-hand side). In example (a), overfitting on the training data leads to poor results on test data. In example (b), the danger of overfitting is diminished because the test distribution is included in the training distribution. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Preliminary 2D super-resolution results on the CH data set for an upscaling factor of 2?. The PSNR and the SSIM values are reported for various patch sizes and different numbers of filters. For models with 7 ? 7 patches, we report the inference time (in seconds) per CT slice measured on an Nvidia GeForce 940MX GPU with 2GB of RAM.</figDesc><table><row><cell cols="4">Number of filters Input size SSIM PSNR Time (in seconds)</cell></row><row><cell>32</cell><cell>4 ? 4</cell><cell>0.9165 35.36</cell><cell>-</cell></row><row><cell>32</cell><cell>7 ? 7</cell><cell>0.9270 36.22</cell><cell>0.05</cell></row><row><cell>32</cell><cell cols="2">14 ? 14 0.8987 33.83</cell><cell>-</cell></row><row><cell>32</cell><cell cols="2">17 ? 17 0.8934 33.42</cell><cell>-</cell></row><row><cell>64</cell><cell>7 ? 7</cell><cell>0.9279 36.28</cell><cell>0.08</cell></row><row><cell>128</cell><cell>7 ? 7</cell><cell>0.9276 36.28</cell><cell>0.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Ablation 2D super-resolution results on the CH data set for an upscaling factor of 2?. The PSNR and the SSIM values are reported various ablated versions of our CNN model. The best results are highlighted in bold. Results of ablated models marked with ? are significantly worse than our complete model, according to paired McNemar's testing<ref type="bibr" target="#b49">[50]</ref> for the significance level 0.001.</figDesc><table><row><cell>Second conv block</cell><cell>Intermediate loss Short-skip connections Long-skip connection Variable standard deviation</cell><cell>SSIM</cell><cell>PSNR</cell></row><row><cell></cell><cell></cell><cell>0.9224  ?</cell><cell>35.58  ?</cell></row><row><cell></cell><cell></cell><cell>0.9236  ?</cell><cell>35.94  ?</cell></row><row><cell></cell><cell></cell><cell>0.9256  ?</cell><cell>36.15  ?</cell></row><row><cell></cell><cell></cell><cell>0.9260  ?</cell><cell>36.17  ?</cell></row><row><cell></cell><cell></cell><cell>0.9234  ?</cell><cell>36.11  ?</cell></row><row><cell></cell><cell></cell><cell>0.9236  ?</cell><cell>35.69  ?</cell></row><row><cell></cell><cell></cell><cell>0.9270</cell><cell>36.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>2D super-resolution results of our CNN model versus a state-of-the-art method<ref type="bibr" target="#b14">[15]</ref> and several interpolation baselines on the CH data set. The PSNR, the SSIM and the IFC values are reported for two upscaling factors, 2? and 4?. The best result on each column is highlighted in bold.</figDesc><table><row><cell>Method</cell><cell></cell><cell>2?</cell><cell></cell><cell></cell><cell>4?</cell><cell></cell></row><row><cell></cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell></row><row><cell>Nearest neighbor</cell><cell>0.8893</cell><cell>32.71</cell><cell>4.40</cell><cell>0.7659</cell><cell>29.06</cell><cell>1.32</cell></row><row><cell>Bilinear</cell><cell>0.8835</cell><cell>33.34</cell><cell>3.73</cell><cell>0.7725</cell><cell>29.73</cell><cell>1.49</cell></row><row><cell>Bicubic</cell><cell>0.9077</cell><cell>34.71</cell><cell>4.59</cell><cell>0.7965</cell><cell>30.41</cell><cell>1.72</cell></row><row><cell>Lanczos</cell><cell>0.9111</cell><cell>35.08</cell><cell>4.93</cell><cell>0.8012</cell><cell>30.57</cell><cell>1.84</cell></row><row><cell>You et al. [15]</cell><cell>0.8874</cell><cell>32.73</cell><cell>4.40</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Our CNN model</cell><cell>0.9291</cell><cell>36.39</cell><cell>5.36</cell><cell>0.8308</cell><cell cols="2">31.59 1.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4 :</head><label>4</label><figDesc>3D super-resolution results of our CNN model versus several interpolation baselines on the CH data set. The PSNR, the SSIM and the IFC values are reported for two upscaling factors, 2? and 4?. The best result on each column is highlighted in bold.</figDesc><table><row><cell>Method</cell><cell></cell><cell>2?</cell><cell></cell><cell></cell><cell>4?</cell><cell></cell></row><row><cell></cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell></row><row><cell>Nearest neighbor</cell><cell>0.8430</cell><cell>30.36</cell><cell>2.19</cell><cell>0.7152</cell><cell>27.32</cell><cell>0.72</cell></row><row><cell>Bilinear</cell><cell>0.8329</cell><cell>30.72</cell><cell>2.18</cell><cell>0.7206</cell><cell>27.93</cell><cell>0.95</cell></row><row><cell>Bicubic</cell><cell>0.8335</cell><cell>26.51</cell><cell>2.47</cell><cell>0.7200</cell><cell>24.05</cell><cell>1.04</cell></row><row><cell>Lanczos</cell><cell>0.8423</cell><cell>27.85</cell><cell>2.58</cell><cell>0.7263</cell><cell>25.06</cell><cell>1.09</cell></row><row><cell>Our CNN model</cell><cell>0.8926</cell><cell>33.04</cell><cell>2.83</cell><cell>0.7819</cell><cell cols="2">29.36 1.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 :</head><label>5</label><figDesc>2D super-resolution results of our CNN model versus several state-of-the-art methods</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6 :</head><label>6</label><figDesc>3D super-resolution results of our CNN model versus a state-of-the-art method<ref type="bibr" target="#b12">[13]</ref> and the Lanczos interpolation baseline on the NAMIC data set. The PSNR, the SSIM and the IFC values are reported for both T1w and T2w images and for two upscaling factors, 2? and 4?. The best results on each column are highlighted in bold.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">T1-weighted</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">T2-weighted</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>2?</cell><cell></cell><cell></cell><cell>4?</cell><cell></cell><cell></cell><cell>2?</cell><cell></cell><cell></cell><cell>4?</cell><cell></cell></row><row><cell></cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell><cell>SSIM</cell><cell>PSNR</cell><cell>IFC</cell></row><row><cell>Lanczos</cell><cell>0.9423</cell><cell>35.72</cell><cell>2.00</cell><cell>0.8690</cell><cell>31.81</cell><cell>0.95</cell><cell>0.9615</cell><cell>37.80</cell><cell>2.29</cell><cell>0.8829</cell><cell>32.08</cell><cell>1.03</cell></row><row><cell>Pham et al. [13]</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>0.9781</cell><cell>38.28</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell cols="2">Our CNN model 0.9687</cell><cell>37.85</cell><cell cols="2">2.38 0.9050</cell><cell>32.88</cell><cell>0.99</cell><cell>0.9835</cell><cell cols="2">40.57 2.67</cell><cell>0.9251</cell><cell>33.54</cell><cell>1.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at http://hdl.handle.net/1926/1687. 2 VOLUME 8, 2020</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">   VOLUME 8, 2020    Georgescu et al.: Convolutional Neural Networks With Intermediate Loss for 3D Super-Resolution of CT and MRI Scans</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">   VOLUME 8, 2020    Georgescu et al.: Convolutional Neural Networks With Intermediate Loss for 3D Super-Resolution of CT and MRI Scans</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">   VOLUME 8, 2020   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">   VOLUME 8, 2020    Georgescu et al.: Convolutional Neural Networks With Intermediate Loss for 3D Super-Resolution of CT and MRI Scans</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>The authors would like to thank the reviewers for their helpful comments. In memory of Tatiana-Mihaela Ionescu, who offered her CT scans to facilitate our study, but died of cancer.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A new approach for brain tumor diagnosis system: single image super resolution based maximum fuzzy entropy segmentation and convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>?zyurt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dogantekin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Hypotheses</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="page">109413</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient and accurate MRI super-resolution using a generative adversarial network and 3D multi-level densely connected network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Christodoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gradient-Guided Convolutional Neural Network for MRI Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page">4874</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accelerated Superresolution MR Image Reconstruction via a 3D Densely Connected Deep Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BIBM</title>
		<meeting>BIBM</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Super-resolution reconstruction of single anisotropic 3D MR images using residual convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gholipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning-based super-resolution applied to dental computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hatvani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Horv?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basarab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kouam?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gy?ngy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Radiation and Plasma Medical Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="120" to="128" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Tensor Factorization Method for 3-D Super Resolution With Application to Dental CT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hatvani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basarab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Tourneret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gy?ngy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kouam?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1524" to="1531" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simultaneous Super-Resolution and Cross-Modality Synthesis of 3D Medical Images Using Weakly-Supervised Joint Convolutional Sparse Coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5787" to="5796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cnn-based superresolution reconstruction of 3d mr images using thick-slice scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Materka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Majos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biocybernetics and Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="111" to="125" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Super-Resolution of Brain MRI Images Using Overcomplete Dictionaries and Nonlocal Similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guizani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25" to="897" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image super-resolution using progressive generative adversarial networks for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bozorgtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="30" to="39" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-input Cardiac Image Super-Resolution Using Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Marvao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>O&amp;apos;regan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of MICCAI</title>
		<meeting>MICCAI</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="246" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multiscale brain MRI super-resolution using deep 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tor-D?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Meunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bednarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fablet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Passat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">101647</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">MR image super-resolution via wide residual networks with fixed skip connection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1129" to="1140" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">CT super-resolution GAN constrained by the identical, residual, and cycle learning ensemble (GAN-CIRCLE)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computed tomography super-resolution using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bramler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3944" to="3948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simultaneous single-and multi-contrast super-resolution for brain MRI images based on a convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="133" to="141" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Channel splitting network for single MR image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5649" to="5662" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual Tracking with Fully Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3119" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Crowd Detection for Drone Safe Landing Through Fully-Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Castellano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mencar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vessio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SOFSEM, 2020</title>
		<meeting>SOFSEM, 2020</meeting>
		<imprint>
			<biblScope unit="page" from="301" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LSTM Fully Convolutional Networks for Time Series Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1662" to="1669" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Line Alberi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="135" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Super-resolution through neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="I" to="I" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Jointly Optimized Regressors for Image Super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="95" to="104" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Via Sparse Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image Super-Resolution Using Very Deep Residual Channel Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="294" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">X-ray microtomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Elliott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Dover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Microscopy</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="211" to="213" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Advanced Non-Destructive Ocular Visualization Methods by Improved X-Ray Imaging Techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Enders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-M</forename><surname>Braig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">U</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>No?l</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rummeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">170633</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tromba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abrami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arfelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Astolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bregant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Casarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chenda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dreossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mancini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Menk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Quai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Quaia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rigon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rokvic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sodini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sanabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schultke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tonutti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vascotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zanconati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Castelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The SYRMEP Beamline of Elettra: Clinical Mammography and Bio-medical Applications</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1266</biblScope>
			<biblScope unit="page" from="18" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual-and Multi-Energy CT: Principles, Technical Approaches, and Clinical Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Mccollough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Fletcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiology</title>
		<imprint>
			<biblScope unit="volume">276</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="637" to="653" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Image quality evaluation of duallayer spectral detector CT of the chest and comparison with conventional CT imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hauger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hickethier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrtus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wybranski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Hokamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haneder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Radiology</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="52" to="58" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Approximation Capabilities of Multilayer Feedforward Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Object-centric Auto-encoders and Dummy Anomalies for Abnormal Event Detection in Video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-I</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speeding up Convolutional Neural Networks with Low Rank Expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rectified Linear Units Improve Restricted Boltzmann Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mean squared error: Love it or leave it? A new look at signal fidelity measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="117" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Single-Image Super-Resolution: A Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="372" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1895" to="1923" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Enhanced Deep Residual Networks for Single Image Super-Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPRW</title>
		<meeting>CVPRW</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1132" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Advantages of laser photoacoustic spectroscopy in radiotherapy characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Popa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patachia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Matei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Romanian Reports in Physics</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="120" to="126" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Calculation of externally applied electric field intensity for disruption of cancer cell proliferation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sardari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electromagnetic Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1-12</biblScope>
			<biblScope unit="page" from="26" to="30" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
