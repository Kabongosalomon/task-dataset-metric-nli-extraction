<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MSAF: Multimodal Split Attention Fusion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Su</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuqing</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofa</forename><surname>Li</surname></persName>
							<email>guofali@szu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Shenzhen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongpu</forename><surname>Cao</surname></persName>
							<email>dongpu@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MSAF: Multimodal Split Attention Fusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multimodal learning mimics the reasoning process of the human multi-sensory system, which is used to perceive the surrounding world. While making a prediction, the human brain tends to relate crucial cues from multiple sources of information. In this work, we propose a novel lightweight multimodal fusion module that learns to emphasize more contributive features across all modalities. Specifically, the proposed Multimodal Split Attention Fusion (MSAF) module splits each modality into channel-wise equal feature blocks and creates a joint representation that is used to generate soft attention for each channel across the feature blocks. Further, the MSAF module is designed to be compatible with features of various spatial dimensions and sequence lengths, suitable for both CNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal networks and utilize existing pretrained unimodal model weights. To demonstrate the effectiveness of our fusion module, we design three multimodal networks with MSAF for emotion recognition, sentiment analysis, and action recognition tasks. Our approach achieves competitive results in each task and outperforms other applicationspecific networks and multimodal fusion benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multimodal learning has been explored in numerous machine learning applications such as audio-visual speech recognition <ref type="bibr" target="#b33">[34]</ref>, action recognition <ref type="bibr" target="#b1">[2]</ref>, and video question answering <ref type="bibr" target="#b21">[22]</ref>, where each modality contains useful information from a different perspective. Although these tasks can benefit from the complementary relationship in multimodal data, different modalities are represented in diverse fashions, making it challenging to grasp their complex correlations.</p><p>Studies in multimodal machine learning are mainly categorized into three fusion strategies: early fusion, intermediate fusion, and late fusion. Early fusion explicitly * Equal Contribution. exploits the cross-modal correlation by joining the representation of the features from each modality at the feature level, which is then used to predict the final outcome. The fusion is typically operated after the feature extractor for each modality, where techniques such as Compact Bilinear Pooling (CBP) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36]</ref> and Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31]</ref> are used to exploit the covariation between modalities. Unfortunately, modalities usually have different natures causing unaligned spatial and temporal dimensions. This creates obstacles in capturing the latent interrelationships in the low-level feature space <ref type="bibr" target="#b3">[4]</ref>. On the other hand, late fusion fuses the decision from each modality into a final decision using a simple mechanism such as voting <ref type="bibr" target="#b32">[33]</ref> and averaging <ref type="bibr" target="#b40">[41]</ref>. Since little training is required, a multimodal system can be promptly deployed by utilizing pretrained unimodal weights, unlike early fusion methods. However, decision-level fusion neglects the crossmodal correlation between the low-level features in modalities, resulting in limited improvement compared to the unimodal models. The intermediate fusion method joins features in the middle of the network, where some feature processing is done for the raw features from the feature extractors. Recent intermediate multimodal fusion networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b16">17]</ref> exploit the modality-wise relationships at different stages of the network, which has shown impressive results. However, there are still a limited number of works that can effectively capture cross-modal dynamics in an efficient way by using pretrained weights while introducing minimal parameters.</p><p>To overcome the aforementioned shortcomings of intermediate fusion methods, we propose a lightweight fusion module, MSAF, taking inspiration from the split-attention block in ResNeSt <ref type="bibr" target="#b52">[53]</ref>. The split-attention mechanism explores cross-channel relationships by dividing the featuremap into several groups and applying attention across the groups based on the global contextual information. We extend split-attention for multimodal applications in the proposed MSAF module to explore inter-and intra-modality relationships while maintaining a compact multimodal context. The MSAF module splits the features of each modality channel-wise into equal-sized feature blocks, which are globally summarized by a channel descriptor. The descrip-arXiv:2012.07175v2 [cs.CV] 26 Jun 2021 tor then learns to emphasize the important feature blocks by generating attention values. Subsequently, the enhanced feature blocks are rejoined for each modality, resulting in an optimized feature space with an understanding of the multimodal context. Our MSAF module is compatible with features of any shape as it operates only on the channel dimension. Thus, MSAF can be added between layers of any CNN or RNN architecture. Furthermore, we boost performance on sequential features by splitting modalities timewise and applying an MSAF module to each time segment. This allows emphasis of different feature blocks in each time segment. To our knowledge, our work is the first independent fusion module that can be used in both CNN-and RNN-based multimodal learning applications.</p><p>We comprehensively evaluate the effectiveness of MSAF in three multimodal learning tasks, namely audiovisual emotion recognition, sentiment analysis, and action recognition. We design a neural network with integrated MSAF modules for each task to demonstrate the ease of applying MSAF to existing unimodal configurations. Our experiments show that MSAF-powered networks outperform other fusion methods and state-of-the-art models designed for each application. Empirically, we observe that MSAF achieves better results while using a similar number of parameters as simple late fusion methods. Our module learns to pinpoint the important features regardless of the modality's complexity.</p><p>In summary, our work provides the following contributions: 1) MSAF -A novel lightweight multimodal fusion module for CNN and RNN networks that effectively fuses intermediate and high-level modality features to leverage the advantages of each modality. 2) We design three multimodal fusion networks corresponding to three applications: emotion recognition, sentiment analysis, and action recognition. Our experiments demonstrate the capabilities of MSAF through competitive results in all applications while utilizing fewer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early Fusion. The majority of works in early fusion integrate features immediately after they are extracted from each modality, whereas occasionally studies perform fusion at the input level, such as <ref type="bibr" target="#b31">[32]</ref>. A simple solution for early fusion is feature concatenation after they are transformed to the same length, followed by fully connected layers. Many early fusion works use CCA to exploit cross-modality correlations. <ref type="bibr" target="#b38">[39]</ref> applies CCA to improve the performance in speaker identification using visual and audio modalities. <ref type="bibr" target="#b0">[1]</ref> proposes deep CCA to learn complex nonlinear transformations between modalities, which inspired multimodal applications such as <ref type="bibr" target="#b27">[28]</ref>. Bilinear pooling is another early fusion method that fuses modalities by calculating their outer product. However, the generated high dimensional feature vectors are very computationally expensive for subsequent analysis. Compact bilinear pooling <ref type="bibr" target="#b12">[13]</ref> significantly mitigates the curse of dimensionality problem <ref type="bibr" target="#b16">[17]</ref> through a novel kernelized analysis while keeping the same discriminative power as the full bilinear representation.</p><p>Late Fusion. Late fusion merges the decision values from each unimodal model into an unified decision using fusion mechanisms such as averaging <ref type="bibr" target="#b40">[41]</ref>, voting <ref type="bibr" target="#b32">[33]</ref> and weighted sum <ref type="bibr" target="#b45">[46]</ref>. In contrast to early fusion, late fusion embraces the end-to-end learning between each modality and the given task. It allows for more flexibility as it can still train or make predictions when one or more modalities are missing. Nevertheless, late fusion lacks the exploration of lower-level correlations between the modalities. Therefore, when it comes to a disagreement between modalities, a simple mechanism acting only on decisions might be too simplified. There are also more complex late fusion approaches that exploit modality-wise synergies. For example, <ref type="bibr" target="#b25">[26]</ref> proposes a multiplicative combination layer that promotes the training of strong modalities per sample and tolerates mistakes made by other modalities.</p><p>Intermediate Fusion. Intermediate fusion exploits feature correlations after some level of processing, therefore the fusion takes place in the middle between the feature extractor and the decision layer. For instance, <ref type="bibr" target="#b47">[48]</ref> applies principle component analysis on the extracted features for each modality, and further processes them respectively before feature concatenation. Recent works continue to improve modality feature alignment to give stronger joint features. CentralNet <ref type="bibr" target="#b44">[45]</ref> coordinates features of each modality by performing a weighted sum of modalities in a central branch at different levels of the network. EmbraceNet <ref type="bibr" target="#b6">[7]</ref> prevents dependency on data of specific modalities and increases robustness to missing data through learning crossmodal correlations by combining selected features from each modality using a multinomial distribution. <ref type="bibr" target="#b19">[20]</ref> utilizes the squeeze and excitation module from SENet <ref type="bibr" target="#b17">[18]</ref> to enable slow modality fusion by channel-wise feature recalibration at different stages of the network. Our work aims to effectively fuse features of modalities while maintaining efficiency. We adopt the split attention <ref type="bibr" target="#b52">[53]</ref> concept for multimodal fusion where modalities are broken down into feature map groups with hidden complement relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We first formulate the multimodal fusion problem in an MSAF module. Let M be the number of modalities and the feature map of modality m ? {1, 2, ? ? ? , M } be F m ? IR N1?N2?????N K ?Cm . Here, K is the number of spatial dimensions of modality m and C m is the number of channels in modality m. Generally, an MSAF module takes the feature maps {F 1 , ? ? ? , F M } and generates optimized feature maps {F 1 , ? ? ? ,F M } activated by the corresponding per channel block-wise attention. An MSAF module consists of three operations: 1) split, 2) join, and 3) highlight, which are summarized in <ref type="figure" target="#fig_0">Figure 1</ref>. We explicate the three steps below. Split. We start by splitting each feature map channelwise into equal-channel feature blocks where the number of channels in each block is C. We denote the set of the feature blocks that belong to modality m as B m , where |B m | = C m /C , m ? {1, ? ? ? , M }, B i m being the ith feature block in B m , i ? {1, ? ? ? , |B m |}. When C m is not a multiple of C, the last block is padded with zeros in the missing channels.</p><p>Join. The join operation is a crucial step that learns the multimodal global context which is used to generate per channel block-wise attention. We join the blocks that belong to modality m into a shared representation D m , by calculating the element-wise sum S m over B m , followed by global average pooling on the spatial dimensions:</p><formula xml:id="formula_0">D m (c) = 1 K i=1 N i (n1,??? ,n K )</formula><p>S m (n 1 , n 2 , ? ? ? , n K , c)</p><p>(1) Each channel descriptor is now a feature vector of the common length C that summarizes the feature blocks within a modality. To obtain multimodal contextual information, we calculate the element-wise sum of the per modality descriptors {D 1 , ? ? ? , D M } to form a multimodal channel descriptor G. We capture the channel-wise dependencies by a fully connected layer with a reduction factor r followed by a batch normalization layer and a ReLU activation function. The transformation maps G to the joint representation Z ? IR C , C = C/r which helps with generalization for complex models.</p><formula xml:id="formula_1">Z = W Z G + b Z (2) where W Z ? IR C ?C , b Z ? IR C .</formula><p>As advised in <ref type="bibr" target="#b19">[20]</ref> and evident in our experiments, a reduction factor of 4 is ideal for two modalities. As the number of modalities increase, we recommend reducing the reduction factor to accommodate mores features in the joint representation. For example, when fusing 3 modalities, a reduction factor of 2 was optimal in our results for sentiment analysis.</p><p>Highlight. The multimodal channel descriptor contains generalized but rich knowledge of the global context. In this step, for a block B i m , we generate the corresponding logits U i m by applying a linear transformation on Z and obtain the block-wise attention weights A i m using the softmax activation.</p><formula xml:id="formula_2">U i m = W i m Z + b i m (3) A i m = exp(U i m ) M k |B k | j exp(U j k )<label>(4)</label></formula><p>where W i m ? IR C?C and b i m ? IR C are weights and bias of the corresponding fully connected layer. Since soft attention values are dependent on the total number of feature blocks, features may be over-suppressed. The effect is more apparent in complex tasks which results in insufficient information for accurate predictions. Thus, we present a hyperparameter ? ? [0, 1] that controls the suppression power of MSAF. Intuitively, ? can be understood as a regularizer for the lowest attention of a split. We obtain an optimized feature blockB i m using attention signals A i m and ?:</p><formula xml:id="formula_3">B i m = [? + (1 ? ?) ? A i m ] B i m<label>(5)</label></formula><p>Finally, the feature blocks belonging to modality m are merged by channel-wise concatenation to produceF m .</p><formula xml:id="formula_4">F m = [B 1 m ,B 2 m , ? ? ? ,B m |Bm| ]<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">BlockDropout</head><p>To lessen the dependencies on certain strong feature blocks and ease overfitting, we propose a dropout method for the feature blocks called BlockDropout. BlockDropout generates a binary mask that randomly drops feature blocks from the set of all feature blocks from each modality B, and applies the same mask on the block's attention. Let the dropout probability p ? [0, 1), we draw |B| samples from a Bernoulli distribution with the probability of success (1 ? p), resulting in a binary mask for dropping out the feature blocks. Subsequently, the mask is scaled by <ref type="bibr" target="#b0">1</ref> 1?p and is applied to the generated attention vectors. This is not to be confused with DropBlock <ref type="bibr" target="#b13">[14]</ref> which is used in ResNeSt to regularize convolutional layers by randomly masking out local block regions in the feature map. Whereas Block-Dropout is applied to feature blocks after the first step of MSAF which are split in the channel dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Enhancing MSAF for RNNs</head><p>As features pass through a CNN, the number of channels gradually increases while other dimensions shrink from convolving filters and pooling layers. In a multilayer RNN, the length of the feature sequence remains the same between layers. When applying split attention to features of longer sequence length, as is the case for RNNs, the original MSAF module does not have the flexibility to adjust the attention vector of each block over the sequence. For example, in audiovisual emotion recognition, there may be a segment where the audio modality should be highlighted over the video modality and vice versa in another segment of the same sequence.</p><p>We achieve this by splitting each modality into q segments. We define the sequence length of a modality as S then the length of each segment for modality m is S m /q . The modalities of each segment are passed into separate MSAF blocks. Optimally the sequence lengths are the same or evenly divisible by q. Otherwise, the extra segment is combined with the second last segment. This process is visualized in <ref type="figure" target="#fig_1">Figure 2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Applications</head><p>In this section, we apply the MSAF module to fuse unimodal networks in three applications. We describe each unimodal network and our configuration for the MSAF modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Emotion Recognition</head><p>Multimodal emotion recognition (MER) is a classification task that categorizes human emotions using multiple interacting signals. Although numerous works have utilized more complex modalities such as EEG <ref type="bibr" target="#b53">[54]</ref> and body gesture <ref type="bibr" target="#b8">[9]</ref>, video and audio remain as dominant modalities used for this task. Thus, we design a multimodal network that fuses a 3D CNN for video and a 1D CNN for audio using MSAF. Video data has dependencies in both spatial and temporal dimensions, therefore requires a network with 3D kernels to learn both the facial expression and its movement. Considering both network performance and training efficiency, we choose the 3D ResNeXt50 network <ref type="bibr" target="#b48">[49]</ref> as suggested by <ref type="bibr" target="#b14">[15]</ref> with cardinality set to 32. For the audio modality, recent works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b46">47]</ref> have demonstrated the effectiveness of deep learning based methods built on Melfrequency cepstral coefficients (MFCC) features. We design a simple 1D CNN for the MFCC features and fuse the two modalities via two MSAF modules as shown in <ref type="figure" target="#fig_3">Figure 3</ref>. We configured the two MSAF modules with 16 and 32 channels per block respectively and BlockDropout with p = 0.2. Finally, we sum the logits of both networks followed by a softmax function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Sentiment Analysis</head><p>Sentiment analysis is the use of Natural Language Processing (NLP) to interpret and classify people's opinions from text. In recent years, the multimodal nature of human language has led to the incorporation of other modalities such as visual and audio data in NLP tasks. Similar to works such as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16]</ref>, we apply our module on audio, visual and text modalities. Our architecture uses two LSTM layers for each modality with an MSAF module in between to fuse features from the first LSTM layer. We use 32, 64 and 128 hidden units for both LSTM layers in the visual, audio and text modality respectively. For the MSAF module, each modality is split into 5 segments sequence-wise (q = 5) and each segment is passed into a separate MSAF block with 16 channels per feature block and BlockDropout with p = 0.2. Lastly, we concatenate the final output from each LSTM and pass that to a fully connected layer to generate the sentiment value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Action Recognition</head><p>With the development of depth cameras, depth and skeleton data became crucial modalities in the action recognition task along with RGB videos. Multiple works such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref> have achieved competitive performance using RGB videos associated with skeleton sequences. We follow <ref type="bibr" target="#b19">[20]</ref> which utilizes I3D <ref type="bibr" target="#b5">[6]</ref> for the video data, and HCN <ref type="bibr" target="#b22">[23]</ref> for the skeleton stream. As illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, we deploy two MSAF modules, one at an intermediate-level in both networks, and the other one for high-level feature recalibration. The HCN framework proposes two strategies to be scalable to multi-person scenarios. The first type stacks the joints from all persons and feeds it as the input of the network in an early fusion style. The second type adapts late fusion that passes the inputs of multiple persons through the same subnetwork, whose Conv6 channel-wise concatenates or element-wise maximizes the group of features of persons. The latter generalizes better to various numbers of persons than the other, which needs a predefined maximum number of persons. <ref type="bibr" target="#b19">[20]</ref> follows the multi-person late fusion strategy and utilizes their first fusion module on one of the two persons universally. We take a different approach by considering all available individuals in a sample because either can send important signals during a multi-person interaction. Our first MSAF module has 64 channels per block and is inserted between the second last Inception layer in I3D and the Conv5 outputs of each person. The second MSAF has 256 channels per block and is positioned between the last Inception layer in I3D and the FC7 layer in HCN. A suppression power of ? = 0.5 is used for both modules. Finally, we average the logits of both networks followed by a softmax function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we discuss our dataset choice, data preprocessing and training details for each application. Further, we evaluate our method and compare with other stateof-the-art works. Validation set accuracy was used to select the optimal hyperparameters for benchmarks and our proposed method in our tables. To verify the effectiveness of MSAF and the proposed hyperparameters, we conduct an ablation study for each task and analyze the module through its complexity, computation cost and visualization of attention signals. All of our experiments were conducted using a single Nvidia 2080 Ti GPU in Ubuntu 20.04 with Python 3.6 and PyTorch 1.7.1. To ensure the reproducibility of our results, our code is made publicly available on GitHub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Emotion Recognition</head><p>Data Preparation. There are many emotion recognition datasets that contain both facial expression and audio signals including <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b24">25]</ref>. We chose the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) <ref type="bibr" target="#b28">[29]</ref> dataset due to its high quality in both video and audio recording and the sufficient number of video clips. RAVDESS contains 1440 videos of short speech from 24 actors (12 males, 12 females), performed under the emotion they are told to act upon. Eight emotions are included in the dataset: neutral, calm, happy, sad, angry, fearful, disgust and surprised. We extracted 30 consecutive images from each video. For each image, we used the 2D facial landmarks provided to crop the face area and then resized to (224, 224). Random crop, horizontal flip, and normalization were used for data augmentation. For the audio modality, since the first 0.5 seconds usually contains no sound, we cropped the first 0.5 seconds and took the next 2.45 seconds for consistency. As suggested by <ref type="bibr" target="#b18">[19]</ref>, we extracted the first 13 MFCC features for each cropped audio clip. We proceed with a 6 fold cross-validation based on the actors for the RAVDESS dataset. We split the 24 actors in a 5:1 ratio for the training and testing sets. Since the gender of the actors is indicated by even or odd actor IDs, we keep the genders evenly distributed by rotating through 4 consecutive actor IDs as the testing set for each fold.</p><p>Training. We first fine-tuned the unimodal models for each fold on RAVDESS. For both unimodal and multimodal training, we used the Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with a constant learning rate of 10 ?3 . The final accuracy reported is the average accuracy over the 6 folds. Benchmarks. For this task, we implemented multiple recent multimodal fusion algorithms as our benchmarks. We categorize them into the following: 1) simple feature concatenation followed by fully connected layers based on <ref type="bibr" target="#b36">[37]</ref> and MCBP <ref type="bibr" target="#b11">[12]</ref> as two early fusion methods, 2) MMTM <ref type="bibr" target="#b19">[20]</ref> as the state-of-the-art intermediate fusion method, 3) averaging, multiplication are two standard late fusion methods; multiplicative layer <ref type="bibr" target="#b25">[26]</ref> is a late fusion method that adds a down-weighting factor to CE loss to suppress weaker modalities.</p><p>Results. <ref type="table" target="#tab_0">Table 1</ref> presents the accuracy of the proposed method in comparison with the implemented benchmarks. Our network surpasses unimodal baselines by over 10% verifying the importance of multimodal fusion. Early fusion methods did not exceed standard late fusion benchmarks by a significant number, indicating the challenge of finding cross-modal correlations between the complex video network and the 1D audio model in early stages. As expected, intermediate fusion methods out performed late and early methods as they are able to highlight features while they are developed to identify areas of focus in each modality. Our model outperforms the top performer MMTM by 1.74% while using 19% less parameters. Compared to the unimodal models, we only introduced 30K parameters in the fusion module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Sentiment Analysis</head><p>Data Preparation. CMU-MOSI <ref type="bibr" target="#b51">[52]</ref> and CMU-MOSEI <ref type="bibr" target="#b2">[3]</ref> are commonly used datasets for multimodal sentiment analysis. We chose to evaluate our method using CMU-MOSEI since it is the next generation of CMU-MOSI provided by the same authors. The CMU-MOSEI dataset contains 22852 annotated video segments (utterances) from 1000 distinct speakers and 250 topics gathered from online video sharing websites. Each utterance is annotated with a sentiment intensity from <ref type="bibr">[-3, 3]</ref>. The train, validation and test set contain 16322, 1871, and 4659 samples respectively. Following recent works, we evaluate: 1) mean absolute error (MAE) and Pearson correlation (Corr) for regression, 2) binary accuracy (Acc-2) and F-score, 3) 7 class accuracy (Acc-7) from -3 to 3. For binary classification, we consider [-3, 0) labels as negative and (0, 3] as positive.</p><p>In order to draw comparisons with recent works <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16]</ref>. COVAREP <ref type="bibr" target="#b9">[10]</ref>, FACET 1 and BERT <ref type="bibr" target="#b10">[11]</ref> features were selected for audio, visual and text modalities respectively. We acquired aligned COVAREP, FACET features and raw text from the public CMU-MultimodalSDK v1.2.0 2 with a sequence length of 50 words for all modalities. The raw text of individual utterances was passed into a pretrained uncased BERT model (not fine-tuned on CMU-MOSEI) and the final encoder layer output was used for text features. Each utterance was passed in separate epochs to avoid adding additional context.</p><p>Training. Since the network is relatively small, the unimodal models were not pretrained. The multimodal network was trained using mean squared error (MSE) loss and the Adam optimizer was used with a constant learning rate of 10 ?3 . For Acc-7, we rounded the output to the nearest integer and clipped to -3 and 3.</p><p>Benchmarks. We summarize various multimodal fusion methods proposed for sentiment analysis as follows: 1) DCCA <ref type="bibr" target="#b42">[43]</ref> and ICCN <ref type="bibr" target="#b41">[42]</ref> use Deep CCA to correlate text with audio-visual features, 2) TFN <ref type="bibr" target="#b50">[51]</ref> and LMF <ref type="bibr" target="#b29">[30]</ref> perform outer-products on modalities to create a joint representation, 3) MFM <ref type="bibr" target="#b43">[44]</ref> and MISA <ref type="bibr" target="#b15">[16]</ref> separates features into modality-specific and modality-invariant. While MFM optimizes a joint generative-discriminative objective allowing for classification and reconstruction of missing modalities, MISA trains modality-specific and multimodal encoders which are then fused using multi-headed self-attention before prediction.  <ref type="table">Table 2</ref>. Comparison between multimodal fusion benchmarks and ours for sentiment analysis on CMU-MOSEI. * from original papers and from <ref type="bibr" target="#b41">[42]</ref>. The standard error over 5 runs are in italics.</p><p>Results. <ref type="table">Table 2</ref> shows the results of our experiments in comparison with the state-of-the-art and recent works using BERT. Our multimodal model outperforms all unimodal models confirming that audio-visual features improve sentiment analysis. MSAF achieves better or similar performance on all metrics compared to state-of-the-art multimodal methods while using a simpler network architecture. Compared to MISA and others, our training method is simpler, allowing MSAF to be easily applied to existing unimodal networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Action Recognition</head><p>Data Preparation. NTU RGB+D <ref type="bibr" target="#b39">[40]</ref> is a large-scale human action recognition dataset. It contains 60 action classes and 56,880 video samples associated with 3D skeleton data. Cross-Subject (CS) and Cross-View (CV) are two recommended protocols. CS splits the training set and testing set by the subject IDs, whereas CV splits the samples based on different camera views. Recent methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8]</ref> have achieved decent CV accuracies; however, CS still remains a more challenging evaluation method based on the reported performance compared to the CV counterpart. We adopt the CS evaluation and split the 40 subjects based on the specified rule. For data preprocessing, video frames are extracted at 32 FPS and we adopt the same data augmentation approach as <ref type="bibr" target="#b19">[20]</ref>.</p><p>Training. The Adam optimizer with a base learning rate of 10 ?3 and a weight decay of 10 ?4 is used. The learning rate is reduced to 10 ?4 at epoch 5, where the loss is near saturation in our experiment.</p><p>Benchmarks. We summarize the multimodal fusion benchmarks for action recognition based on RGB videos and skeletons as follows: 1) SGM-Net <ref type="bibr" target="#b23">[24]</ref> proposed a skeleton guidance block to enhance RGB features, 2) Cen-tralNet <ref type="bibr" target="#b44">[45]</ref> adds a central branch that learns the weighted  <ref type="table">Table 3</ref>. Comparison between multimodal fusion benchmarks and ours on the NTU RGB+D Cross-Subject protocol. * from original papers and from <ref type="bibr" target="#b19">[20]</ref>. The standard error for Inflated ResNet50 and I3D over 5 runs is 0.04 and 0.03 respectively. sum of skeleton and RGB features at various locations, 3) MFAS <ref type="bibr" target="#b37">[38]</ref> is a generic search algorithm that finds an optimal architecture for a given dataset, 4) PoseMap <ref type="bibr" target="#b26">[27]</ref> uses CNNs to process pose estimation maps and skeletons independently with late fusion for final prediction, 5) MMTM <ref type="bibr" target="#b19">[20]</ref> recalibrates features at different stages achieving stateof-the-art in RGB and skeleton fusion.</p><p>Results. <ref type="table">Table 3</ref> reports the accuracy of the proposed MSAF network in comparison with other action recognition models using RGB videos and skeletons. To compare with state-of-the-art intermediate fusion methods, we also evaluate the performance of MSAF applied to Inflated ResNet50 <ref type="bibr" target="#b4">[5]</ref> and HCN. Our model outperforms all intermediate fusion methods and application-specific models, achieving state-of-the-art performance in RGB+pose action recognition in the NTU RGB+D CS protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>To obtain the configurations used for each application, we conduct the ablation study on all three datasets with the following hyperparameters: the number of channels in a block C, attention regularizer ? (default value is 0), Block-Dropout (with p = 0.2), and the number of segments q for RNNs (default value is 1). <ref type="table" target="#tab_3">Table 4</ref> reports the accuracy of the configurations building up to the best configuration. We observe the optimal number of channels in a block, C, for each dataset can be derived from min {C 1 , ? ? ? , C M }/2 which serves as a good starting point when tuning C for other applications. Hyperparameter ? plays an important role in NTU by avoiding over-suppression of features for more complex tasks. BlockDropout is essential to the performance in RAVDESS and MOSEI but not NTU as dropout tends to be more effective on smaller datasets to prevent overfitting. The importance of q is shown in MO-SEI, where a sequence length of 5 words was optimal for a sequence length of 50 words. As q increases, the number of MSAF modules increases which increases the number of parameters and thus results in overfitting.</p><p>The location of a MSAF module in a multimodal network architecture is an important factor for effective feature fusion. On one hand, placing a MSAF module at an earlier part of the network can help unimodal models learn to correlate raw features of each other. On the other hand, using MSAF to fuse high-level features generates more apparent bias towards specific unimodal patterns as the highlevel features are more tailored to the task. To analyze the effect of MSAF in different fusion locations on model performance, we define three positions to place MSAF in our action recognition network (I3D + HCN). In the early location, a MSAF receives the concatenated Conv4 features from the two actors in HCN and the third last Inception layer of I3D. The intermediate location is set to be between the Conv5 layer of HCN and the second last Inception layer of I3D. Finally, the late location is at the last I3D Inception layer and the FC7 layer of HCN. We follow C = min {C 1 , ? ? ? , C M }/2 while keeping other parameters the same. We train the multimodal network with different combinations of the above fusion locations and report our results in <ref type="table" target="#tab_4">Table 5</ref>.</p><p>We observe that the combination of intermediate and late fusion achieves the best result among all seven experiments. Interestingly, all experiments that involve early fusion yield similar performance at around 91.9%. Further, deploying MSAF in all three locations does not achieve better performance than using only intermediate and late fusion. We believe this is because the low-level features at the early position are still underdeveloped to show enough correlation for effective fusion, which results in sub-optimal performance. In summary, we find that multimodal fusion using MSAF is the most effective when applied to a combination of intermediate and high-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Parameter and Attention Analysis</head><p>Reflecting on our objective to design an effective fusion module that is also lightweight, we analyze the number of parameters of the MSAF module. Ideally, the fusion module should introduce minimal parameters to the unimodal networks combined despite the feature map size of the modalities. The split and join steps in MSAF ensure the joint feature space depends on the channel number of the feature blocks instead of the channel of the modalities. Therefore, the number of parameters is significantly reduced. In <ref type="figure" target="#fig_5">Figure 5</ref>, we compare the number of parameters of MSAF to MMTM <ref type="bibr" target="#b19">[20]</ref>. For both methods, we use two example modalities with shape (4, #Channels, 3, 128, 128) where #Channels is indicated on the x-axis. The reduction factor is set to 4 for both modules and we set C = min {C 1 , ? ? ? , C M }/2 for MSAF. As shown, MSAF utilizes parameters more efficiently, reaching a maximum of 330K parameters. In terms of computational cost, the number of FLOPs for MSAF has a similar trend as the number of parameters. For instance, when #Channels is 64 and 1024, MSAF has 10.4K and 2.6M FLOPs, whereas MMTM has 131.6K and 33.6M FLOPs respectively. To further understand the MSAF module and its effectiveness, we analyze the attention signals produced on the RAVDESS dataset. We first compare the attention signals averaged per emotion. <ref type="figure" target="#fig_6">Figure 6</ref> shows the attention signals from the second MSAF module and sum the attention values for the blocks of the same modality. The video modality has higher attention weights when summed together since it has more blocks and is the stronger modality. However, we observe that for some emotions such as happy, a number of channels in the audio modality have similar weights as the video modality. This shows that the MSAF module is able to optimize how the modalities are used together depending on the emotion. Next, we examine the attention signals from the first MSAF module versus the second MSAF module. In <ref type="figure">Figure 7</ref>, the first MSAF module gives blocks of each modality similar levels of attention since the features are lower-level whereas the second MSAF module learns that the audio modality has fewer blocks and gives them higher attention values compared to the video modality blocks. <ref type="bibr">Figure 7</ref>. Comparison between attention values of 2 MSAF modules in RAVDESS. Blocks 14-16 belong to the audio modality and part of the video modality is shown due to size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we present a lightweight multimodal fusion module, MSAF, that learns to exploit the complementary relationships between the modalities and highlight features for optimal multimodal learning. MSAF enables easy deployment of high-performance multimodal models due to its compatibility with CNNs and RNNs. We implement three multimodal networks with MSAF for emotion recognition, sentiment analysis, and action recognition. Our experiments demonstrate the module's ability to coordinate various types of modalities through competitive evaluation results in all three tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Breakdown of the MSAF module with steps, split, join and highlight, numbered on the left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Enhancing the MSAF module for RNNs. For easy visualization, the sequence lengths are evenly divisible by q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Proposed architecture for emotion recognition</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Proposed architecture for action recognition. "Inc." denotes an inception module from<ref type="bibr" target="#b5">[6]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Number of parameters comparison between an MSAF module and an MMTM<ref type="bibr" target="#b19">[20]</ref> module. Each module receives two modalities with the same channel number indicated by the x-axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Visualization of attention values from the second MSAF module averaged for each emotion in the RAVDESS dataset and summed modality-wise (V=video, A=audio).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between multimodal fusion benchmarks and ours for emotion recognition on RAVDESS.</figDesc><table><row><cell>Model</cell><cell cols="3">Fusion Stage Accuracy #Params</cell></row><row><cell>3D ResNeXt50 (Vis.)</cell><cell>-</cell><cell>62.99</cell><cell>25.88 M</cell></row><row><cell>1D CNN (Aud.)</cell><cell>-</cell><cell>56.53</cell><cell>0.03 M</cell></row><row><cell>Averaging</cell><cell>Late</cell><cell>68.82</cell><cell>25.92 M</cell></row><row><cell>Multiplicative ?=0.3</cell><cell>Late</cell><cell>70.35</cell><cell>25.92 M</cell></row><row><cell>Multiplication</cell><cell>Late</cell><cell>70.56</cell><cell>25.92 M</cell></row><row><cell>Concat + FC</cell><cell>Early</cell><cell>71.04</cell><cell>26.87 M</cell></row><row><cell>MCBP</cell><cell>Early</cell><cell>71.32</cell><cell>51.03 M</cell></row><row><cell>MMTM</cell><cell>Inter.</cell><cell>73.12</cell><cell>31.97 M</cell></row><row><cell>MSAF</cell><cell>Inter.</cell><cell>74.86</cell><cell>25.94 M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Ablation study of MSAF module hyperparameters. For CMU-MOSEI, Acc-7 is shown for Acc.</figDesc><table><row><cell>Dataset</cell><cell>C</cell><cell>?</cell><cell>BlockDropout</cell><cell>q</cell><cell>Acc.</cell></row><row><cell></cell><cell>8, 16</cell><cell></cell><cell></cell><cell></cell><cell>71.01</cell></row><row><cell></cell><cell>16, 32</cell><cell></cell><cell></cell><cell></cell><cell>72.99</cell></row><row><cell>RAVDESS</cell><cell>32, 64 32, 64</cell><cell></cell><cell></cell><cell></cell><cell>73.40 72.29</cell></row><row><cell></cell><cell>16, 32</cell><cell></cell><cell></cell><cell></cell><cell>74.86</cell></row><row><cell></cell><cell>16, 32</cell><cell>0.25</cell><cell></cell><cell></cell><cell>74.37</cell></row><row><cell></cell><cell>8</cell><cell></cell><cell></cell><cell></cell><cell>51.6</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell></cell><cell>51.7</cell></row><row><cell>MOSEI</cell><cell>32 16</cell><cell></cell><cell></cell><cell></cell><cell>51.1 52.3</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>5</cell><cell>52.4</cell></row><row><cell></cell><cell>16</cell><cell></cell><cell></cell><cell>10</cell><cell>52.2</cell></row><row><cell></cell><cell>32, 128</cell><cell></cell><cell></cell><cell></cell><cell>91.04</cell></row><row><cell></cell><cell>64, 256</cell><cell></cell><cell></cell><cell></cell><cell>91.56</cell></row><row><cell>NTU</cell><cell>126, 512 64, 256</cell><cell>0.25</cell><cell></cell><cell></cell><cell>91.05 92.00</cell></row><row><cell></cell><cell>64, 256</cell><cell>0.5</cell><cell></cell><cell></cell><cell>92.24</cell></row><row><cell></cell><cell>64, 256</cell><cell>0.5</cell><cell></cell><cell></cell><cell>92.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of the placement of MSAF modules in early, intermediate and late feature levels on NTU RGB+D.</figDesc><table><row><cell>Early Intermediate Late Acc. (CS)</cell></row><row><cell>91.93</cell></row><row><cell>92.08</cell></row><row><cell>92.11</cell></row><row><cell>91.81</cell></row><row><cell>91.88</cell></row><row><cell>92.24</cell></row><row><cell>91.88</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://imotions.com/platform/ 2 https://github.com/A2Zadeh/CMU-MultimodalSDK</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galen</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML)<address><addrLine>Atlanta, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1247" to="1255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sequential deep learning for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christophe</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atilla</forename><surname>Baskurt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Behavior Understanding</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirali Bagher</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2236" to="2246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Glimpse clouds: Human activity recognition from unstructured feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Embracenet: A robust deep learning architecture for multimodal classification. Information Fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Ho</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Seok</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="259" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Infrared and 3d skeleton feature fusion for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Main De Boissiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Noumeir</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12886</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards recognizing emotion with affective dimensions through body gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P Ravindra De</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minetada</forename><surname>Osano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashu</forename><surname>Marasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajith P</forename><surname>Madurapperuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Automatic Face and Gesture Recognition (FGR06)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="269" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Covarep -a collaborative voice analysis repository for speech technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="960" to="964" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ning Zhang, and Trevor Darrell. Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Misa: Modality-invariant and -specific representations for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03545</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dense multimodal fusion for hierarchically joint representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiping</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3941" to="3945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Speech emotion recognition with acoustic and lexical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4749" to="4753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mmtm: Multimodal transfer module for cnn fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Hamid Reza Vaezi Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13289" to="13299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations (ICLR)</title>
		<meeting>the 3rd International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tvqa: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1369" to="1379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="786" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sgm-net: Skeleton-guided multimodal network for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuemei</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingzhe</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">107356</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A spontaneous driver emotion facial expression (defe) dataset for intelligent vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yintao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guofa</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongpu</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08626</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learn to combine modalities in multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11730</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recognizing human actions as the evolution of pose estimation maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multimodal emotion recognition using deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie-Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05349</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Locally confined modality fusion network with a global perspective for multimodal human affective computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="122" to="137" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep convolutional and lstm recurrent neural networks for multimodal wearable activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Majority vote of diverse classifiers for late fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilie</forename><surname>Morvant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Ayache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural, Syntactic, and Statistical Pattern Recognition</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="153" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Audio visual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chalapathy</forename><surname>Neti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerasimos</forename><surname>Potamianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Luettin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitra</forename><surname>Vergyri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">June</forename><surname>Sison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azad</forename><surname>Mashari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IDIAP</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Moddrop: Adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1692" to="1706" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep spatio-temporal feature fusion with compact bilinear pooling for multimodal emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kien</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridha</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clinton</forename><surname>Fookes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Multimodal fusion with deep neural networks for audio-video emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Senoussaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">L</forename><surname>Cardinal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koerich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03196</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mfas: Multimodal fusion architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>Perez-Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6966" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multimodal speaker identification using canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engin</forename><surname>Mehmet Emre Sargin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y?cel</forename><surname>Erzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yemez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murat Tekalp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Acoustics Speech and Signal Processing Proceedings</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="613" to="616" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Black holes and white rabbits: Metaphor identification with visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Shutova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="160" to="170" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathusha</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Palo Alto, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-modal sentiment analysis using deep canonical correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Prathusha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">P</forename><surname>Sethares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bucy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1323" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning factorized multimodal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR 2019</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Centralnet: a multilayer approach for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="575" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved weight assignment approach for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aarohi</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chirag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mita</forename><surname>Paunwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paunwala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 International Conference on Circuits, Systems, Communication and Information Technology Applications (CSCITA)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="70" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Speech emotion recognition with dual-sequence lstm architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enmao</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vahid</forename><surname>Tarokh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6474" to="6478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dnn multimodal fusion techniques for predicting video sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramona</forename><surname>Comanescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leimin</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)</title>
		<meeting>Grand Challenge and Workshop on Human Multimodal Language (Challenge-HML)<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="64" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Feedback graph convolutional network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunda</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07564</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1103" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Mosi: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06259</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Resnest: Splitattention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multimodal emotion recognition using eeg and eye tracking data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Long</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo-Nan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bao-Liang</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5040" to="5043" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
