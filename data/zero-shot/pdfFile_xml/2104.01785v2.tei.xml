<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Annotating Columns with Pre-trained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-12">2022. June 12-17, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
							<email>jinfeng@megagon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
							<email>yuliang@megagon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
							<email>dan_z@megagon.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?a?atay</forename><surname>Demiralp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Megagon</forename><surname>Labs</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen@megagon</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
							<email>wangchiew@fb.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meta</forename><surname>Ai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?a?atay</forename><surname>Demiralp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Megagon Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Sigma Computing</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Annotating Columns with Pre-trained Language Models</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 2022 International Conference on Management of Data (SIGMOD &apos;22)</title>
						<meeting>the 2022 International Conference on Management of Data (SIGMOD &apos;22) <address><addrLine>Philadelphia, PA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published" when="2022-06-12">2022. June 12-17, 2022</date>
						</imprint>
					</monogr>
					<note>ACM Reference Format: USA. ACM, New York, NY, USA, 15 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Inferring meta information about tables, such as column headers or relationships between columns, is an active research topic in data management as we find many tables are missing some of this information. In this paper, we study the problem of annotating table columns (i.e., predicting column types and the relationships between columns) using only information from the table itself. We develop a multi-task learning framework (called Doduo) based on pre-trained language models, which takes the entire table as input and predicts column types/relations using a single model. Experimental results show that Doduo establishes new state-of-the-art performance on two benchmarks for the column type prediction and column relation prediction tasks with up to 4.0% and 11.9% improvements, respectively. We report that Doduo can already outperform the previous state-of-the-art performance with a minimal number of tokens, only 8 tokens per column. We release a toolbox 1 and confirm the effectiveness of Doduo on a real-world data science problem through a case study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Meta information about tables, such as column types and relationships between columns (or column relations), is crucial to a variety of data management tasks, including data quality control <ref type="bibr" target="#b43">[45]</ref>, schema matching <ref type="bibr" target="#b39">[41]</ref>, and data discovery <ref type="bibr" target="#b7">[8]</ref>. Recently, there is an increasing interest in identifying semantic column types and relations <ref type="bibr" target="#b19">[21,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b65">66]</ref>. Semantic column types such as "population", "city", and "birth_date" provide contain finer-grained, richer information than standard DB types such as integer or string. Similarly, semantic column relations such as a binary relation "is_birthplace_of" connecting a "name" and a "city" column can provide valuable information for understanding semantics of the table. For example, commercial systems (e.g., Google Data Studio <ref type="bibr" target="#b16">[18]</ref> , Tableau <ref type="bibr" target="#b44">[46]</ref>) leverage such meta information for better table understanding. However, semantic column types and relations are typically missing in tables while annotating such meta information manually can be quite expensive. Thus, it is essential to build models that can automatically assign meta information to tables. <ref type="figure">Figure 2</ref> shows two tables with missing column types and column relations. The table in <ref type="figure">Figure 2</ref>(a) is about animation films and the corresponding directors/producers/release countries of the films. In the second and third columns, person names will require context, both in the same column and the other columns, to determine the correct column types. For example, George Miller 2 <ref type="figure">Figure 2</ref>: Two example tables from the WikiTable dataset. (a) The task is to predict the column type of each column based on the table values. (b) The task is to predict both column types and relationships between columns. The column types (the column relations) are depicted at the top (at the bottom) of the table. This example also shows that column types and column relations are inter-dependent and hence, our motivation to develop a unified model for predicting both tasks. appears in both columns as a director and a producer, and it is also a common name. Observing other names in the column helps better understand the semantics of the column. Furthermore, a column type is sometimes dependent on other columns of the table. Hence, by taking contextual information into account, the model can learn that the topic of the table is about (animation) films and understand that the second and third columns are less likely to be politician or athlete. To sum up, this example shows that the table context and both intra-column and inter-column context can be very useful for column type prediction. <ref type="figure">Figure 2</ref>(b) depicts a table with predicted column types and column relations. The column types person and location are helpful for predicting the relation place_of_birth. However, it will still need further information to distinguish whether the location is place_of_birth or place_of_death.</p><p>The example above shows that column type and column relation prediction tasks are intrinsically related. Thus it will be synergistic to solve the two tasks simultaneously using a single framework. To combine the synergies of column type prediction and column relation prediction tasks, we develop Doduo that: (1) learns column representations, <ref type="bibr" target="#b1">(2)</ref> incorporates table context, and (3) uniformly handles both column annotation tasks. Most importantly, our solution <ref type="bibr" target="#b3">(4)</ref> shares knowledge between the two tasks.</p><p>Doduo leverages a pre-trained Transformer-based language models (LMs) and adopts multi-task learning into the model to appropriately "transfer" shared knowledge from/to the column type/relation prediction task. The use of the pre-trained Transformerbased LM makes Doduo a data-driven representation learning system <ref type="bibr" target="#b2">3</ref> (i.e., feature engineering and/or external knowledge bases are not needed) (Challenge 1.) Pre-trained LM's contextualized representations and our table-wise serialization enable Doduo to naturally incorporate table context into the prediction (Challenge 2) and to handle different tasks using a single model (Challenge 3.) Lastly, training such a table-wise model via multi-task learning helps "transfer" shared knowledge from/to different tasks (Challenge 4.) <ref type="figure" target="#fig_0">Figure 1</ref> depicts the model architecture of Doduo. Doduo takes as input values from multiple columns of a table after serialization and predicts column types and column relations as output. Doduo considers the table context by taking the serialized column values of all columns in the same table. This way, both intra-column (i.e., co-occurrence of tokens within the same column) and inter-column (i.e., co-occurrence of tokens in different columns) information is accounted for. Doduo appends a dummy symbol [CLS] at the beginning of each column and uses the corresponding embeddings as learned column representations for the column. The output layer on top of a column embedding (i.e., <ref type="bibr">[CLS]</ref>) is used for column type prediction, whereas the output layer for the column relation prediction takes the column embeddings of each column pair.</p><p>Contributions Our contributions are:</p><p>? We develop Doduo, a unified framework for both column type prediction and column relation prediction. Doduo incorporates table context through the Transformer architecture and is trained via multi-task learning. ? Our experimental results show that Doduo establishes new stateof-the-art performance on two benchmarks, namely the Wik-iTable and VizNet datasets, with up to 4.0% and 11.9% improvements compared to TURL and Sato. ? We show that Doduo is data-efficient as it requires less training data or less input data. Doduo achieves competitive performance against previous state-of-the-art methods using less than half of the training data or only using 8 tokens per column as input. ? We release the codebase and models as a toolbox, which can be used with just a few lines of Python code. We test the performance of the toolbox on a real-world data science problem and verify the effectiveness of Doduo even on out-domain data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Existing column type prediction models enjoyed the recent advances in machine learning by formulating column type prediction as a multi-class classification task. <ref type="bibr">Hulsebos</ref>   <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">28,</ref><ref type="bibr" target="#b27">29,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b53">54]</ref>. A column relation is a semantic label between a pair of columns in a table, which offers more fine-grained information about the table. For example, a relation place_of_birth can be assigned to a pair of columns person and location to describe the relationship between them. Venetis et al. <ref type="bibr" target="#b53">[54]</ref> use an Open IE tool <ref type="bibr" target="#b61">[62]</ref> to extract triples to find relations between entities in the target columns. Mu?oz et al. <ref type="bibr" target="#b32">[34]</ref> use machine learning models to filter triple candidates created from DBPedia. Cannaviccio et al. <ref type="bibr" target="#b3">[4]</ref> use a language model-based ranking method <ref type="bibr" target="#b64">[65]</ref>, which is trained on a large-scale web corpus, to re-rank relations extracted by an open relation extraction tool <ref type="bibr" target="#b33">[35]</ref>. Cappuzo et al. <ref type="bibr" target="#b4">[5]</ref> represent table structure as a graph and then learn the embeddings from the descriptive summaries generated from the graph.</p><p>Recently, pre-trained Transformer-based Language Models (LMs) such as BERT, which were originally designed for NLP tasks, have shown success in data management tasks. Li et al. <ref type="bibr" target="#b24">[26]</ref> show that pre-trained LMs is a powerful base model for entity matching. Macdonald et al. <ref type="bibr" target="#b27">[29]</ref> proposed applications for entity relation detection. Tang et al. <ref type="bibr" target="#b48">[49]</ref> propose RPTs as a general framework for automating human-easy data preparation tasks like data cleaning, entity resolution and information extraction using pre-trained masked language models. The power of Transformer-based pre-trained LMs can be summarized into two folds. First, using a stack of Transformer blocks (i.e., self-attention layers), the model is able to generate contextualized embeddings for structured data components like table cells, columns, or rows. Second, models pre-trained on large-scale textual corpora can store "semantic knowledge" from the training text in the form of model parameters. For example, BERT might know that George Miller is a director/producer since the name frequently appears together with "directed/produced by" in the text corpus used for pre-training. In fact, recent studies have shown that pre-trained LMs store a significant amount of factual knowledge, which can be retrieved by template-based queries <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42]</ref>.</p><p>Those pre-trained models have also shown success in data management tasks on tables. TURL <ref type="bibr" target="#b12">[13]</ref> is a Transformer-based pretraining framework for table understanding tasks. Contextualized representations for tables are learned in an unsupervised way during pre-training and later applied to 6 different tasks in the finetuning phase. SeLaB <ref type="bibr" target="#b51">[52]</ref> leverages pre-trained LMs for column annotation while incorporating table context. Their approach uses fine-tuned BERT models in a two-stage manner. TaPaS <ref type="bibr" target="#b18">[20]</ref> conducts weakly supervised parsing via pre-training, and TaBERT <ref type="bibr" target="#b62">[63]</ref> pre-trains for a joint understanding of textual and tabular data for the text-to-SQL task. TUTA <ref type="bibr" target="#b57">[58]</ref> makes use of different pre-training objectives to obtain representations at token, cell, and table levels and propose a tree-based structure to describe spatial and hierarchical information in tables. TCN <ref type="bibr" target="#b56">[57]</ref> makes use of both information within the table and across external tables from similar domains to predict column type and pairwise column relations.</p><p>In this paper, we empirically compare Doduo with Sherlock <ref type="bibr" target="#b20">[22]</ref>, Sato <ref type="bibr" target="#b65">[66]</ref>, and TURL <ref type="bibr" target="#b12">[13]</ref> as baseline methods. Sherlock is a singlecolumn model while Doduo is multi-column by leveraging table context to predict column types and relations more accurately. Sato leverages topic model (LDA) features as table context while Doduo can additionally take into account fine-grained, token-level interactions among columns via its built-in self-attention mechanism. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>In this section, we formally define the two column annotation tasks: column type prediction and column relation annotation. We also provide a brief background on pre-trained language models (LMs) and how to fine-tune them for performing column annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>The goal of the column type prediction task is to classify each column to its semantic type, such as "country name", "population", and "birthday" instead of the standard column types such as string, int, or Datetime. See also <ref type="figure">Figure 2</ref> for more examples. For column relation annotation, our goal is to classify the relation of each pair of columns. In <ref type="figure">Figure 2</ref>, the relation between the "person" column and the "location" column can be "place_of_birth". As summarized in <ref type="table" target="#tab_1">Table 1</ref>, more formally, we consider a standard relational data model where a relation (i.e., table) consists of a set of attributes = ( 1 , . . . ) (i.e., columns.) We denote by val( . ) the sequence of data values stored at the column . For each value ? val( . ), we assume to be of the string type and can be split into a sequence of input tokens = [ 1 , . . . , ] to pre-trained LMs. This approach of casting cell values into text might seem restricted since tables columns can be of numeric types such as float or date. There has been extensions of the Transformer models to support numeric data <ref type="bibr" target="#b59">[60]</ref> and providing such direct support of numeric data is important future work. We also provide a brief analysis on Doduo's performance on numeric column types in Section 5.4. Problem 1 (Column type prediction). Given a table and a column in , a column type prediction model M with type vocabulary C type predicts a column type M ( , ) ? C type that best describes the semantics of . Problem 2 (Column relation prediction). Given a table and a pair of columns ( , ) in , a column relation prediction model M with relation vocabulary C rel predicts a relation M ( , , ) ? C rel that best describes the semantics of the relation between and .</p><p>In Doduo, we consider the supervised setting of multi-class classification. This means that we assume a training data set train of tables annotated with columns types and relations from two fixed vocabularies (C type , C rel ). Note that Doduo does not restrict itself to specific choices of vocabularies (C type , C rel ) which are customizable by switching the training set train . In practice, the choice of (C type , C rel ) is ideally application-dependent. For example, if the  downstream task requires integration with a Knowledge Base (KB), it is ideal to have (C type , C rel ) aligned with the KB's type/relation vocabulary. In our experiment, we evaluated Doduo on datasets annotated with (1) KB types <ref type="bibr" target="#b1">[2]</ref> and (2) DBPedia types <ref type="bibr" target="#b34">[36]</ref>.</p><p>The size and quality of the training set are also important for training high-quality column annotation models. While manually creating such datasets can be quite expensive, the datasets used in our experiments rely on heuristics that map table meta-data (e.g., header names, entity links) to type names to create training sets of large scale. See Section 5.1 for more details.</p><p>While KB can work as a training example provider, Doduo does not require the training examples to be from a single source but can combine labels from any resources such as human annotations, labeling rules, and meta-data that can be transformed into the column type/relation label format.</p><p>We also note that the learning goal of Doduo is to train column annotation models with high accuracy while being generalizable to unannotated tables (e.g., as measured by an unseen test set test ). The column type/relation prediction models of Doduo only considers the table content (i.e., cell values) as input. This setting allows Doduo to be more flexible to practical applications without replying on auxiliary information such as column names, table titles/captions, or adjacent tables typically required by existing works (See Section 2 for a comprehensive overview).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pre-trained Language Models</head><p>Pre-trained Language Models (LMs) emerge as general-purpose solutions to tackle various natural language processing (NLP) tasks. Representative LMs such as BERT <ref type="bibr" target="#b13">[14]</ref> and ERNIE <ref type="bibr" target="#b46">[47]</ref> have shown leading performance among all solutions in NLP benchmarks such as GLUE <ref type="bibr">[17,</ref><ref type="bibr" target="#b55">56]</ref>. These models are pre-trained on large text corpora such as Wikipedia pages and typically employ multi-layer Transformer blocks <ref type="bibr" target="#b52">[53]</ref> to assign more weights to informative words and less weight to stop words for processing raw texts. During pretraining, a model is trained on self-supervised language prediction tasks such as missing token prediction and next-sentence prediction. The purpose is to learn the semantic correlation of word tokens (e.g., synonyms), such that correlated tokens can be projected to similar vector representations. After pre-training, the model is able to learn the lexical meaning of the input sequence in the shallow layers and the syntactic and semantic meanings in the deeper layers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>A special component of pre-trained LMs is the attention mechanism, which embeds a word into a vector based on its context (i.e., surrounding words). The same word has different vectors if it appears in different sentences, and this is very different from other embedding mechanisms such as word2vec <ref type="bibr" target="#b30">[32]</ref>, GloVe <ref type="bibr" target="#b37">[39]</ref>, and fastText <ref type="bibr" target="#b0">[1]</ref>, which always generate the same vector for the same word in any context. Pre-trained LMs' embeddings are contextdependent and thus offer two strengths. First, it can discern polysemy. For example, the person name George Miller referring to a producer is different from the same name that refers to a director. Pre-trained LMs discern the difference and generate different vectors. Second, the embedding deals with synonyms well. For example, the words Derrick Henry and Derrick Lamar Henry Jr (respectively, (USA, US), (Oregon, OR)) are likely the same given their respective contexts. Pre-trained LMs will generate similar word vectors accordingly. Due to the two favorable strengths, pre-trained models should enable the best performance to column annotation tasks, where each cell value is succinct, and its meaning highly depends on its surrounding cells.</p><p>The pre-trained model does not know what to predict for specific tasks unless the task is exactly the same the pre-training task. Thus, a pre-trained LM needs to be fine-tuned with task-specific training data, so the model can be tailored for the task. A task-specific output layer is attached to the final layer of the pre-trained LM, and the loss value (e.g., cross-entropy loss) is back-propagated from the output layer to the pre-trained LM for a minor adjustment.</p><p>In this paper, we fine-tune the popular 12-layer BERT Base model <ref type="bibr" target="#b13">[14]</ref>. However, Doduo is independent of the choice of pretrained LMs, and Doduo can potentially perform even better with larger pre-trained LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-task learning</head><p>Multi-task learning <ref type="bibr" target="#b6">[7]</ref> is a type of supervised machine learning framework, where the objective function is calculated based on more than one task. Generally, different types of labels are used, and the labels may be or may not be annotated on the same example. The intuition and an assumption behind multi-task learning is that the tasks intrinsically share knowledge, and thus, training with the same base model benefits each other.</p><p>The major benefit of multi-task learning is that it can help improve the generalization performance of the model, especially when the training data is not sufficient. Multi-task learning can be easily applied to Deep Learning models <ref type="bibr" target="#b42">[44]</ref> by attaching different output layers to the main model, which is considered a "learned" representation encoder that converts input data to dense representations.</p><p>There are a variety of approaches for multi-task learning <ref type="bibr" target="#b42">[44]</ref>, depending on how to model and optimize shared parameters. Multitask learning models can be split into two categories based on how parameters are shared. With hard parameter sharing <ref type="bibr" target="#b5">[6]</ref>, models for multiple tasks share the same parameters, whereas soft parameter sharing <ref type="bibr" target="#b60">[61]</ref> adds constraints on distinct models for different tasks. In this paper, we consider hard parameter sharing as it is a more cost-effective approach. Among hard parameter sharing models, we choose a joint multi-task learning framework <ref type="bibr" target="#b17">[19]</ref> that uses the same base model with different output layers for different tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MODEL</head><p>In this section, we first introduce a baseline single-column model that fine-tunes a pre-trained LM on individual columns. Then, we describe Doduo's model architecture and training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single-column Model</head><p>Since LMs take token sequences (i.e., text) as input, one first has to convert a table into token sequences so that they can be meaningfully processed by LMs. A straightforward serialization strategy is to simply concatenate column values to make a sequence of tokens and feed that sequence as input to the model. That is, suppose a column has column values 1 , . . . , the serialized sequence is serialize single ( ) ::</p><formula xml:id="formula_0">= [CLS] 1 . . . [SEP],</formula><p>where [CLS] and [SEP] are special tokens used to mark the beginning and end of a sequence <ref type="bibr" target="#b3">4</ref> . For example, the first column of the first table in <ref type="figure">Figure 2</ref> is serialized as: [CLS] Happy Feet Cars Flushed Away [SEP]. This serialization converts the problem into a sequence classification task. Thus, it is straightforward to fine-tune a BERT model using training data. The column relation prediction task can be also formulated as a sequence classification task by converting a pair of columns (instead of a single column) into a token sequence in a similar manner. For this case, we also insert additional [SEP] between values of two columns to help the pre-trained LM distinguish the two columns. Namely, given two columns = 1 , . . . , and ? = ? 1 , . . . , ? , the single-column model serializes the pair as:</p><formula xml:id="formula_1">serialize single ( , ? ) ::= [CLS] 1 . . . [SEP] ? 1 . . . ? [SEP]</formula><p>. Using the above serialization scheme, we can cast the column type and relation prediction tasks as sequence classification and sequence-pair classification tasks, which can be solved by LM finetuning. However, such sequence classifications predict column types independently, even if they are in the same table. We refer to this method as the single-column model. Although the singlecolumn model can leverage the language understanding capability and knowledge learned by the LM via pre-training, it has an obvious drawback of treating columns in the same table as independent sequences. As a result, the single-column model fails to capture the table context, which is known to be important for the column annotation tasks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b65">66</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Table Serialization</head><p>In contrast to the single-column model described above, Doduo is a multi-column (or table-wise) model that takes an entire table as input. Doduo serializes data entries as follows: for each table that has columns = ( ) =1 , where each column has column values = ( ) =1 . We let</p><formula xml:id="formula_2">serialize( ) ::= [CLS] 1 1 . . . [CLS] 1 . . . [SEP]</formula><p>. For example, the first table in <ref type="figure">Figure 2</ref> is serialized as: </p><formula xml:id="formula_3">[CLS]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Contextualized Column Representations</head><p>We describe how Doduo obtains table context through contextualized column embeddings using the Transformer-architecture. Thus, an attention vector for the same symbol (e.g., George) can be different when it appears in a different context. This resolves the ambiguity issue of conventional word embedding techniques such as word2vec or GloVe.</p><p>After encoding tokens into token embeddings, a Transformer layer converts a token embedding into key (K), query (Q), and value (V) embeddings. A contextualized embedding for a token is calculated by the weighted average of value embeddings of all token embeddings, where the weights are calculated by the similarity between the query embedding and key embeddings. By having key embeddings and query embeddings separately, the model is able to calculate contextualized embeddings in an asymmetric manner. That is, the importance of Happy Feet for George Miller, which should be a key signal to disambiguate the person name, may not be necessarily equal to that of George Miller for Happy Feet. Furthermore, a Transformer-based model usually has multiple attention heads (e.g., 12 attention heads for the BERT base model.) Different attention heads have different parameters for K, Q, V calculation so that they can capture different characteristics of input data holistically. Finally, the output of a Transformer block is converted into the same dimension size as that of the input (e.g, 768 for BERT) so that the output of the previous Transformer block can be directly used as the input to the next Transformer block. The same procedure is carried out as many as the number of Transformer blocks (i.e., 12 blocks for the BERT Base model.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Column representations. Since Doduo inserts dummy [CLS]</head><p>symbols for each column, we can consider the output embeddings of the pre-trained LM for those symbols as contextualized column representations. Note that Doduo is a table-wise model, which takes the entire table as input and thus contextualized column representations take into account table context in a holistic manner. For column type prediction, Doduo attaches an additional dense layer followed by output layer with the size of |C type |. Column-pair representations. For column relation prediction, Doduo concatenates a corresponding pair of contextualized column representations as a contextualized column-pair representation. The additional dense layer should capture combinatorial information between two column-level representations. Same as the column representations, column-pair representations are also table-wise representations. In the experiment, we also tested a variant of Doduo that only takes a single column (a single column pair) as input for the column type (column relation) prediction task.</p><p>More formally, given a table , the language model LM takes the serialized sequence serialize( ) = { 1 , . . . , } of tokens as input and outputs a sequence LM( ) where each element LM( ) is a -dimensional context-aware embedding of the token . Let { 1 , . . . , } be the indices of the inserted special [CLS] tokens. Let type be the column type prediction dense layer of dimension ? |C type |. The column type model computes:</p><p>softmax( type (LM( ) )) (1) as the predicted column type of the -th column. Similarly, for column relation prediction with dense layer rel of dimension 2 ? |C rel |, the column relation model computes:</p><p>softmax( rel (LM( ) ? LM( ) )) (2) as the predicted relation between the -th and the -th column of table . The ? symbol denotes concatenation of two vectors. Doduo then feeds the predictions and the groundtruth labels into a cross entropy loss function to update the model parameters (Line 9-10, Algorithm 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Learning from Multiple Tasks</head><p>In the training phase, Doduo fine-tunes a pre-trained LM using two different training data and two different objectives. As shown in Algorithm 1, Doduo switches the task every epoch and updates the parameters for different objectives using different optimization schedulers. This design choice enables Doduo to naturally handle imbalanced training data for different tasks. Furthermore, with a single objective function and a single optimizer, we need to carefully choose hyper-parameter(s) that balance different objective terms to create a single objective function (e.g., ? = ? 1 + (1 ? )? 2 like TCN <ref type="bibr" target="#b56">[57]</ref>.) With our strategy, we can avoid adjusting the hyperparameter. Also, in Section 6, we will show that Doduo can be robustly trained with imbalanced training data.</p><p>Note that Doduo is not limited to training with just two tasks. By adding more output layers and corresponding loss functions, Doduo can be used for more than two tasks. We also note that finding relevant tasks is challenging as adding new tasks might not necessarily improve the model's performance. This can be due to the tasks not sharing enough common knowledge to improve each other, or noisy labels in training sets which can propagate among </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION 5.1 Dataset</head><p>We used two benchmark datasets for evaluation. The WikiTable dataset <ref type="bibr" target="#b12">[13]</ref> is a collection of tables collected from Wikipedia, which consists of 580,171 tables in total. The dataset provides both annotated column types and relations for training and evaluation. For column type prediction, the dataset provides 628,254 columns from 397,098 tables annotated by 255 column types. For column relations, the dataset provides 62,954 column pairs annotated with 121 relation types from 52,943 tables for training. According to <ref type="bibr" target="#b12">[13]</ref>, the type and relation labels are from FreeBase <ref type="bibr" target="#b1">[2]</ref> and are obtained by aggregating entity links attached to the original tables. For both tasks, we used the same train/valid/test splits as TURL <ref type="bibr" target="#b12">[13]</ref>. Each column/column-pair allows to have more than one annotation, and thus, the task is a multi-label classification task. The VizNet dataset <ref type="bibr" target="#b65">[66]</ref> is a collection of WebTables, which is a subset of the original VizNet corpus <ref type="bibr" target="#b19">[21]</ref>. The dataset is for the column type prediction task. The dataset has 78,733 tables, and 119,360 columns are annotated with 78 column types. The dataset constructed the columns types by mapping column headers to DBpedia types <ref type="bibr" target="#b34">[36]</ref> by a set of mapping rules. We used the same splits for the cross-validation to make the evaluation results directly comparable to <ref type="bibr" target="#b65">[66]</ref>. Each column has only one label, and thus, the task is a multi-class classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>TURL <ref type="bibr" target="#b12">[13]</ref> is a recently developed pre-trained Transformer-based LM for tables. TURL further pre-trains a pre-trained LM using table data, so the model becomes more suitable for tabular data. Since TURL relies on entity-linking and meta information such as table headers and table captions, which are not available in our scenario, we used a variant of TURL pre-trained on table values for a fair comparison. Note that to perform column type/relation annotation, we fine-tuned the pre-trained TURL model on the same training sets as for Doduo and other baselines. Sherlock <ref type="bibr" target="#b20">[22]</ref> is a single-column prediction model that uses multiple feature sets, including character embeddings, word embeddings, paragraph embeddings, and column statistics (e.g., mean, std of numerical values.) A multi-layer "sub" neural network is applied to each column-wise feature set to calculate compact dense vectors except for the column statistics feature set, which are already continuous values. The output of the subnetworks and the column statistics features are fed into the "primary" neural network that consists of two fully connected layers. Sato <ref type="bibr" target="#b65">[66]</ref> is a multi-column prediction model, which extends Sherlock by adding LDA features to capture table context and a CRF layer to incorporate column type dependency into prediction. Sato is the state-of-the-art column type prediction on the VizNet dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Settings</head><p>We used Adam optimizer with an of 10 ?8 . The initial learning rate was set to be 5 ? 10 ?5 with a linear decay scheduler with no warmup. We trained Doduo for 30 epochs and chose the checkpoint with the highest F1 score on the validation set.</p><p>Since the WikiTable dataset can have multiple labels on each column/column pair, we used Binary Cross Entropy loss to formulate as a multi-label prediction task. For the VizNet dataset, which only has a single annotation on each column, we used Cross Entropy loss to formulate as a multi-class prediction task. Models and experiments were implemented with PyTorch <ref type="bibr" target="#b36">[38]</ref> and the Transformers library <ref type="bibr" target="#b58">[59]</ref>. All experiments were conducted on an AWS p3.8xlarge instance (V100 (16GB)).</p><p>Following the previous studies <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b65">66]</ref>, we use micro F1 for the WikiTable dataset, and micro F1 and macro F1 for the VizNet dataset, as evaluation metrics. The micro F1 score is the weighted average of F1 values based on the sample size of each class, while the macro F1 score is the simple average of F1 values for all classes. Additional results and analysis can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Main Results</head><p>WikiTable <ref type="table" target="#tab_5">Table 3</ref> shows the micro F1 performance for the column type prediction and column relation prediction tasks on the WikiTable dataset. Doduo significantly outperforms the state-ofthe-art method TURL on both of the tasks with improvements of 4.0% and 0.9%, respectively.</p><p>A significant difference in the model architecture between Doduo and TURL is whether the model uses full self-attention. In TURL, the model uses the self-attention mechanism with the "cross-column" edges removed, which they referred to as visibility matrix <ref type="bibr" target="#b12">[13]</ref>. Let us use the example in <ref type="figure" target="#fig_1">Figure 3</ref>, which depicts how the contextualized embedding for the second column is calculated. TURL's visibility matrix removes the connections to [CLS] 2 from the cells "Happy Feet", "Cars", "USA", and "UK", whereas our Doduo uses the full set of connections.</p><p>Since TURL is designed for tables with meta information (e.g., table captions or column headers), we consider the major benefit of this design (i.e., the visibility matrix) to effectively incorporate descriptions in the meta information into table values. From the results, Doduo with the full self-attention performs better than TURL, which indicates that some direct intersections between tokens in different columns and different rows are useful for the column annotation problem.</p><p>We also tested Doduo with metadata, which appends the column name to column values for each column before serialization. As shown in <ref type="table" target="#tab_5">Table 3</ref>, by using column names, Doduo slightly improves the performance and performs competitively against TURL with metadata. This indicates that TURL relies on metadata and Doduo performs better and more robustly than TURL when metadata is not available.</p><p>VizNet <ref type="table" target="#tab_6">Table 4</ref> shows the results on the VizNet dataset. Note that Doduo is trained only using the column prediction task for the VizNet dataset, as column relation labels are not available for the dataset. The results show that Doduo outperforms Sherlock and Sato, the SoTA method for the dataset, by a large margin and establishes new state-of-the-art performance with micro F1 (macro F1) improvements of 11.9% (6.7%.) As described in Section 2, Sato is a multi-column model that incorporates table context by using LDA features. Different from the LDA features that provide multi-dimensional vector representations for the entire table, the Transformer-based architecture enables Doduo to capture more fine-grained inter-token relationships through the self-attention mechanism. Furthermore, Doduo's table-wise design naturally helps incorporate inter-column information into the model. Performance on numeric columns. As mentioned in Section 3, Doduo casts all cell values as strings thus it may be weak in handling numeric columns. <ref type="table" target="#tab_7">Table 5</ref> shows Doduo performance on the top-15 most numeric column types from the VizNet dataset. Doduo did have low performance on some numeric types such as ranking (33.21%) and capacity (62.55%). However, on these 15 types, Doduo achieves an average F1 score of 86.9% which is comparable to the overall macro F1 (84.6%) and even slightly better. This can be due to the Transformer model being able to recognize digit patterns in the numeric values to predict the correct types. This results aligns with the findings from the NLP literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b54">55]</ref> that Transformer models can partially handle numeric data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ANALYSIS 6.1 Ablation Analysis</head><p>To verify the effectiveness of multi-task learning and multi-column architecture, we tested variants of Doduo. Dosolo is a Doduo model without multi-task learning. Thus, we trained Doduo models only using training data for the target task (i.e., column type prediction or column relation prediction.) Dosolo SCol is a single column model that only uses column values of the target column  (or target column pair for column relation prediction.) Dosolo SCol is also trained without multi-task learning. <ref type="table" target="#tab_8">Table 6</ref> shows the results of the ablation study. For both of the tasks, Dosolo degraded the performance compared to the multitask learning of Doduo. As Dosolo SCol shows significantly lower performance than the others, the results also confirm that the multicolumn architecture of Doduo successfully captures table context to improve the performance on the column type prediction and column relation prediction tasks.</p><p>The same analysis with Dosolo SCol on the VizNet dataset is shown in <ref type="table" target="#tab_9">Table 7</ref>. As expected, the multi-column model (Doduo) performs significantly better than the single-column model (Dosolo SCol .) The results further confirm the strengths of the multi-column model. We would like to emphasize that Dosolo SCol outperforms Sato, which incorporates table context as LDA features.</p><p>The pre-trained LM (e.g., BERT) is sensitive to the input sequence order, which may not reflect the property of tables that rows/columns are order-invariant. To verify if Doduo has this limitation, we trained and evaluated Doduo on two versions of the WikiTable dataset, where the input table's rows (columns) were randomly shuffled. As shown in <ref type="table" target="#tab_8">Table 6</ref>, somewhat surprisingly, Doduo shows only subtle degradation for shuffled rows and no substantial difference for shuffled columns. We conjecture that Doduo successfully tailors the original position embeddings to be aligned with the table structure during the fine-tuning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Data Efficiency</head><p>Learning Efficiency Pre-trained LMs are known for its capability of training high-quality models using a relatively small number of labeled examples. Furthermore, multi-task learning (i.e., training with multiple tasks simultaneously) should further stabilize the performance with fewer training data for each task. To verify the effectiveness of Doduo and Dosolo with respect to label efficiency, we compared the Doduo models trained with different training data sizes (10%, 25%, 50%, and 100%) and evaluated the performance on the column type prediction and column relation prediction tasks.</p><p>As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, Doduo consistently outperforms Dosolo and achieves higher than 0.9 F1 scores on both tasks even when trained with half of the training data. In particular, with only 50% or fewer labeled examples, Doduo outperforms the SoTA method TURL on column-type prediction and achieves comparable performance on column relation prediction. Input Data Efficiency A major challenge of applying pre-trained LMs to data management tasks is their limits of the maximum input sequence length. For example, LMs like BERT can only take at most   Thus, we evaluated different variants of Doduo with shorter input token length to discuss the input data efficiency of Doduo. We would like to emphasize that any of the recent studies applying pre-trained Transformer-based LMs to data management tasks (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b24">26,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b56">57]</ref>) did not conduct this kind of analysis. Thus, it is still not clear how many tokens should we feed to the model to obtain reasonable task performance. <ref type="table" target="#tab_11">Table 8</ref> shows the results of Doduo with different max token sizes on the WikiTable dataset. We simply truncated column values if the number of tokens exceeded the threshold. As shown in the table, the more tokens used, the better performance Doduocan achieve, as expected. However, somewhat surprisingly, Doduo already outperforms TURL using just 8 tokens per column for column type prediction (TURL has micro F1 of 88.86 on WikiTable). For the column relation prediction task, Doduo needs to use more tokens to outperform TURL (i.e., 32 tokens to beat TURL's score of 90.94.) This is understandable, as column relation prediction is more contextual than column type prediction, and thus it requires more signals to further narrow down to the correct prediction. For the VizNet dataset, we confirm that Doduo with 8 max tokens per column with Doduo (92.5 F1) outperforms the state-of-theart method (i.e., Sato) (88.4 F1) on the task. <ref type="table" target="#tab_11">Table 8</ref> reports how many columns each variant can support under the maximum token configuration. The average numbers of columns in Web Tables, Enterprise Data, and Open Data are reported to be 4, 12, and 16, respectively <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">33]</ref>. Thus, we confirm that Doduo has a nice input data efficiency property, so it can be also used for "wide" tables.</p><p>We note that 16 columns may not be sufficient for tables outside of WebTables. For such cases, a reasonable option is to first split the wide table into clusters of relevant columns (maybe by some user-defined rules), then apply Doduo on each cluster. In this case, Doduo still has the advantage of leveraging partial context of the input table to improve prediction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CASE STUDY: CLUSTERING COLUMNS</head><p>We apply Doduo to a real data science application scenario of clustering relevant columns. On a daily basis, data scientists collect information from multiple sources and incorporate data from various tables. Therefore, as a first step of data exploration and data integration <ref type="bibr" target="#b35">[37]</ref>, it is essential to know which columns are semantically similar. However, this is not always straightforward as different column names can be assigned to semantically similar columns. In this section, we demonstrate the usefulness of Doduo's contextualized column embeddings with a case study of clustering semantically similar columns on an enterprise database.</p><p>For this case study, we use an in-production enterprise database from the HR domain. Different teams create and update tables about job seekers and companies that are hiring, etc. Despite some company-wide naming conventions, we observe that semantically similar columns are sometimes given different column names across tables. Some tables have additional meta information describing the meaning of each column, which helps to understand the similarity of columns. However, the meta information is missing for many columns and is not always reliable as they are worded differently by different teams. Next, we simulate a workflow of a data scientist performing analysis related to job search and review of companies.</p><p>Scenario: Our data scientist Sofia starts by filtering tables with keywords "jobsearch" and "review" and gets 10 tables with 50 columns in total (29 columns of type "string" and 21 columns of type "integer".) To group similar columns together, she can simply create contextualized column embedding using the Doduo toolbox for all the columns and then apply her favorite clustering algorithm to form the final groups.</p><p>To evaluate Sofia's column groups, we generate an initial cluster using both the column names and descriptions and manually refine the results to form the ground-truth 5 as shown below: date, IP address, job title, timestamp (unixtime), timestamp (hhmm), counts, status, file path, browser, location, search term, rating, company ID, review ID, user ID Sofia use the Doduo model trained on the WikiTable dataset to obtain contextualized column embeddings for each column (Doduo+ column value emb.)</p><p>We compared the method with three baselines and two traditional schema matching approaches. We directly used Doduo's column type predictions as the clustering criteria where columns with the same predicted types got assigned into the same cluster (Doduo+predicted type). We tested fastText <ref type="bibr" target="#b0">[1]</ref> to verify how non-contextualized column embeddings perform for the task. We use column value embeddings (fastText+column value emb) and column name embeddings (fastText+column name emb) with fast-Text. We choose fastText as a baseline as it offers a widely used off-the-shelf toolbox, which is a "go-to" option for data scientists. To achieve a fair evaluation of the embedding quality, we use the same k-means clustering algorithm for all models. In addition, we compared with more traditional schema matching approaches tested in the experiment suite Valentine <ref type="bibr" target="#b23">[25]</ref>. We picked the two most effective approaches from Valentine's empirical study: COMA <ref type="bibr" target="#b14">[15]</ref> and DistributionBased <ref type="bibr" target="#b66">[67]</ref>. To generate the clustering label for comparison, we went over all possible pairs of tables and connected matched columns to assign the same cluster labels. Since those schema matching methods take two tables as input and return pairs of matched columns, we regard returned pairs as connected nodes in a graph and merge them into connected components to obtain column clusters. We use Homogeneity (Precision), Completeness (Recall), V-Measure (F1) to evaluate the quality of clusters using the ground-truth cluster assignment. As shown in <ref type="table" target="#tab_12">Table 9</ref>, Doduo's column embeddings show the best clustering performance with respect to Precision and F1. The results confirm that contextualized column embeddings are more useful than predicted column types and can be more accurate representations than column name/value embeddings created by fastText. Compared to Doduo, fastText tends to generate similar embeddings even for semantically different columns, which leads to creating unnecessarily large clusters that contain many irrelevant columns. This significantly increases (decreases) fastText methods' Recall (Precision) values in <ref type="table" target="#tab_12">Table 9</ref>. COMA shows reasonably good performance and DistributionBased falls short on precision, given both column name and content. Doduo outperforms both matchingbased approaches using the contextualized embedding. Note that both the column names and column descriptions are not given to the Doduo model as input, and the model was trained on the WikiTable dataset (i.e., different domain.) Thus, this case study also indicates the transferability of a Doduo model trained on one domain (i.e., Wikipedia tables) to another domain (i.e., enterprise database.) so that data scientists can apply the Doduo model in the toolbox for their own needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we present Doduo, a unified column annotation framework based on pre-trained Transformer language models and Multi-task learning. Experiments on two benchmark datasets show that Doduo achieves new state-of-the-performance. With a series of analyses, we confirm that the improvements are benefited from the multi-task learning framework. Through the analysis, we also confirm that Doduo is data-efficient, as it can achieve competitive performance as the previous state-of-the-art methods only using 8 tokens per column or about 50% of training data. We conduct a case study and verify the effectiveness of Doduo on a real-world data science problem. We believe our toolbox will further help researcher/data scientists easily apply the state-of-theart column annotation model to a wide variety of data science and data management problems. <ref type="figure" target="#fig_4">Figure 5</ref> shows the F1 score of Doduo and Sato for each class on the VizNet (Full) and VizNet (Multi-column only) datasets. The improvements against Sato on the two variants of the VizNet datasets indicate that Doduo robustly and consistently performs better than Sato, especially for single-column tables, where Sato cannot benefit from the table features and CRF. We found that Sato shows zero or very poor F1 values for religion, education, organisation. The labeled columns in the training data in the 1st fold of the VizNet (Full) are only 24, 22, and 14, respectively. Sato should suffer from the lack of training examples for those column types, and probably the skewed column type distribution as well. We show that Doduo robustly performs well on such column types. <ref type="table" target="#tab_1">Table 10</ref> shows the performance of Doduo and Dosolo on the WikiTable dataset for 6 column types/column relations. We confirm that Doduo tends to perform better for the column types/relations that are less clearly distinguishable (e.g., artist vs. writer, place-ofbirth vs. place-lived.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ADDITIONAL ANALYSIS A.1 Detailed Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Input Token Length</head><p>For the VizNet dataset, we tested Doduo and Dosolo SCol with different maximum token numbers per column. <ref type="table" target="#tab_1">Table 11</ref> shows the similar trends as the results on the WikiTable dataset. Doduo with 8 max tokens per column with Doduo outperforms the state-ofthe-art method (i.e., Sato) on the task. As we observe significant differences between the multi-column model (Doduo) and the singlecolumn model (Dosolo SCol ), we consider it is mainly because the Transformer blocks (i.e., self-attention mechanisms) capture the inter-column table context successfully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Learning Efficiency A.4 Inter-column Dependency</head><p>A strength of the Transformer architecture is stacked Transformer blocks that calculate highly contextual information through the selfattention mechanism. As described in Section 4, Doduo uses the [CLS] dummy symbols to explicitly obtain contextualized column representations. The representations not only take into account table context but also explicitly incorporate the inter-column dependency. That is, as we showed in <ref type="figure">Figure 2</ref>, predictions for some columns should be relevant and useful for other columns in the table. To the best of our knowledge, none of the existing work that applies pre-trained LMs to tables has conducted this type of analysis for better understanding how the Transformer-based model captures the semantics of tables.</p><p>Thus, we conduct attention analysis to further understand how the attention mechanisms of Doduo (i.e., the pre-trained LM) captures the inter-column dependency and the semantic similarity between them. Following the literature of attention analysis in NLP <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">43,</ref><ref type="bibr" target="#b50">51]</ref>, we look into attention weights for the analysis. It is known that in pre-trained Transformer-based LMs, the deep layer focuses on semantic similarity between tokens <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51]</ref>. Therefore, to investigate the high-level (semantic) similarity between columns, we looked into the attention weights of the last Transformer block.</p><p>We used the VizNet dataset (Multi-column only) for the analysis. Specifically, we focus on attention weights between [CLS] tokens (i.e., column representations.) Since Transformer-based LMs usually have multiple attention heads (e.g., 12 heads in the BERT Base model,) we aggregate attention weights of all attention heads. As a result, we obtain an ? matrix, where denotes the input sequence length. We disregard aggregated attention weights other than those for [CLS] tokens. After masking out any attention weights other than [CLS] tokens, we averaged the matrices obtained from all tables in the dataset so that we can create a single |C type | ? |C type | matrix that represents the dependency between column types. This gives us aggregated information about the dependency between column types.</p><p>Each element ( , ) in the final matrix represents how much the column type relies on the other column type for its contextualized representation. Note that the dependency of column type (or ) for column type (or ) can be different, and thus the matrix is not symmetric. For example, age highly relies on the type origin, whereas the opposite direction has negative attention weight showing a low degree of dependency. To eliminate the influence of the co-occurrence of column types, we counted the co-occurrence of column types in the same table and normalized the matrix to make the reference point to be zero for more straightforward interpretation. As a result, the final matrix consists of relative importance scores, and higher/lower values mean more/less influence from the column type. <ref type="figure">Figure 6</ref> depicts the final matrix in heatmap visualization. Higher values (colored in red) indicate stronger dependency of column types (in -axis) against other column types (in -axis). For example, gender (age) has a higher value against country (origin.) We can interpret that majority of information, which composes the contextualized column representations for gender columns, is derived from origin. On the other hand, the gender column seems not to be important for the origin column. The results confirm that Doduo learns the inter-column dependency through the selfattention mechanism and the learned semantic similarity values between different pairs of column types have different weights, which the co-occurrence cannot simply explain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Language Model Probing</head><p>Recent applications of pre-trained LMs to data management tasks including entity matching <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">26]</ref> and column annotation tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b56">57]</ref> have shown great success by improving previous SoTA performance by a large margin. However, little is known about how well pretrained LMs inherently know about the problem in the first place. Pre-trained LMs are trained on large-scale textual corpora, and the pre-training process helps the pre-trained LM to memorize and generalize the knowledge stored in the pre-training corpus.</p><p>There is a line of recent work that aims to investigate how well pre-trained LMs know about factual knowledge <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42]</ref>. The studies have shown that pre-trained LMs store a significant amount of factual knowledge through pre-training on large-scale corpora. Therefore, we hypothesized that Doduo's performance was partly boosted by the knowledge obtained from the pre-training corpus that might store knowledge relevant to the task. To verify the hypothesis, we evaluated if the BERT model, which we used as the base model for the experiments, stored relevant knowledge for the column annotation problem.    <ref type="figure">Figure 6</ref>: Inter-column dependency based on attention analysis on the VizNet dataset. A higher value (red) indicates that the column type ( -axis) "relies on" the other column type ( -axis) for prediction. Each row denotes the degree of "dependence" against each column. For example, age in -axis has a high attention weight against origin in -axis, indicating that predicting age relies on signals from the origin column. In the analysis, following the line of work <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b38">40,</ref><ref type="bibr" target="#b40">42]</ref>, we use the template-based approach to test if a pre-trained LM knows the factual knowledge. Specifically, we use a template that has a blank field for the column type like below:</p><p>Judy Morris is _____.</p><p>In this example, "director" should be a better fit than other column types (e.g., "actor", "player", etc.) In this way, we conclude that the model knows the fact if the model judges the template with the true column type (e.g., "director") more likely than other sentences that use different column types (e.g., "actor", "player", etc.). To evaluate the likelihood of a sequence after filling the blank in the template, we use the perplexity score of a sequence using the pre-trained LM. Perplexity is used to measure how well the LM can predict each token from the context (i.e., the other tokens in the sequence 6 ) and is calculated by the average likelihood of a sequence. It is a common metric to evaluate how "natural" the text is from the LM perspective. The perplexity of a sequence of tokens = ( 1 , 2 , . . . , ) is defined by:</p><formula xml:id="formula_4">( ) = exp ? 1 ?? log ( | \ ) ,<label>(3)</label></formula><p>where ( | \ ) denotes the probability of an LM (e.g., BERT) predicting a target token given the context in with masked out. With the same LM, the lower perplexity score for a sentence indicates it is easier for the LM to generate the sentence. We use the perplexity to score column types for each column value (e.g., Judy Morris) by filling each column type name in the template. Then, we can evaluate if the ground truth label (i.e., "director" in this case) has the best (i.e., lowest) perplexity among all candidates. For the analysis, we use the vanilla BERT (bert-base-uncased) model, which is the same base model used for Doduo in the experiments. We use the average rank and the normalized PPL (= PPL / Avg. PPL, where Avg. PPL denotes the average perplexity of all column types for evaluation.) Since perplexity values for sequences with different lengths are not directly comparable, we selected column types that are tokenized into a single token by the BERT tokenizer <ref type="bibr" target="#b6">7</ref> . As a result, 80 (out of 255) and 75 (out of 78) column types were selected for the WikiTable and VizNet datasets for the analysis, respectively.</p><p>We can use the same framework for the column relation prediction task as well. In this case, we consider a different template that has a blank field for the column relation. For example, Derrick Henry _____ Yulee, Florida. In this example, the likelihood of a sentence with "was born in" (place_of_birth) should be judged higher than that with "was died in" (place_of_death), which requires factual knowledge about the person. Since the column relation types are not written in plain language, we manually converted column relation type names so that they better fit in the template (entity 1, relation, entity 2). Examples of converted column relation names are (place_of_birth, "was born in"), (directed_by, "is directed by"). We filtered 34 (out of 121) column relation types to make sure the converted relation type names have the exact same number of tokens.</p><p>As a more direct way to test the hypothesis, we also evaluated a variant of Doduo that randomly initialized model parameters <ref type="bibr" target="#b5">6</ref> It would be the next token from the previous tokens if the model were an autoregressive model that only considers backward dependency in each step (e.g., GPT-2.) Since BERT has bi-directional connections between any tokens in the input sequence, the perplexity should take into account any other tokens in the input sequence to evaluate the likelihood of the target token. <ref type="bibr" target="#b6">7</ref> Technically, column type names in the WikiTable contain hierarchical information, which is represented by URI. We used the leaf node as the column type name. instead of using the pre-trained parameters of BERT. In this way, we can test the performance when the model with the identical architecture is trained from scratch only using training data of the target task. The model did not show meaningful performance (i.e., approximately zero F1 value.) We consider this is mainly because the model is too large to be trained on only the training data (i.e., without pre-training.) Thus, we decided to use the "language model probing" method to test the hypothesis. Results. <ref type="table" target="#tab_1">Table 12</ref> shows the results on the WikiTable dataset. We observe that some column types (e.g., goverment.election, geography.river) show lower average rank and PPL / Avg. PPL (i.e., the BERT model knows about the facts), whereas some column types (e.g., biology.organism, royalty.kingdom) show poor performance on the language probing analysis. For example, the "government.election" column type is ranked at 6.74 on average and shows a smaller PPL than the average PPL. That means values in the columns that have the "government.election" ground-truth labels are considered "more natural" to appear with the term "election" 8 than other column type names by the pre-trained LM (i.e., BERT.) As we used 80 column types for the analysis, the "royalty.kingdom" column type is almost always ranked at the bottom by the LM. The poor performance could be attributed to the lower frequency of the term "kingdom" than other terms in the pre-training corpus.</p><p>For the column relations on the WikiTable dataset, the results in <ref type="table" target="#tab_1">Table 12</ref> (Right) indicate that the LM knows about factual knowledge of persons as the probing performance for relations such as place_of_birth and position is higher. Compared to the probing results for the column types, the results show less significant differences between top-5 and bottom column relations. This is mainly because the template has three blank fields for two entities and one relation, which has a higher chance to create an unnatural sentence for the LM than that for column types.</p><p>The probing analysis on the VizNet dataset shows the same trend as in the WikiTable dataset. In <ref type="figure" target="#fig_4">Figure 5</ref>, we confirm that Doduo has better performance than Sato for all the top-5 column types. Meanwhile, birthPlace and nationality, which are in the bottom-5 column types for the language model probing analysis, are among the few column types where Doduo underperforms Sato. The results support that Doduo may not benefit from relevant factual knowledge stored in the pre-trained LM for the column type.</p><p>Note that the BERT model used for the analysis is not fine-tuned on the WikiTable/VizNet dataset, but the vanilla BERT model. Thus, the language model probing analysis shows the inherent ability of the BERT model, and we confirm that the pre-trained LM does store factual knowledge that is useful for the column annotation problem. This especially explains the significant improvements over the previous SoTA method that does not use pre-trained LM (i.e., Sato), as shown in <ref type="figure" target="#fig_4">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B LIMITATIONS AND FUTURE WORK</head><p>We have discussed why Doduo performs well for the column annotation problem through the series of analysis. In this section, we summarize the limitations of Doduo and our findings in the paper to discuss open questions for future work. The average rank becomes 1 if the language model always judges the column type (column relation) as the most "natural" choice among 80 (34) candidates for the target column value (the target column value pair.) We consider the language model has more prior knowledge about the column types (column relations) in Top-5 than those in Bottom-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Column type</head><p>Avg. rank (?) PPL / Avg.PPL (?)    <ref type="bibr" target="#b56">[57]</ref> has developed a framework that incorporates signals from external tables for better column annotation performance. Thus, we consider joint modeling of multiple tables should be a future direction. Clean data vs. dirty data. Third, our framework assumes that table values are "correct and clean", which may not always be true in real-world settings. The input table value should be of the high-quality, especially when we limit the max input token size for better efficiency. Recent studies that applied pre-trained LMs to tables <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b25">27]</ref> have shown that the pre-trained LM-based approach achieves robust improvements even on "dirty" datasets, where some table values are missing or misplaced. Following the error detection/correction research <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31]</ref>, which has been studied independently, implementing functionality that alleviates the influence from the incorrect table values is part of the future work.</p><p>Multi-task learning with more tasks. Lastly and most importantly, there are many open questions in applying multi-task learning to data management tasks. Although we have shown that multitask learning is useful for the column annotation task, it is not yet very clear what types of relevant tasks are useful for the target task. A line of work in Machine Learning has studies on the task similarity and the transferability of the model <ref type="bibr" target="#b63">[64]</ref>. Therefore, it is also important for us to understand the relationship between task similarity and benefits of the multi-task learning framework. We acknowledge that our study is still preliminary with respect to this point. However, we believe that our study established the first step in this research topic toward a wider scope of multi-task learning applicability to other data management tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of Doduo's model architecture. Doduo serializes the entire table into a sequence of tokens to make it compatible with the Transformer-based architecture. To handle the column type prediction and column relation extraction tasks, Doduo implements two different output layers on top of column representations and a pair of column representations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>How Doduo computes contextualized column embeddings using the Transformer layers. Each Transformer block calculates an embedding vector for every token based on surrounding tokens.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3</head><label>3</label><figDesc>depicts how each Transformer block of the Doduo aggregates contextual information from all columns values (including dummy [CLS] symbols and themselves) in the same table. Specifically, this example illustrates the first Transformer layer calculates the attention vector by aggregating embeddings of other tokens based on the similarity against the second column's [CLS] token.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Performance improvements over increasing the training data size on the WikiTable dataset. The dashed lines in the plots denote the state-of-the-art methods (TURL.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Class F1 values by Doduo and Sato on the VizNet dataset (Above: Full set; Below: Multi-column only.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>line of work focuses on column relations between pairs of columns in the sametable for better understanding tables</figDesc><table><row><cell>et al. [22] developed</cell></row><row><cell>a deep learning model called Sherlock, which applies neural net-</cell></row><row><cell>works on multiple feature sets such as word embeddings, character</cell></row><row><cell>embeddings, and global statistics extracted from individual column</cell></row><row><cell>values. Zhang et al. [66] developed Sato, which extends Sherlock</cell></row><row><cell>by incorporating table context and structured output prediction</cell></row><row><cell>to better model the nature of the correlation between columns</cell></row><row><cell>in the same table. Other models such as ColNet [9], HNN [10],</cell></row><row><cell>Meimei [48], 2 [24] use external Knowledge Bases (KBs) on top</cell></row><row><cell>of machine learning models to improve column type prediction.</cell></row></table><note>Those techniques have shown success on column type prediction tasks, outperforming classical machine learning models. While those techniques identify the semantic types of individual columns, another</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Notations. Description = ( 1 , 2 , . . . , )Columns in a table. = ( 1 , 2 , . . . , )</figDesc><table><row><cell></cell><cell cols="2">Symbol</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Column values.</cell></row><row><cell>= (</cell><cell>,1 ,</cell><cell>,2 , . . . ,</cell><cell cols="2">, )</cell><cell>A single column value.</cell></row><row><cell></cell><cell></cell><cell>( ) type ,</cell><cell>( ) rel</cell><cell>=1</cell><cell>Training data</cell></row></table><note>train = ( ) ,type = ( 1 , 2 , . . . , ),* ? C type Column type labels.rel = ( 1,2 , 1,3 , . . . , 1, ), * , * ? C rel Column relation labels.TURL is also a Transformer-based model like Doduo but it requires additional meta table information such as table headers for pre- training. Doduo is more generic as it predicts column types and relations only relying on cell values in the table. See Section 5 for a more detailed comparison.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Happy Feet ... George ..</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>E'1,[CLS]</cell><cell>E'1,Val 1</cell><cell>E'1,Val 2</cell><cell>...</cell><cell>E'2,[CLS]</cell><cell>E'2,Val 3</cell><cell>...</cell><cell>E'3,[CLS]</cell><cell>E'3,Val 5</cell><cell>...</cell><cell>E'3,[SEP]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Transformer layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>...</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>USA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Transformer layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Transformer layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cars</cell><cell>Darla ...</cell><cell>UK</cell><cell>E1,[CLS]</cell><cell>E1,Val 1</cell><cell>E1,Val 2</cell><cell>...</cell><cell>E2,[CLS]</cell><cell>E2,Val 3</cell><cell>...</cell><cell>E3,[CLS]</cell><cell>E3,Val 5</cell><cell>...</cell><cell>E3,[SEP]</cell></row><row><cell></cell><cell></cell><cell></cell><cell>[CLS]</cell><cell>Val 1</cell><cell>Val 2</cell><cell>...</cell><cell>[CLS]</cell><cell>Val 3</cell><cell>...</cell><cell>[CLS]</cell><cell>Val 5</cell><cell>...</cell><cell>[SEP]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Happy Feet, . . . [CLS] George Miller, . . . [CLS] USA, . . . , France [SEP]. Different from the single-column model, which always has a single [CLS] token in the input, Doduo's serialization method inserts as many [CLS] tokens as the number of columns in the input table. This difference makes a change in the classification formulation. While the single-column model classifies a single sequence (i.e., a single column) by predicting a single label, Doduo predicts as many labels as the number of [CLS] tokens in the input sequence. The 4 Note that [CLS] and [SEP] are the special tokens for BERT and other LMs may have other special tokens, which are usually implemented as part of their tokenizers. learning procedure of Doduo starts by serializing and tokenizing all tables in the datasets (Line 3-4 of Algorithm 1.) Algorithm 1: Training procedure of Doduo Input: Number of training epochs Epoch ; For each task ? [1, ], training set , loss function L , and optimizer Output: Annotation model M = (LM, { 1 , . . . , }) with a language model LM and heads 1 to // Initialize model weights 1 Initialize LM using its pre-trained weights; 2 Initialize { 1 , . . . , } randomly; // Serialize all tables in each training set 3 for = 1 to do Randomly split into mini-batches { 1 , . . . }; 8 for in { 1 , . . . } do // Evaluate the L loss func on the head</figDesc><table><row><cell>4</cell><cell>? {serialize( ) for ?</cell><cell>};</cell></row><row><cell>9</cell><cell cols="2">? L ( , LM ? );</cell></row><row><cell></cell><cell cols="2">// Back-propagate to update model weights</cell></row><row><cell>10</cell><cell cols="2">M ? back-propagate( M, , / (LM ? ));</cell></row><row><cell cols="2">11 return M;</cell><cell></cell></row></table><note>5 for ep = 1 to Epoch do6 for = 1 to do7</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Dataset description.</figDesc><table><row><cell>Name</cell><cell># tables</cell><cell># col</cell><cell cols="2"># col types # col rels</cell></row><row><cell>WikiTable</cell><cell cols="2">580,171 3,230,757</cell><cell>255</cell><cell>121</cell></row><row><cell>VizNet</cell><cell>78,733</cell><cell>119,360</cell><cell>78</cell><cell>-</cell></row><row><cell cols="5">tasks. Finding more relevant tasks and testing Doduo on them are</cell></row><row><cell cols="2">part of our future work.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Performance on the WikiTable dataset.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Col type</cell><cell></cell><cell></cell><cell>Col rel</cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Sherlock</cell><cell>88.40</cell><cell>70.55</cell><cell>78.47</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TURL</cell><cell>90.54</cell><cell>87.23</cell><cell>88.86</cell><cell>91.18</cell><cell>90.69</cell><cell>90.94</cell></row><row><cell>Doduo</cell><cell cols="6">92.69 92.21 92.45 91.97 91.47 91.72</cell></row><row><cell>TURL+metadata</cell><cell>92.75</cell><cell>92.63</cell><cell>92.69</cell><cell>92.90</cell><cell>93.80</cell><cell>93.35</cell></row><row><cell>Doduo+metadata</cell><cell>93.25</cell><cell>92.34</cell><cell>92.79</cell><cell>91.20</cell><cell>94.50</cell><cell>92.82</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Performance on the VizNet dataset.</figDesc><table><row><cell></cell><cell>Full</cell><cell></cell><cell cols="2">Multi-column only</cell></row><row><cell>Method</cell><cell cols="4">Macro F1 Micro F1 Macro F1 Micro F1</cell></row><row><cell>Sherlock</cell><cell>69.2</cell><cell>86.7</cell><cell>64.2</cell><cell>87.9</cell></row><row><cell>Sato</cell><cell>75.6</cell><cell>88.4</cell><cell>73.5</cell><cell>92.5</cell></row><row><cell>Doduo</cell><cell>84.6</cell><cell>94.3</cell><cell>83.8</cell><cell>96.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :int, float, date).</head><label>5</label><figDesc>Doduo's column type prediction performance on the 15 most numeric types of the VizNet dataset. We use %num to measure how many cell values of a type can be cast as a numeric type (e.g.,</figDesc><table><row><cell cols="2">type %num</cell><cell>F1</cell><cell>type</cell><cell>%num</cell><cell>F1</cell><cell>type</cell><cell>%num</cell><cell>F1</cell></row><row><cell cols="3">plays 100.00 88.55</cell><cell>fileSize</cell><cell cols="3">87.84 88.23 grades</cell><cell cols="2">67.18 97.68</cell></row><row><cell>rank</cell><cell cols="6">93.01 94.52 elevation 87.39 92.14 weight</cell><cell cols="2">60.41 97.59</cell></row><row><cell cols="4">depth 92.86 88.45 ranking</cell><cell cols="2">86.88 33.21</cell><cell>isbn</cell><cell cols="2">43.77 96.51</cell></row><row><cell>sales</cell><cell cols="2">92.05 75.13</cell><cell>age</cell><cell cols="5">81.04 98.53 capacity 42.06 62.55</cell></row><row><cell>year</cell><cell cols="5">91.47 98.94 birthDate 67.85 95.64</cell><cell>code</cell><cell cols="2">35.93 95.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on the WikiTable dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Type prediction Relation prediction</cell></row><row><cell>Doduo</cell><cell>92.50</cell><cell>91.90</cell></row><row><cell>w/ shuffled rows</cell><cell>91.94</cell><cell>91.61</cell></row><row><cell>w/ shuffled cols</cell><cell>92.68</cell><cell>91.98</cell></row><row><cell>Dosolo</cell><cell>91.37 (1.23% ?)</cell><cell>91.24 (0.7% ?)</cell></row><row><cell>Dosolo SCol</cell><cell>82.45 (21.9% ?)</cell><cell>83.08 (9.6% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Ablation study on the VizNet dataset (Full.)</figDesc><table><row><cell></cell><cell>Macro F1</cell><cell>Micro F1</cell></row><row><cell>Doduo</cell><cell>84.6</cell><cell>94.3</cell></row><row><cell>Dosolo SCol</cell><cell cols="2">77.4 (8.5% ?) 90.2 (4.3% ?)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Comparisons of Doduo with different input token size on the WikiTable dataset.</figDesc><table><row><cell cols="3">MaxToken/col Col type (F1) Col rel (F1)</cell><cell>Max. # of cols</cell></row><row><cell>8</cell><cell>89.8</cell><cell>88.9</cell><cell>56</cell></row><row><cell>16</cell><cell>91.4</cell><cell>90.7</cell><cell>30</cell></row><row><cell>32</cell><cell>92.4</cell><cell>91.7</cell><cell>15</cell></row></table><note>512 tokens so ingesting a full wide table may not be feasible for the LMs. Doduo (or the multi-column model in general) has the advantage that it is input data efficient, meaning that it can make table-wise predictions accurately by only taking a small number of samples of each column. This makes Doduo more attractive in practice as it can handle large tables with many columns.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Case study results.</figDesc><table><row><cell>Method</cell><cell>Prec.</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>Doduo+column value emb</cell><cell>68.19</cell><cell>70.40</cell><cell>69.28</cell></row><row><cell>Doduo+predicted type</cell><cell>44.87</cell><cell>61.32</cell><cell>51.82</cell></row><row><cell>fastText+column value emb</cell><cell>35.90</cell><cell>76.61</cell><cell>48.89</cell></row><row><cell>fastText+column name emb</cell><cell>56.62</cell><cell>74.68</cell><cell>64.40</cell></row><row><cell>COMA (with column name)</cell><cell>58.47</cell><cell>66.06</cell><cell>62.03</cell></row><row><cell>DistributionBased (with column name)</cell><cell>23.87</cell><cell>69.51</cell><cell>35.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Further analysis on column type prediction (left) and on column relation prediction (right.)</figDesc><table><row><cell>Column type</cell><cell cols="2">Doduo (F1) Dosolo (F1)</cell><cell>Column relation</cell><cell cols="2">Doduo (F1) Dosolo (F1)</cell></row><row><cell>music.artist</cell><cell>84.03</cell><cell>81.87</cell><cell>film.film.production_companies</cell><cell>80.95</cell><cell>74.29</cell></row><row><cell>music.genre</cell><cell>93.33</cell><cell>87.50</cell><cell>film.film.produced_by</cell><cell>43.90</cell><cell>38.89</cell></row><row><cell>music.writer</cell><cell>75.00</cell><cell>40.00</cell><cell>film.film.story_by</cell><cell>100.00</cell><cell>90.91</cell></row><row><cell>american_football.football_coach</cell><cell>70.59</cell><cell>66.67</cell><cell>people.person.place_of_birth</cell><cell>92.00</cell><cell>90.79</cell></row><row><cell>american_football.football_conference</cell><cell>44.44</cell><cell>36.36</cell><cell>people.person.place_lived</cell><cell>85.98</cell><cell>77.67</cell></row><row><cell>american_football.football_team</cell><cell>86.67</cell><cell>86.36</cell><cell>people.person.nationality</cell><cell>100.00</cell><cell>98.80</cell></row><row><cell>origin birthDate location manufacturer country city nationality owner county region birthPlace type area class address team description company state language family name gender duration age product capacity elevation</cell><cell></cell><cell>0.15 0.10 0.05 0.00 0.05 0.10 0.15</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">elevation capacity product age duration gender name family language state company description team address class area type birthPlace region county owner nationality city country manufacturer location birthDate origin</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 :</head><label>11</label><figDesc>Comparisons with different input token size on the VizNet (Full) dataset.</figDesc><table><row><cell>Method</cell><cell cols="3">MaxToken/col Macro F1 Micro F1</cell></row><row><cell>Doduo</cell><cell>8</cell><cell>81.0</cell><cell>92.5</cell></row><row><cell></cell><cell>16</cell><cell>83.6</cell><cell>93.6</cell></row><row><cell></cell><cell>32</cell><cell>83.4</cell><cell>94.2</cell></row><row><cell>Dosolo SCol</cell><cell>8</cell><cell>72.7</cell><cell>87.2</cell></row><row><cell></cell><cell>16</cell><cell>76.1</cell><cell>89.1</cell></row><row><cell></cell><cell>32</cell><cell>77.4</cell><cell>90.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Language model probing results on the WikiTable dataset (Left: column type prediction. Right: Column relation prediction.)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 13 :</head><label>13</label><figDesc>Language model probing results on the VizNet dataset. We observe that the language model stores a certain amount of factual knowledge about column types listed in Top-5, compared to Bottom-5. The general trend is consistent withTable 12.</figDesc><table><row><cell></cell><cell cols="3">Column type Avg. rank (?) PPL / Avg.PPL (?)</cell></row><row><cell></cell><cell>year</cell><cell>6.60</cell><cell>0.799</cell></row><row><cell>Top-5</cell><cell>manufacturer day state</cell><cell>20.19 14.21 16.88</cell><cell>0.810 0.819 0.825</cell></row><row><cell></cell><cell>language</cell><cell>17.23</cell><cell>0.840</cell></row><row><cell>Bottom-5</cell><cell>organisation nationality creator affiliation</cell><cell>61.83 65.81 57.39 63.85</cell><cell>1.146 1.218 1.232 1.239</cell></row><row><cell></cell><cell>birthPlace</cell><cell>72.30</cell><cell>1.334</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Table values only vs. with meta information. First, Doduo takes table value only. In most cases, we believe this assumption makes the framework more flexible to be practical. For example, spreadsheets and data frames, which are common data format choices for data analysis, do not have table captions and often lack meaningful table headers. Nevertheless, we acknowledge that, in some cases, meta information plays an important role to complementing table values to compose the table semantics. As recent work [13, 57] has shown the effectiveness of meta information for the table tasks, understanding when meta information becomes essential for the task is still an open question. Single-table model vs. multi-table model. Second, Doduo assumes the input table to be self-contained. That means columns that are necessary to compose table context should be stored in the same table. Web Tables generally follow the assumption, and Doduo shows the strong performance on the WikiTable and VizNet datasets. However, Doduo was not tested on relational tables, where chunks of information can be split into multiple tables after database normalization. In such a scenario, we need to consider inter-table relations and information to incorporate key signals outside the target table. In fact, contemporaneous work</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In this context, George Miller refers to an Australian filmmaker, but there exist more than 30 different Wikipedia articles that refer to different George Miller.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In other words, Doduo relies on the general knowledge obtained from text corpora (e.g., Wikipedia) and a training set of tables annotated with column types and relations.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We used column name and description for ground-truth creation purpose only as it is not available for all tables in practice. The initial clustering uses a combination of TF-IDF vectors and fastText embeddings.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">Again, we used the leaf node of each column type as the term for the template.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Jin Wang and Estevam Hruschka for their valuable feedback on the draft.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00051</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00051" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Entity Matching with Transformer Architectures -A Step Forward in Data Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursin</forename><surname>Brunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Stockinger</surname></persName>
		</author>
		<idno type="DOI">10.5441/002/edbt.2020.58</idno>
		<ptr target="https://doi.org/10.5441/002/edbt.2020.58" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Extending Database Technology</title>
		<editor>Dan Olteanu, George H. L. Fletcher, Arijit Khan, and Bin Yang</editor>
		<meeting>the 23rd International Conference on Extending Database Technology<address><addrLine>Copenhagen, Denmark; Angela Bonifati, Yongluan Zhou, Marcos Antonio Vaz Salles</addrLine></address></meeting>
		<imprint>
			<publisher>Alexander B?hm</publisher>
			<date type="published" when="2020-03-30" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="463" to="473" />
		</imprint>
	</monogr>
	<note>OpenProceedings.org</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">WWW &apos;18). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Cannaviccio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Merialdo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178876.3186029</idno>
		<ptr target="https://doi.org/10.1145/3178876.3186029" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference</title>
		<meeting>the 2018 World Wide Web Conference<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1316" />
		</imprint>
	</monogr>
	<note>Towards Annotating Relational Data on the Web with Language Models</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Creating Embeddings of Heterogeneous Relational Datasets for Data Integration Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Cappuzzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saravanan</forename><surname>Thirumuruganathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3389742</idno>
		<ptr target="https://doi.org/10.1145/3318464.3389742" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2020 ACM SIGMOD International Conference on Management of Data<address><addrLine>Portland, OR, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1335" to="1349" />
		</imprint>
	</monogr>
	<note>SIGMOD &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multitask Learning: A Knowledge-Based Source of Inductive Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on International Conference on Machine Learning<address><addrLine>Amherst, MA, USA; San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multitask Learning. Machine Learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dataset search: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriane</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Simperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Koesten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Konstantinidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis-Daniel</forename><surname>Ib??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilia</forename><surname>Kacprzak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="251" to="272" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ColNet: Embedding the Semantics of Web Tables for Column Type Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Jim?nez-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.330129</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.330129" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="29" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Semantic Annotations for Tabular Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernesto</forename><surname>Jim?nez-Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/289</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2019/289" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
		<editor>Sarit Kraus (Ed.). ijcai.org</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-10" />
			<biblScope unit="page" from="2088" to="2094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">What Does BERT Look at? An Analysis of BERT&apos;s Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Urvashi</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BlackBoxNLP &apos;19</title>
		<meeting>BlackBoxNLP &apos;19</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="276" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Data Civilizer System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><forename type="middle">Castro</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziawasch</forename><surname>Abedjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">K</forename><surname>Elmagarmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ihab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mourad</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<ptr target="http://cidrdb.org/cidr2017/papers/p44-deng-cidr17.pdf" />
	</analytic>
	<monogr>
		<title level="m">8th Biennial Conference on Innovative Data Systems Research</title>
		<meeting><address><addrLine>Chaminade, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-01-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TURL: Table Understanding through Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyssa</forename><surname>Lees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.14778/3430915.3430921</idno>
		<ptr target="https://doi.org/10.14778/3430915.3430921" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="307" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">COMA-a system for flexible combination of schema matching approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB&apos;02: Proceedings of the 28th International Conference on Very Large Databases</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="610" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Injecting Numerical Reasoning Skills into Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.89</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.89" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="946" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://datastudio.google.com/" />
		<title level="m">Google Data Studio</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1206</idno>
		<ptr target="https://doi.org/10.18653/v1/D17-1206" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1923" to="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TaPas: Weakly Supervised Table Parsing via Pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.398</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.398" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">VizNet: Towards A Large-Scale Visualization Learning and Benchmarking Repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">&amp;apos;</forename><forename type="middle">S</forename><surname>Snehalkumar &amp;apos;neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Gaikwad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?a?atay</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demiralp</surname></persName>
		</author>
		<idno type="DOI">10.1145/3290605.3300892</idno>
		<ptr target="https://doi.org/10.1145/3290605.3300892" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems<address><addrLine>Glasgow, Scotland Uk; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>CHI &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sherlock: A Deep Learning Approach to Semantic Data Type Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuel</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?agatay</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?sar</forename><surname>Hidalgo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3292500.3330993</idno>
		<ptr target="https://doi.org/10.1145/3292500.3330993" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>Anchorage, AK, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1500" to="1508" />
		</imprint>
	</monogr>
	<note>KDD &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How Can We Know What Language Models Know?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Araki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00324</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00324" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="423" to="438" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic Concept Annotation for Tabular Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udayan</forename><surname>Khurana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainyam</forename><surname>Galhotra</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482295</idno>
		<ptr target="https://doi.org/10.1145/3459637.3482295" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="844" to="853" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kyriakos Psarakis, Jerry Brons, Marios Fragkoulis, Christoph Lofi, Angela Bonifati, and Asterios Katsifodimos. 2021. Valentine: Evaluating Matching Techniques for Dataset Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Koutras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Siachamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andra</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 37th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="468" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep entity matching with pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.14778/3421424.3421431</idno>
		<ptr target="https://doi.org/10.14778/3421424.3421431" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2020-09" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="50" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Entity Matching: Challenges and Opportunities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Hirota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3431816</idno>
		<ptr target="https://doi.org/10.1145/3431816" />
	</analytic>
	<monogr>
		<title level="j">J. Data and Information Quality</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2021-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Annotating and Searching Web Tables Using Entities, Types and Relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girija</forename><surname>Limaye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.14778/1920841.1921005</idno>
		<ptr target="https://doi.org/10.14778/1920841.1921005" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2010-09" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1338" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Neural Relation Extraction on Wikipedia Tables for Augmenting Knowledge Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erin</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412164</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412164" />
		<imprint>
			<date type="published" when="2020" />
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="2133" to="2136" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Baran: Effective error correction via a unified context representation and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziawasch</forename><surname>Abedjan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment (PVLDB)</title>
		<meeting>the VLDB Endowment (PVLDB)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1948" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Raha: A configuration-free error detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziawasch</forename><surname>Abedjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><forename type="middle">Castro</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mourad</forename><surname>Ouzzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stonebraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Management of Data (SIGMOD)</title>
		<meeting>the International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="865" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1301.3781" />
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations</title>
		<editor>Bengio and Yann LeCun</editor>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-02" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Open Data Integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.14778/3229863.3240491</idno>
		<ptr target="https://doi.org/10.14778/3229863.3240491" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2130" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Using Linked Data to Mine RDF from Wikipedia&apos;s Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emir</forename><surname>Mu?oz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandra</forename><surname>Mileo</surname></persName>
		</author>
		<idno type="DOI">10.1145/2556195.2556266</idno>
		<ptr target="https://doi.org/10.1145/2556195.2556266" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 7th ACM International Conference on Web Search and Data Mining<address><addrLine>New York, New York, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
	<note>WSDM &apos;14)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PATTY: A Taxonomy of Relational Patterns with Semantic Types</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ndapandula</forename><surname>Nakashole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D12-1104" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1135" to="1145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">MTab4DBpedia: Semantic Annotation for Tabular Data with DBpedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phuc</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natthawut</forename><surname>Kertkeidkachorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>n.d.. n. d.</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Data-Driven Domain Discovery for Structured Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayo</forename><surname>Ota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juliana</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</author>
		<idno type="DOI">10.14778/3384345.3384346</idno>
		<ptr target="https://doi.org/10.14778/3384345.3384346" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020-03" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="953" to="967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://doi.org/10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Language Models as Knowledge Bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1250" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of approaches to automatic schema matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhard</forename><surname>Rahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">A</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="334" to="350" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How Much Knowledge Can You Pack Into the Parameters of a Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.437</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.emnlp-main.437" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5418" to="5426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A Primer in BERTology: What We Know About How BERT Works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00349</idno>
		<ptr target="https://doi.org/10.1162/tacl_a_00349" />
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">An Overview of Multi-Task Learning in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automating Large-Scale Data Quality Verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Schelter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meltem</forename><surname>Celikel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Bie?mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Grafberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1781" to="1794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<title level="m">Tableau Software</title>
		<imprint/>
	</monogr>
	<note>n.d.</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tableau</surname></persName>
		</author>
		<ptr target="https://www.tableau.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">ERNIE 2.0: A Continual Pre-Training Framework for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8968" to="8975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Meimei: An Efficient Probabilistic Approach for Semantically Annotating Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunihiro</forename><surname>Takeoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masafumi</forename><surname>Oyamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Okadome</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.3301281</idno>
		<ptr target="https://doi.org/10.1609/aaai.v33i01.3301281" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019-07" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="281" to="288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">RPT: Relational Pre-Trained Transformer is Almost All You Need towards Democratizing Data Preparation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ju</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhong</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mourad</forename><surname>Ouzzani</surname></persName>
		</author>
		<idno type="DOI">10.14778/3457390.3457391</idno>
		<ptr target="https://doi.org/10.14778/3457390.3457391" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2021-04" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1254" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">BERT Rediscovers the Classical NLP Pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">BERT Rediscovers the Classical NLP Pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1452</idno>
		<ptr target="https://doi.org/10.18653/v1/P19-1452" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Heflin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16037</idno>
		<title level="m">Semantic Labeling Using a Deep Contextualized Language Model</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS &apos;17</title>
		<meeting>NIPS &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Venetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayant</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Pa?ca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warren</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengxin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.14778/2002938.2002939</idno>
		<ptr target="https://doi.org/10.14778/" />
		<title level="m">Recovering Semantics of Tables on the Web. Proc. VLDB Endow</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="528" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Do NLP Models Know Numbers? Probing Numeracy in Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1534</idno>
		<ptr target="https://doi.org/10.18653/v1/D19-1534" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5307" to="5315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">TCN: Table Convolutional Network for Web Table Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prashant</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lockard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binxuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><forename type="middle">Luna</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450090</idno>
		<ptr target="https://doi.org/10.1145/3442381.3450090" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW &apos;21)</title>
		<meeting>the Web Conference 2021 (Ljubljana, Slovenia) (WWW &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4020" to="4032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">TUTA: Tree-Based Transformers for Generally Structured Table Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467434</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467434" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining (Virtual Event, Singapore) (KDD &apos;21)</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining (Virtual Event, Singapore) (KDD &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-Art Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.emnlp-demos.6" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Deep transformer models for time series forecasting: The influenza prevalence case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn O&amp;apos;</forename><surname>Banion</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08317</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Trace Norm Regularised Deep Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR &apos;17 Workshop Track</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">TextRunner: Open Information Extraction on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Broadhead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Cafarella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/N07-4013" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)</title>
		<meeting>Human Language Technologies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT)<address><addrLine>Rochester, New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="25" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yih</forename><surname>Wen-Tau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.745</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.acl-main.745" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8413" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling Task Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">B</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Statistical Language Models for Information Retrieval: A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<idno type="DOI">10.1561/1500000008</idno>
		<ptr target="https://doi.org/10.1561/1500000008" />
	</analytic>
	<monogr>
		<title level="j">Critical Review. Found. Trends Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="137" to="213" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sato: Contextual Semantic Type Detection in Tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelon</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?a?atay</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.14778/3407790.3407793</idno>
		<ptr target="https://doi.org/10.14778/3407790.3407793" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1835" to="1848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automatic discovery of attributes in relational databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meihui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Hadjieleftheriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><surname>Beng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><forename type="middle">M</forename><surname>Ooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divesh</forename><surname>Procopiuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM SIGMOD International Conference on Management of data</title>
		<meeting>the 2011 ACM SIGMOD International Conference on Management of data</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
