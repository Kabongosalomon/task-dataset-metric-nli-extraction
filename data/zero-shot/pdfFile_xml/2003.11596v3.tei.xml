<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Multi-Scale Photo Exposure Correction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Afifi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre (SAIC)</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">York University</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre (SAIC)</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Heidelberg University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Centre (SAIC)</orgName>
								<address>
									<settlement>Toronto</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Multi-Scale Photo Exposure Correction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Capturing photographs with wrong exposures remains a major source of errors in camera-based imaging. Exposure problems are categorized as either: (i) overexposed, where the camera exposure was too long, resulting in bright and washed-out image regions, or (ii) underexposed, where the exposure was too short, resulting in dark regions. Both under-and overexposure greatly reduce the contrast and visual appeal of an image. Prior work mainly focuses on underexposed images or general image enhancement. In contrast, our proposed method targets both over-and underexposure errors in photographs. We formulate the exposure correction problem as two main sub-problems: (i) color enhancement and (ii) detail enhancement. Accordingly, we propose a coarse-to-fine deep neural network (DNN) model, trainable in an end-to-end manner, that addresses each subproblem separately. A key aspect of our solution is a new dataset of over 24,000 images exhibiting the broadest range of exposure values to date with a corresponding properly exposed image. Our method achieves results on par with existing state-of-the-art methods on underexposed images and yields significant improvements for images suffering from overexposure errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The exposure used at capture time directly affects the overall brightness of the final rendered photograph. Digital cameras control exposure using three main factors: (i) capture shutter speed, (ii) f-number, which is the ratio of the focal length to the camera aperture diameter, and (iii) the ISO value to control the amplification factor of the received pixel signals. In photography, exposure settings are represented by exposure values (EVs), where each EV refers to different combinations of camera shutter speeds and f-numbers that result in the same exposure effect-also referred to as 'equivalent exposures' in photography.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Photographs with over-and underexposure errors and the results of our method using a single model for exposure correction. These sample input images are taken from outside our dataset to demonstrate the generalization of our trained model. Digital cameras can adjust the exposure value of captured images for the purpose of varying the brightness levels. This adjustment can be controlled manually by users or performed automatically in an auto-exposure (AE) mode. When AE is used, cameras adjust the EV to compensate for low/high levels of brightness in the captured scene using through-the-lens (TTL) metering that measures the amount of light received from the scene <ref type="bibr" target="#b52">[53]</ref>.</p><p>Exposure errors can occur due to several factors, such as errors in measurements of TTL metering, hard lighting conditions (e.g., very low lighting and backlighting), dramatic changes in the brightness level of the scene, and errors made by users in the manual mode. Such exposure errors are introduced early in the capture process and are thus hard to correct after rendering the final 8-bit image. This is due to the highly nonlinear operations applied by the camera image signal processor (ISP) afterwards to render the final 8-bit standard RGB (sRGB) image <ref type="bibr" target="#b31">[32]</ref>. <ref type="figure">Fig. 1</ref> shows typical examples of images with exposure errors. In <ref type="figure">Fig. 1</ref>, exposure errors result in either very bright image regions, due to overexposure, or very dark regions, caused by underexposure errors, in the final rendered images. Correcting images with such errors is a challenging task even for well-established image enhancement software packages, see <ref type="figure" target="#fig_4">Fig. 9</ref>. Although both over-and underexposure errors are common in photography, most prior work is mainly focused on correcting underexposure errors <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref> or generic image quality enhancement <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>. Contributions We propose a coarse-to-fine deep learning method for exposure error correction of both over-and underexposed sRGB images. Our approach formulates the exposure correction problem as two main sub-problems: (i) color and (ii) detail enhancement. We propose a coarse-tofine deep neural network (DNN) model, trainable in an endto-end manner, that begins by correcting the global color information and subsequently refines the image details. In addition to our DNN model, a key contribution to the exposure correction problem is a new dataset containing over 24,000 images 1 rendered from raw-RGB to sRGB with different exposure settings with broader exposure ranges than previous datasets. Each image in our dataset is provided with a corresponding properly exposed reference image. Lastly, we present an extensive set of evaluations and ablations of our proposed method with comparisons to the state of the art. We demonstrate that our method achieves results on par with previous methods dedicated to underexposed images and yields significant improvements on overexposed images. Furthermore, our model generalizes well to images outside our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The focus of our paper is on correcting exposure errors in camera-rendered 8-bit sRGB images. We refer the reader to <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref> for representative examples for rendering linear raw-RGB images captured with low light or exposure errors. Exposure Correction Traditional methods for exposure correction and contrast enhancement rely on image histograms to re-balance image intensity values <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b73">74]</ref>. Alternatively, tone curve adjustment is used to correct images with exposure errors. This process is performed by relying either solely on input image information <ref type="bibr" target="#b66">[67]</ref> or trained deep learning models <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b65">66]</ref>. The majority of prior work adopts the Retinex theory <ref type="bibr" target="#b34">[35]</ref> by assuming that improperly exposed images can be formulated as a pixel-wise multiplication of target images, captured with correct exposure settings, by illumination maps. Thus, the goal of these methods is to predict illumination maps to recover the well-exposed target images. Representative Retinex-based methods include <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70]</ref> 1 Project page: https://github.com/mahmoudnafifi/Exposure Correction Emulates camera ISP processes based on metadata in each DNG file  <ref type="figure">Figure 2</ref>: Dataset overview. Our dataset contains images with different exposure error types and their corresponding properly exposed reference images. Shown is a t-SNE visualization <ref type="bibr" target="#b44">[45]</ref> of all images in our dataset and the lowlight (LOL) paired dataset (outlined in red) <ref type="bibr" target="#b61">[62]</ref>. Notice that LOL covers a relatively small fraction of the possible exposure levels, as compared to our introduced dataset. Our dataset was rendered from linear raw-RGB images taken from the MIT-Adobe FiveK dataset <ref type="bibr" target="#b4">[5]</ref>. Each image was rendered with different relative exposure values (EVs) by an accurate emulation of the camera ISP processes. and the most recent deep learning ones <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b70">71]</ref>. Most of these methods, however, are restricted to correcting underexposure errors <ref type="bibr">[23, 60, 62-64, 70, 71, 73]</ref>. In contrast to the majority of prior work, our work is the first deep learning method to explicitly correct both overexposed and underexposed photographs with a single model. HDR Restoration and Image Enhancement HDR restoration is the process of reconstructing scene radiance HDR values from one or more low dynamic range (LDR) input images. Prior work either require access to multiple LDR images <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b45">46]</ref> or use a single LDR input image, which is converted to an HDR image by hallucinating missing information <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b49">50]</ref>. Ultimately, these reconstructed HDR images are mapped back to LDR for perceptual visualization. This mapping can be directly performed from the input multi-LDR images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, the reconstructed HDR image <ref type="bibr" target="#b64">[65]</ref>, or directly from the single input LDR image without the need for radiance HDR reconstruction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>There are also methods that focus on general image enhancement that can be applied to enhancing images with poor exposure. In particular, work by <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>   <ref type="figure">Figure 3</ref>: Motivation behind our coarse-to-fine exposure correction approach. Example of an overexposed image and its corresponding properly exposed image shown in (A) and (B), respectively. The Laplacian pyramid decomposition allows us to enhance the color and detail information sequentially, as shown in (C) and (D), respectively.</p><p>of exposure errors. <ref type="figure">Fig. 2</ref> shows a comparison between our dataset and the LOL dataset in terms of the number of images and the variety of exposure errors in each dataset. The LOL dataset covers a relatively small fraction of the possible exposure levels, as compared to our introduced dataset. Our dataset is based on the MIT-Adobe FiveK dataset <ref type="bibr" target="#b4">[5]</ref> and is accurately rendered by adjusting the high tonal values provided in camera sensor raw-RGB images to realistically emulate camera exposure errors. An alternative worth noting is to use a large HDR dataset to produce training data-for example, the Google HDR+ dataset <ref type="bibr" target="#b23">[24]</ref>. One drawback, however, is that this dataset is a composite of a varying number of smartphone captured raw-RGB images that were first aligned to a composite raw-RGB image. The target ground truth image is based on an HDR-to-LDR algorithm applied to this composite raw-RGB image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. We opt instead to use the FiveK dataset as it starts with a single high-quality raw-RGB image and the ground truth result is generated by an expert photographer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Dataset</head><p>To train our model, we need a large number of training images rendered with realistic over-and underexposure errors and corresponding properly exposed ground truth images. As discussed in Sec. 2, such datasets are currently not publicly available to support exposure correction research. For this reason, our first task is to create a new dataset. Our dataset is rendered from the MIT-Adobe FiveK dataset <ref type="bibr" target="#b4">[5]</ref>, which has 5,000 raw-RGB images and corresponding sRGB images rendered manually by five expert photographers <ref type="bibr" target="#b4">[5]</ref>.</p><p>For each raw-RGB image, we use the Adobe Camera Raw SDK <ref type="bibr" target="#b0">[1]</ref> to emulate different EVs as would be applied by a camera <ref type="bibr" target="#b56">[57]</ref>. Adobe Camera Raw accurately emulates the nonlinear camera rendering procedures using metadata embedded in each DNG raw file <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b56">57]</ref>. We render each raw-RGB image with different digital EVs to mimic real exposure errors. Specifically, we use the relative EVs ?1.5, ?1, +0, +1, and +1.5 to render images with underexposure errors, a zero gain of the original EV, and overexposure errors, respectively. The zero-gain relative EV is equivalent to the original exposure settings applied onboard the camera during capture time.</p><p>As the ground truth images, we use images that were manually retouched by an expert photographer (referred to as Expert C in <ref type="bibr" target="#b4">[5]</ref>) as our target correctly exposed images, rather than using our rendered images with +0 relative EV. The reason behind this choice is that a significant number of images contain backlighting or partial exposure errors in the original exposure capture settings. The expert adjusted images were performed in ProPhoto RGB color space <ref type="bibr" target="#b4">[5]</ref> (rather than raw-RGB), which we converted to a standard 8-bit sRGB color space encoding.</p><p>In total, our dataset contains 24,330 8-bit sRGB images with different digital exposure settings. We discarded a small number of images that had misalignment with their corresponding ground truth image. These misalignments are due to different usage of the DNG crop area metadata by Adobe Camera Raw SDK and the expert. Our dataset is divided into three sets: (i) training set of 17,675 images, (ii) validation set of 750 images, and (iii) testing set of 5,905 images. The training, validation, and testing sets, use different images taken from the FiveK dataset. This means the training, validation, and testing images do not share any images in common. <ref type="figure">Fig. 2</ref> shows examples of our generated 8-bit sRGB images and the corresponding properly exposed 8-bit sRGB reference images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Method</head><p>Given an 8-bit sRGB input image, I, rendered with the incorrect exposure setting, our method aims to produce an output image, Y, with fewer exposure errors than those in I. As we simultaneously target both over-and underexposed errors, our input image, I, is expected to contain regions of nearly over-or under-saturated values with corrupted color and detail information. We propose to correct color and detail errors of I in a sequential manner. Specifically, we process a multi-resolution representation of I, rather than directly dealing with the original form of I. We use the Laplacian pyramid <ref type="bibr" target="#b3">[4]</ref> as our multiresolution decomposition, which is derived from the Gaussian pyramid of I.  <ref type="figure">Figure 4</ref>: Overview of our image exposure correction architecture. We propose a coarse-to-fine deep network to progressively correct exposure errors in 8-bit sRGB images. Our network first corrects the global color captured at the final level of the Laplacian pyramid and then the subsequent frequency layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Coarse-to-Fine Exposure Correction</head><p>Let X represent the Laplacian pyramid of I with n levels, such that X (l) is the l th level of X. The last level of this pyramid (i.e., X (n) ) captures low-frequency information of I, while the first level (i.e., X (1) ) captures the highfrequency information. Such frequency levels can be categorized into: (i) global color information of I stored in the low-frequency level and (ii) image coarse-to-fine details stored in the mid-and high-frequency levels. These levels can be later used to reconstruct the full-color image I. <ref type="figure">Fig. 3</ref> motivates our coarse-to-fine approach to exposure correction. Figs. 3-(A) and (B) show an example overexposed image and its corresponding well-exposed target, respectively. As observed, a significant exposure correction can be obtained by using only the low-frequency layer (i.e., the global color information) of the target image in the Laplacian pyramid reconstruction process, as shown in <ref type="figure">Fig. 3</ref>-(C). We can then improve the final image by enhancing the details in a sequential way by correcting each level of the Laplacian pyramid, as shown in <ref type="figure">Fig. 3</ref>-(D). Practically, we do not have access to the properly exposed image in <ref type="figure">Fig. 3-(B)</ref> at the inference stage, and thus our goal is to predict the missing color/detail information of each level in the Laplacian pyramid.</p><p>Inspired by this observation and the success of coarseto-fine architectures for various other computer vision tasks (e.g., <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b57">58]</ref>), we design a DNN that corrects the global color and detail information of I in a sequential manner using the Laplacian pyramid decomposition. The remaining parts of this section explain the technical details of our model (Sec </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Coarse-to-Fine Network</head><p>Our image exposure correction architecture sequentially processes the n-level Laplacian pyramid, X, of the input image, I, to produce the final corrected image, Y. The proposed model consists of n sub-networks. Each of these sub-networks is a U-Net-like architecture <ref type="bibr" target="#b55">[56]</ref> with untied weights. We allocate the network capacity in the form of weights based on how significantly each sub-problem (i.e., global color correction and detail enhancement) contributes to our final result. <ref type="figure">Fig. 4</ref> provides an overview of our network. As shown, the largest (in terms of weights) subnetwork in our architecture is dedicated to processing the global color information in I (i.e., X (n) ). This sub-network (shown in yellow in <ref type="figure">Fig. 4</ref>) processes the low-frequency level X (n) and produces an upscaled image Y (n) . The upscaling process scales up the output of our sub-network by a factor of two using strided transposed convolution with trainable weights. Next, we add the first mid-frequency level X (n?1) to Y (n) to be processed by the second subnetwork in our model. This sub-network enhances the corresponding details of the current level and produces a residual layer that is then added to Y (n) + X (n?1) to reconstruct image Y (n?1) , which is equivalent to the corresponding Gaussian pyramid level n ? 1. This refinement-upsampling process proceeds until the final output image, Y, is produced. Our network is fully differentiable and thus can be trained in an end-to-end manner. Additional details of our network are provided in the supplementary materials. The code and weights for our model will be released to support reproducibility and facilitate future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Losses</head><p>We train our model end-to-end to minimize the following loss function:</p><formula xml:id="formula_0">L = L rec + L pyr + L adv ,<label>(1)</label></formula><p>where L rec denotes the reconstruction loss, L pyr the pyramid loss, and L adv the adversarial loss. The individual losses are defined next.</p><p>Reconstruction Loss: We use the L 1 loss function between the reconstructed and properly exposed reference images. This loss can be expressed as follows:</p><formula xml:id="formula_1">L rec = 3hw p=1 |Y(p) ? T(p)| ,<label>(2)</label></formula><p>where h and w denote the height and width of the training image, respectively, and p is the index of each pixel in our corrected image, Y, and the corresponding properly exposed reference image, T, respectively.</p><p>Pyramid Loss: To guide each sub-network to follow the Laplacian pyramid reconstruction procedure, we introduce dedicated losses at each pyramid level. Let T (l) denote the l th level of the Gaussian pyramid of our reference image, T, after upsampling by a factor of two. We use a simple interpolation process for the upsampling operation <ref type="bibr" target="#b45">[46]</ref>. Our pyramid loss is computed as follows:</p><formula xml:id="formula_2">L pyr = n l=2 2 (l?2) 3h l w l p=1 Y (l) (p) ? T (l) (p) ,<label>(3)</label></formula><p>where h l and w l are twice the height and width of the l th level in the Laplacian pyramid of the training image, respectively, and p is the index of each pixel in our corrected image at the l th level Y (l) and the properly exposed reference image at the same level T (l) , respectively. The pyramid loss not only gives a principled interpretation of the task of each sub-network but also results in less visual artifacts compared to training using only the reconstruction loss, as can be seen in <ref type="figure" target="#fig_2">Fig. 5</ref>. Notice that without the intermediate pyramid losses, the multi-scale reconstructions, shown in <ref type="figure">Fig</ref>    <ref type="figure">Figure 6</ref>: We evaluate the results of input images against all five expert photographers' edits from the FiveK dataset <ref type="bibr" target="#b4">[5]</ref>.</p><p>and appeal, we also consider an adversarial loss as a regularizer. This adversarial loss term can be described by the following equation <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_3">L adv = ?3hwn log (S (D (Y))) ,<label>(4)</label></formula><p>where S is the sigmoid function and D is a discriminator DNN that is trained together with our main network. We provide the details of our discriminator network and visual comparisons between our results using non-adversarial and adversarial training in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Inference Stage</head><p>Our network is fully convolutional and can process input images with different resolutions. While our model requires a reasonable memory size (?7M parameters), processing high-resolution images requires a high computational power that may not always be available. Furthermore, processing images with considerably higher resolution (e.g., 16-megapixel) than the range of resolutions used in the training process can affect our model's robustness with large homogeneous image regions. This issue arises because our network was trained on a certain range of effective receptive fields, which is very low compared to the receptive fields required for images with very high resolution. To that end, we use the bilateral guided upsampling method <ref type="bibr" target="#b8">[9]</ref> to process high-resolution images. First, we resize the input test image to have a maximum dimension of 512 pixels. Then, we process the downsampled version of the input image using our model, followed by applying the fast upsampling technique <ref type="bibr" target="#b8">[9]</ref> with a bilateral grid of 22 ? 22 ? 8 cells. This process allows us to process a 16megapixel image in ?4.5 seconds on average. This time includes ?0.5 seconds to run our network on an NVIDIA GeForce GTX 1080 TM GPU and ?4 seconds on an Intel Xeon E5-1607 @ 3.10 GHz machine for the guided upsampling process. Note the runtime of the guided upsampling step can be significantly improved with a Halide implementation <ref type="bibr" target="#b54">[55]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training Details</head><p>In our implementation, we use a Laplacian pyramid with four levels (i.e., n = 4) and thus we have four sub-networks in our model-an ablation study evaluating the effect on the  number of Laplacian levels, including a comparison with a vanilla U-Net architecture, is presented in the supplementary materials. We trained our model on patches randomly extracted from training images with different dimensions. We first train on patches of size 128?128 pixels. Next, we continue training on 256 ? 256 patches, followed by training on 512?512 patches. We use the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> to minimize our loss function in Eq. 1. Inspired by previous work <ref type="bibr" target="#b42">[43]</ref>, we initially train without the adversarial loss term L adv to speed up the convergence of our main network. Upon convergence, we then add the adversarial loss term L adv and fine-tune our network to enhance our initial results. Additional training details are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Empirical Evaluation</head><p>We compare our method against a broad range of existing methods for exposure correction and image enhancement. We first present quantitative results and comparisons in Sec. 5.1, followed by qualitative comparisons in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Quantitative Results</head><p>To evaluate our method, we use our test set, which consists of 5,905 images rendered with different exposure settings, as described in Sec. 3. Specifically, our test set includes 3,543 well-exposed/overexposed images rendered with +0, +1, and +1.5 relative EVs, and 2,362 underexposed images rendered with ?1 and ?1.5 relative EVs.</p><p>We adopt the following three standard metrics to evaluate the pixel-wise accuracy and the perceptual quality of our results: (i) peak signal-to-noise ratio (PSNR), (ii) structural similarity index measure (SSIM) <ref type="bibr" target="#b71">[72]</ref>, and (iii) perceptual index (PI) <ref type="bibr" target="#b2">[3]</ref>. The PI is given by:</p><formula xml:id="formula_4">PI = 0.5(10 ? Ma + NIQE),<label>(5)</label></formula><p>where both Ma <ref type="bibr" target="#b41">[42]</ref> and NIQE <ref type="bibr" target="#b47">[48]</ref> are no-reference image quality metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image Photoshop HDR Ours</head><p>By Justin Chiaratti (Flickr: CC BY-NC-SA 2.0) DPE <ref type="figure">Figure 8</ref>: Qualitative comparison with Adobe Photoshop's local adaptation HDR function <ref type="bibr" target="#b10">[11]</ref> and DPE <ref type="bibr" target="#b9">[10]</ref>. Input images are taken from Flickr.</p><p>For the pixel-wise error metrics -namely, PSNR and SSIM -we compare the results not only against the properly exposed rendered images by Expert C but also with all five expert photographers in the MIT-Adobe FiveK dataset <ref type="bibr" target="#b4">[5]</ref>. Though the expert photographers may render the same image in different ways due to differences in the camera-based rendering settings (e.g., white balance and tone mapping), a common characteristic over all rendered images by the expert photographers is that they all have fairly proper exposure settings <ref type="bibr" target="#b4">[5]</ref> (see <ref type="figure">Fig. 6</ref>). For this reason, we evaluate our method against the five expert rendered images as they all represent satisfactory exposed reference images.</p><p>We also evaluate a variety of previous non-learning and learning-based methods on our test set for comparison: histogram equalization (HE) <ref type="bibr" target="#b17">[18]</ref>, contrast-limited adaptive histogram equalization (CLAHE) <ref type="bibr" target="#b73">[74]</ref>, the weighted variational model (WVM) <ref type="bibr" target="#b15">[16]</ref>, the low-light image enhancement method (LIME) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, HDR CNN <ref type="bibr" target="#b13">[14]</ref>, DPED models <ref type="bibr" target="#b26">[27]</ref>, deep photo enhancer (DPE) models <ref type="bibr" target="#b9">[10]</ref>, the high-quality exposure correction method (HQEC) <ref type="bibr" target="#b69">[70]</ref>, RetinexNet <ref type="bibr" target="#b61">[62]</ref>, deep underexposed photo enhancer (UPE) <ref type="bibr" target="#b59">[60]</ref>, and the zero-reference deep curve estimation method (Zero-DCE) <ref type="bibr" target="#b19">[20]</ref>. To render the reconstructed HDR images generated by the HDR CNN method <ref type="bibr" target="#b13">[14]</ref> back into LDR, we tested both the deep reciprocating HDR transformation method (RHT) <ref type="bibr" target="#b64">[65]</ref>, and Adobe Photoshop's (PS) HDR tool <ref type="bibr" target="#b10">[11]</ref>. <ref type="table">Table 1</ref> summarizes the quantitative results obtained by each method. As shown in the top portion of the table, our method achieves the best results for overexposed images under all metrics. In the underexposed image correction set- <ref type="table">Table 1</ref>: Quantitative evaluation on our introduced test set. The best results are highlighted with green and bold. The second-and third-best results are highlighted in yellow and red, respectively. We compare each method with properly exposed reference image sets rendered by five expert photographers <ref type="bibr" target="#b4">[5]</ref>. For each method, we present peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) <ref type="bibr" target="#b71">[72]</ref>, and perceptual index (PI) <ref type="bibr" target="#b2">[3]</ref>. We denote methods designed for underexposure correction in gray. Non-deep learning methods are marked by * . Generalization We further evaluate the generalization ability of our method on the following standard image datasets used by previous low-light image enhancement methods: (i) LIME (10 images) <ref type="bibr" target="#b22">[23]</ref>, (ii) NPE (75 images) <ref type="bibr" target="#b60">[61]</ref>, (iii) VV (24 images) <ref type="bibr" target="#b58">[59]</ref>, and DICM (44 images) <ref type="bibr" target="#b35">[36]</ref>. Note that in</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>Photoshop HDR <ref type="bibr" target="#b10">[11]</ref> iPhone Photo Enhancer these experiments, we report results of our model trained on our training set without further tuning or re-training on any of these datasets. Similar to previous methods, we use the NIQE perceptual score <ref type="bibr" target="#b47">[48]</ref> for evaluation. <ref type="table" target="#tab_6">Table 2</ref> compares results by our method and the following methods: LIME <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>, WVM <ref type="bibr" target="#b15">[16]</ref>, RetinexNet (RNet) <ref type="bibr" target="#b61">[62]</ref>, "kindling the darkness" (KinD) <ref type="bibr" target="#b70">[71]</ref>, enlighten GAN (EGAN) <ref type="bibr" target="#b28">[29]</ref>, and deep bright-channel prior (BCP) <ref type="bibr" target="#b37">[38]</ref>. As can be seen in <ref type="table" target="#tab_6">Table 2</ref>, our method generally achieves perceptually superior results in correcting low-light 8-bit images of other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Qualitative Results</head><p>We compare our method qualitatively with a variety of previous methods. Note we show results using the model trained with the adversarial loss term, as it produces perceptually superior results (see the perceptual metric results in <ref type="table" target="#tab_6">Tables 1 and 2</ref>). <ref type="figure" target="#fig_3">Fig. 7</ref> shows our results on different overexposed and underexposed images. As shown, our results are arguably visually superior to the other methods, even when input images have hard backlight conditions, as shown in the second row in <ref type="figure" target="#fig_3">Fig. 7 (right)</ref>. Generalization We also ran our model on several images from Flickr that are outside our introduced dataset, as shown in <ref type="figure" target="#fig_4">Figs. 1, 8, and 9</ref>. As with the images from our introduced dataset, our results on the Flickr images are arguably superior to the compared methods. Additional qualitative results and comparisons are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Limitations</head><p>Our method produces unsatisfactory results in regions that have insufficient semantic information, as shown in <ref type="figure" target="#fig_6">Fig. 10</ref>. For example, the input image shown in the first row in <ref type="figure" target="#fig_6">Fig. 10</ref> is completely saturated and contains almost no details in the region of the man's face. We can see that our network cannot constrain the color inside the face region due to the lack of semantic information. In the supplementary materials, we provide a way to interactively control the output results by scaling each layer of the Laplacian pyramid before feeding them to the network. In that way, one can control the output results to reduce such color bleeding problems. It also can be observed that our method may introduce noise when the input image has extreme dark regions, as shown in the second example in <ref type="figure" target="#fig_6">Fig. 10</ref>. These challenging conditions prove difficult for other methods as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>We proposed a single coarse-to-fine deep learning model for overexposed and underexposed image correction. We employed the Laplacian pyramid decomposition to process input images in different frequency bands. Our method is designed to sequentially correct each of the Laplacian pyramid levels in a multi-scale manner, starting with the global color in the image and progressively addressing the image details.</p><p>Our method is enabled by a new dataset of over 24,000 images rendered with the broadest range of exposure errors to date. Each image in our introduced dataset has a ref-  rapher with well-exposure compensation. Through extensive evaluation, we showed that our method produces compelling results compared to available solutions for correcting images rendered with exposure errors and it generalizes well. We believe that our dataset will help future work on improving exposure correction for photographs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Implementation Details</head><p>In the main paper, we proposed a coarse-to-fine network to correct exposure errors in photographs. In this section, we provide the implementation details of our network, the discriminator network used in the adversarial training process, and additional training details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Main Network</head><p>Our main network consists of four sub-networks with ?7M parameters trained in an end-to-end manner. The largest network capacity is dedicated to the first sub-network with decreasing amounts of capacity as we move from coarseto-fine scales. Each sub-network accepts a different representation of the input image extracted from the Laplacian pyramid decomposition. The first sub-network is a four-layer encoder-decoder network with skip connections (i.e., U-Net-like architecture <ref type="bibr" target="#b55">[56]</ref>). The output of the first convolutional (conv) layer has 24 channels. Our first subnetwork has ?4.4M learnable parameters and accepts the low-frequency band level of the Laplacian pyramid, i.e., X <ref type="bibr" target="#b3">(4)</ref> . The result of the first sub-network is then upscaled using a 2 ? 2 ? 3 transposed conv layer with three output channels and a stride of two. This processed layer is then added to the first mid-frequency band level of the Laplacian pyramid (i.e., X <ref type="bibr" target="#b2">(3)</ref> ) and is fed to the second sub-network.</p><p>The second sub-network is a three-layer encoderdecoder network with skip connections. It has 24 channels in the first conv layer of the encoder, with a total of ?1.1M learnable parameters. The second sub-network processes the upscaled input from the first sub-network and outputs a residual layer, which is then added back to the input to the second sub-network followed by a 2?2?3 transposed conv layer with three output channels and a stride of two. The result is added to the second mid-frequency band level of the Laplacian pyramid (i.e., X <ref type="bibr" target="#b1">(2)</ref> ) and is fed to the third sub-network, which generates a new residual that is added back again to the input of this sub-network.</p><p>The third sub-network has the same design as the second network. Finally, the result is added to the high-frequency band level of the Laplacian pyramid (i.e., X <ref type="bibr" target="#b0">(1)</ref> ) and is fed to the fourth sub-network to produce the final processed image.</p><p>The final sub-network is a three-layer encoder-decoder network with skip connections and has ?482.2K learnable parameters, where the output of the first conv layer in its encoder has 16 channels. We provide the details of the main encoder-decoder architecture of each sub-network in <ref type="figure">Fig.  S1</ref>-(A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Discriminator Network</head><p>In the adversarial training of our network, we use a lightweight discriminator network with ?1M learnable parameters. We provide the details of the discriminator in <ref type="figure">Fig.  S1-(B)</ref>. Notice that unlike our main network, we resize all input image patches to have 256?256 pixels before being processed by the discriminator. The output of the last layer in our discriminator is a single scalar value which is then used in our loss during the optimization, as described in the main paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">Additional Training Details</head><p>We use He et al.'s method <ref type="bibr" target="#b24">[25]</ref> to initialize the weights of our encoder and decoder conv layers, while the bias terms are initialized to zero. We minimize our loss functions using the Adam optimizer <ref type="bibr" target="#b32">[33]</ref> with a decay rate ? 1 = 0.9 for the exponential moving averages of the gradient and a decay rate ? 2 = 0.999 for the squared gradient. We use a learning rate of 10 ?4 to update the parameters of our main network and a learning rate of 10 ?5 to update our discriminator's parameters.</p><p>We train our network on patches with different dimensions. Training begins without the adversarial loss, L adv , then L adv is added to fine-tune the results of our initial training <ref type="bibr" target="#b42">[43]</ref>. Specifically, we begin our training without L adv on 176,590 patches with dimensions of 128?128 pixels extracted randomly from our training images for 40 epochs. The mini-batch size is set to 32. The learning rate is decayed by a factor of 0.5 after the first 20 epochs. Then, we continue training on another 105,845 patches with dimensions of 256 ? 256 pixels for 30 epochs with a mini-batch size of eight. At this stage, we train our main network without L adv for 15 epochs and continue training for another 15 epochs with L adv . The learning rates for the main network and the discriminator network are decayed by a factor of 0.5 every 10 epochs. Finally, we fine-tune the trained networks on another 69,515 training patches with dimensions of 512 ? 512 pixels for 20 epochs with a mini-batch size of four and a learning rate decay of 0.5 applied every five epochs.</p><p>We discard any training patches that have an average intensity less than 0.02 or higher than 0.98. We also discard homogeneous patches that have an average gradient magnitude less than 0.06. We randomly left-right flip training patches for data augmentation. In the adversarial training, we optimize both the main network and the discriminator in an iterative manner. At each optimization step, the learnable parameters of each network are updated to minimize its own loss function. Our main network's loss function is described in the main paper. The discriminator is trained to minimize the following loss function <ref type="bibr" target="#b18">[19]</ref>:</p><formula xml:id="formula_5">L dsc = r (T) + c (Y) ,<label>(6)</label></formula><p>where r (T) refers to the discriminator loss of recognizing the properly exposed reference image T, while c (Y) refers to the discriminator loss of recognizing our corrected image Y. The r (T) and c (Y) loss functions are given by the following equations:</p><formula xml:id="formula_6">r (T) = ? log (S (D (T))) ,<label>(7)</label></formula><formula xml:id="formula_7">c (Y) = ? log (1 ? S (D (Y))) ,<label>(8)</label></formula><p>where S denotes the sigmoid function and D is the discriminator network described in <ref type="figure">Fig. S1-(B)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Ablation Studies (Loss Function)</head><p>Our loss function (Eq. 1 in the main paper) includes three main terms. The first term is the standard reconstruction loss (i.e., L 1 loss). The second and third terms consist of the pyramid and adversarial losses, respectively, which are introduced to further improve the reconstruction and perceptual quality of the output images. In the following, we discuss the effect of these loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Pyramid Loss Impact</head><p>In <ref type="figure" target="#fig_2">Fig. 5</ref> of the main paper, we show the output of each sub-network when we train our model with and without the pyramid loss. We observe that the pyramid loss helps to provide additional supervision to guide each sub-network to follow a coarse-to-fine reconstruction. In this ablation study, we aim to quantitatively evaluate the effect of the pyramid loss on our final results.</p><p>We train two light-weight models of our main network with and without our pyramid loss term. Each model has  <ref type="figure">Figure S2</ref>: Comparisons between our results with (w/) and without (w/o) the adversarial loss for training. The peak signalto-noise ratio (PSNR), structural similarity index measure (SSIM) <ref type="bibr" target="#b71">[72]</ref>, and perceptual index (PI) <ref type="bibr" target="#b2">[3]</ref>   <ref type="figure">Figure S3</ref>: Comparison of results by varying the number of Laplacian pyramid levels. The peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) <ref type="bibr" target="#b71">[72]</ref>, and perceptual index (PI) <ref type="bibr" target="#b2">[3]</ref> are shown for each result. Notice that higher PSNR and SSIM values are better, while lower PI values indicate better perceptual quality. The input image is taken from our validation set.</p><p>Overexposed input image Our result Properly exposed input image Our result <ref type="figure">Figure S4</ref>: Our framework can deal with both improperly and properly exposed input images producing compelling results. The input images are taken from our test set. <ref type="table">Table S1</ref>: Results of our ablation study on 500 images randomly selected from our validation set. We show the effects of: (i) the pyramid loss, L pyr , and (ii) the number of Laplacian levels, n, in the main network. For each experiment, we show the values of the peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM) <ref type="bibr" target="#b71">[72]</ref>. The best PSNR/SSIM values are indicated with bold for each experiment. The training is performed on a sub-set of our training data for ?150,000 iterations on 80,000 128?128 patches, ?100,000 iterations on 40,000 256 ? 256 patches, and ?25,000 iterations on 25,000 512?512 patches. <ref type="table">Table S1</ref> shows the results on 500 randomly selected images from our validation set. The results show that the pyramid loss not only helps in providing a better interpretation of the task of each sub-network but also improves the final results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Adversarial Loss Impact</head><p>In the main paper, we show quantitative results of our method with and without the adversarial loss term. Our trained model with the adversarial loss term achieves better perceptual quality (i.e., lower perceptual index (PI) values <ref type="bibr" target="#b2">[3]</ref>) than training without the adversarial loss term. <ref type="figure">Fig. S2</ref> shows qualitative comparisons of our results with and without the adversarial loss. As shown, the network trained without the adversarial training tends to produce darker images with slightly unrealistic colors in some cases, while the adversarial regularization improves the perceptual quality of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Ablation Studies (Number of Laplacian Pyramid Levels)</head><p>We repeat the same experimental setup described in Sec. 7.2.1 with a varying number of Laplacian pyramid levels (sub-networks). Specifically, we train a network with n = 1 levels-this network is equivalent to a vanilla U-Net-like architecture <ref type="bibr" target="#b55">[56]</ref>. Additionally, we train another network with n = 2 (i.e., two sub-networks).</p><p>For a fair comparison, we fix the total number of parameters in each model by changing the number of filters in the conv layers. Specifically, we set the number of output chan-  <ref type="bibr" target="#b13">[14]</ref> with Adobe Photoshop's HDR tool <ref type="bibr" target="#b10">[11]</ref>. (C) Our results. (G) Properly exposed reference images. The input images are taken from our test set.</p><p>nels of the first layer in the encoder to 48 for the trained model with n = 1, while we decrease it to 34 for the twosub-net model (i.e., n = 2) to have approximately the same number of learnable parameters. Thus, the trained model in Sec. 7.2.1, used to study the pyramid loss impact, and the additional two trained models have approximately the same number of parameters. <ref type="table">Table S1</ref> shows the results obtained by each model on the same random validation image subset used to study the pyramid loss impact in Sec. 7.2.1. <ref type="figure">Fig. S3</ref> shows a qualitative comparison. As can be seen, the best quantitative and qualitative results are obtained using the four-sub-net model (i.e., n = 4 levels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Additional Results and Comparisons</head><p>In this section, we provide additional qualitative results. <ref type="figure">Fig. S4</ref> shows our results when the input image has no exposure errors. As can be seen, our method produces consistent output images regardless of the exposure setting of the input image. Additional qualitative comparisons with other methods on our testing set are shown in <ref type="figure" target="#fig_2">Fig. S5-S9</ref>.</p><p>Generalization We provide additional results on images that are outside our training/testing sets. <ref type="figure" target="#fig_6">Fig. S10</ref> shows qualitative comparisons with the methods of Yuan and Sun <ref type="bibr" target="#b66">[67]</ref> and Guo et al. <ref type="bibr" target="#b20">[21]</ref>, which were designed to correct overexposure errors in photographs. The source code of these methods is not available. Thus, the presented input images and corresponding results by the methods of Yuan and Sun <ref type="bibr" target="#b66">[67]</ref> and Guo et al. <ref type="bibr" target="#b20">[21]</ref> are taken from the original papers <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b66">67]</ref>. As shown in <ref type="figure" target="#fig_6">Fig. S10</ref>, our method produces compelling results. <ref type="figure" target="#fig_13">Fig. S11</ref> shows a qualitative comparison using the DICM image set. <ref type="figure">Fig. S12</ref> shows a qualitative comparison the SID dataset <ref type="bibr" target="#b7">[8]</ref>. In the shown example, we rendered the raw-RGB images provided in the SID dataset to 8-bit JPEG compressed sRGB image. This 8-bit compressed format is more challenging compared to dealing with the 12-bit linear raw images as used by prior work. Though our method is not targeting this kind of "dark" scenes, it is arguable that our result is visually on par with the recently proposed method for low-light image enhancement-namely, the Zero-DCE method <ref type="bibr" target="#b19">[20]</ref>.</p><p>We further examined our model on the testing set used in <ref type="bibr" target="#b59">[60]</ref>. This set has no overlap with our training examples taken from the MIT-Adobe FiveK dataset <ref type="bibr" target="#b4">[5]</ref> and its input images were processed using a different rendering/degradation procedure, as described in <ref type="bibr" target="#b59">[60]</ref>. <ref type="figure">Fig. S13</ref> shows a qualitative comparison between our method and the recent Zero-DCE method <ref type="bibr" target="#b19">[20]</ref> for low-light image enhancement. The quantitative results using the testing set used in <ref type="bibr" target="#b59">[60]</ref>, are reported in <ref type="table" target="#tab_6">Table S2</ref>.</p><p>As can be seen, our method achieves on par, sometimes better, results compared to the state-of-the-art methods designed specifically to deal with underexposure errors. Unlike these methods, our method can effectively deal with both under-and overexposure errors, as discussed in the main paper. Note that we did not re-train our method on either the SID dataset or the testing set used in <ref type="bibr" target="#b59">[60]</ref>, before reporting our results. Additional qualitative comparisons using images taken from Flickr are shown in <ref type="figure">Fig. S14</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.">Potential Applications</head><p>In this section, we highlight two potential applications of our method: (i) photo editing and (ii) image preprocessing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Photo Editing</head><p>The main potential application of the proposed method is to post-capture correct exposure errors in images. This correction process can be performed in a fully   <ref type="figure" target="#fig_6">Figure S10</ref>: Qualitative comparison with the methods of Yuan and Sun <ref type="bibr" target="#b66">[67]</ref> and Guo et al. <ref type="bibr" target="#b20">[21]</ref>. The input images are taken from <ref type="bibr" target="#b66">[67]</ref> and <ref type="bibr" target="#b20">[21]</ref>, respectively. sult Y. In particular, this scaling operation is performed as a pre-processing of each level in the pyramid X as follows: S (l=i) X (l=i) , s.t. i ? {1, 2, 3, 4}. The values of the scale vector S can be interactively controlled by the user to edit our network results. <ref type="figure" target="#fig_2">Fig. S15</ref> shows different results obtained by our network in an interactive way through our graphical user interface (GUI). Our GUI can be used as a photo editing tool to apply different visual effects and filters on the input images. Note that we used S = [1.8, 1.8, 1.8, 1.12] in our experiments in the main paper, as we found it gives the best compelling results (see <ref type="figure" target="#fig_14">Fig. S16</ref>).</p><p>Image Preprocessing Our method can also improve the results of computer vision tasks by using it as a preprocessing step to correct exposure errors in input images. <ref type="figure" target="#fig_3">Fig. S17</ref> shows example applications. In these examples, we show results of face and facial landmark detection of the work in <ref type="bibr" target="#b67">[68]</ref> and image semantic segmentation results obtained by the work in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. As shown, the results of face detection and semantic segmentation are improved by pre-processing the input images using our method. In future work, we plan to investigate the impact of our exposure correction method on a variety of computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input image</head><p>Zero-DCE Ours Properly exposed ref. image <ref type="figure">Figure S12</ref>: Qualitative example from the SID dataset <ref type="bibr" target="#b7">[8]</ref>. We compare our result with the recent Zero-DCE method <ref type="bibr" target="#b19">[20]</ref>.</p><p>Input image Zero-DCE Ours Properly exposed ref. image <ref type="figure">Figure S13</ref>: Qualitative comparison with the recent Zero-DCE method <ref type="bibr" target="#b19">[20]</ref> on the testing set, used in <ref type="bibr" target="#b9">[10]</ref>. Our corrected image and its semantic segmentation mask. We use the cascaded convolutional networks proposed in <ref type="bibr" target="#b67">[68]</ref> for face and facial landmark detection. For image semantic segmentation, we use RefineNet <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. The input images are taken from Flickr.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. 4.2), including details of the losses (Sec. 4.3), inference phase (Sec. 4.4), and training (Sec. 4.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. 5 (right-top), deviate widely from the intermediate Gaussian targets compared to using the pyramid loss at each scale, as shown inFig. 5(right-bottom). We provide supporting justification for this loss with an ablation study in the supplementary materials.Adversarial Loss: To perceptually enhance the reconstruction of the corrected image output in terms of realism4 th level Input image and its 4-level Laplacian pyramid Output of each sub-network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Multiscale losses. Shown are the output of each sub-net trained with and without the pyramid loss (Eq. 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of correcting images with exposure errors. Shown are the input images from our test set, results from the DPED<ref type="bibr" target="#b26">[27]</ref>, results from the Deep UPE<ref type="bibr" target="#b9">[10]</ref>, our results, and the corresponding ground truth images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 9 :</head><label>9</label><figDesc>Comparisons with commercial software packages. The input images are taken from Flickr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>erence image properly rendered by a well-trained photog-Input image Photoshop HDR Ours By Dr. D. (Flickr: CC BY-NC-SA 2.0) By eviljohnius (Flickr: CC BY 2.0)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 10 :</head><label>10</label><figDesc>Failure examples of correcting (top) overexposed and (bottom) underexposed images. The input images are taken from Flickr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>??</head><label></label><figDesc>Output of 3?3 covn layers with stride 1 and padding 1 Output of Leaky ReLU (LReLU) layers Output of 2?2 max-pooling layers with stride 2 Output of 2?2 transposed conv layers Output of depth concatenation layers Output of 1?1 covn layer with stride 1 and padding 1 Skip connections input width and input height output channels of 1 st level in the encoder and number of levels in the encoder/decoder (A) Encoder-decoder architecture used in each sub-network Output of 4?4 covn layers with stride 2 and padding 1 Output of LReLU layers Output of batch normalization layer (B) Discriminator architecture used in our adversarial training Output of 4?4 covn layers with stride 2 and padding 0 Figure S1: Details of the architectures used in our work. (A) Encoder-decoder architecture [56] used to design our subnetworks in the main network. (B) Discriminator architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure S5 :</head><label>S5</label><figDesc>Additional qualitative results. (A) Input images. (B) Results of HDR CNN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure S6 :</head><label>S6</label><figDesc>Additional qualitative comparisons with other methods in correcting underexposed images. (A) Input images. (B) Results of CLAHE [74]. (C) Results of WVM [16]. (D) Results of HDR CNN [14] with Adobe Photoshop's HDR tool [11]. (E) Results of DPED [27]. (F) Results of DPE [10]. (G) Results of Deep UPE [60]. (H) Our results. The input images are taken from our test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure S7 :</head><label>S7</label><figDesc>Additional qualitative comparisons with other methods in correcting overexposed images. (A) Input images. (B) Results of histogram equalization (HE) [18]. (C) Results of the contrast-limited adaptive histogram equalization (CLAHE) [74]. (D) Results of the local Laplacian filter [51]. (E) Results of HDR CNN [14] with Adobe Photoshop's (PS) HDR tool [11]. (F) Results of the DSLR Photo Enhancement dataset (DPED) trained model [27]. (G) Results of deep photo enhancer (DPE) [10]. (H) Our results. The input images are taken from our test set. automated way (as described in the main paper) or can be performed in an interactive way with the user. Specifically, we introduce a scale vector S = [S 1 , S 2 , S 3 , S 4 ] that can be used to independently scale each level in the pyramid X in the inference stage. The scale vector S is introduced to produce different visual effects in the final re-(A) Input image (B) DPED (C) Ours (D) Properly exposed ref. image Figure S8: Additional qualitative results of correcting overexposed images. (A) Input images. (B) Results of DPED [27]. (C) Our results. (G) Properly exposed reference images. The input images are taken from our test set. (A) Input image (B) Deep UPE (C) Ours (D) Properly exposed ref. image Figure S9: Additional qualitative results of correcting underexposed images. (A) Input images. (B) Results of Deep UPE [60]. (C) Our results. (G) Properly exposed reference images. The input images are taken from our test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure S11 :</head><label>S11</label><figDesc>Additional qualitative results of correcting overexposed images. (A) Input image. (B) Result of LIME [22, 23]. (C) Result of HQEC [70]. (D) Our result. The input image is taken from the DICM image set<ref type="bibr" target="#b35">[36]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure S16 :</head><label>S16</label><figDesc>Flickr: CC BY-NC-SA 2.0) By Gabriele (Flickr: CC BY-NC-SA 2.0) By Darlene Acero (Flickr: CC BY-NC-SA 2.0) By julochka (Flickr: CC BY-NC 2.0) Figure S14: Comparison with the recent Zero-DCE method [20] using images taken from Flickr. (A) Input image By Kazuyoshi Wada (Flickr: CC BY-NC-SA 2.0) (B) Our results using different settingsFigure S15: Our GUI photo editing tool. (A) Input image. (B) Our results using different pyramid level scaling settings set by the user in an interactive way. The input image is taken from Flickr. The effect of the scale vector S on our final results. (A) Input images. (B-D) Our results using different scale values, S. The shown input images are taken from our validation set. (A) Failure case of face and facial landmark detection (B) Face and facial landmark detection after our correction (D) Semantic segmentation result on our corrected image (C) Semantic segmentation result on original image By Dan Machold (Flickr: CC BY-NC-SA 2.0) By Vetatur Fumare (Flickr: CC BY-NC 2.0) Figure S17: Applying our method as a pre-processing step can improve results of different computer vision tasks. (A) False negative result of face and facial landmark detection due to the overexposure error in the input image. (B) Our corrected image and the results of face and facial landmark detection. (C) Underexposed input image and its semantic segmentation mask. (D)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>The terms U and S stand for unsupervised and supervised, respectively. Notice that higher PSNR and SSIM values are better, while lower PI values indicate better perceptual quality. RHT [65] 14.547 0.456 14.347 0.427 14.068 0.441 13.025 0.398 11.957 0.379</figDesc><table><row><cell>Method</cell><cell cols="3">Expert A PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM Expert B Expert C Expert D Expert E</cell><cell>Avg. PSNR</cell><cell>SSIM</cell><cell>PI</cell></row><row><cell></cell><cell cols="3">+0, +1, and +1.5 relative EVs (3,543 properly exposed and overexposed images)</cell><cell></cell><cell></cell></row><row><cell>HE [18]  *</cell><cell cols="3">16.140 0.686 16.277 0.672 16.531 0.699 16.643 0.669 17.321 0.691</cell><cell>16.582</cell><cell cols="2">0.683 2.351</cell></row><row><cell>CLAHE [74]  *</cell><cell cols="3">13.934 0.568 14.689 0.586 14.453 0.584 15.116 0.593 15.850 0.612</cell><cell>14.808</cell><cell cols="2">0.589 2.270</cell></row><row><cell>WVM [16]  *</cell><cell cols="3">12.355 0.624 13.147 0.656 12.748 0.645 14.059 0.669 15.207 0.690</cell><cell>13.503</cell><cell cols="2">0.657 2.342</cell></row><row><cell>LIME [22, 23]  *</cell><cell>09.627 0.549 10.096 0.569</cell><cell>9.875</cell><cell>0.570 10.936 0.597 11.903 0.626</cell><cell>10.487</cell><cell cols="2">0.582 2.412</cell></row><row><cell cols="4">HDR CNN [14] w/ RHT [65] 13.151 0.475 13.637 0.478 13.622 0.497 14.177 0.479 14.625 0.503</cell><cell>13.842</cell><cell cols="2">0.486 4.284</cell></row><row><cell>HDR CNN [14] w/ PS [11]</cell><cell cols="3">14.804 0.651 15.622 0.689 15.348 0.670 16.583 0.685 18.022 0.703</cell><cell>16.076</cell><cell cols="2">0.680 2.248</cell></row><row><cell>DPED (iPhone) [27]</cell><cell cols="3">12.680 0.562 13.422 0.586 13.135 0.581 14.477 0.596 15.702 0.630</cell><cell>13.883</cell><cell cols="2">0.591 2.909</cell></row><row><cell>DPED (BlackBerry) [27]</cell><cell cols="3">15.170 0.621 16.193 0.691 15.781 0.642 17.042 0.677 18.035 0.678</cell><cell>16.444</cell><cell cols="2">0.662 2.518</cell></row><row><cell>DPED (Sony) [27]</cell><cell cols="3">16.398 0.672 17.679 0.707 17.378 0.697 17.997 0.685 18.685 0.700</cell><cell>17.627</cell><cell cols="2">0.692 2.740</cell></row><row><cell>DPE (HDR) [10]</cell><cell cols="3">14.399 0.572 15.219 0.573 15.091 0.593 15.692 0.581 16.640 0.626</cell><cell>15.408</cell><cell cols="2">0.589 2.417</cell></row><row><cell>DPE (U-FiveK) [10]</cell><cell cols="3">14.314 0.615 14.958 0.628 15.075 0.645 15.987 0.647 16.931 0.667</cell><cell>15.453</cell><cell cols="2">0.640 2.630</cell></row><row><cell>DPE (S-FiveK) [10]</cell><cell cols="3">14.786 0.638 15.519 0.649 15.625 0.668 16.586 0.664 17.661 0.684</cell><cell>16.035</cell><cell cols="2">0.661 2.621</cell></row><row><cell>HQEC [70]  *</cell><cell cols="3">11.775 0.607 12.536 0.631 12.127 0.627 13.424 0.652 14.511 0.675</cell><cell>12.875</cell><cell cols="2">0.638 2.387</cell></row><row><cell>RetinexNet [62]</cell><cell cols="3">10.149 0.570 10.880 0.586 10.471 0.595 11.498 0.613 12.295 0.635</cell><cell>11.059</cell><cell cols="2">0.600 2.933</cell></row><row><cell>Deep UPE [60]</cell><cell cols="3">10.047 0.532 10.462 0.568 10.307 0.557 11.583 0.591 12.639 0.619</cell><cell>11.008</cell><cell cols="2">0.573 2.428</cell></row><row><cell>Zero-DCE [20]</cell><cell cols="6">10.116 0.503 10.767 0.502 10.395 0.514 11.471 0.522 12.354 0.557 11.0206 0.5196 2.774</cell></row><row><cell>Our method w/o L adv</cell><cell cols="3">18.976 0.743 19.767 0.731 19.980 0.768 18.966 0.716 19.056 0.727</cell><cell>19.349</cell><cell cols="2">0.737 2.189</cell></row><row><cell>Our method w/ L adv</cell><cell cols="3">18.874 0.738 19.569 0.718 19.788 0.760 18.823 0.705 18.936 0.719</cell><cell>19.198</cell><cell cols="2">0.728 2.183</cell></row><row><cell></cell><cell cols="3">?1 and ?1.5 relative EVs (2,362 underexposed images)</cell><cell></cell><cell></cell></row><row><cell cols="4">HE [18]  0.694</cell><cell>14.643</cell><cell cols="2">0.671 2.462</cell></row><row><cell cols="5">HDR CNN [14] w/ 13.589</cell><cell cols="2">0.420 5.072</cell></row><row><cell>HDR CNN [14] w/ PS [11]</cell><cell cols="3">17.324 0.692 18.992 0.714 18.047 0.696 18.377 0.689 19.593 0.701</cell><cell>18.467</cell><cell cols="2">0.698 2.294</cell></row><row><cell>DPED (iPhone) [27]</cell><cell cols="3">18.814 0.680 21.129 0.712 20.064 0.683 19.711 0.675 19.574 0.676</cell><cell>19.858</cell><cell cols="2">0.685 2.894</cell></row><row><cell>DPED (BlackBerry) [27]</cell><cell cols="3">19.519 0.673 22.333 0.745 20.342 0.669 19.611 0.683 18.489 0.653</cell><cell>20.059</cell><cell cols="2">0.685 2.633</cell></row><row><cell>DPED (Sony) [27]</cell><cell cols="3">18.952 0.679 20.072 0.691 18.982 0.662 17.450 0.629 15.857 0.601</cell><cell>18.263</cell><cell cols="2">0.652 2.905</cell></row><row><cell>DPE (HDR) [10]</cell><cell cols="3">17.625 0.675 18.542 0.705 18.127 0.677 16.831 0.665 15.891 0.643</cell><cell>17.403</cell><cell cols="2">0.673 2.340</cell></row><row><cell>DPE (U-FiveK) [10]</cell><cell cols="3">19.130 0.709 19.574 0.674 19.479 0.711 17.924 0.665 16.370 0.625</cell><cell>18.495</cell><cell cols="2">0.677 2.571</cell></row><row><cell>DPE (S-FiveK) [10]</cell><cell cols="3">20.153 0.738 20.973 0.697 20.915 0.738 19.050 0.688 17.510 0.648</cell><cell>19.720</cell><cell cols="2">0.702 2.564</cell></row><row><cell>HQEC [70]  *</cell><cell cols="3">15.801 0.692 17.371 0.718 16.587 0.700 17.090 0.705 17.675 0.716</cell><cell>16.905</cell><cell cols="2">0.706 2.532</cell></row><row><cell>RetinexNet [62]</cell><cell cols="3">11.676 0.607 12.711 0.611 12.132 0.621 12.720 0.618 13.233 0.637</cell><cell>12.494</cell><cell cols="2">0.619 3.362</cell></row><row><cell>Deep UPE [60]</cell><cell cols="3">17.832 0.728 19.059 0.754 18.763 0.745 19.641 0.737 20.237 0.740</cell><cell>19.106</cell><cell cols="2">0.741 2.371</cell></row><row><cell>Zero-DCE [20]</cell><cell cols="6">13.935 0.585 15.239 0.593 14.552 0.589 15.202 0.587 15.893 0.614 14.9642 0.5936 3.001</cell></row><row><cell>Our method w/o L adv</cell><cell cols="3">19.432 0.750 20.590 0.739 20.542 0.770 18.989 0.723 18.874 0.727</cell><cell>19.685</cell><cell cols="2">0.742 2.344</cell></row><row><cell>Our method w/ L adv</cell><cell cols="3">19.475 0.751 20.546 0.730 20.518 0.768 18.935 0.715 18.756 0.719</cell><cell>19.646</cell><cell cols="2">0.737 2.342</cell></row><row><cell></cell><cell cols="3">Combined over and underexposed images (5,905 images)</cell><cell></cell><cell></cell></row><row><cell>HE [18]  *</cell><cell cols="3">16.148 0.685 16.283 0.671 16.525 0.696 16.639 0.668 17.305 0.688</cell><cell>16.580</cell><cell cols="2">0.682 2.405</cell></row><row><cell>CLAHE [74]  *</cell><cell cols="3">14.884 0.589 15.669 0.610 15.383 0.599 15.452 0.601 15.737 0.610</cell><cell>15.425</cell><cell cols="2">0.602 2.317</cell></row><row><cell>WVM [16]  *</cell><cell cols="3">14.488 0.665 15.803 0.699 15.117 0.678 15.863 0.693 16.469 0.704</cell><cell>15.548</cell><cell cols="2">0.688 2.415</cell></row><row><cell>LIME [22, 23]</cell><cell cols="3">11.154 0.591 11.828 0.610 11.517 0.607 12.638 0.628 13.613 0.653</cell><cell>12.150</cell><cell cols="2">0.618 2.432</cell></row><row><cell cols="4">HDR CNN [14] w/ RHT [65] 13.709 0.467 13.921 0.458 13.800 0.474 13.716 0.446 13.558 0.454</cell><cell>13.741</cell><cell cols="2">0.460 4.599</cell></row><row><cell>HDR CNN [14] w/ PS [11]</cell><cell cols="3">15.812 0.667 16.970 0.699 16.428 0.681 17.301 0.687 18.650 0.702</cell><cell>17.032</cell><cell cols="2">0.687 2.267</cell></row><row><cell>DPED (iPhone) [27]</cell><cell cols="3">15.134 0.609 16.505 0.636 15.907 0.622 16.571 0.627 17.251 0.649</cell><cell>16.274</cell><cell cols="2">0.629 2.903</cell></row><row><cell>DPED (BlackBerry) [27]</cell><cell cols="3">16.910 0.642 18.649 0.713 17.606 0.653 18.070 0.679 18.217 0.668</cell><cell>17.890</cell><cell cols="2">0.671 2.564</cell></row><row><cell>DPED (Sony) [27]</cell><cell cols="3">17.419 0.675 18.636 0.701 18.020 0.683 17.554 0.660 17.778 0.663</cell><cell>17.881</cell><cell cols="2">0.676 2.806</cell></row><row><cell>DPE (HDR) [10]</cell><cell cols="3">15.690 0.614 16.548 0.626 16.305 0.626 16.147 0.615 16.341 0.633</cell><cell>16.206</cell><cell cols="2">0.623 2.417</cell></row><row><cell>DPE (U-FiveK) [10]</cell><cell cols="3">16.240 0.653 16.805 0.646 16.837 0.671 16.762 0.654 16.707 0.650</cell><cell>16.670</cell><cell cols="2">0.655 2.606</cell></row><row><cell>DPE (S-FiveK) [10]</cell><cell cols="3">16.933 0.678 17.701 0.668 17.741 0.696 17.572 0.674 17.601 0.670</cell><cell>17.510</cell><cell cols="2">0.677 2.621</cell></row><row><cell>HQEC [70]  *</cell><cell cols="3">13.385 0.641 14.470 0.666 13.911 0.656 14.891 0.674 15.777 0.692</cell><cell>14.487</cell><cell cols="2">0.666 2.445</cell></row><row><cell>RetinexNet [62]</cell><cell cols="3">10.759 0.585 11.613 0.596 11.135 0.605 11.987 0.615 12.671 0.636</cell><cell>11.633</cell><cell cols="2">0.607 3.105</cell></row><row><cell>Deep UPE [60]</cell><cell cols="3">13.161 0.610 13.901 0.642 13.689 0.632 14.806 0.649 15.678 0.667</cell><cell>14.247</cell><cell cols="2">0.640 2.405</cell></row><row><cell>Zero-DCE [20]</cell><cell cols="6">11.643 0.536 12.555 0.539 12.058 0.544 12.964 0.548 13.769 0.580 12.5978 0.5494 2.865</cell></row><row><cell>Our method w/o L adv</cell><cell cols="3">19.158 0.746 20.096 0.734 20.205 0.769 18.975 0.719 18.983 0.727</cell><cell>19.483</cell><cell cols="2">0.739 2.251</cell></row><row><cell>Our method w/ L adv</cell><cell cols="3">19.114 0.743 19.960 0.723 20.080 0.763 18.868 0.709 18.864 0.719</cell><cell>19.377</cell><cell cols="2">0.731 2.247</cell></row></table><note>* 16.158 0.683 16.293 0.669 16.517 0.692 16.632 0.665 17.280 0.684 16.576 0.679 2.486 CLAHE [74] * 16.310 0.619 17.140 0.646 16.779 0.621 15.955 0.613 15.568 0.608 16.350 0.621 2.387 WVM [16] * 17.686 0.728 19.787 0.764 18.670 0.728 18.568 0.729 18.362 0.724 18.615 0.735 2.525 LIME [22, 23] * 13.444 0.653 14.426 0.672 13.980 0.663 15.190 0.673 16.177ting, our results (middle portion of table) are on par with the state-of-the-art methods. Finally, in contrast to most of the existing methods, the results in the bottom portion of the table show that our method can effectively deal with both types of exposure errors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Perceptual quality evaluation. Summary of NIQE scores<ref type="bibr" target="#b47">[48]</ref> on different low-light image datasets. In these dataset, there are no ground-truth images provided for fullreference quality metrics (e.g., PSNR). Highlights are in the same format asTable 1.</figDesc><table><row><cell>Method</cell><cell cols="5">LIME [23] NPE [61] VV [59] DICM [36] Avg.</cell></row><row><cell>NPE [61]  *</cell><cell>3.91</cell><cell>3.95</cell><cell>2.52</cell><cell>3.76</cell><cell>3.54</cell></row><row><cell>LIME [23]  *</cell><cell>4.16</cell><cell>4.26</cell><cell>2.49</cell><cell>3.85</cell><cell>3.69</cell></row><row><cell>WVM [16]  *</cell><cell>3.79</cell><cell>3.99</cell><cell>2.85</cell><cell>3.90</cell><cell>3.63</cell></row><row><cell>RNet [62]</cell><cell>4.42</cell><cell>4.49</cell><cell>2.60</cell><cell>4.20</cell><cell>3.93</cell></row><row><cell>KinD [71]</cell><cell>3.72</cell><cell>3.88</cell><cell>-</cell><cell>-</cell><cell>3.80</cell></row><row><cell>EGAN [29]</cell><cell>3.72</cell><cell>4.11</cell><cell>2.58</cell><cell>-</cell><cell>3.50</cell></row><row><cell>DBCP [38]</cell><cell>3.78</cell><cell>3.18</cell><cell>-</cell><cell>3.57</cell><cell>3.48</cell></row><row><cell>Ours w/o L adv</cell><cell>3.76</cell><cell>3.20</cell><cell>2.28</cell><cell>2.55</cell><cell>2.95</cell></row><row><cell>Ours w/ L adv</cell><cell>3.76</cell><cell>3.18</cell><cell>2.28</cell><cell>2.50</cell><cell>2.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>are shown for each result. Notice that higher PSNR and SSIM values are better, while lower PI values indicate better perceptual quality. The input images are taken from our test set.</figDesc><table><row><cell></cell><cell>PSNR = 13.52</cell><cell>PSNR = 16.56</cell><cell>PSNR = 21.56</cell><cell></cell></row><row><cell></cell><cell>SSIM = 0.728</cell><cell>SSIM = 0.766</cell><cell>SSIM = 0.824</cell><cell></cell></row><row><cell></cell><cell>PI = 1.985</cell><cell>PI = 1.795</cell><cell>PI = 1.782</cell><cell></cell></row><row><cell>Input image</cell><cell>Result of n=1</cell><cell>Result of n=2</cell><cell>Result of n=4</cell><cell>Properly exposed ref. image</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S2 :</head><label>S2</label><figDesc>Comparison with other methods for low-light image enhancement using the test set used in<ref type="bibr" target="#b59">[60]</ref>.</figDesc><table><row><cell></cell><cell>Method</cell><cell>PSNR</cell></row><row><cell></cell><cell>White-Box [26]</cell><cell>18.57</cell></row><row><cell>.</cell><cell cols="2">Distort-and-Recover [52] 20.97 Deep UPE [60] 23.04</cell></row><row><cell></cell><cell>Zero-DCE [20]</cell><cell>15.455</cell></row><row><cell></cell><cell>Ours</cell><cell>21.02</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Color and camera raw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adobe</forename></persName>
		</author>
		<idno>2020-11-12. 3</idno>
		<ptr target="https://helpx.adobe.com/ca/photoshop-elements/using/color-camera-raw.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">When color constancy goes wrong: Correcting improperly white-balanced images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Afifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The 2018 PIRM challenge on perceptual image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yochai</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roey</forename><surname>Mechrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The Laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input / output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep single image contrast enhancer from multi-exposure images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2049" to="2062" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Contextual and variational contrast enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turgay</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tardi</forename><surname>Tjahjadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3431" to="3441" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bilateral guided upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep photo enhancer: Unpaired learning for image enhancement from photographs with GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ching</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man-Hsin</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Danae</forename><surname>Dayley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brad</forename><surname>Dayley</surname></persName>
		</author>
		<title level="m">Photoshop CS5 Bible</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recovering high dynamic range radiance maps from photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIG-GRAPH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">HDR image reconstruction from a single exposure using deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Eilertsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Kronander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyorgy</forename><surname>Denes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafa?</forename><surname>Mantiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Unger</surname></persName>
		</author>
		<idno>178:1-178:15</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep reverse tone mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiro</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Mitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="177" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep bilateral learning for realtime image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Durand</surname></persName>
		</author>
		<idno>118:1-118:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woods</surname></persName>
		</author>
		<title level="m">Digital Image Processing</title>
		<imprint>
			<publisher>Addison-Wesley Longman Publishing Co., Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-reference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runmin</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Correcting over-exposure in photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">LIME: A method for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LIME: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exposure: A white-box photo postprocessing framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<idno>26:1-26:17</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DSLR-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">WESPE: Weakly supervised photo enhancer for digital cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Enlightengan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06972</idno>
		<title level="m">Deep light enhancement without paired supervision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A multiscale Retinex for bridging the gap between color images and the human observation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziaur</forename><surname>Daniel J Jobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><forename type="middle">A</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woodell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="965" to="976" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep high dynamic range imaging of dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khademi</forename><surname>Nima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="144" to="145" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A software platform for manipulating the camera imaging pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Hakki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Karaimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep Laplacian pyramid networks for fast and accurate super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Retinex theory of color vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Land</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific American</title>
		<imprint>
			<biblScope unit="volume">237</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="108" to="129" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrast enhancement based on layered difference representation of 2D histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulwoo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5372" to="5384" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised low-light image enhancement using bright channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Min</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="251" to="255" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Handheld mobile photography in very low light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orly</forename><surname>Liba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ta</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Karnad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiurui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yael</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RefineNet: Multi-path refinement networks for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">RefineNet: Multipath refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning a no-reference quality metric for single-image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">158</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tinne Tuytelaars, and Luc Van Gool. Pose guided person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient and fast real-world noisy image denoising by combining pyramid neural network and two-pathway unscented Kalman filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3927" to="3940" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exposure fusion: A simple and practical alternative to high dynamic range photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Van Reeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">High dynamic range image rendering with a Retinex-based adaptive filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Meylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2820" to="2830" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajiv</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="209" to="212" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepLPF: Deep local parametric filters for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Marza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Mcdonagh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Parisot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Slabaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Hybrid loss for learning single-image-based HDR reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenta</forename><surname>Moriwaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Yoshihashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rei</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Naemura</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.07134</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Local Laplacian filters: Edge-aware image processing with a Laplacian pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">15</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Donggeun Yoo, and In So Kweon. Distort-and-recover: Color enhancement using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Understanding exposure: How to shoot great photographs with any camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Peterson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>AmPhoto Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adaptive histogram equalization and its variations. Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stephen M Pizer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">D</forename><surname>Philip Amburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Austin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Cromartie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trey</forename><surname>Geselowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Greer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ter Haar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Romeny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Zimmerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics, and Image Processing</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="355" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Halide: A language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ragan-Kelley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?do</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saman</forename><surname>Amarasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI, 2015. 4, 9</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Real World Camera Raw with Adobe Photoshop CS5. Pearson Education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><surname>Fraser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sin-GAN: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Busting image enhancement and tonemapping algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Vonikakis</surname></persName>
		</author>
		<ptr target="https://sites.google.com/site/vonikakis/datasets" />
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2020" to="2031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Wing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Miao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3538" to="3548" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Deep Retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning to restore low-light images via decomposition-andenhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baocai</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">From fidelity to perceptual quality: A semisupervised approach for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Image correction via deep reciprocating HDR transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">DeepExposure: Learning to expose photos with asynchronously reinforced adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runsheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Automatic exposure correction of consumer photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">21</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dual illumination estimation for robust exposure correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongwei</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">High-quality exposure correction of underexposed photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganzhao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Kindling the darkness: A practical low-light image enhancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Image quality assessment: From error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Zhou Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">EEMEFN: Low-light image enhancement via edgeenhanced multi-exposure fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Contrast limited adaptive histogram equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Zuiderveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphics Gems IV</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
