<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Chen</surname></persName>
							<email>anchen@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaian</forename><surname>Chen</surname></persName>
							<email>cheneh@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhixiang</surname></persName>
							<email>zhixiangwei@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Xiao</forename><surname>Xin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Reusing the Task-specific Classifier as a Discriminator: Discriminator-free Adversarial Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Adversarial learning has achieved remarkable performances for unsupervised domain adaptation (UDA). Existing adversarial UDA methods typically adopt an additional discriminator to play the min-max game with a feature extractor. However, most of these methods failed to effectively leverage the predicted discriminative information, and thus cause mode collapse for generator. In this work, we address this problem from a different perspective and design a simple yet effective adversarial paradigm in the form of a discriminator-free adversarial learning network (DALN), wherein the category classifier is reused as a discriminator, which achieves explicit domain alignment and category distinguishment through a unified objective, enabling the DALN to leverage the predicted discriminative information for sufficient feature alignment. Basically, we introduce a Nuclear-norm Wasserstein discrepancy (NWD) that has definite guidance meaning for performing discrimination. Such NWD can be coupled with the classifier to serve as a discriminator satisfying the K-Lipschitz constraint without the requirements of additional weight clipping or gradient penalty strategy. Without bells and whistles, DALN compares favorably against the existing state-of-the-art (SOTA) methods on a variety of public datasets. Moreover, as a plug-and-play technique, NWD can be directly used as a generic regularizer to benefit existing UDA algorithms. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks (DNNs) have achieved a significant progress in many computer vision tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42]</ref>. However, the success of these methods highly depends on large amounts of annotated data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b56">57]</ref>, which is extremely time-consuming and expensive to obtain. Moreover, due to * indicates equal contribution. ? Corresponding author.  <ref type="figure" target="#fig_10">Figure 1</ref>. Illustration of different adversarial paradigms, in which G, C, and D denote the feature extractor, task-specific classifier, and discriminator, respectively. Different from typical paradigms that adopt an (a) additional classifier C (called bi-classifier) or (b) additional discriminator D, we present a different perspective for UDA and introduce a simple but effective adversarial paradigm illustrated in (c), in which the original task-specific classifier C is reused as a implicit discriminator, achieving explicit domain alignment and category distinguishment via a unified objective.</p><p>the discrepancy <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> between training data and real-world testing data, the DNN model trained on annotated data may suffer from a dramatic performance decline in testing set despite extensive annotation efforts. To address this problem, unsupervised domain adaptation (UDA) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b53">54]</ref>, which aims to transfer knowledge from a labeled source domain to an unlabeled target domain in the presence of a domain shift, has been deeply explored. Inspired by the theoretical analysis of Ben-David et al. <ref type="bibr" target="#b1">[2]</ref>, the existing UDA methods usually explore the idea of learning domain-invariant feature representations. Generally, these methods can be categorized into two branches, i.e., moment matching methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55]</ref> and adversarial learning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44]</ref>. Moment matching methods explicitly reduce the domain shift by matching a well-defined distribution discrepancy of the source and target domain features. Adversarial learning methods implicitly mitigate the domain shift by playing an adversarial min-max two-player game, which drives the generator to extract indistinguishable features to fool the discriminator.</p><p>Encouraged by the remarkable performance achieved by adversarial learning, increasingly more researchers have been devoted to developing a UDA method based on an adversarial paradigm <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>Basically, adversarial learning-based UDA methods usually follow two lines of adversarial paradigms. One line <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44]</ref> leverages the disparity of two task-specific classifiers C and C (as shown in <ref type="figure" target="#fig_10">Fig. 1(a)</ref>), which can be deemed as a discriminator, to implicitly achieve adversarial learning and improve feature transferability. This paradigm enables UDA methods to reduce the class-level domain discrepancy. However, the methods following this paradigm are prone to be affected by ambiguous predictions and thus hinder the adaption optimization. The other line <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref> directly constructs an additional domain discriminator D as shown in <ref type="figure" target="#fig_10">Fig. 1(b)</ref>, which improves the feature transferability by sufficiently confusing the cross-domain feature representations. However, the methods following this paradigm usually focus on the domain-level feature confusion, which may hurt the category-level information and thus cause mode collapse problem <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b47">48]</ref>.</p><p>To address these problems, we present a different perspective for UDA and introduce a simple but effective adversarial paradigm illustrated in <ref type="figure" target="#fig_10">Fig. 1(c)</ref>. In this paradigm, the original task-specific classifier is coupled with a novel discrepancy to serve as a discriminator/critic, which simultaneously achieves domain alignment and category distinguishment through a unified objective, enabling the model to leverage the predicted discriminative information to capture the multi-modal structures <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b26">27]</ref> of the feature distributions. Particularly, when classifier C is used for classification, it helps achieve category-level distinguishment; furthermore, when C serves as a discriminator, it achieves feature-level alignment. The novel discrepancy, called Nuclear-norm Wasserstein discrepancy (NWD), leverages the advantages of the Nuclear norm and 1-Wasserstein distance to encourage the prediction determinacy and diversity. Different from the discrepancy metrics used in existing adversarial methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56]</ref>, the NWD not only has a promising theoretical generalization bound but also has definite guidance meaning for performing discrimination, i.e., naturally giving high scores to the source domain samples and low scores to the target domain samples due to the supervised training on the source domain. Such guidance encourages the intra-class and inter-class correlations of the target domain to approach those of the source domain. Moreover, in contrast to the existing Wasserstein discrepancy used in recent work <ref type="bibr" target="#b44">[45]</ref>, the NWD enables the adversarial UDA paradigm to satisfy the K-Lipschitz constraint without the need to set up an additional weight clipping <ref type="bibr" target="#b0">[1]</ref> or gradient penalty <ref type="bibr" target="#b16">[17]</ref>.</p><p>Based on the introduced paradigm, we propose a discriminator-free adversarial learning network (DALN), which achieves adversarial UDA classification without ex-plicit domain discriminator. Benefiting from the definite guidance of the NWD, the proposed DALN converges rapidly and achieves competitive prediction determinacy and diversity. Note that, the DALN is considerably different from recent approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b55">56]</ref> that integrate the discriminator into the classifier. DALN directly reuses the original task-specific classifier without requiring any additional components, making it quite simple and efficient. Extensive experiments on a variety of datasets demonstrate that the proposed DALN outperforms the existing state-of-the-art (SOTA) methods. Moreover, we show that the proposed NWD is general and plug-and-play, which can be used as a regularizer to benefit the existing methods, which helps them achieve more competitive performance. The main contributions of this work are summarized as follows:</p><p>? We present a different perspective for UDA by introducing a simple yet effective adversarial paradigm, in which the original task-specific classifier is reused as a discriminator. Based on this, we propose a new UDA method, namely DALN, which can leverage the predicted discriminative information for sufficient feature alignment. ? We introduce a new discrepancy, termed NWD, which has a theoretical generalization bound and definite guidance meaning. Such discrepancy enables the implicitly constructed discriminator to satisfy the K-Lipschitz constraint without the requirements of additional weight clipping and gradient penalty strategies. ? Without bells and whistles but only a few lines of code, the proposed method achieves highly competitive performance on various public datasets. By taking the proposed NWD as a regularizer for existing methods, these methods can achieve more competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>The existing UDA methods can be mainly divided into two categories, i.e., moment matching methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b54">55]</ref> and adversarial learning methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>. Moment Matching Methods. Moment matching methods learn domain-invariant feature representations by matching a well-defined moment-based distribution discrepancy <ref type="bibr" target="#b56">[57]</ref> across domains. Typically, DDC <ref type="bibr" target="#b48">[49]</ref> attempted to explicitly align the learned feature distributions across domains by minimizing the maximum mean discrepancy (MMD). Later, methods in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28]</ref> improved DDC by performing alignment with multi-kernel maximum mean discrepancy (MK-MMD) and joint maximum mean discrepancy (JMMD), respectively. In addition, MDD <ref type="bibr" target="#b54">[55]</ref> proposed margin disparity discrepancy (MDD) to reduce the distribution discrepancy. Adversarial Learning Methods. Inspired by generative adversarial network (GAN) <ref type="bibr" target="#b15">[16]</ref>, adversarial learning methods learn domain-invariant features via a min-max two-player  <ref type="figure" target="#fig_11">Figure 2</ref>. An overview of the adversarial paradigm in the form of DALN, which consists of a feature extractor G and a task-specific classifier C. L cls is used to guarantee a low source risk for the source domain, and L nwd is used to empirically estimate the NWD that can be coupled with classifier C to implicitly serve as a discriminator. The gradient reverse layer is used to help perform the adversarial learning.</p><p>game. As one of the earliest attempts, DANN <ref type="bibr" target="#b12">[13]</ref> introduced an additional discriminator to distinguish the features generated by the feature extractor, which successfully achieves the domain-level adaptation. The success of the DANN exhibits the ability to improve UDA with the GAN model. Later, FGDA <ref type="bibr" target="#b13">[14]</ref> leveraged a discriminator to distinguish the gradient distribution of features, which achieved better performance for reducing domain discrepancy. Inspired by the conditional GAN <ref type="bibr" target="#b30">[31]</ref>, methods in <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b36">37]</ref> combined the predicted discriminative information with learned features to improve feature alignment. Additionally, DADA <ref type="bibr" target="#b47">[48]</ref> attempted to couple the task-specific classifier with the domain discriminator to align the joint distributions of two domains. Although these methods successfully learn domain-invariant features, they cannot guarantee an appropriate divergence used for the discriminator when the support sets of two distributions do not overlap with each other <ref type="bibr" target="#b0">[1]</ref>.</p><p>In addition to the methods adopting an additional discriminator, some studies attempted to use two task-specific classifiers (called bi-classifier), in which the disparity of two task-specific classifiers can be deemed as a discriminator <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>, to implicitly achieve the adversarial learning. Representatively, MCD <ref type="bibr" target="#b43">[44]</ref> simply used the L1 distance to measure the intra-class divergence of two classifiers. SWD <ref type="bibr" target="#b20">[21]</ref> proposed using sliced Wasserstein discrepancy instead of L1 distance to obtain a more geometrically meaningful intra-class divergence. CGDM <ref type="bibr" target="#b10">[11]</ref> additionally introduced the cross-domain gradient discrepancy to further alleviate the domain discrepancy. Although these methods have achieved considerable improvements in reducing domain discrepancy, most of them consider only the intra-class divergence between predictions, which may result in ambiguous predictions. Different from the aforementioned methods adopting an additional discriminator or classifier, we reuse the original task-specific classifier by coupling it with the designed NWD, implicitly constructing a discriminator/critic satisfying the K-Lipschitz constraint without the requirements of additional weight clipping or gradient penalty strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Recap of Preliminary Knowledge</head><formula xml:id="formula_0">Given a labeled source domain set {(x s i , y s i )} N s i=1 with N s samples drawn from source domain D S , where x s i ? X s , y s</formula><p>i ? Y s , and label y s covers k classes, and an unlabeled domain target set {x t i } N t i=1 with N t samples drawn from target domain D T , where x t i ? X t , the goal of this work is to learn a deep UDA model for learning domain-invariant representations and achieving reliable predictions on the target domain. This model consists of a feature generator G (?) that maps the input data to the features f ? R d , i.e., f s = G (x s ) and f t = G (x t ), and a task-specific classifier C (?) that generates corresponding predictions p ? R k , i.e., p s = C (f s ) and p t = C (f t ). To this end, the existing adversarial UDA approaches usually take an additional discriminator or classifier. Typically, many popular methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref> use an additional discriminator D (?) to achieve adversarial UDA by optimizing object classification loss L cls and domain adversarial loss L adv :</p><formula xml:id="formula_1">L cls = E (x s i ,y s i )?DS L ce (C (G (x s i )) , y s i ),<label>(1)</label></formula><formula xml:id="formula_2">L adv = E G(x s i )?Ds log [D (G (x s i ))] + E G(x t i )?Dt log 1 ? D G x t i ,<label>(2)</label></formula><p>whereD s andD t denote the induced feature distributions of D S and D T , respectively, and L ce (?, ?) is the cross-entropy loss function. However, we find that the original task-specific classifier C has an implicit discriminative ability for the source domain and target domain, and can be directly used as a discriminator (see Sec. 3.2). Inspired by this observa-tion, as shown in <ref type="figure" target="#fig_11">Fig. 2</ref>, we propose a simple yet effective adversarial paradigm for adversarial UDA: reusing the taskspecific classifier as a discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Reusing the Classifier as a Discriminator</head><p>Motivation Re-clarification. As we claimed before, the original task-specific classifier has an implicit discriminative ability for the source domain and the target domain. <ref type="figure" target="#fig_3">Fig. 3</ref> presents the self-correlation matrices of the predictions on the source and target domains based on a model trained with the source-only data. For the source domain, benefiting from the supervised training, the values of the selfcorrelation matrix are concentrated on the main diagonal. In contrast, for the target domain, the prediction generates larger values on the off-diagonal elements due to the lack of supervision. Therefore, the intra-class and inter-class correlations represented in the self-correlation matrix are capable of constructing the adversarial critic.    Rethinking the Intra-class and Inter-class Correlations. Given a prediction matrix Z ? R b?k predicted by C that contains the prediction probabilities of k categories multiplied by b samples, the self-correlation matrix R ? R k?k can be calculated by R = Z T Z, where the prediction matrix</p><formula xml:id="formula_3">Z = C (f ) satisfies k j=1 Z i,j = 1 ?i ? 1 . . . b Z i,j ? 0 ?i ? i . . . b, j ? 1 . . . k.<label>(3)</label></formula><p>For a self-correlation matrix R, the main diagonal elements represent the intra-class correlation and the off-diagonal elements denote the inter-class correlation or confusion <ref type="bibr" target="#b18">[19]</ref>. For convenient, in this work, we define the overall intra-class correlation as I a and the overall inter-class correlation as I e :</p><formula xml:id="formula_4">I a = k i,j=1 R ij I e = k i =j R ij .<label>(4)</label></formula><p>For the source domain, the prediction contributes to a large I a and a small I e ; while for the target domain, the prediction generally produces a relatively small I a and large I e due to the lack of supervised training. Thus, I a ? I e can be used to represent the domain discrepancy. According to <ref type="bibr">Equation 3</ref>, I a and I e satisfy I a + I e = b. Meanwhile, I a is equal to the Frobenius norm of prediction matrix Z, i.e, I a = Z F . Thus, we have I a ? I e = 2 Z F ? b. Z is predicted via the classifier C, so we can use 2 C F ? b as a correlation critic function, which naturally gives high scores for the source domain samples and low scores for the target domain samples due to the supervised training on source domain. Moreover, considering weight 2 and bias b are both constants, the C F can be directly used as a correlation critic function.</p><p>From Correlations Critic to 1-Wasserstein Distance. Inspired by the WGAN <ref type="bibr" target="#b0">[1]</ref>, a straightforward idea is to introduce an additional discriminator D to learn a K-Lipschitz critic function h expected to give high scores to source representations f ?D s and low scores to the target representations f ?D t , and measure the 1-Wasserstein distance</p><formula xml:id="formula_5">W 1 D s ,D t between two feature distributionsD s ,D t by W 1 D s ,D t = sup h L ?K E f ?Ds [h (f )] ? E f ?Dt [h (f )] ,<label>(5)</label></formula><p>where ? L denotes the Lipschitz semi-norm <ref type="bibr" target="#b51">[52]</ref>, and K denotes the Lipschitz constant. But, as we claimed above, C F has exactly definite critic meaning to serve as D. Then, the domain discrepancy can be written as</p><formula xml:id="formula_6">W F = sup C F L ?K ED s [ C (f ) F ] ? ED t [ C (f ) F ] ,<label>(6)</label></formula><p>where W F is short for W F D s ,D t , which denotes the Frobenius norm-based 1-Wasserstein distance of two domain distributions. In this way, we can achieve explicit domain alignment and category distinguishment through a unified objective, contributing to leveraging the predicted discriminative information for capturing the multi-modal structures of the feature distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Adversarial Learning with the NWD</head><p>From Frobenius Norm to Nuclear Norm. The constructed discriminator/critic D = C F can perform adversarial training with the generator G, which helps achieve transferable and discriminative representations while improving the prediction determinacy. However, adversarial learning based on the Frobenius-norm 1-Wasserstein distance may reduce the prediction diversity because it tends to push the category with a small number of samples to the neighbouring category containing large amounts of samples far from the decision boundary <ref type="bibr" target="#b8">[9]</ref>. Inspired by recent works on the Nuclear norm <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b46">47]</ref>, which has been demonstrated to be bound with the Frobenius norm, we attempt to replace the Frobenius norm ? F with the Nuclear norm ? * because maximizing Z * means maximizing the rank of Z when the ? F is nearby ? b <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, which improves the prediction diversity. Therefore, the domain discrepancy can be rewritten as</p><formula xml:id="formula_7">W N = sup C * L ?K ED s [ C (f ) * ] ? ED t [ C (f ) * ] ,<label>(7)</label></formula><p>where W N is short for W N D s ,D t , which denotes the Nuclear-norm 1-Wasserstein discrepancy (NWD) of two domain distributions. Then, our discriminator can be rewritten as D = C * . When classifier C is used for classification, it helps achieve category-level distinguishment, but when C serves as a discriminator, it achieves feature-level alignment. Note that our classifier consists of a fully connected layer and a softmax activation function. It can be demonstrated that all the components of our implicit discriminator satisfy the K-Lipschitz constraint (see supplementary material for the proof), which enables the proposed model to be trained without the requirements of additional weight clipping and gradient penalty strategies. Therefore, we can approximately estimate the empirical NWD? N by maximizing the domain critic loss L nwd :</p><formula xml:id="formula_8">L nwd x s , x t = 1 N s Ns i=1 D (G (x s i )) ? 1 N t Nt j=1 D G x t i , (8) W N = max D L nwd x s , x t .<label>(9)</label></formula><p>Adversarial Learning for DALN. In this work, we build a DALN consisting of a generator G based on a pretrained ResNet and a classifier C constructed with a fully connected layer and a softmax layer. To avoid tedious alternating updates for the DALN, a gradient reverse layer (GRL) <ref type="bibr" target="#b12">[13]</ref>, which does not include the above mentioned gradient penalty or weight clipping, is used to help achieve updating within one back propagation. In this way, DALN can be trained by playing the min-max game as</p><formula xml:id="formula_9">min G max C L nwd x s , x t .<label>(10)</label></formula><p>Moreover, to ensure the fidelity of UDA classification, we need to guarantee a low source risk for the source domain. Therefore, generator G and classifier C should also be optimized by minimizing the supervised classification loss L cls for the source domain as</p><formula xml:id="formula_10">L cls (x s , y s ) = 1 N s ns i=1 L ce (C (G (x s i )) , y s i ) .<label>(11)</label></formula><p>In short, the overall loss used to optimize the classification model can be written as</p><formula xml:id="formula_11">min C,G L cls (x s , y s ) + ? max C L nwd x s , x t ,<label>(12)</label></formula><p>where ? is used to balance L cls and L nwd . In this work, ? is set to 1. With the help of adversarial learning, the DALN learns transferable and discriminative representations while promising the prediction determinacy and diversity. Generalization Bound. Here, we present the theoretical guarantees for the proposed method. Following <ref type="bibr" target="#b1">[2]</ref>, we consider a binary classification instance. Then, let F (f ? F) denote a fixed representation space and C : F ? [0, 1] be a family of source classifiers, where C belongs to hypothesis space H. We assume that the risk of C on the source domain is described as</p><formula xml:id="formula_12">? s (C) = E f ?Ds [C (f ) = y],</formula><p>wher? D s is the feature distribution induced by the data distribution of source domain D S and y is the label corresponding to the induced feature f . Moreover, given two classifiers C 1 , C 2 ? H, we define the risk of these two classifiers on the source domain as ? s (C 1 ,</p><formula xml:id="formula_13">C 2 ) = E f ?Ds [C 1 (f ) = C 2 (f )].</formula><p>In the same way, we define the risk on the target domain, i.e, ? t (C) and ? t (C 1 , C 2 ). Then, the ideal joint hypothesis is written as C * = arg min C ? s (C) + ? t (C), which can be used to minimize the combined risk on the source and target domains. Therefore, according to <ref type="bibr" target="#b1">[2]</ref>, the probabilistic bound of ? t (C) can be written as</p><formula xml:id="formula_14">? t (C) ? ? s (C) + |? s (C, C * ) ? ? t (C, C * )| + ? * ,<label>(13)</label></formula><p>where ? * = ? s (C * ) + ? t (C * ) is a sufficiently small constant representing the ideal combined risk. Thus, the goal of UDA classification is to reduce the domain discrepancy term |? s (C, C * ) ? ? t (C, C * )|.</p><p>Lemma 1. Let ? s , ? t ? P (F) denote the probability measures of the source and target domain features, ? (f s , f t ) be the cost of transporting a unit of material from location</p><formula xml:id="formula_15">f s satisfying f s ? ? s to location f t satisfying f t ? ? t , W 1 (? s , ? t )</formula><p>represent the NWD, and K denote a Lipschitz constant. Given a family of classifiers C ? H 1 and a ideal classifier C * ? H 1 satisfying the K-Lipschitz constraint, where H 1 is a subspace of H, the following holds for every C, C * ? H 1 .</p><formula xml:id="formula_16">|? s (C, C * ) ? ? t (C, C * )| 2KW 1 (? s , ? t ) .<label>(14)</label></formula><p>Theorem 1. Based on Lemma 1, for every C ? H 1 , the following holds</p><formula xml:id="formula_17">? t (C) ? ? s (C) + 2KW 1 (? s , ? t ) + ? * ,<label>(15)</label></formula><p>where ? * = ? s (C * ) + ? t (C * ) is the risk of ideal joint hypothesis, which is a sufficiently small constant. (b) VisDA-2017.</p><p>Method Avg ResNet-101 <ref type="bibr" target="#b17">[18]</ref> 52.4 WDGRL ? (18) <ref type="bibr" target="#b44">[45]</ref> 61.3 MCD(18) <ref type="bibr" target="#b43">[44]</ref> 71.9 BSP(19) <ref type="bibr" target="#b6">[7]</ref> 75.9 SWD(19) <ref type="bibr" target="#b20">[21]</ref> 76.4 BNM(20) <ref type="bibr" target="#b7">[8]</ref> 70.4 GVB-GD ? (20) <ref type="bibr" target="#b9">[10]</ref> 77.2 DADA(20) <ref type="bibr" target="#b47">[48]</ref> 79.8 TSA(21) <ref type="bibr" target="#b23">[24]</ref> 78.6 SCDA ? (21) <ref type="bibr" target="#b24">[25]</ref> 79  Therefore, the risk of the target domain can be bounded by the risk of the source domain and the introduced NWD, providing theoretical guarantees for the proposed approach. Limited by space, all the proofs and more details about the empirical measure of the target risk are provided in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Regularizer to Existing UDA Methods</head><p>The proposed NWD can be easily integrated into existing methods to improve the prediction determinacy and diversity. Specifically, a gradient reverse layer is first added to the original task-specific classifier. Subsequently, the taskspecific classifier with the introduced NWD can serve as a discriminator, which performs adversarial learning with the feature extractor. Formally, assuming the original loss L ori of the model written as L ori = L cls +L spe , where L cls is the standard supervised classification loss as that of the proposed method and L spe is the special loss used in these methods, the reconstructed loss L rec can be described as</p><formula xml:id="formula_18">L rec = L cls +L spe + ?L nwd ,<label>(16)</label></formula><p>where ? is the balance weight. For convenience, in our experiments, the values of ? for all other methods are set to 0.01. The results of taking the proposed discrepancy as a regularizer to benefit other UDA algorithms are presented in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the proposed DALN and compare it with the SOTA methods for UDA classification. Additionally, we evaluate the effectiveness of NWD as a regularizer to benefit existing methods including DANN <ref type="bibr" target="#b12">[13]</ref>, CDAN <ref type="bibr" target="#b26">[27]</ref>, MDD <ref type="bibr" target="#b54">[55]</ref>, and MCC <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>Dataset Description. We use four datasets including Office-Home <ref type="bibr" target="#b50">[51]</ref>, Office-31 <ref type="bibr" target="#b42">[43]</ref>, ImageCLEF <ref type="bibr" target="#b3">[4]</ref>, and VisDA-2017 <ref type="bibr" target="#b38">[39]</ref> to perform the comparison experiments. Office-Home is a large-scale dataset that includes 15500 images and 65 categories. This dataset has four extremely different domains, i.e., Art (A), Clip Art (C), Product (P), and Real-World (R). VisDA-2017 is a large-scale synthetic-to-real dataset that has two domains (synthetic (S) and real (R)). The dataset contains over 280K images across 12 classes.</p><p>Office-31 has a total number of 4110 images, which includes three domains, i.e., Amazon (A), Webcam (W), and DSLR (D); and each domain contains 31 categories. ImageCLEF consists of three domains derived from three public datasets: Caltech (C), ImageNet (I), and Pascal (P). Each domain contains 12 categories and each category has 50 images. Implementation Details. The proposed method is implemented based on the PyTorch <ref type="bibr" target="#b34">[35]</ref> framework running on a GPU (Tesla-V100). The SGD optimizer is used to train the model with a moment of 0.9, a weight decay of 1e-3, a batch size of 36, and a cropped image size of 224 ? 224. The initial learning rate of classifier C is set to 5e-3, which is 10 times larger than that of feature extractor G. Other details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison Results</head><p>Results on Office-Home are shown in <ref type="table" target="#tab_2">Table 1</ref>(a). Compared with SOTA methods, the proposed method achieves dramatic improvements in terms of classification accuracy. Particularly, in the case of domains suffering from large shifts and extremely unbalanced classes, e.g., A?R and C?R, the proposed method achieves 2.9% and 2.2% improvements compared to the existing SOTA methods. Moreover, by integrating the proposed NWD into the DANN, the average accuracy is improved by 7.9%. Combining the proposed NWD with MCC, it achieves SOAT performance of 72.6%, attaining 3.2% improvements. Dramatic improvements are obviously exhibited in P?R, C?A, and C?P tasks. These obtained gains come from the paradigm leveraging the predicted discriminative information and the introduced NWD encouraging the prediction determinacy and diversity. Results on VisDA-2017 are displayed in <ref type="table" target="#tab_2">Table 1</ref>(b). Despite the tremendous domain shift existing in synthetic and real data, the DALN achieves an average accuracy of 80.6%, outperforming the existing SOTA methods. Combining the proposed NWD with other methods, the performances of these methods are substantially improved by 22.6%, 7.5%, 5.2%, and 4.9% for the DANN, CDAN, MDD, and MCC, respectively. In particular, with the help of the proposed NWD, MCC achieves SOTA accuracy of 83.7%, demonstrating the effectiveness of the proposed method. Results on Office-31 are presented in <ref type="table" target="#tab_4">Table 2</ref>(a). The proposed DALN achieves SOTA performances in five adaptation sub-tasks, and attains the best performance on the average accuracy. Particularly, compared with WDGRL <ref type="bibr" target="#b44">[45]</ref>, which uses 1-Wasserstein distance with an additional discriminator, the proposed DALN dramatically improves the average accuracy by 11.8%. Additionally, taking the NWD as a regularizer, the typical methods can be improved by at least 0.6%, and the average accuracy of DANN is even improved by 4.9%. We note that the improvements are evidently achieved in the task of adapting a domain with a small number of samples (e.g., D and W) to a domain (e.g., A) containing large amounts of samples. These results occur because the proposed method encourages the prediction determinacy and diversity, which are highly important in such cases. Results on ImageCLEF-2014 are provided in <ref type="table" target="#tab_4">Table 2</ref>(b). Without bells and whistles, the proposed method achieves an average accuracy of 89.7%, which is same as the most competitive method CKB-MMD <ref type="bibr" target="#b29">[30]</ref>. Moreover, the proposed method can be used as a regularizer, which is capable of improving the performance of the existing methods and thus contributes to SOTA performances. Specifically, the proposed method enables MCC to compare favorably against the SOTA methods. These results demonstrate that the proposed method is still highly effective in the case of all the domains containing the same samples and categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Insight Analysis</head><p>Here, we provide insight analyses for the proposed DALN and NWD. Limited by space, more detailed analyses regarding toy experiment, Proxy A distance, self-correlation matrix, convergence, and trade-off parameters (? and ?) are provided in supplementary material. Confusion Matrix. The comparison of the confusion matrices is shown in <ref type="figure" target="#fig_7">Fig. 4</ref>. The figure shows that the model trained on the source-only data suffers from severe class confusion. The DANN focuses on domain-level feature adaptation, but ignores feature discriminability, which results in misclassification for some categories (e.g., computers are misclassified as monitors and projectors). In contrast, benefiting from the introduced paradigm, DALN generates large values for the main diagonal elements of the confusion matrix. Moreover, by integrating the NWD into DANN and MDD, the off-diagonal elements of their confusion matrix are considerably decreased, demonstrating the effectiveness of NWD <ref type="table">.  back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser  trash_can   Predicted label   back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser</ref>   <ref type="table">Predicted label   back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser</ref>   <ref type="table">bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser  trash_can   Predicted label   back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser</ref>   <ref type="table">bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser  trash_can   Predicted label   back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser</ref>   <ref type="table">bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser  trash_can   Predicted label   back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser</ref>  Determinacy. To evaluate the determinacy, we calculate the ratio of the correctly classified samples that have high prediction certainty. Here, we consider task Ar?Rw of Office-Home. The prediction probability in the range of 0.9 to 1 is regarded as a high certainty prediction. As shown in <ref type="figure">Fig. 5(a)</ref>, the model trained on the source-only data nearly cannot generate high certainty prediction. DANN and MDD considerably improve the ratio of high certainty prediction, but the improvements achieved by these methods cannot compete with those achieved by the proposed DALN. By taking the NWD as a regularizer, the ratios of high certainty prediction for DANN and MDD are improved, demonstrating the effectiveness of NWD in improving the determinacy. Diversity. As shown in <ref type="figure">Fig. 5(b)</ref>, we compute the number of correctly classified samples for some typical categories that have a large or small number of samples. Compared with other methods, the proposed DALN correctly classifies more samples in the categories that have a small number of samples. Moreover, by adopting the NWD as a regularizer for the DANN and MDD, these methods achieve considerable improvements for categories that have a small number of samples. These results demonstrate the effectiveness of the NWD in improving prediction diversity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work, we present a simple yet effective adversarial paradigm, i.e., reusing the task-specific classifier as a discriminator. To achieve this paradigm, we designed a new discrepancy NWD that has definite guidance meaning and correspondingly built a discriminator-free adversarial UDA model, i.e., DALN, which learns transferable and discriminative representations while promising prediction determinacy and diversity. Moreover, we demonstrated that the proposed NWD can be used as a plug-and-play regularizer to the existing methods, which helps these methods achieve more competitive performance. Extensive experiments on a variety of datasets demonstrate the effectiveness and superiority of the proposed method. Reusing the Task-specific Classifier as a Discriminator:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head><p>Discriminator-free Adversarial Domain Adaptation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material provides more details that are not presented in the main paper due to space limitations. In the following sections, we first provide the proof of the implicitly constructed discriminator D = C * satisfying the K-Lipschitz constraint. Then, we prove that the expected target risk can be bounded by the expected and empirical measures of the Nuclear-norm 1-Wasserstein discrepancy (NWD) on the source and target domains. Finally, more implementation details, experimental results, and insight analysis are presented, including the detailed comparisons on the VisDA-2017 dataset, the extra comparisons on the Domain-Net <ref type="bibr" target="#b37">[38]</ref> dataset, and analyses regarding toy experiments, Proxy A-distance, self-correlation matrix, convergence, and trade-off parameters (? and ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. K-Lipschitz constraint</head><p>To prove that the implicitly constructed discriminator D = C * satisfies the K-Lipschitz constraint, where the classifier C consists of a fully connected layer L c (?) and a softmax function S m (?), we first analyze L c (?) and S m (?), respectively.  </p><formula xml:id="formula_19">h L = sup m1 =m2 d N (h (m 1 ) , h (m 2 )) d M (m 1 , m 2 ) ? K.<label>(17)</label></formula><formula xml:id="formula_20">, f 2 ? F , if f 1 = f 2 , we have |h (f 1 ) ? h (f 2 )| = K |f 1 ? f 2 | = 0, and if f 1 = f 2 , we have |L c (f 1 ) ? L c (f 2 )| = |(W f 1 + b) ? (W f 2 + b)| = |W (f 1 ? f 2 )| .</formula><p>Meanwhile, the spectral norm of the matrix W induced by |f | is defined as</p><formula xml:id="formula_21">W 2 = max f =0 |W f | |f | = ? max ,</formula><p>where ? max is the maximum singular value obtained by singular value decomposition (SVD) on the matrix W . Therefore, according to Definition 1, the Lipschitz constant K is W 2 . Additionally, the Frobenius norm of the matrix W is defined as</p><formula xml:id="formula_22">W F = k i=1 d j=1 W 2 i,j = r i=1 ? 2 i ,</formula><p>where r = min {k, d}, ? i denotes the i-th singular value.</p><p>Thus, for every f 1 , f 2 ? F , we have</p><formula xml:id="formula_23">|L c (f 1 ) ? L c (f 2 )| = |W (f 1 ? f 2 )| W 2 |f 1 ? f 2 | = K |f 1 ? f 2 | W F |f 1 ? f 2 | .</formula><p>According to Proposition 1, W F can be an upper bound of the Lipschitz constant K. As a widely used strategy, the weight decay (implemented with Frobenius norm regularization), which improves the generalization performance of a DNN model through minimizing an additional term ? 0 W 2 F (where ? 0 is a trade-off parameter), can simultaneously enforce the fully connected layer to satisfy the K-Lipschitz constraint.  Proof. Let p = S m (o), where p i is in the range from 0 to 1, for ?i ? 1 . . . k, then, we have</p><formula xml:id="formula_24">p i = e oi k j=1 e oj ?i ? 1 . . . k k i=1 p i = 1 ?i ? 1 . . . k.</formula><p>For simplicity, we denote k j=1 e oj as ?. Then, the Jacobian matrix can be written as</p><formula xml:id="formula_25">J = ? ? ? ?p1 ?o1 ? ? ? ?p1 ?o k . . . . . . . . . ?p k ?o1 ? ? ? ?p k ?o k ? ? ? .</formula><p>According to the quotient rule, when i = j, we have</p><formula xml:id="formula_26">?p i ?o j = e oi ? ? e oj e oi ? 2 = e oi ? ? ? ? e oj ? = p i (1 ? p j ) .</formula><p>When i = j, we have</p><formula xml:id="formula_27">?p i ?o j = 0 ? e oj e oi ? 2 = ? e oi ? ? e oj ? = ?p i p j .</formula><p>Therefore, we have</p><formula xml:id="formula_28">J i,j = p i (1 ? p j ) i = j ?p i p j i = j.</formula><p>Therefore, given p i in the range from 0 to 1, J i,j can be bounded by 1. Thus, according to Definition 2, softmax function satisfies the 1-Lipschitz constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generalization Bound</head><p>In this section, we first provide the proof for Lemma 2 and Theorem 2. Then, based on Theorem 2, we prove that the expected target risk can be bounded by the expected and empirical measures of NWD on the source and target domains. </p><formula xml:id="formula_29">|? s (C, C * ) ? ? t (C, C * )| 2KW 1 (? s , ? t ) .<label>(18)</label></formula><p>Proof. For every C, C * ? H 1 satisfying the K-Lipschitz constraint, according to the triangle inequality, we have</p><formula xml:id="formula_30">|C (f s ) ? C * (f s )| C (f s ) ? C f t + C f t ? C * (f s ) C (f s ) ? C f t + C f t ? C * f t + C * (f s ) ? C * f t .</formula><p>Therefore, for every f 1 , f 2 ? F, the following holds,</p><formula xml:id="formula_31">||C (f s ) ? C * (f s )| ? |C (f t ) ? C * (f t )|| ? (f s , f t ) 2K.</formula><p>For simplicity, we denote the discrepancy term |? s (C, C * ) ? ? t (C, C * )| as dis. Thus, given f 1 , f 2 ? F, and C denotes the labeling function C : F ? [0, 1], for every C, C * ? H 1 , we have</p><formula xml:id="formula_32">dis = E ?t C f t ? C * f t ? E ?s [|C (f s ) ? C * (f s )|] sup C L K E ?s [C (f s )] ? E ?t C f t = 2KW 1 (? s , ? t ) .</formula><p>Theorem 2. Based on Lemma 2, for every C ? H 1 , the following holds</p><formula xml:id="formula_33">? t (C) ? s (C) + 2KW 1 (? s , ? t ) + ? * ,<label>(19)</label></formula><p>where ? * = ? s (C * ) + ? t (C * ) is the risk of ideal joint hypothesis, which is a sufficiently small constant.</p><p>Proof. Based on Lemma 2, we have</p><formula xml:id="formula_34">? t (C) ? t (C * ) + ? t (C * , C) = ? t (C * ) + ? s (C, C * ) + ? t (C * , C) ? ? s (C, C * ) = ? t (C * ) + ? s (C, C * ) + ? t (C, C * ) ? ? s (C, C * ) ? t (C * ) + ? s (C, C * ) + 2KW 1 (? s , ? t ) ? t (C * ) + ? s (C) + ? s (C * ) + 2KW 1 (? s , ? t ) = ? s (C) + 2KW 1 (? s , ? t ) + ? * .</formula><p>Therefore, the expected target risk can be bounded by the expected measures of the NWD on the source and target domain distributions. Furthermore, we show the convergence of the empirical measures to the expected measures of the NWD on the source and target domain samples. </p><formula xml:id="formula_35">W (? , ?) 2 ? H (? |?),<label>(20)</label></formula><p>where</p><formula xml:id="formula_36">H (? |?) = d? d? log d? d? d?,<label>(21)</label></formula><p>holds for any probability measure ? .</p><p>Lemma 3. (Theorem 2.1 of <ref type="bibr" target="#b2">[3]</ref>; Theorem 1 of <ref type="bibr" target="#b40">[41]</ref>; Theorem 2 of <ref type="bibr" target="#b44">[45]</ref>) Let ? ? P (F) be a probability measure</p><formula xml:id="formula_37">in representation space F, where F is a subspace of R d , satisfying T 1 (? * ) inequality. Let? = 1 N N i=1 ? fi be its as- sociated empirical measure defined on a sample set {f i } N i=1</formula><p>of size N drawn i.i.d from ?. Then for any d &gt; d and ? &lt; ? * , there exists some constant N 0 depending on d and some square exponential moment of ? such that for any &gt; 0 and N N 0 max ?(d+2) , 1 , the following holds</p><formula xml:id="formula_38">P [W 1 (?,?) &gt; ] exp ? ? 2 N 2 ,<label>(22)</label></formula><p>where d , ? can be calculated explicitly. Nt Nt i=1 ? f t i be the associated empirical measures. Then for any d &gt; d and ? &lt; ? * , there exists some constant N 0 depending on d such that for any ? &gt; 0 and min (N s , N t ) N 0 max ? ?(d +2) , 1 with probability at least 1 ? ? for all C, the following holds</p><formula xml:id="formula_39">? t (C) ? s (C) + 2KW 1 (? s ,? t ) + ? * + 2K 2 log 1 ? ? 1 N s + 1 N t ,<label>(23)</label></formula><p>where ? * = ? s (C * )+? t (C * ) is a sufficiently small constant representing the ideal combined risk.</p><p>Proof. Based on Lemma 2 and Lemma 3, we have</p><formula xml:id="formula_40">? t (C) ? s (C) + 2KW 1 (? s , ? t ) + ? * ? s (C) + 2KW 1 (? s ,? s ) + 2KW 1 (? s , ? t ) + ? * ? s (C) + 2KW 1 (? s ,? t ) + 2KW 1 (? t , ? t ) + ? * + 2K 2 log 1 ? N s ? ? s (C) + 2KW 1 (? s ,? t ) + ? * + 2K 2 log 1 ? ? 1 N s + 1 N t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation details</head><p>The proposed method is implemented based on the Py-Torch framework running on a GPU (Tesla-V100 32 GB). Following the existing methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>, the ResNet50 or ResNet101 pretrained on the ImageNet is used as the feature extractor G, in which we use the bottleneck layer used in <ref type="bibr" target="#b18">[19]</ref> to replace the last fully connected layer. Classifier C is a fully connected layer depending on the specific task . The setting of the gradient reverse layer follows that of <ref type="bibr" target="#b12">[13]</ref>. The SGD optimizer is used to train the model with a moment of 0.9, a weight decay of 1e-3, a batch size of 36, and a cropped image size of 224?224. The initial learning rate of classifier C is set to 5e-3, which is 10 times larger than that of the feature extractor G. Additionally, to facilitate model training, we use the annealing strategy <ref type="bibr" target="#b11">[12]</ref> for the decay of the learning rate. One can refer to our provided code for more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Detailed results on VisDA-2017</head><p>The detailed results on VisDA-2017 are shown in <ref type="table">Table 3</ref>. The proposed DALN achieves an average accuracy of 80.6 %, outperforming the existing SOTA methods. Combining the proposed NWD with other methods, the performances of these methods are substantially improved by 22.6%, 7.5%, 5.2%, and 4.9% for the DANN, CDAN, MDD, and MCC, respectively. In particular, the improvements are evidently exhibited on categories including bus, car, person, and truck. These results come from the proposed NWD helping these methods distinguish some confusing class pairs such as bus and car, and train and truck.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Extra experiments on DomainNet</head><p>We further conduct an experiment on DomainNet <ref type="bibr" target="#b37">[38]</ref> (containing 0.6 million images, 345 categories, and 6 subdomains), which consists of 30 sub-experiments. And the batch size is 64 for DomainNet. As the results shown in <ref type="table" target="#tab_10">Table 4</ref>, DALN outperforms the previous SOTA methods impressively in terms of the average accuracy. Such encouraging results demonstrate the superiority of DALN for processing complex datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Insight Analysis</head><p>Toy examples. We perform toy experiments to discuss the inter twinning moons 2D problem <ref type="bibr" target="#b35">[36]</ref>, helping analyze the learned decision boundary. The presented examples consider two cases. One case is generating balanced samples for both the source and target domains, and the other studies the class imbalance problem by reducing the samples of one class in the target domain. Specifically, for the first case, we generate 300 samples for two classes of the source samples labeled 0 and 1, and each class has 150 samples. As shown <ref type="table">Table 3</ref>. Classification accuracy (%) on VisDA-2017 for unsupervised domain adaptation (using ResNet-101 as the backbone). ? denotes that the results are reproduced using the publicly released code. The best accuracy is indicated in bold red and the second best accuracy is indicated in undelined blue.  in <ref type="figure" target="#fig_16">Fig. 7</ref>, samples corresponding to 0 are denoted by an upper moon, while samples corresponding to 1 are denoted by a lower moon. Then, the target samples are generated by rotating the data distribution of the source samples by 30 degrees, resulting in a domain shift for the target domain. In this case, the number of samples of each class in the two domains are equal. In contrast, for the second case, we reduce the number of samples to 38 for the upper moon of the target domain via a sampling strategy, which thus generates imbalanced class samples. As shown in <ref type="figure" target="#fig_16">Fig. 7</ref>  Proxy A-distance. As shown in <ref type="figure">Fig. 8</ref>, we calculate the proxy A-distance of the feature representations achieved by different methods based on task A?W of Office-31. Note that a smaller proxy A-distance denotes better transferability. The proposed DALN achieves the lowest proxy A-distance, demonstrating its superiority in learning transferable features. Moreover, by taking the NWD as a regularizer for DANN and MDD, their proxy A-distances are considerably decreased, demonstrating the effectiveness of the proposed NWD in improving the transferability of the features.  <ref type="figure">Figure 8</ref>. Visualization of the proxy A-distance on task A?W of Office-31. Note that a smaller proxy A-distance denotes better transferability.</p><p>Self-correlation matrix. As shown in <ref type="figure" target="#fig_19">Fig. 9</ref>, the model trained on the source-only data generates large values on the off-diagonal elements for the target domain samples. In contrast, with the adaptation of the proposed paradigm, the values of the self-correlation matrix generated from the target samples are highly concentrated on the main diagonal as shown in <ref type="figure" target="#fig_19">Fig. 9(b)</ref>. Thus, the intra-class correlation I a is increased and the inter-class correlation I e is decreased, which demonstrates the effectiveness of the proposed method.</p><p>Convergence. We present the convergence curves of the test accuracy, NWD, and MMD with respect to the number of <ref type="table">back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser  trash_can   Predicted label   back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler</ref>   <ref type="table">back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler  tape_dispenser  trash_can   Predicted label   back_pack  bike  bike_helmet  bookcase  bottle  calculator  desk_chair  desk_lamp  desktop_computer  file_cabinet  headphones  keyboard  laptop_computer  letter_tray  mobile_phone  monitor  mouse  mug  paper_notebook  pen  phone  printer  projector  punchers  ring_binder  ruler  scissors  speaker  stapler</ref>  iterations on tasks A?W and W?A of Office-31, as shown in <ref type="figure" target="#fig_10">Fig. 10</ref>. Benefiting from the definite guidance meaning, DALN achieves rapid convergence with competitive accuracy. In particular, it can be observed that minimizing the NWD can also effectively decrease the widely-used maximum mean discrepancy (MMD), which also demonstrates the effectiveness of the NWD.   Discussion of trade-off parameters ? and ?. ? is used to balance the losses L cls and L nwd . ? is also a balance weight used for taking the proposed NWD as a regularizer. Here, we conduct influence analysis for these two parameters based on tasks A?W and W?A of Office-31. As shown in <ref type="figure" target="#fig_10">Fig. 11</ref>, DALN achieves the best performance when ? ranges from 0.75 to 1.25. For the parameter ?, pleasing results occur when ? is in the range of 0.005 to 0.01. Similar trends can also be observed in other datasets. For simplicity, in this work, we set ? to 1 and ? to 0.01 for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Limitations</head><p>Despite the simplicity and the impressive performance of our method, here comes two problems in the training process. One problem is that the SVD process takes some time to compute the Nuclear norm, and the other problem is that the performance always reaches the highest value early in the training process and then decreases slowly. These two problems will be explored in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The self-correlation matrices of the predictions on the source and target domains based on a DNN model trained only with the source domain data on task A?W of Office-31. (Zoom in for a clear visualization.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>The confusion matrices of different methods of the target domain on task A ? W of Office-31. (Zoom in for a clear visualization.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .Figure 6</head><label>56</label><figDesc>Visualizations of (a) determinacy and (b) diversity on task A?R of Office-Home. The ratio in (a) is the proportion of the number of correctly classified samples, whose prediction probability is in the range of 0.9 to 1, to the total number of correctly classified samples in the target domain. The number of correctly classified samples in (b) is calculated for 8 typical categories that have a large or small number of samples.t-SNE Visualization. The feature representations of the ResNet-50, DANN, MDD, DALN, DANN+NWD, and MDD+NWD are visualized inFig. 6using t-SNE<ref type="bibr" target="#b49">[50]</ref>. Compared with the DANN and MDD, the proposed DALN not only confuses the feature representations, but also contributes to a more compact intra-class distribution and a more dispersed inter-class distribution, indicating that the features learned by the DALN are more discriminative. Combining the DANN and MDD with the proposed NWD, the intra-class features are pulled together while the inter-class features are pushed apart, demonstrating that the NWD can help them improve the discriminability. . t-SNE visualizations of feature distributions learned by different methods on task A?W of Office-31. Blue and red points represent source and target features, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>This work was supported in part by the National Natural Science Foundation of China under Grant 61727809, in part by the Special Fund for Key Program of Science and Technology of Anhui Province under Grant 201903c08020002, and in part by the National Key Research and Development Program of China under Grant 2019YFC0117800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Definition 1 .</head><label>1</label><figDesc>Given two metric spaces (M, d m ) and (N, d n ), where d m denotes the metric on the compact set M ? R m and d n is the metric on the compact set N ? R n , a function h : M ? N is called K-Lipschitz continuous if there exists a real constant K ? 0 (the minimum K called Lipschitz constant) such that, for ?m 1 , m 2 ? M , the following holds</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Definition 2 .</head><label>2</label><figDesc>(Remark 4.6.10 in<ref type="bibr" target="#b45">[46]</ref>) Given a function h : M ? N , where M and N denote the compact subsets of R m and R n , h will satisfy K-Lipschitz continuous if there exists a real constant K ? 0 (the minimum K called Lipschitz constant), such that, for any m 1 , m 2 ? M , m 1 = m 2 , all the first partial derivatives are bounded by K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Proposition 2 .</head><label>2</label><figDesc>Given the compact output set O ? R k and prediction set P ? R k , the softmax function S m (?) : O ? P mapping the output o ? O to the prediction p ? P satisfies the 1-Lipschitz constraint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Lemma 2 .</head><label>2</label><figDesc>(Lemma 1 of<ref type="bibr" target="#b40">[41]</ref>; Lemma1 of<ref type="bibr" target="#b44">[45]</ref>) Let ? s , ? t ? P (F) denote the probability measures of the source and target domain features, ? (f s , f t ) be the cost of transporting a unit of material from location f s satisfying f s ? ? s to location f t satisfying f t ? ? t , W 1 (? s , ? t ) represent the NWD, and K denote a Lipschitz constant. Given a family of classifiers C ? H 1 and a ideal classifier C * ? H 1 satisfying the K-Lipschitz constraint, where H 1 is a subspace of H, the following holds for every C, C * ? H 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Definition 3 .</head><label>3</label><figDesc>Given p 1 and ? &gt; 0, a probability measure ? on F satisfies T p (?) if the inequality</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Theorem 3 . 1 Ns Ns i=1 ? f s i and ? t = 1</head><label>311</label><figDesc>(Theorem 3 of<ref type="bibr" target="#b40">[41]</ref>; Theorem 3 of<ref type="bibr" target="#b44">[45]</ref>) Under the assumption of Lemma 2 and Lemma 3, let two probability measures ? s , ? t ? P (F) of the source and target domain features satisfy the T 1 (? * ) inequality, F s and F t be two sample sets of size N s and N t drawn i.i.d from ? s and ? t , respectively. Let? s =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 .</head><label>7</label><figDesc>Comparisons of decision boundaries on a toy example dataset. Red points and green points denote classes 0 and 1 of source data, respectively. Blue points are target data generated by rotating the source data distribution by 30 degrees. In the second row, we reduce the number of samples to 1/4 for the upper moon of the target domain via a sampling strategy, which thus generates imbalanced class samples. The orange and green regions are classified as classes 0 and 1 by the final decision boundary, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 9 .</head><label>9</label><figDesc>The self-correlation matrices of the predictions on the target domain on task A?W of Office-31. (Zoom in for a clear visualization.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure 10 .</head><label>10</label><figDesc>The test accuracy, NWD and MMD convergence curves of the target domain on tasks A?W and W?A of Office-31.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Figure 11 .</head><label>11</label><figDesc>The influence of ? and ? on tasks A?W and W?A of Office-31.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Classification accuracy (%) on (a) Office-Home and (b) VisDA-2017 for unsupervised domain adaptation (using ResNet-50 and ResNet-101 as the backbone, respectively). ? denotes that the results are reproduced using the publicly released code. The best accuracy is indicated in bold red and the second best accuracy is indicated in undelined blue. See supplementary material for more details.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">(a) Office-Home.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="12">A?C A?P A?R C?A C?P C?R P?A P?C P?R R?A R?C R?P Avg</cell></row><row><cell>ResNet-50 [18]</cell><cell>34.9</cell><cell>50.0</cell><cell>58.0</cell><cell>37.4</cell><cell>41.9</cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9 46.1</cell></row><row><cell>WDGRL  ? (18) [45]</cell><cell>44.1</cell><cell>63.8</cell><cell>74.0</cell><cell>47.3</cell><cell>57.1</cell><cell>61.7</cell><cell>51.8</cell><cell>39.1</cell><cell>72.1</cell><cell>64.9</cell><cell>45.9</cell><cell>76.5 58.2</cell></row><row><cell>MCD(18) [44]</cell><cell>48.9</cell><cell>68.3</cell><cell>74.6</cell><cell>61.3</cell><cell>67.6</cell><cell>68.8</cell><cell>57.0</cell><cell>47.1</cell><cell>75.1</cell><cell>69.1</cell><cell>52.2</cell><cell>79.6 64.1</cell></row><row><cell>BSP(19) [7]</cell><cell>52.0</cell><cell>68.6</cell><cell>76.1</cell><cell>58.0</cell><cell>70.3</cell><cell>70.2</cell><cell>58.6</cell><cell>50.2</cell><cell>77.6</cell><cell>72.2</cell><cell>59.3</cell><cell>81.9 66.3</cell></row><row><cell>BNM(20) [8]</cell><cell>52.3</cell><cell>73.9</cell><cell>80.0</cell><cell>63.3</cell><cell>72.9</cell><cell>74.9</cell><cell>61.7</cell><cell>49.5</cell><cell>79.7</cell><cell>70.5</cell><cell>53.6</cell><cell>82.2 67.9</cell></row><row><cell>GVB-GD(20) [10]</cell><cell>57.0</cell><cell>74.7</cell><cell>79.8</cell><cell>64.6</cell><cell>74.1</cell><cell>74.6</cell><cell>65.2</cell><cell>55.1</cell><cell>81.0</cell><cell>74.6</cell><cell>59.7</cell><cell>84.3 70.4</cell></row><row><cell>FGDA(21) [14]</cell><cell>52.3</cell><cell>77.0</cell><cell>78.2</cell><cell>64.6</cell><cell>75.5</cell><cell>73.7</cell><cell>64.0</cell><cell>49.5</cell><cell>80.7</cell><cell>70.1</cell><cell>52.3</cell><cell>81.6 68.3</cell></row><row><cell>TSA(21) [24]</cell><cell>53.6</cell><cell>75.1</cell><cell>78.3</cell><cell>64.4</cell><cell>73.7</cell><cell>72.5</cell><cell>62.3</cell><cell>49.4</cell><cell>77.5</cell><cell>72.2</cell><cell>58.8</cell><cell>82.1 68.3</cell></row><row><cell>CKB-MMD(21) [30]</cell><cell>54.2</cell><cell>74.1</cell><cell>77.5</cell><cell>64.6</cell><cell>72.2</cell><cell>71.0</cell><cell>64.5</cell><cell>53.4</cell><cell>78.7</cell><cell>72.6</cell><cell>58.4</cell><cell>82.8 68.7</cell></row><row><cell>SCDA(21) [25]</cell><cell>57.5</cell><cell>76.9</cell><cell>80.3</cell><cell>65.7</cell><cell>74.9</cell><cell>74.5</cell><cell>65.5</cell><cell>53.6</cell><cell>79.8</cell><cell>74.5</cell><cell>59.6</cell><cell>83.7 70.5</cell></row><row><cell>MetaAlign(21) [54]</cell><cell>59.3</cell><cell>76.0</cell><cell>80.2</cell><cell>65.7</cell><cell>74.7</cell><cell>75.1</cell><cell>65.7</cell><cell>56.5</cell><cell>81.6</cell><cell>74.1</cell><cell>61.1</cell><cell>85.2 71.3</cell></row><row><cell>DALN(Ours)</cell><cell>57.8</cell><cell>79.9</cell><cell>82.0</cell><cell>66.3</cell><cell>76.2</cell><cell>77.2</cell><cell>66.7</cell><cell>55.5</cell><cell>81.3</cell><cell>73.5</cell><cell>60.4</cell><cell>85.3 71.8</cell></row><row><cell>DANN(16) [13]</cell><cell>45.6</cell><cell>59.3</cell><cell>70.1</cell><cell>47.0</cell><cell>58.5</cell><cell>60.9</cell><cell>46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8 57.6</cell></row><row><cell>DANN+NWD</cell><cell>51.8</cell><cell>63.3</cell><cell>73.9</cell><cell>56.6</cell><cell>66.1</cell><cell>68.6</cell><cell>59.3</cell><cell>54.6</cell><cell>79.0</cell><cell>70.5</cell><cell>61.5</cell><cell>80.4 65.5</cell></row><row><cell>CDAN(18) [27]</cell><cell>50.7</cell><cell>70.6</cell><cell>76.0</cell><cell>57.6</cell><cell>70.0</cell><cell>70.0</cell><cell>57.4</cell><cell>50.9</cell><cell>77.3</cell><cell>70.9</cell><cell>56.7</cell><cell>81.6 65.8</cell></row><row><cell>CDAN+NWD</cell><cell>54.8</cell><cell>70.7</cell><cell>77.9</cell><cell>60.5</cell><cell>69.6</cell><cell>71.8</cell><cell>61.2</cell><cell>55.0</cell><cell>80.9</cell><cell>74.6</cell><cell>59.4</cell><cell>83.4 68.3</cell></row><row><cell>MDD(19) [55]</cell><cell>54.9</cell><cell>73.7</cell><cell>77.8</cell><cell>60.0</cell><cell>71.4</cell><cell>71.8</cell><cell>61.2</cell><cell>53.6</cell><cell>78.1</cell><cell>72.5</cell><cell>60.2</cell><cell>82.3 68.1</cell></row><row><cell>MDD+NWD</cell><cell>55.8</cell><cell>76.1</cell><cell>79.1</cell><cell>64.3</cell><cell>73.3</cell><cell>73.2</cell><cell>63.6</cell><cell>55.0</cell><cell>80.2</cell><cell>73.8</cell><cell>61.1</cell><cell>84.0 70.0</cell></row><row><cell>MCC(20) [19]</cell><cell>55.1</cell><cell>75.2</cell><cell>79.5</cell><cell>63.3</cell><cell>73.2</cell><cell>75.8</cell><cell>66.1</cell><cell>52.1</cell><cell>76.9</cell><cell>73.8</cell><cell>58.4</cell><cell>83.6 69.4</cell></row><row><cell>MCC+NWD</cell><cell>58.1</cell><cell>79.6</cell><cell>83.7</cell><cell>67.7</cell><cell>77.9</cell><cell>78.7</cell><cell>66.8</cell><cell>56.0</cell><cell>81.9</cell><cell>73.9</cell><cell>60.9</cell><cell>86.1 72.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Classification accuracy (%) on (a) Office-31 and (b) ImageCLEF-2014 for unsupervised domain adaptation (using ResNet-50 as the backbone). ? denotes that the results are reproduced using the publicly released code. The best accuracy is indicated in bold red and the second best is indicated in undelined blue.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b) ImageCLEF-2014.</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="7">A?W D?W W?D A?D D?A W?A Avg</cell><cell>Method</cell><cell cols="3">I?P P?I I?C C?I C?P P?C Avg</cell></row><row><cell>ResNet-50 [18]</cell><cell>68.4</cell><cell>96.7</cell><cell>99.3</cell><cell>68.9</cell><cell>62.5</cell><cell>60.7</cell><cell>76.1</cell><cell>ResNet-50 [18]</cell><cell>74.8 83.9 91.5 78.0</cell><cell>65.5</cell><cell>91.2 80.7</cell></row><row><cell>WDGRL  ? (18) [45]</cell><cell>72.6</cell><cell>97.1</cell><cell>99.2</cell><cell>79.5</cell><cell>63.7</cell><cell>59.5</cell><cell>78.6</cell><cell>WDGRL  ? (18) [45]</cell><cell>76.8 87.0 91.7 87.2</cell><cell>75.2</cell><cell>90.3 84.7</cell></row><row><cell>MCD(18) [44]</cell><cell>88.6</cell><cell>98.5</cell><cell>100.0</cell><cell>92.2</cell><cell>69.5</cell><cell>69.7</cell><cell>86.5</cell><cell>MCD(18) [44]</cell><cell>77.3 89.2 92.7 88.2</cell><cell>71.0</cell><cell>92.3 85.1</cell></row><row><cell>SWD(19) [21]</cell><cell>90.4</cell><cell>98.7</cell><cell>100.0</cell><cell>94.7</cell><cell>70.3</cell><cell>70.5</cell><cell>87.4</cell><cell>SWD(19) [21]</cell><cell>78.1 89.6 95.2 89.3</cell><cell>73.4</cell><cell>92.8 86.4</cell></row><row><cell>BNM(20) [8] DADA(20) [48] GVB-GD(20) [10] FGDA(21) [14] MetaAlign(21) [54] TSA(21) [24] SCDA(21) [25] DALN(Ours) DANN(16) [13] DANN+NWD CDAN(18) [27] CDAN+NWD MDD(19) [55]</cell><cell>91.5 92.3 94.8 93.3 93.0 94.8 94.2 95.2 82.0 92.1 94.1 93.7 94.5</cell><cell>98.5 99.2 98.7 99.1 98.6 99.1 98.7 99.1 96.9 98.2 98.6 98.5 98.4</cell><cell>100.0 100.0 100.0 100.0 100.0 100.0 99.8 100.0 99.1 100.0 100.0 100.0 100.0</cell><cell>90.3 93.9 95.0 93.2 94.5 92.6 95.2 95.4 79.7 84.7 92.9 91.0 93.5</cell><cell>70.9 74.4 73.4 73.2 75.0 74.9 75.7 76.4 68.2 74.5 71.0 74.4 74.6</cell><cell>71.6 74.2 73.7 72.7 73.6 74.4 76.2 76.5 67.4 73.0 69.3 73.0 72.2</cell><cell>87.1 89.0 89.3 88.6 89.2 89.3 90.0 90.4 82.2 87.1 87.7 88.4 88.9</cell><cell cols="2">BNM(20) [8] GVB-GD  ? (20) [10] DADA  ? (20) [48] CKB-MMD(21) [30] 80.7 92.2 96.5 92.2 77.2 91.2 96.2 91.7 78.2 92.7 96.5 91.5 78.7 92.3 97.2 91.6 SCDA  ? (21) [25] 78.7 91.8 96.7 92.8 TSA  ? (21) [24] 78.6 92.8 97.0 92.8 DALN(Ours) 80.5 93.8 97.5 92.8 DANN(16) [13] 75.0 86.0 96.2 87.0 DANN+NWD 78.0 89.2 97.3 93.3 CDAN(18) [27] 77.7 90.7 97.7 91.3 CDAN+NWD 78.6 92.5 97.2 91.7 MDD  ? (19) [55] 77.3 90.2 96.8 89.5</cell><cell>75.7 78.2 78.5 79.9 78.5 79.0 78.3 74.3 78.5 74.2 79.3 77.6</cell><cell>96.7 88.1 95.0 88.7 95.3 88.9 96.7 89.7 95.2 89.0 95.2 89.2 95.0 89.7 91.5 85.0 92.0 88.1 94.3 87.7 94.6 89.0 94.2 87.6</cell></row><row><cell>MDD+NWD</cell><cell>95.5</cell><cell>98.7</cell><cell>100.0</cell><cell>94.9</cell><cell>76.6</cell><cell>74.0</cell><cell>90.0</cell><cell>MDD+NWD</cell><cell>78.9 91.7 97.5 91.7</cell><cell>78.9</cell><cell>95.4 89.0</cell></row><row><cell>MCC(20) [19]</cell><cell>95.5</cell><cell>98.6</cell><cell>100.0</cell><cell>94.4</cell><cell>72.9</cell><cell>74.9</cell><cell>89.4</cell><cell>MCC  ? (20) [19]</cell><cell>78.3 94.5 97.3 92.3</cell><cell>77.3</cell><cell>96.3 89.3</cell></row><row><cell>MCC+NWD</cell><cell>95.5</cell><cell>98.7</cell><cell>100.0</cell><cell>95.4</cell><cell>75.0</cell><cell>75.1</cell><cell>90.0</cell><cell>MCC+NWD</cell><cell>79.8 94.5 98.0 94.2</cell><cell>80.0</cell><cell>97.5 90.7</cell></row></table><note>(a) Office-31.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Proposition 1 .</head><label>1</label><figDesc>Given two metric spaces (F, |?|) and (O, |?|), where F ? R d and O ? R k denote the compact input feature set and output set, respectively, |?| denotes the Frobenius norm in O or F . Then, for every input feature f ? F , the Lipschitz constant K of the fully connected layer L c (f ) = W f + b, where L c (?) : F ? O maps the feature f ? F to the output o ? O, W ? R k?d denotes the weight matrix, and b ? R k denotes the bias vector, has a upper bound W F .</figDesc><table /><note>Proof. Given features f 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>, for the first case, the model trained on the source-only data can correctly classify the source samples, but cannot perform properly in the overall target samples. DANN improves the decision boundary, but some samples in the upper moon are misclassified. MDD and DALN successfully classify both the source and target samples, but the proposed DALN achieves better classification performances compared with MDD. For the second case, except our DALN, both the DANN and MDD cannot learn a favorable decision boundary for the target samples. Some samples in the upper moon and lower moon are misclassified.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 .</head><label>4</label><figDesc>Accuracy(%) on DomainNet for UDA. In each sub-table, the column-wise domains are selected as the source domain and the row-wise domains are selected as the target domain.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Quantitative concentration inequalities for empirical measures on non-compact spaces. Probability Theory and Related Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Bolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Guillin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="page" from="541" to="593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview and analysis of the results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesus</forename><surname>Martinez-Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burak</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Novi</forename><surname>Patricia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neda</forename><surname>Marvasti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Suzan?sk?darl?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cazorla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="192" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transferability vs. discriminability: Batch spectral penalization for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast batch nuclear-norm maximization and minimization for robust domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06154</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradually vanishing bridge for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="12455" to="12464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cross-domain gradient discrepancy minimization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhekai</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient distribution alignment certificates better adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shufei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiufeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoliang</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="8937" to="8946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Minimum class confusion for versatile domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Looking back at labels: A class based domain adaptation technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Kumar Kurmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sliced wasserstein discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Haris</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Enhanced transport distance for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ming</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You-Wei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Fei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Xian</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13936" to="13944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bi-classifier determinacy maximization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangrui</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binhui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Transferable semantic augmentation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mixue</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Semantic concentration for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mixue</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangrui</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep transfer learning with joint adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stochastic classifiers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihe</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Conditional bures metric for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Xian</forename><surname>You-Wei Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fixbi: Bridging domain spaces for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heechul</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Hyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1094" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">T</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ga?l</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multi-adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">Visda: The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Fazel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Parrilo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="471" to="501" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Theoretical analysis of domain adaptation with optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ievgen</forename><surname>Redko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaury</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Basic real analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Houshang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohrab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">231</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Maximum-margin matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1329" to="1336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Discriminative adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3474</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C?dric</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Metaalign: Coordinating domain alignment and classification for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bridging theory and algorithm for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianle</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Domainsymmetric networks for adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A review of single-source deep unsupervised visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><forename type="middle">L</forename><surname>Sangiovanni-Vincentelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanjit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seshia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">clp inf pnt qdr rel skt Avg. DALN(Ours) clp inf pnt qdr rel skt Avg</title>
		<imprint>
			<biblScope unit="page">19</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
