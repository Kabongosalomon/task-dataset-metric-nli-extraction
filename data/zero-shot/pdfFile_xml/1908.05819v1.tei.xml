<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mixed High-Order Attention Network for Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
							<email>chenbinghui@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
							<email>whdeng@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
							<email>jnhu@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mixed High-Order Attention Network for Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Attention has become more attractive in person reidentification (ReID) as it is capable of biasing the allocation of available resources towards the most informative parts of an input signal. However, state-of-the-art works concentrate only on coarse or first-order attention design, e.g. spatial and channels attention, while rarely exploring higher-order attention mechanism. We take a step towards addressing this problem. In this paper, we first propose the High-Order Attention (HOA) module to model and utilize the complex and high-order statistics information in attention mechanism, so as to capture the subtle differences among pedestrians and to produce the discriminative attention proposals. Then, rethinking person ReID as a zero-shot learning problem, we propose the Mixed High-Order Attention Network (MHN) to further enhance the discrimination and richness of attention knowledge in an explicit manner. Extensive experiments have been conducted to validate the superiority of our MHN for person ReID over a wide variety of state-of-the-art methods on three large-scale datasets, including Market-1501, DukeMTMC-ReID and CUHK03-NP. Code is available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the quest for algorithms that enable cognitive abilities is an important part of machine learning, person reidentification (ReID) has become more attractive, where the model is requested to be capable of correctly matching images of pedestrians across videos captured from different cameras. This task has drawn increasing attention in many computer vision applications, such as surveillance <ref type="bibr" target="#b48">[49]</ref>, activity analysis <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and people tracking <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b43">44]</ref>. It is also challenging because the images of pedestrians are captured from disjoint views, the lighting-conditions/personposes differ across cameras, and occlusions are frequent in real-world scenarios.</p><p>Affected by the aforementioned factors, the discrimina- (1) Spatial attention uses softmax-like gated functions to produce a spatial mask. <ref type="bibr" target="#b0">(2)</ref> Channel attention <ref type="bibr" target="#b17">[19]</ref> uses global average pooling and fully connected layers to produce a scale vector. (3) Our high-order attention uses high-order polynomial predictor to produce scale maps that contain high-order statistics of convolutional activations.</p><p>tion of feature representations of pedestrian images actually is not good enough. In order to obtain discriminative feature representations, many research works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b19">21,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b44">45]</ref> resort to attention mechanism so as to highlight the informative parts (e.g. spatial locations) of convolutional responses and suppress the noisy patterns (e.g. background). Specifically, spatial attention <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b51">52</ref>] is a form of visual attention that involves directing attention to a location in space, it allows CNN to selectively process visual information of an area within the visual field. While, in spatial attention, the processing strategy of spatial masking is coarse and has no intrinsic effect on modulating the finegrained channel-knowledge. Recently, channel attention <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b26">27]</ref> is proposed to adaptively recalibrates channelwise convolutional responses by explicitly modelling interdependencies among channels. And the combination of spatial and channel attention has also been successfully applied in person ReID <ref type="bibr" target="#b26">[27]</ref>. However, we emphasize that these commonly used attention methods (i.e. spatial and channel attention) are either coarse or first-order, being confined to mining only simple and coarse information, in person ReID cases, they are insufficiently rich to capture the complex/high-order interactions of visual parts and the subtle differences among pedestrians caused by various viewpoints/person poses, as a result, the produced attention maps are neither discriminative or detailed. To this end, we dedicate to modeling the attention mechanism via high-order statistics of convolutional activations so as to capture more complex and high-order relationships among parts and to produce powerful attention proposals. Moreover, we rethink the problem of person ReID as a zero-shot learning (ZSL) task where there is no intersection of pedestrian identities between training and testing sets. Zero-shot learning has large gap with conventional full-shot learning (e.g. classification on CIFAR <ref type="bibr" target="#b6">[8,</ref><ref type="bibr" target="#b5">7]</ref>, Imagenet <ref type="bibr" target="#b37">[38]</ref>), and in zero-shot settings, the phenomenon of 'partial/biased learning behavior of deep model' <ref type="bibr" target="#b3">[5]</ref> largely affects the embedding performance, i.e. the deep model will only focus on the biased visual knowledges that only benefit to the seen identities and ignore the other helpful ones that might be useful for identifying the unseen identities. In other words, deep models easily learn to focus on surface statistical regularities rather than more general abstract concepts. However, many ReID works ignore this intrinsic problem of zero-shot learning. To this end, proposing detail-preserving attention framework remains important.</p><p>In this paper, we first propose High-Order Attention (HOA) module, a novel and powerful attention mechanism, to model the complex and high-order relationships among visual parts via high-order polynomial predictor, such that the subtle differences among pedestrian can be captured and discriminative attention results can be produced. Then, rethinking person ReID as a zero-shot problem, we propose Mixed High-Order Attention Network (MHN) to prevent the problem of 'biased learning behavior of deep model' <ref type="bibr" target="#b3">[5]</ref> and to encourage the richness of attention information. It is mainly achieved by employing multiple HOA modules with different orders to model diverse high-order statistics, such that all-sided attention knowledge can be preserved and thus the unseen pedestrian identity can be successfully recognized. Additionally, we introduce the adversarial learning constraint for MHN to further prevent the order collapse problem during training 1 , so as to explicitly enhance the discrimination of MHN. Our contributions can be summarized as follows:</p><p>? The High-Order Attention (HOA) module is proposed to capture and use high-order attention distributions. To our knowledge, it is the first work to propose and apply high-order attention module in Person-ReID.</p><p>? We rethink ReID as zero-shot learning task and propose the Mixed High-Order Attention Network (MHN) to efficiently utilize multiple HOA modules, so as to enhance the richness of attention by explicitly suppressing the 'biased learning behavior of deep model'. And adversary learning constraint is introduced to further prevent the problem of order collapse.</p><p>? MHN is a generally applicable and model-agnostic framework, it can be easily applied in the popular baseline architectures, such as IDE <ref type="bibr" target="#b62">[63]</ref> and PCB <ref type="bibr" target="#b42">[43]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Person ReID &amp; Attention Mechanism: Person ReID intends to correctly match images of pedestrians across videos captured from different cameras, it has been widely studied, such as ranking by pairwise constraints <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b47">48]</ref>, metric learning <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b50">51]</ref>, deep embedding learning <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b42">43]</ref>, re-ranking <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b14">16]</ref> and attributes learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b59">60]</ref>. Recently, attention methods <ref type="bibr" target="#b8">[10,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b17">19,</ref><ref type="bibr" target="#b45">46]</ref> in deep community are more attractive, in this paper, we focus on improving the performance of ReID via attention strategy.</p><p>Attention serves as a tool to bias the allocation of available resources towards the most informative parts of an input. Li et al. <ref type="bibr" target="#b22">[24]</ref> propose a part-aligning CNN network for locating latent regions (i.e. hard attention) and then extract and exploit these regional features for ReID. Zhao et at. <ref type="bibr" target="#b58">[59]</ref> employ the Spatial Transformer Network <ref type="bibr" target="#b18">[20]</ref> as the hard attention model for finding discriminative image parts. Except hard attention methods, soft attention strategies are also proposed to enhance the performance of ReID. For example, Li et at. <ref type="bibr" target="#b23">[25]</ref> use multiple spatial attention modules (by softmax function) to extract features at different spatial locations. Xu et al. <ref type="bibr" target="#b51">[52]</ref> propose to mask the convolutional maps via pose-guided attention module. Li et al. <ref type="bibr" target="#b26">[27]</ref> employ both the softmax-based spatial attention module and channel-wise attention module <ref type="bibr" target="#b17">[19]</ref> to enhance the convolutional response maps. However, spatial attention and channel attention are coarse and first-order respectively, and are not capable of modeling the complex and high-order relationships among parts, resulting in loss of fine-grained information. Thus, to capture detailed and complex information, we propose High-Order Attention (HOA) module.</p><p>High-order statistics: It has been widely studied in traditional machine learning due to its powerful representation ability. And recently, the progresses of challenging finegrained visual categorization task demonstrates integration of high-order pooling representations with deep CNNs can bring promising improvements. For example, Lin et al. <ref type="bibr" target="#b28">[29]</ref> proposed bilinear pooling to aggregate the pairwise feature interactions. Gao et al. <ref type="bibr" target="#b13">[15]</ref> proposed to approximate the second-order statistics via Tensor Sketch <ref type="bibr" target="#b34">[35]</ref>. Yin et al. <ref type="bibr" target="#b10">[12]</ref> aggregated higher-order statistics by iteratively applying the Tensor Sketch compression to the features. Cai et al. <ref type="bibr" target="#b0">[2]</ref> used high-order pooling to aggregate hierarchical convolutional responses. Moreover, the bilinear pooling and highorder pooling methods are also applied in Visual-Question-Answering task, such as <ref type="bibr" target="#b12">[14,</ref><ref type="bibr" target="#b20">22,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>. However, different from these above methods which mainly focus on using high-order statistics on top of feature pooling, resulting in high-dimensional feature representations that are not suitable for efficient/fast pedestrian search, we instead intend to enhance the feature discrimination by attention learning. We model high-order attention mechanism to capture the high-order and subtle differences among pedestrians, and to produce the discriminative attention proposals.</p><p>Zero-Shot Learning: In ZSL, the model is required to learn from the seen classes and then to be capable of utilizing the learned knowledge to distinguish the unseen classes. It has been studied in image classification <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">4]</ref>, video recognition <ref type="bibr" target="#b11">[13]</ref> and image retrieval/clustering <ref type="bibr" target="#b3">[5]</ref>. Interestingly, person ReID matches the setting of ZSL well where training identities have no intersection with testing identities, but most the existing ReID works ignore the problem of ZSL. To this end, we propose Mixed High-Order Attention Network (MHN) to explicitly depress the problem of 'biased learning behavior of deep model' <ref type="bibr" target="#b3">[5,</ref><ref type="bibr" target="#b4">6]</ref> caused by ZSL, allowing the learning of all-sided attention information which might be useful for unseen identities, preventing the learning of biased attention information that only benefits to the seen identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>In this section, we will first provide the formulation of the general attention mechanism in Sec. 3.1, then detail the proposed High-Order Attention (HOA) module in Sec. 3.2, finally show the overall framework of our Mixed High-Order Attention Network (MHN) in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>Attention acts as a tool to bias the allocation of available resources towards the most informative parts of an input. In convolutional neural network (CNN), it is commonly used to reweight the convolutional response maps so as to highlight the important parts and suppress the uninformative ones, such as spatial attention <ref type="bibr" target="#b23">[25,</ref><ref type="bibr" target="#b26">27]</ref> and channel attention <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b26">27]</ref>. We extend these two attention methods to a general case. Specifically, for a convolutional activation output, a 3D tensor X , encoded by CNN and coming from the given input image. We have X ? R C?H?W , where C, H, W indicate the number of channel, height and width, resp. As aforementioned, the goal of attention is to reweight the convolutional output, we thus formulate this process as:</p><formula xml:id="formula_0">Y = A(X ) X (1)</formula><p>where A(X ) ? R C?H?W is the attention proposal output by a certain attention module, is the Hadamard Product (element-wise product). As A(X ) serves as a reweighting term, the value of each element of A(X ) should be in the interval [0, 1]. Based on the above general formulation of attention, A(X ) can take many different forms. For example, However, in spatial attention or channel attention, A(X ) is coarse and unable to capture the high-order and complex interactions among parts, resulting in less discriminative attention proposals and failing in capturing the subtle differences among pedestrians. To this end, we dedicate to modeling A(X ) with high-order statistics.</p><formula xml:id="formula_1">if A(X ) = rep[M ]| C where M ? R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">High-Order Attention Module</head><p>To model the complex and high-order interactions within attention, we first define a linear polynomial predictor on top of the high-order statistics of x, where x ? R C denotes a local descriptor at a specific spatial location of X :</p><formula xml:id="formula_2">a(x) = R r=1 w r , ? r x<label>(2)</label></formula><p>where ?, ? indicates the inner product of two same-sized tensors, R is the number of order, ? r x is the r-th order outer-product of x that comprises all the degree-r monomials in x, and w r is the r-th order tensor to be learned that contains the weights of degree-r variable combinations in x.</p><p>Considering w r with large r will introduce excessive parameters and incur the problem of overfitting, we suppose that when r &gt; 1, w r can be approximated by D r rank-1 tensors by Tensor Decomposition <ref type="bibr" target="#b21">[23]</ref></p><formula xml:id="formula_3">, i.e. w r = D r d=1 ? r,d u r,d 1 ? ? ? ? ? u r,d r when r &gt; 1, where u r,d 1 ? R C , . . . , u r,d</formula><p>r ? R C are vectors, ? is the outer-product, ? r,d is the weight for d-th rank-1 tensor. Then according to the tensor algebra, Eq. 2 can be reformulated as:</p><formula xml:id="formula_4">a(x) = w 1 , x + R r=2 D r d=1 ? r,d u r,d 1 ? ? ? ? ? u r,d r , ? r x = w 1 , x + R r=2 D r d=1 ? r,d r s=1 u r,d s , x = w 1 , x + R r=2 ? r , z r<label>(3)</label></formula><p>where ? r = [? r,1 , ? ? ? , ? r,D r ] T is the weight vector, z r = [z r,1 , ? ? ? , z r,D r ] T with z r,d = r s=1 u r,d s , x . For later convenience, Eq. 3 can also be written as:</p><formula xml:id="formula_5">a(x) = 1 T (w 1 x) + R r=2 1 T (? r z r )<label>(4)</label></formula><p>where is Hadamard Product and 1 T is a row vector of ones. Then, to obtain a vector-like predictor a(x) ? R C , Eq. 4 is generalized by introducing the auxiliary matrixes P r :</p><formula xml:id="formula_6">a(x) = P 1 T (w 1 x) + R r=2 P r T (? r z r )<label>(5)</label></formula><p>where P 1 ? R C?C , P r ? R D r ?C with r &gt; 1. Since all P r , w 1 , ? r are parameters to be learned, for implementation convenience, we can integrate {P 1 , w 1 } into a new single matrix w 1 ? R C?C according to matrix algebra, and</p><formula xml:id="formula_7">{P r , ? r } into ? r ? R D r ?C (simple proof is in Supple- mentary file)</formula><p>. Then Eq. 5 can be expressed as:</p><formula xml:id="formula_8">a(x) = w 1 T x + R r=2 ? r T z r<label>(6)</label></formula><p>The above equation contains two terms, for clarity, we intend to formulate it into a more general case. Suppose w 1 can be approximated by the multiplication of two matrixes v ? R C?D 1 and ? 1 ? R D 1 ?C , i.e. w 1 = v ? 1 . then Eq. 6 can be reformulated as:</p><formula xml:id="formula_9">a(x) = ? 1 T ( v T x) + R r=2 ? r T z r = R r=1 ? r T z r (7)</formula><p>where z 1 = v T x, and when r &gt; 1, z r is the same as in Eq. 3. ? r ? R D r ?C are the trainable parameters.</p><p>In Eq.7, a(x) is capable of modeling and using the highorder statistics of the local descriptor x, thus, we can obtain the high-order vector-like attention 'map' by performing Sigmoid function on Eq. 7:</p><formula xml:id="formula_10">A(x) = sigmoid(a(x)) = sigmoid( R r=1 ? r T z r ) (8) where A(x) ? R C and the value of each element in A(x) is in the interval [0, 1].</formula><p>Nonlinearity: Moreover, in order to further improve the representation capacity of this high-order attention 'map', inspired by the common design of CNN, we provide a variation of Eq.8 by introducing nonlinearity as follows:</p><formula xml:id="formula_11">A(x) = sigmoid( R r=1 ? r T ?(z r ))<label>(9)</label></formula><p>where ? denotes an arbitrary non-linear activation function, here, we use ReLU <ref type="bibr" target="#b32">[33]</ref> function. A(x) in Eq.9 is finally employed as the required high-order attention 'map' for the corresponding local descriptor x. The experimental comparisons between Eq.8 and Eq.9 are in Sec. 4. Full module: As aforementioned, A(x) is defined on a local descriptor x, to obtain A(X ) which is defined on 3D tensor X , we generalize Eq.9. Specifically, we share the learnable weights in A(x) among different spatial locations of X and let A(X ) = {A(x (1,1) ), ? ? ? , A(x (H,W ) )}, where x (h,w) indicates a local descriptor at spatial location point (h, w) of X . Employing this attention map A(X ) in CNN has two benefits. (1) sharing weights among different spatial locations will not incur excessive parameters. (2) A(X ) can be easily implemented by 1x1 convolution layers. After obtaining the high-order attention map A(X ), our High-Order Attention (HOA) module can be formulated in the same way as Eq. 1, i.e. Y = A(X ) X .</p><p>Implementation: Since the learnable parameters are shared among spatial locations, all operations in A(X ) can be implemented by Convolution. As illustrated in <ref type="figure" target="#fig_1">Fig. 2.(a)</ref>, when R = 1, matrixes { v, ? 1 } are implemented by 1x1 convolution layers with D 1 and C output channels, resp. When R &gt; 1, r &gt; 1, we first employ {u r,d s } d=1,??? ,D r as a set of D r 1x1 convolutional filters on X so as to produce a set of feature maps Z r s with channels D r , then feature maps {Z r s } s=1,??? ,r are combined by element-wise product to obtain Z r = Z r 1 ? ? ? Z r r , where Z r = {z r }, and ? r can also be implemented by 1x1 convolution layer. A toy example of HOA when R = 3 is illustrated in <ref type="figure" target="#fig_1">Fig.2.(b)</ref>.</p><p>Remark: The proposed HOA module can be easily implemented by the commonly used operations, such as 1x1 convolution and element-wise product/addition. Equipped by the powerful high-order predictor, the attention proposals could be more discriminative and is capable of modeling the complex and high-order relationships among parts. Moreover, the channel attention module in <ref type="bibr" target="#b17">[19,</ref><ref type="bibr" target="#b26">27]</ref> is called to be first-order because (1) GAP layer only collects firstorder statistics while neglecting richer higher-order ones, suffering from limited representation ability <ref type="bibr" target="#b9">[11]</ref> (2) fullyconnected layers can be regarded as 1x1 convolution layers and thus the two cascaded fully-connected layers used in <ref type="figure">Figure 3</ref>. Illustration of Mixed High-Order Attention Network (MHN). Our MHN is model-agnostic, it can be applied in both IDE <ref type="bibr" target="#b62">[63]</ref> and PCB <ref type="bibr" target="#b42">[43]</ref> architectures, here for clarity, we take ResNet50 <ref type="bibr" target="#b16">[18]</ref> based IDE for example. The adversary constraint is used to regularize the order of HOA modules. channel-attention <ref type="bibr" target="#b17">[19]</ref> are equivalent to our HOA module when R = 1 (regardless of the spatial dimensions and see <ref type="figure" target="#fig_1">Fig.2.(a)</ref>). In summary, the full channel attention module can only collect and utilize the first-order information, being insufficiently rich to capture the complex interactions and to produce the discriminative attention maps. And if without using GAP, the channel attention module can be regarded as a special case of our HOA with R = 1, further demonstrating it indeed is first-order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Mixed High-Order Attention Network</head><p>Considering that Person ReID essentially pertains to zero-shot learning (ZSL), where there is no intersection between training identities and testing identities, we should explicitly suppress the problem of 'biased learning behavior of deep model' caused by zero-shot settings <ref type="bibr" target="#b3">[5]</ref>. Specifically, in ZSL, the deep model easily learn to focus on surface statistical regularities rather than more general abstract concepts, in other words, deep model will selectively learn the biased knowledge that are only useful to distinguish the seen identities, while ignore the knowledge that might be useful for unseen ones. Therefore, to correctly recognize the unseen identities, we propose Mixed High-Order Attention Network (MHN) to utilize multiple HOA modules with different orders such that the diverse and complementary high-order information can be explicitly used, encouraging the richness of the learned features and preventing the learning of partial/biased visual information.</p><p>For a toy example as shown in <ref type="figure">Fig. 3</ref>, the proposed MHN is constituted by several different HOA modules such that the diverse statistics of visual knowledge could be modeled and used. In particular, ResNet50 is first decomposed into two parts, i.e. P 1 (from conv1 to layer2 2 ) and P 2 (from layer3 to GAP). P 1 is used to encode the given image from raw pixel space to mid-level feature space, P 2 is used to encode the attentional information to the high-level feature space where the data can be classified. HOA modules with 2 Named in pytorch <ref type="bibr" target="#b35">[36]</ref> manner. different orders (e.g. {R = 1, 2, 3}) are placed between P 1 and P 2 so as to produce the diverse high-order attention maps and intensify the richness within learned knowledge. Worthy of mention is that our MHN won't introduce excessive parameters since P 2 modules share the same weights across different attention streams.</p><p>However, simply employing multiple HOA modules with different orders won't lead the best performance of MHN, since one HOA module with higher order might collapse to a relatively lower order module due to 'the partial/biased learning behavior of deep model'. Specifically, from Eq. 7, one can observe that for a k-th order HOA module, a(x) also contains the l-th order sub-term (where l &lt; k). In theory, HOA module with R = k can capture and use the k-th order statistics of local descriptor x, but in actual, especially in zero-shot learning settings, due to the fact that the deep model will selectively learn surface statistical regularities that are the easiest ones to distinguish the seen classes <ref type="bibr" target="#b3">[5]</ref>, the k-th order attention module might collapse to a lower-order counterpart as lower-order statistics are common and are much easier to collect than higher-order statistics. Therefore, these HOA modules with different Rs actually collapse to some similar lower-order counterparts, and the wanted diverse higher-order attention information are not captured. To this end, inspired by GAN <ref type="bibr" target="#b15">[17]</ref>, we introduce the adversary constraint for regularizing the order of HOA to be different, as shown in <ref type="figure">Fig. 3</ref>, it can be formulated as:</p><formula xml:id="formula_12">max HOA| R=k R=1 min F (L adv ) = max HOA| R=k R=1 min F ( k j,j ,j =j F (fj)?F (f j ) 2 2 )<label>(10)</label></formula><p>where HOA| R=k R=1 means there are k HOA modules (from first-order to k-th order) in MHN, F indicates the encoding function parameterized by the adversary network which contains two fully-connected layers, f j is the feature representation vector learned from the corresponding HOA module with R = j. In Eq. 10, the adversary network F tries to minimize the discrepancies among features f j while HOA modules try to maximize these discrepancies. After obtaining the Nash Equilibrium, the orders of HOA modules will be different with each other, since during the optimization of Eq.10, P 2 shares across streams and the only differentiating parts in MHN are HOA modules, when maximizing the feature discrepancies, the only solution is to make the HOA modules have different orders and produce diverse attention knowledge. In other words, only diverse HOA modules will make L adv large. Thus the problem of order collapse can be suppressed.</p><p>Then, the overall objective function of MHN is as:</p><formula xml:id="formula_13">min(L ide ) + ?( max HOA| R=k R=1 min F (L adv ))<label>(11)</label></formula><p>where L ide indicates the identity loss based on Softmax classifier, ? is the coefficient. Remark: From Eq.11, one can observe that we regularize the order/diversity of HOA modules by imposing constraint on the encoded feature vectors, instead of directly on the high-order attention maps, since these attention maps come from the complex high-order statistics and the definition of the order difference of HOA modules in the attention space is too hard to be artificially made. Thus, the order constraint is imposed on the feature vectors. Moreover, it seems that using Hinge loss based constraint instead of the adversary strategy to maximize the feature discrepancies is also feasible. However, we want to emphasize that in Hinge loss based function there is another margin-controller 'm' which needs extra tuning, and the discrepancies between features that coming from different HOA modules will be heterogeneous, thus to determine the optimal margin 'm', many redundant experiments must be executed. To this end, we employ the adversary constraint so as to allow the automatic learning of the optimal discrepancies.</p><p>By preventing the problem of order collapse, the HOA modules are explicitly regularized to model the wanted high-order attention distributions and thus can produce the discriminative and diverse attention maps which could be benefit for recognizing the unseen identities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets: We use three popular benchmark datasets based on zero-shot learning (ZSL) settings, i.e. Market-1501 <ref type="bibr" target="#b60">[61]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b64">65]</ref> and CUHK03-NP <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b65">66]</ref>. Market-1501 have 12,936 training images with 751 different identities. Gallery and query sets have 19,732 and 3,368 images respectively with another 750 identities. DukeMTMC-ReID includes 16,522 training images of 702 identities, 2,228 query and 17,661 gallery images of another 702 identities. CUHK03-NP is a new training-testing split protocol for CUHK03, it contains two subsets which provide labeled and detected (from a person detector) person images. The detected CUHK03 set includes 7,365 training images, 1,400 query images and 5,332 gallery images. The labeled set contains 7,368 training, 1,400 query and 5,328 gallery images respectively. The new protocol splits the training and testing sets into 767 and 700 identities.</p><p>Implementation: The proposed MHN is applied on both ResNet50-based IDE <ref type="bibr" target="#b62">[63]</ref> and PCB <ref type="bibr" target="#b42">[43]</ref> architectures. For both architectures, we adopt the SGD optimizer with a momentum factor of 0.9, set the start learning rate to be 0.01 for backbone CNN and ten times learning rate for the new added layers, and a total of 70 epochs with the learning rate decreased by a factor of 10 each 20 epochs. The dimension of feature f j is 256 and the two FC layers in F have 128, 128 neurons resp, we set all D r | R r=1 to be 64. For IDE, the images are resized to 288x144. For PCB, the images are resized to 336x168. We set the batch size to 32 in all experiments and use one 1080Ti GPU. MHN is implemented by Pytorch <ref type="bibr" target="#b35">[36]</ref> and modified from the public code[1], random erasing <ref type="bibr" target="#b66">[67]</ref> is also applied. Notation: We use 'MHN-k' to denote that in MHN there are k HOA modules with orders  <ref type="table">Table 3</ref>. Results comparisons over CUHK03-NP <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b65">66]</ref>. * indicates the re-implementation by our code. The best/second results are shown in red/blue, resp. CUHK03-NP <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b65">66]</ref> DukeMTMC-ReID <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b64">65]</ref> Market-1501 <ref type="bibr">[</ref>  <ref type="table">Table 4</ref>. Effect (%) of attention modules. * indicates the re-implementation and 'era' means random erasing. R = {1, ? ? ? , k} resp, and 'MHN-k (IDE/PCB)' to denote using IDE/PCB architectures, resp.</p><p>Evaluation: In testing, the feature representations f j , j ? {1, ? ? ? , k} are concatenated after L2 normalization. Then, the metrics of cumulative matching characteristic (CMC) and mean Average Precision (mAP) are used for evaluation. No re-ranking tricks are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-Art Methods</head><p>In order to highlight the significance of the proposed MHN for person ReID task, we compare it with some recent remarkable works, including methods of alignment <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b42">43]</ref>, deep supervision <ref type="bibr" target="#b49">[50]</ref>, architectures <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b42">43]</ref>, attention <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b46">47]</ref> and others <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">9,</ref><ref type="bibr" target="#b1">3]</ref>, over the popular used benchmarks Market-1501, DukeMTMC-ReID and CUHK03-NP. For fair comparison, we re-implement the baseline models, i.e. ResNet50-based IDE and PCB, with the same training configurations as ours. MHN is then applied over both IDE and PCB architectures. The comparison results are listed in Tab. 1, Tab. 2 and Tab. 3. From these tables, one can observe that by explicitly intensify the discrimination and diversity within the deep embedding via high-order attention modules, our MHN-6 can significantly improve the performances over both the baseline methods IDE and PCB (e.g. comparing with PCB, MHN-6 (PCB) has 2%/6.4% improvements of R-1/mAP on Market and 5.2%/7.5% improvements of R-1/mAP on DukeMTMC), demonstrating the effectiveness of our high-order attention idea. And our MHN-6 (PCB) achieves the new SOTA performances on all these three benchmarks, showing the superiority of our method. mances will further increase with the number of HOA modules, e.g. on CUHK03-NP Labeled dataset, applying MHN on PCB, when increasing the number of HOA modules from 2 to 6 the performance of R-1 will be increased from 71.2% to 77.2%, the same phenomenon can be observed in other datasets and architecture. This phenomenon also shows that employing multiple HOA modules is benefit for modeling diverse and discriminative information for recognizing the unseen identities, and MHN-6 outperforms all the baseline models by a large margin over all the three benchmarks, demonstrating the effectiveness of our method. However, when further increase the number of HOA modules, e.g. k = 8, the performance improvements are few, thus we don't report it here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Component Analysis</head><p>Effect of Adversary Constraint: From Tab. 5, when comparing {MHN-6 (IDE) w/o L adv } with {IDE} and comparing {MHN-6 (PCB) w/o L adv } with {PCB}, one can observe that on both DukeMTMC and Market datasets the performances of R-1 and mAP can be improved by simply employing multiple HOA modules without any regularizing constraint, showing that using higher-order attention information will indeed increase the discrimination of the learned knowledge in ZSL settings. However, as mentioned in Sec. 3.3, the task of person ReID pertains to zero-shot settings, the problem of 'partial/biased learning behavior of deep model' will incur the problem of order collapse of HOA modules, i.e. the deep model will partially model the easy and lower-order information regardless the theoretical capacity of HOA module. Therefore, we introduce the adversary constraint to explicitly prevent the problem of order collapse. After equipping with L adv , MHN-6(IDE/PCB) can further improve the performances over both the benchmarks, demonstrating the effectiveness of L adv and implying that explicitly learning diverse high-order attention information is essential for recognizing the unseen identities.</p><p>Effect of Nonlinearity: The nonlinearity comparisons are listed in Tab. 6, from this table, one can observe that by adding nonlinearity into the high-order attention modules, the performances can be further improved.</p><p>Comparison to other attention methods: To demonstrate the effectiveness of our idea of high-order attention, we compare with some other attention methods as in Tab. 7. Specifically, our MHN-6(IDE) outperforms both the spatial and channel attention methods, i.e. HA-CNN <ref type="bibr" target="#b26">[27]</ref> and SENet50 3 <ref type="bibr" target="#b17">[19]</ref>, showing the superiority of high-order attention model to these coarse/first-order attention methods. Moreover, although {SpaAtt+Q} <ref type="bibr" target="#b23">[25]</ref> employs multiple diverse attention modules like MHN to enhance the richness of attention information, the used attention method is spatial attention which is coarse and insufficiently rich to capture the complex and high-order interactions of parts, failing in producing more discriminative attention proposals and thus performing worse than MHN-6(IDE). {CASN+IDE} <ref type="bibr" target="#b63">[64]</ref> regularizes the attention maps of the paired images belonging to the same identity to be similar and indeed improves the results, but it still performs worse than MHN-6(IDE) since the consistence constraint for attention maps is only based on the the coarse spatial attention maps.</p><p>In summary, because of the ability of modeling and using complex and high-order information, the proposed MHN can significantly surpass all the listed coarse/first-order attention methods as shown in Tab. 7.</p><p>Ablation study on the configurations of P 1 &amp; P 2 : As mentioned in Sec. 3.3, the HOA modules are placed between P 1 and P 2 , to investigate the effect of the placed position of HOA modules, we conduct experiments as in Tab. 8. One can observe that placing HOA modules after <ref type="bibr" target="#b1">3</ref> We fine-tune the pre-trained SENet50 released at https:// github.com/moskomule/senet.pytorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Market-1501 R-1 mAP 1 :P 1 ={conv1?layer1},P 2 ={layer2?GAP} 92.2 81.8 2 :P 1 ={conv1?layer2},P 2 ={layer3?GAP} 93.6 83.6 3 :P 1 ={conv1?layer3},P 2 ={layer4?GAP} 92.7 82.1 <ref type="table">Table 8</ref>. Ablation study on the configurations of P1 and P2. All the layer names are shown in Pytorch manner. Here, for convenience we conduct experiments with MHN-6 (IDE) and test three configurations, i.e. 1 , 2 and 3 . 'layer2' (i.e. using the configuration 2 ) performs the best since when placing it at the relatively lower layer (i.e. using the configuration 1 ) the knowledge input to HOA module is more relevant to the low-level texture information and contains much noise, while placing it at relatively higher layer (i.e. using the configuration 3 ), some useful knowledge for recognizing the unseen identities might be already lost during the forward propagation of information as a result of partial/biased learning behavior. To this end, we use the configuration 2 for both IDE and PCB architectures throughout the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models</head><p>Model size: We compare the model size as in Tab. 9, from this table, one can observe that the parameter number of our MHN increases with the order. While comparing with SENet50 <ref type="bibr" target="#b17">[19]</ref>, the total parameter number of each MHN is not so much, and in terms of the performance, each MHN can outperform SENet50, showing that our MHN is indeed 'light and sweet'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we first propose the High-Order Attention (HOA) module so as to increase the discrimination of attention proposals by modeling and using the complex and highorder statistics of parts. Then, considering the fact that the person-ReID task pertains to zero-shot learning where the deep model will easily learn the biased knowledge, we propose the Mixed High-Order Attention Network (MHN) to utilize the HOA modules at different orders, preventing the learning of partial/biased visual information that only benefit to the seen identities. The adversary constraint is further introduced to prevent the problem of order collapse of HOA module. And Extensive experiments have been conducted over three popular benchmarks to validate the necessity and effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Attention comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of High-Order Attention (HOA) modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>indicates the re-implementation by our code. The best/second results are shown in red/blue, resp. Results comparisons over DuckMTMC-ReID<ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b64">65]</ref>.</figDesc><table><row><cell>Market-1501 (%)</cell></row></table><note>** indicates the re-implementation by our code. The best/second results are shown in red/blue, resp.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>[63] 52.9 48.5 50.4 46.3 80.1 90.7 93.5 64.2 89.0 95.7 97.3 73.9 IDE * +era 61.4 55.71 56.9 51.3 83.6 92.1 94.3 67.4 90.3 96.5 97.6 75.9 MHN-2 (IDE) 65.9 59.1 60.9 54.8 84.5 92.6 94.7 68.9 90.6 96.1 97.6 76.1 MHN-4 (IDE) 67.4 60.3 62.7 55.8 86.3 93.1 95.6 72.4 91.8 97.6 98.5 80.1 MHN-6 (IDE) 69.7 65.1 67.0 61.2 87.5 93.8 95.6 75.2 93.6 97.7 98.6 83.6 PCB</figDesc><table><row><cell></cell><cell>Labeled</cell><cell>Detected</cell><cell>61]</cell></row><row><cell>Methods</cell><cell cols="3">R-1 mAP R-1 mAP R-1 R-5 R-10 mAP R-1 R-5 R-10 mAP</cell></row><row><cell>IDE  [43]</cell><cell cols="3">61.9 56.8 60.6 54.4 83.9 91.8 94.4 69.7 93.1 97.5 98.5 78.6</cell></row><row><cell>PCB  *  +era</cell><cell cols="3">57.4 52.5 54.3 49.9 83.4 91.5 94.3 68.2 91.9 97.4 98.4 76.8</cell></row><row><cell cols="4">MHN-2 (PCB) 71.2 66.3 67.9 61.9 86.9 93.3 95.3 73.5 94.0 97.8 98.5 82.5</cell></row><row><cell cols="4">MHN-4 (PCB) 75.1 70.6 71.6 66.1 88.7 94.4 95.9 76.8 94.5 98.0 98.6 84.2</cell></row><row><cell cols="4">MHN-6 (PCB) 77.2 72.4 71.7 65.4 89.1 94.6 96.2 77.2 95.1 98.1 98.9 85.0</cell></row></table><note>**</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Effect of MHN:We conduct quantitative comparisons on MHN as in Tab. 4. From this table, one can observe that the proposed MHN can significantly improve the performances of person ReID task over both IDE and PCB baseline architectures. Specifically, comparing MHN-2(IDE/PCB) with IDE/PCB, we can see that using higherorder attention information indeed encourage the discrimination of the learned embedding. Moreover, the perfor-</figDesc><table><row><cell></cell><cell cols="3">DukeMTMC-ReID Market-1501</cell></row><row><cell>Methods</cell><cell>R-1</cell><cell>mAP</cell><cell>R-1 mAP</cell></row><row><cell>IDE  *  [63]</cell><cell>80.1</cell><cell>64.2</cell><cell>89.0 73.9</cell></row><row><cell cols="2">MHN-6 (IDE) w/o L adv 85.5</cell><cell>70.8</cell><cell>91.8 80.0</cell></row><row><cell>MHN-6 (IDE)</cell><cell>87.5</cell><cell>75.2</cell><cell>93.6 83.6</cell></row><row><cell>PCB  *  [43]</cell><cell>83.9</cell><cell>69.7</cell><cell>93.1 78.6</cell></row><row><cell cols="2">MHN-6 (PCB) w/o L adv 87.7</cell><cell>75.4</cell><cell>93.9 83.2</cell></row><row><cell>MHN-6 (PCB)</cell><cell>89.1</cell><cell>77.1</cell><cell>95.1 85.0</cell></row><row><cell cols="4">Table 5. Effect (%) of adversary constraint.  *  indicates the re-</cell></row><row><cell>implementation by our code.</cell><cell cols="3">DukeMTMC-ReID Market-1501</cell></row><row><cell>Methods</cell><cell>R-1</cell><cell>mAP</cell><cell>R-1 mAP</cell></row><row><cell cols="2">MHN-6 (IDE) w/o nonli 87.1</cell><cell>74.9</cell><cell>93.3 83.1</cell></row><row><cell>MHN-6 (IDE)</cell><cell>87.5</cell><cell>75.2</cell><cell>93.6 83.6</cell></row><row><cell cols="2">MHN-6 (PCB) w/o nonli 88.7</cell><cell>76.8</cell><cell>95.0 84.5</cell></row><row><cell>MHN-6 (PCB)</cell><cell>89.1</cell><cell>77.1</cell><cell>95.1 85.0</cell></row><row><cell cols="3">Table 6. Effect (%) of nonlinearity.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .</head><label>7</label><figDesc>Comparison to other attention methods (%).</figDesc><table><row><cell></cell><cell cols="3">DukeMTMC-ReID Market-1501</cell></row><row><cell>Methods</cell><cell>R-1</cell><cell>mAP</cell><cell>R-1 mAP</cell></row><row><cell>IDE  *  [63]</cell><cell>80.1</cell><cell>64.2</cell><cell>89.0 73.9</cell></row><row><cell>SENet50  *  [19]</cell><cell>81.2</cell><cell>64.8</cell><cell>90.0 75.6</cell></row><row><cell>HA-CNN [27]</cell><cell>80.5</cell><cell>63.8</cell><cell>91.2 75.7</cell></row><row><cell cols="2">SpaAtt+Q  *  [25] 84.7</cell><cell>69.6</cell><cell>91.6 77.4</cell></row><row><cell cols="2">CASN+IDE [64] 84.5</cell><cell>67.0</cell><cell>92.0 78.0</cell></row><row><cell>MHN-6 (IDE)</cell><cell>87.5</cell><cell>75.2</cell><cell>93.6 83.6</cell></row></table><note>* indicates our reproducing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Model size comparisons. PN means Parameter Number.</figDesc><table><row><cell></cell><cell cols="3">PN (million) Depth R-1 (on Market)</cell></row><row><cell>IDE [63]</cell><cell>24.2</cell><cell>50</cell><cell>89.0%</cell></row><row><cell>SENet50 [19]</cell><cell>27.4</cell><cell>50</cell><cell>90.0%</cell></row><row><cell>MHN-2 (IDE)</cell><cell>24.4</cell><cell>50</cell><cell>90.6%</cell></row><row><cell>MHN-4 (IDE)</cell><cell>25.2</cell><cell>50</cell><cell>91.8%</cell></row><row><cell>MHN-6 (IDE)</cell><cell>26.8</cell><cell>50</cell><cell>93.6%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although, the proposed high-order attention module has ability to capture complex and high-order statistics, but suffering from 'biased learning behavior of deep model', in zero-shot settings, the high-order module might collapse to lower-order module.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was partially supported by the National Natural Science Foundation of China under Grant Nos. 61871052, 61573068, 61471048, and BUPT Excellent Ph.D. Students Foundation CX2019307.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Synthesized classifiers for zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5327" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Energy confused adversarial metric learning for zero-shot image retrieval and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hybrid-attention based decoupled metric learning for zero-shot image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2750" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Noisy softmax: Improving the generalization ability of dcnn via postponing the early softmax saturation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Virtual class enhanced discriminative embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1946" to="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep filter banks for texture recognition and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3828" to="3836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Zero-shot video retrieval using content and concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Mirajkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</title>
		<meeting>the 22nd ACM international conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1857" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person re-identification ranking optimisation by discriminant context information analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Martinel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Micheloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Gardel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1305" to="1313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mahdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emrah</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhittin</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>G?kmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mustafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tensor decompositions and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="455" to="500" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning deep context-aware features over body and latent parts for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dangwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on</title>
		<meeting>the IEEE Conference on</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative learning of latent features for zero-shot recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junge</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiqi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7463" to="7471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hydraplus-net: Attentive deep features for pedestrian analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multicamera activity correlation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen Change Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1988" to="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Timedelayed correlation analysis for multi-camera activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Chen Change Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning to rank in person re-identification with metric ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sakrapee</forename><surname>Paisitkriangkrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1846" to="1855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fast and scalable polynomial kernels via explicit feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ninh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmus</forename><surname>Pagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="239" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">End-to-end deep kronecker-product matching for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep attributes driven multi-camera person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Part-aligned bilinear representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Svdnet for pedestrian retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Multiple people tracking by lifted multicut and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3539" to="3548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mancs: A multi-task attentional network with curriculum sampling for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taiqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Intelligent multi-camera video surveillance: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Resource aware person re-identification across multiple resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lequn</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Octavia Camps, and Mario Sznaier. Person re-identification using kernel-based metric learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengran</forename><surname>Gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Attention-aware compositional network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2119" to="2128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep metric learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 22nd International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Harry potter&apos;s marauder&apos;s map: Localizing and tracking multiple persons-of-interest by nonnegative discretization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Shoou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3714" to="3720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multimodal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1821" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Beyond bilinear: generalized multimodal factorized high-order pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoqing</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning mid-level filters for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="144" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Query-adaptive late fusion for image search and person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqiong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1741" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Person re-identification: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Re-identification with consistent attentive siamese networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
