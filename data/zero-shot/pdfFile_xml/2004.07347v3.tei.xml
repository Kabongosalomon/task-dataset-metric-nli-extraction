<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
							<email>wenhuchen@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
							<email>hwzha@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
							<email>zhiyuchen@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
							<email>hongwang600@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Santa Barbara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HybridQA: A Dataset of Multi-Hop Question Answering over Tabular and Textual Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing question answering datasets focus on dealing with homogeneous information, based either only on text or KB/ <ref type="table">Table information</ref> alone. However, as human knowledge is distributed over heterogeneous forms, using homogeneous information alone might lead to severe coverage problems. To fill in the gap, we present HybridQA 1 , a new large-scale question-answering dataset that requires reasoning on heterogeneous information. Each question is aligned with a Wikipedia table and multiple free-form corpora linked with the entities in the table. The questions are designed to aggregate both tabular information and text information, i.e., lack of either form would render the question unanswerable. We test with three different models: 1) a table-only model.</p><p>2) text-only model. 3) a hybrid model that combines heterogeneous information to find the answer. The experimental results show that the EM scores obtained by two baselines are below 20%, while the hybrid model can achieve an EM over 40%. This gap suggests the necessity to aggregate heterogeneous information in HybridQA. However, the hybrid model's score is still far behind human performance. Hence, HybridQA can serve as a challenging benchmark to study question answering with heterogeneous information.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Question answering systems aim to answer any form of question of our interests, with evidence provided by either free-form text like Wikipedia passages <ref type="bibr" target="#b15">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b2">Chen et al., 2017;</ref> or structured data like Freebase/WikiData <ref type="bibr" target="#b0">(Berant et al., 2013;</ref><ref type="bibr" target="#b11">Kwiatkowski et al., 2013;</ref><ref type="bibr" target="#b23">Yih et al., 2015;</ref><ref type="bibr" target="#b19">Weston et al., 2015)</ref> and WikiTables <ref type="bibr" target="#b14">(Pasupat and Liang, 2015)</ref>. Both 1 https://github.com/wenhuchen/HybridQA forms have their advantages, the free-form corpus has in general better coverage while structured data has better compositionality to handle complex multi-hop questions. Due to the advantages of different representation forms, people like to combine them in real world applications. Therefore, it is sometime not ideal to assume the question has answer in a passage. This paper aims to simulate a more realistic setting where the evidences are distributed into heterogeneous data, and the model requires to aggregate information from different forms for answering a question. There has been some pioneering work on building hybrid QA systems <ref type="bibr" target="#b16">(Sun et al., 2019</ref><ref type="bibr" target="#b17">(Sun et al., , 2018</ref><ref type="bibr" target="#b20">Xiong et al., 2019)</ref>. These methods adopts KB-only datasets <ref type="bibr" target="#b0">(Berant et al., 2013;</ref><ref type="bibr" target="#b23">Yih et al., 2015;</ref><ref type="bibr" target="#b18">Talmor and Berant, 2018)</ref> to simulate a hybrid setting by randomly masking KB triples and replace them with text corpus. Experimental results have proved decent improvement, which shed lights on the potential of hybrid question answering systems to integrate heterogeneous information.</p><p>Though there already exist numerous valuable questions answering datasets as listed in <ref type="table" target="#tab_1">Table 1</ref>, these datasets were initially designed to use either structured or unstructured information during annotation. There is no guarantee that these questions need to aggregate heterogeneous information to find the answer. Therefore, designing hybrid question answering systems would probably yield marginal benefits over the non-hybrid ones, which greatly hinders the research development in building hybrid question answering systems.</p><p>To fill in the gap, we construct a heterogeneous QA dataset HYBRIDQA, which is collected by crowdsourcing based on Wikipedia tables. During annotation, each crowd worker is presented with a table along with its hyperlinked Wikipedia passages to propose questions requiring aggregating both forms of information. The dataset consists of   <ref type="bibr" target="#b18">(Talmor and Berant, 2018)</ref>, MetaQA , WikiTableQuestion <ref type="bibr" target="#b14">(Pasupat and Liang, 2015)</ref>. 2) Text-only datasets with single passage: like SQuAD <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref>, DROP <ref type="bibr" target="#b7">(Dua et al., 2019)</ref>. 3) open-domain Text-Only dataset: Trivi-aQA <ref type="bibr" target="#b9">(Joshi et al., 2017)</ref>, HotpotQA , Natural Questions <ref type="bibr" target="#b12">(Kwiatkowski et al., 2019</ref>  models, we carefully employ different strategies to calibrate the annotation process. An example is demonstrated in <ref type="figure">Figure 1</ref>. This table is aimed to describe Burmese flag bearers over different Olympic events, where the second column has hyperlinked passages about the Olympic event, and the fourth column has hyperlinked passages about biography individual bearers. The dataset is both multi-hop and hybrid in the following senses: 1) the question requires multiple hops to achieve the answer, each reasoning hop may utilize either tabular or textual information.</p><p>2) the answer may come from either the table or a passage.</p><p>In our experiments, we implement three models, namely Table-only model, Passage-only, and a heterogeneous model HYBRIDER, which combines both information forms to perform multi-hop reasoning. Our Experiments show that two homogeneous models only achieve EM lower than 20%, while HYBRIDER can achieve an EM over 40%, which concludes the necessity to do multihop reasoning over heterogeneous information on HYBRIDQA. As the HYBRIDER is still far behind human performance, we believe that it would be a challenging next-problem for the community.</p><p>In this section, we describe how we crawl highquality tables with their associated passages, and then describe how we collect hybrid questions. The statistics of HYBRIDQA is in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>Table/Passage Collection To ease the annotation process, we apply the following rules during table crawling. 1) we need tables with rows between 5-20, columns between 3-6, which is appropriate for the crowd-workers to view. 2) we restrain the tables from having hyperlinked cells over 35% of its total cells, which provide an abundant amount of textual information. For each hyperlink in the table, we retrieve its Wikipedia page and crop at most the first 12 sentences from its introduction session as the associated passage. 3) we apply some additional rules to avoid improper tables and finally collect 13,000 high-quality tables.</p><p>Question/Answer Collection We release 13K HITs (human intelligence task) on the Amazon Mechanical Turk platform, where each HIT presents the crowd-worker with one crawled Wikipedia table along with its hyperlinked passages. We require the worker to write six questions as well as their answers. The question annotation phase is not trivial, as we specifically need questions that rely on both tabular and textual information. In order to achieve that, we exemplify abundant examples in our Amazon Turker interface with detailed explanations to help crowd-workers to understand the essence of the "hybrid" question. The guidelines are described as follows:</p><p>? The question requires multiple steps over two information forms of reasoning to answer.</p><p>? <ref type="table" target="#tab_12">Table reasoning</ref> step specifically includes (i) filter our table rows based on equal/greater/less, e.g. "For the XXXI Olympic event", (ii)) superlative operation over a column, e.g. "the earliest Olympic event", (iii) hop between two cells, e.g. "Which event ... participate in ...", (iv) extract information from table, e.g. "In which year did the player ... ".</p><p>? Text reasoning step specifically includes (i) select passages based on the certain mentions, e.g. "the judoka bearer", (ii) extract a span from the passage as the answer.</p><p>? The answer should be a minimum text span from either a table cell or a specific passage.</p><p>Based on the above criteria, we hire five CSmajored graduate students as our "human expert" to decide the acceptance of a HIT. The average completion time for one HIT is 12 minutes, and payment is $2.3 U.S. dollars/HIT.</p><p>Annotation De-biasing As has been suggested in previous papers <ref type="bibr" target="#b10">(Kaushik and Lipton, 2018;</ref><ref type="bibr" target="#b3">Chen and Durrett, 2019;</ref><ref type="bibr" target="#b5">Clark et al., 2019)</ref>, the existing benchmarks on multi-hop reasoning question answering have annotation biases, which makes designing multi-hop models unnecessary. We discuss different biases and our prevention as follows:</p><p>? <ref type="table">Table Bias</ref>: our preliminary study observes that the annotators prefer to ask questions regarding the top part of the table. In order to deal with this issue, we explicitly highlight certain regions in the table to encourage crowd-workers to raise questions regarding the given uniformly-distributed regions.</p><p>? Passage Bias: the preliminary study shows that the annotators like to ask questions regarding the first few sentences in the passage.</p><p>In order to deal with such a bias, we use an algorithm to match the answer with linked passages to find their span and reject the HITs, which have all the answers centered around the first few sentences.</p><p>? Question Bias: the most difficult bias to deal with is the "fake" hybrid question like "when is 2012 Olympic Burmese runner flag bearer born?" for the table listed in <ref type="figure">Figure 1</ref>. Though it seems that "2012 Olympic" is needed to perform hop operation on the table, the "runner flag bearer" already reviews the bearer as "Zaw Win Thet" because there is no other runner bearer in the table. With that said, reading the passage of "Zaw Win Thet" alone can simply lead to the answer. In order to cope with such a bias, we ask "human experts" to spot such questions and reject them.</p><p>Statistics After we harvest the human annotations from 13K HITs (78K questions), we trace back the answers to its source (table or passage). Then we apply several rules to further filter out lowquality annotations: 1) the answer cannot be found from either table or passage, 2) the answer is longer than 20 words, 3) using a TF-IDF retriever can directly find the answer passage with high similarity without relying on tabular information. We filter the question-answer pairs based on the previous criteria and release the filtered version. As our goal is to solve multi-hop hybrid questions requiring a deeper understanding of heterogeneous information. We follow HotpotQA  to construct a more challenging dev/test split in our benchmark. Specifically, we use some statistical features like the "size of the table", "similarity between answer passage and question", "whether question directly mentions the field", etc. to roughly classify the question into two difficulty levels: simple (65%) and hard (35%). We construct our dev and test set by sampling half-half from the two categories. We match the answer span against all the cells and passages in the table and divide the answer source into three categories: 1) the answer comes from a text span in a table cell, 2) the answer comes from a certain linked passage, 3) the answer is computed by using numerical operation like 'count', 'add', 'average', etc. The matching process is approximated, not guaranteed to be 100% correct. We summarize our findings in <ref type="table" target="#tab_5">Table 3</ref>. In the following experiments, we will report the EM/F1 score for these fine-grained question types to better understand our results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Data Analysis</head><p>In this section, we specifically analyze the different aspects of the dataset to provide the overall characteristics of the new dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Question Types</head><p>We heuristically identified question types for each collected question. To identify the question type, we locate the central question word (CQW) in the question and take the neighboring three tokens  to determine the question types. We visualize the distribution in <ref type="figure" target="#fig_1">Figure 2</ref>, which demonstrates the syntactic diversity of the questions in HYBRIDQA.</p><p>how much many whose home into for in in h o w m a n y  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Answer Types</head><p>We further sample 100 examples from the dataset and present the types of answers in <ref type="table" target="#tab_7">Table 4</ref>. As can be seen, it covers a wide range of answer types. Compared to , our dataset covers more number-related or date-related questions, which reflects the nature of tabular data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference Types</head><p>We analyze multi-hop reasoning types in <ref type="figure" target="#fig_2">Figure 3</ref>. According to our statistics, most of the questions require two or three hops to find the answer. 1) Type I question (23.4%) uses  hyperlinked cell as the answer.</p><p>2) Type II question (20.3%) uses Passage ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head><p>In this section, we propose three models we use to perform question answering on HYBRIDQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Table-Only Model</head><p>In this setting, we design a model that can only rely on the tabular information to find the answer. Our model is based on the SQL semantic parser (Zhong   <ref type="bibr" target="#b21">Xu et al., 2017)</ref>, which uses a neural network to parse the given questions into a symbolic form and execute against the table. We follow the SQLNet <ref type="bibr" target="#b21">(Xu et al., 2017)</ref> to flatten the prediction of the whole SQL query into a slot filling procedure. More specifically, our parser model first encode the input question q using BERT  and then decode the aggregation, target, condition separately as described in <ref type="figure" target="#fig_3">Figure 4</ref>. The aggregation slot can have the following values of "argmax, argmin, argmaxdate, argmin-date", the target and condition slots have their potential values based on the table field and its corresponding entries. Though we do not have the ground-truth annotation for these simple SQL queries, we can use heuristics to infer them from the denotation. We use the synthesized question-SQL pairs to train the parser model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Passage-Only Model</head><p>In this setting, we design a model that only uses the hyperlinked passages from the given table to find the answer. Our model is based on DrQA <ref type="bibr" target="#b2">(Chen et al., 2017)</ref>, which first uses an ensemble of several retrievers to retrieve related documents and then concatenate several documents together to do reading comprehension with the state-of-the-art BERT model . The basic architecture is depicted in <ref type="figure" target="#fig_3">Figure 4</ref>, where we use the retriever to retrieve the top-5 passages from the pool and then concatenate them as a document for the MRC model, and the maximum length of the concatenated document is set to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HYBRIDER</head><p>In order to cope with heterogeneous information, we propose a novel architecture called HYBRIDER. We divide the model into two phases as depicted in <ref type="figure" target="#fig_5">Figure 6</ref> and describe them separately below:</p><p>Linking This phase is aimed to link questions to their related cells from two sources: -Cell Matching: it aims to link cells explicitly mentioned by the question. The linking consists of three criteria, 1) the cell's value is explicitly mentioned, 2) the cell's value is greater/less than the mentioned value in question, 3) the cell's value is maximum/minimum over the whole column if the question involves superlative words. -Passage Retriever: it aims to link cells implicitly mentioned by the question through its hyperlinked passage. The linking model consists of a TD-IDF retriever with 2-3 gram lexicon and a longest-substring retriever, this ensemble retriever calculates the distances with all the passages in the pool and highlight the ones with cosine distance lower than a threshold ? . The retrieved passages are mapped back to the linked cell in the  We call the set of cells from these two sources as "retrieved cells" denotes by C. Each retrieved cell c is encoded by 5-element tuple (content, location, description, source, score). Content represents the string representation in the table, Content refers to the absolute row and column index in the table, description refers to the evidence sentence in the hyperlinked passage, which gives highest similarity score to question, source denotes where the entry comes from (e.g. equal/argmax/passage/etc), score denotes the score of linked score normalized to <ref type="bibr">[0,</ref><ref type="bibr">1]</ref>.</p><p>Reasoning This phase is aimed to model the multi-hop reasoning in the table and passage, we specifically break down the whole process into three stages, namely the ranking stage p f (c|q, C), hoping stage p h (c |q, c), and the reading comprehension stage p r (a|P, q). These three stages are modeled with three different neural networks. We first design a cell encoding scheme to encode each cell in the table as depicted in <ref type="figure" target="#fig_4">Figure 5: 1)</ref> for "retrieved cells", it contains information for retrieval source and score, 2) for "plain cells" (not retrieved), we set the information in source and score to empty. We concatenate them with their table field and question, and then fed into a encoder module (BERT) to obtain its vector representation H c . feed them all into the cell encoder to obtain their representations {H c }. The representations are aggregated and further fed to a feed-forward neural network to obtain a score s c , which is normalized over the whole set of linked cell C as follows:</p><formula xml:id="formula_0">p f (c|q, C) = exp(sc) c ?C exp(s c )<label>(1)</label></formula><p>2) Hop model: this model takes the predicted cell from the previous stage and then decide which neighboring cell or itself to hop to. Specifically, we represent each hop pair (c ? c ) using their con-</p><formula xml:id="formula_1">catenated representation H c,c = [H c , H c ].</formula><p>The representation is fed to a feed-forward neural network to obtain a hop score s c,c , which is normalized over all the possible end cells as follows:</p><formula xml:id="formula_2">p f (c |q, c) = exp(s c,c ) c ?Nc?c exp(s c,c )<label>(2)</label></formula><p>3) RC model: this model finally takes the hopped cell c from last stage and find answer from it. If the cell is not hyperlinked, the RC model will simply output its plain text as the answer, otherwise, the plain text of the cell is prepended to the linked passage P (c) for reading comprehension. The prepended passage P and the question are given as the input to the question answering model to predict the score of answer's start and end index as g s (P, q, index) and g e (P, q, index), which are normalized over the whole passage |P | to calculate the likelihood p r (a|P, q) as follows:</p><p>pr(a|P, q) = exp(gs(P, q, as)) i?|P | exp(gs(P, q, i)) gs(P, q, ae) i?|P | ge <ref type="bibr">(P, q, i)</ref> where a s is the start index of answer a and a e is the end index of answer a. By breaking the reasoning process into three stages, we manage to cover the Type-I/II/III/VI questions well. For example, the Type-III question first uses the ranking model to select the most likely cell from retrievers, and then use the hop model to jump to neighboring hyperlinked cell, finally use the RC model to extract the answer. where the marginalization is over all the linked cells c, and all the neighboring cell with answer a in its plain text or linked passages. However, directly maximizing the marginal likelihood is unnecessarily complicated as the marginalization leads to huge computation cost. Therefore, we propose to train the three models independently and then combine them to do inference.</p><p>By using the source location of answers, we are able to 1) infer which cells c in the retrieved set C are valid, which can be applied to train the ranking model, 2) infer which cell it hops to get the answer, which we can be applied to train the hop model. Though the synthesized reasoning paths are somewhat noisy, it is still enough to be used for training the separate models in a weakly supervised manner. For the RC model, we use the passages containing the ground-truth answer to train it. The independent training avoids the marginalization computation to greatly decrease the computation and time cost. During inference, we apply these three models sequentially to get the answer. Specifically, we use greedy search at first two steps to remain only the highest probably cell and finally extract the answer using the RC model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setting</head><p>In the linking phase, we set the retrieval threshold ? to a specific value. All the passages having distance lower than ? will be retrieved and fed as input to the reasoning phase. If there is no passage that has been found with a distance lower than ? , we will simply use the document with the lowest distance as the retrieval result. Increasing ? can increase the recall of correct passages, but also increase the difficulty of the filter model in the reasoning step.</p><p>In the reasoning phase, we mainly utilize BERT  as our encoder for the cells and passages due to its strong semantic understanding. Specifically, we use four BERT variants provided by huggingface library 3 , namely baseuncased, based-cased, large-uncased, and largecased. We train the modules all for 3.0 epochs and save their checkpoint file at the end of each epoch. The filtering, hop, and RC models use AdamW <ref type="bibr" target="#b13">(Loshchilov and Hutter, 2017)</ref> optimizer with learning rates of 2e-6, 5e-6, and 3e-5. We held out a small development set for model selection on the saved checkpoints and use the most performant ones in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dev</head><p>Test In-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>Following previous work <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref>, we use exact match (EM) and F1 as two evaluation metrics. F1 metric measures the average overlap between the prediction and ground-truth answers. We assess human performance on a held-out set from the test set containing 500 instances. To evaluate human performance, we distribute each question along with its table to crowd-workers and compare their answer with the ground-truth answer.</p><p>We obtain an estimated accuracy of EM=88.2 and F1=93.5, which is higher than both SQuAD <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref> and HotpotQA . The higher accuracy is due to the In- <ref type="table" target="#tab_7">Table  questions (over 40%)</ref>, which have much lesser ambiguity than the text-span questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Results</head><p>We demonstrate the experimental results for different models in <ref type="table" target="#tab_13">Table 5</ref>, where we list fine-grained accuracy over the questions with answers in the cell and passage separately. The In- <ref type="table">Table questions</ref> are remarkably simpler than In-Passage question because they do not the RC reasoning step; the overall accuracy is roughly 8-10% higher than its counterpart. With the experimented model variants, the best accuracy is achieved with BERT-large-uncased as backend, which can beat the BERT-base-uncased by roughly 2%. However, its performance is still far lagged behind human performance, leaving ample room for future research.</p><p>Heterogeneous Reasoning From <ref type="table" target="#tab_13">Table 5</ref>, we can clearly observe that using either <ref type="table">Table-</ref>Only or Passage-Only model achieves a poor accuracy below 20%. In contrast, the proposed HYBRIDER can achieve up to 50% EM increase by leveraging both structured and unstructured data during reasoning. It strongly supports the necessity to do heterogeneous reasoning in HYBRIDQA.</p><p>Retriever Threshold We also experiment with a different ? threshold. Having an aggressive retriever increases the recall of the mentioned cells, but it increases the burden for the ranking model. Having a passive retriever can guarantee the precision of predicted cells, but it also potentially miss evidence for the following reasoning phase. There exist trade-offs between these different modes.</p><p>In <ref type="table" target="#tab_13">Table 5</ref>, we experiment with different ? during the retrieval stage and find that the model is rather stable, which means the model is quite insensitive regarding different threshold values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Error Analysis</head><p>To analyze the cause of the errors in HYBRIDER, we propose to break down into four types as <ref type="figure">Figure 7</ref>. Concretely, linking error is caused by the retriever/linker, which fails to retrieve the most relevant cell in the linking phase. In the reasoning phase: 1) ranking error is caused by the ranking model, which fails to assign a high score to the correct retrieved cell. 2) hop error occurs when the correctly ranked cell couldn't hop to the answer cell. 3) RC error refers to the hoped cell is correct, but the RC model fails to extract the correct text span from it. We perform our anal-Linking Acc (87.4%) Ranking Accuracy (87.9%)</p><p>Hop Accuracy (89.2%) RC Acc (62%) <ref type="figure">Figure 7</ref>: The error of HYBRIDER is based on its stages. Pink cell means the answer cell; green means the model's prediction; circle means the current cell.</p><p>ysis on the full dev set based on the bert-largeuncased model (? =0.8), as indicated in <ref type="figure">Figure 7</ref>, the errors are quite uniformly distributed into the four categories except the reading comprehension step is slightly more erroneous. Based on the step-wise error, we can compute its product as 87.4% ? 87.9% ? 89.2% ? 61.9% ? 42% and find that the result consistent well the overall accuracy, which demonstrates the necessity to perform each reasoning step correctly. Such error cascading makes the problem extremely difficult than the previous homogeneous question answering problems. By breaking down the reasoning into steps, HY-BRIDER layouts strong explainability about its rationale, but it also causes error propagation, i.e., the mistakes made in the earlier stage are nonreversible in the following stage. We believe future research on building an end-to-end reasoning model could alleviate such an error propagation problem between different stages in HYBRIDER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Text-Based QA Since the surge of SQuAD (Rajpurkar et al., 2016) dataset, there have been numerous efforts to tackle the machine reading comprehension problem. Different datasets like DrQA <ref type="bibr" target="#b2">(Chen et al., 2017)</ref>, TriviaQA <ref type="bibr" target="#b9">(Joshi et al., 2017)</ref>, SearchQA <ref type="bibr" target="#b8">(Dunn et al., 2017)</ref> and DROP <ref type="bibr" target="#b7">(Dua et al., 2019)</ref> have been proposed. As the SQuAD <ref type="bibr" target="#b15">(Rajpurkar et al., 2016)</ref> questions that are relatively simple because they usually require no more than one sentence in the paragraph to answer. The following datasets further challenge the QA model's capability to handle different scenarios like open-domain, long context, multi-hop, discrete operations, etc. There has been a huge success in proving that the deep learning model can show strong competence in terms of understanding textonly evidence. Unlike these datasets, HYBRIDQA leverages structured information in the evidence form, where the existing models are not able to handle, which distinguishes it from the other datasets.</p><p>KB/Table-Based QA Structured knowledge is known as unambiguous and compositional, which absorbs lots of attention to the QA system built on KB/Tables. There have been multiple datasets like WebQuestion <ref type="bibr" target="#b0">(Berant et al., 2013)</ref>, ComplexWe-bQuestions <ref type="bibr" target="#b18">(Talmor and Berant, 2018)</ref>, <ref type="bibr">WebQues-tionSP (Yih et al., 2015)</ref> on using FreeBase to answer natural questions. Besides KB, structured or semi-structured tables are also an interesting form. Different datasets like WikiTableQuestions <ref type="bibr" target="#b14">(Pasupat and Liang, 2015)</ref>, WikiSQL <ref type="bibr" target="#b27">(Zhong et al., 2017)</ref>, SPIDER <ref type="bibr" target="#b25">(Yu et al., 2018)</ref>, TabFact <ref type="bibr" target="#b4">(Chen et al., 2020)</ref> have been proposed to build models which can interact with such structured information.</p><p>However, both KB and tables are known to suffer from low coverage issues. Therefore, HYBRIDQA combine tables with text as complementary information to answer natural questions.</p><p>Information Aggregation There are some pioneering studies on designing hybrid question answering systems to aggregate heterogeneous information. GRAFT <ref type="bibr" target="#b17">(Sun et al., 2018)</ref> proposes to use the early fusion system and use heuristics to build a question-specific subgraph that contains sentences from corpus and entities, facts from KB. PullNet <ref type="bibr" target="#b16">(Sun et al., 2019)</ref> improves over GRAFT to use an integrated framework that dynamically learns to retrieve and reason over heterogeneous information to find the best answers. More recently, KAReader <ref type="bibr" target="#b20">(Xiong et al., 2019)</ref> proposes to reformulate the questions in latent space by reading retrieved text snippets under KB-incomplete cases. These models simulate a 'fake' KB-incomplete scenario by masking out triples from KB. In contrast, the questions in HYBRIDQA are inherently hybrid in the sense that it requires both information forms to reason, which makes our testbed more realistic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present HYBRIDQA, which is collected as the first hybrid question answering dataset over both tabular and textual data. We release the data to facilitate the current research on using heterogeneous information to answer real-world questions. We design HYBRIDER as a strong baseline and offer interesting insights about the model. We believe HYBRIDQA is an interesting yet challenging nextproblem for the community to solve.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The type of questions in HYBRIDQA, question types are extracted using rules starting at the question words or preposition before them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of different types of multi-hop questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of the table-only and passageonly baselines, both are based on BERT Encoder. et al., 2017;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of cell encoder of retrieved (green) and plain cells (orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Illustration of the proposed model to perform multi-hop reasoning over table and passage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Training &amp; Inference The three-stage decomposition breaks the question answering likelihood p(a|q, T ) into the following marginal probability: c?C p f (c|q, C) c ?Nc;a?P (c ) p f (c |c, q)pr(a|P (c ), q)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:2004.07347v3 [cs.CL] 11 May 2021 Summer Olympics officially known as the Games of the XXXI Olympiad (Portuguese : Jogos da XXXI Olimp?ada) and commonly known as Rio 2016 , was an international multi-sport event ?? Yan Naing Soe ( born 31 January 1979 ) is a Burmese judoka . He competed at the 2016 Summer Olympics in the men 's 100 kg event , ?? He was the flag bearer for Myanmar at the Parade of Nations .</figDesc><table><row><cell></cell><cell cols="5">Name XXXI XXX XXIX XXVIII XXVII The 2016 Win Maung ( born 12 May 1949 ) is a Burmese footballer . He Year Season Flag bearer 2016 Summer Yan Naing Soe 2012 Summer Zaw Win Thet 2008 Summer Phone Myint Tayzar 2004 Summer Hla Win U 2000 Summer Maung Maung Nge Zaw Win Thet ( born 1 March 1991 in Kyonpyaw , Pathein District , Ayeyarwady Division , Myanmar ) is a Burmese runner who ?? ?? Myint Tayzar Phone ( Burmese : ???? ? ?????+ ?? ? ) born July 2 , 1978 ) is a sprint canoer from Myanmar who competed in the late 2000s .</cell></row><row><cell></cell><cell>XX</cell><cell>1972</cell><cell></cell><cell>Summer</cell><cell>Win Maung</cell><cell>competed in the men 's tournament at the 1972 Summer Olympics ?</cell></row><row><cell></cell><cell cols="5">Q: In which year did the judoka bearer participate in the Olympic opening ceremony?</cell><cell>A: 2016</cell></row><row><cell></cell><cell cols="5">Q: Which event does the does the XXXI Olympic flag bearer participate in?</cell><cell>A: men's 100 kg event</cell></row><row><cell>Hardness</cell><cell cols="5">Q: Where does the Burmesse jodoka participate in the Olympic opening ceremony as a flag bearer? Q: For the Olympic event happening after 2014, what session does the Flag bearer participate?</cell><cell>A: Rio A: Parade of Nations</cell></row><row><cell></cell><cell cols="5">Q: For the XXXI and XXX Olympic event, which has an older flag bearer?</cell><cell>A: XXXI</cell></row><row><cell></cell><cell cols="5">Q: When does the oldest flag Burmese bearer participate in the Olympic ceremony?</cell><cell>A: 1972</cell></row><row><cell cols="2">Dataset</cell><cell></cell><cell>Size</cell><cell>#Docs</cell><cell>KB/ Table</cell><cell>Multi-Hop</cell></row><row><cell cols="3">WebQuestions</cell><cell>5.8K</cell><cell>no</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="2">WebQSP</cell><cell></cell><cell>4.7K</cell><cell>no</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="3">WebQComplex</cell><cell>34K</cell><cell>no</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="2">MetaQA</cell><cell></cell><cell>400k</cell><cell>no</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="3">WikiTableQA</cell><cell>22K</cell><cell>no</cell><cell>yes</cell><cell>yes</cell></row><row><cell cols="3">SQuAD-v1</cell><cell>107K</cell><cell>1</cell><cell>no</cell><cell>no</cell></row><row><cell cols="2">DROP</cell><cell></cell><cell>99K</cell><cell>1</cell><cell>no</cell><cell>yes</cell></row><row><cell cols="2">TriviaQA</cell><cell></cell><cell>95K</cell><cell>&gt;1</cell><cell>no</cell><cell>no</cell></row><row><cell cols="2">HotpotQA</cell><cell></cell><cell>112K</cell><cell>&gt;1</cell><cell>no</cell><cell>yes</cell></row><row><cell cols="3">Natural-QA</cell><cell>300K</cell><cell>&gt;1</cell><cell>no</cell><cell>yes</cell></row><row><cell cols="3">HYBRIDQA</cell><cell>70K</cell><cell>&gt;&gt;1</cell><cell>yes</cell><cell>yes</cell></row></table><note>Figure 1: Examples of annotated question answering pairs from Wikipedia page 2 . Underlined entities have hyper- linked passages, which are displayed in the boxes. The lower part shows the human-annotated question-answer pairs roughly categorized based on their hardness.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of existing datasets, where #docs means the number of documents provided for a spe- cific question. 1) KB-only datasets: WebQuestions (Be- rant et al., 2013), WebQSP (Yih et al., 2016), Web- Complex</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Statistics of Table and Passage in our dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Data Split: In-Table means the answer comes from plain text in the table, and In-Passage means the answer comes from certain passage.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Types of answers in HYRBIDQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table chain</head><label>chain</label><figDesc></figDesc><table /><note>, it first uses cues present in the question to retrieve related passage, which traces back to cer- tain hyperlinked cells in the table, and then hop to a neighboring cell within the same row, finally extracts text span from that cell. 3) Type III question (35.1%) uses Passage ?Table?Passage chain, it follows the same pat- tern as Type II, but in the last step, it hops to a hy- perlinked cell and extracts answer from its linked passage. This is the most common pattern. 4) Type IV question (17.3%) uses Passage and Ta- ble jointly to identify a hyperlinked cell based on table operations and passage similarity and then extract the plain text from that cell as the answer. 5) Type V question (3.1%) involves two parallel reasoning chain, while the comparison is involved in the intermediate step to find the answer. 6) Type VI questions (0.8%) involve multiple rea- soning chains, while superlative in involved in the intermediate step to obtain the correct answer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>table .</head><label>.</label><figDesc></figDesc><table><row><cell>Country</cell><cell>Name</cell><cell cols="2">School</cell><cell>Date</cell><cell cols="2">Cell Encoder</cell></row><row><cell>US</cell><cell>Yan</cell><cell>?</cell><cell></cell><cell>Jul 24</cell><cell></cell><cell>!</cell></row><row><cell>CA</cell><cell></cell><cell>?</cell><cell></cell><cell>Jun 27</cell><cell>Encoder</cell></row><row><cell>Content</cell><cell>Yan</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Location</cell><cell>(1, 2)</cell><cell></cell><cell>Content</cell><cell>Jun 27</cell><cell></cell></row><row><cell>Descrip</cell><cell cols="2">Born in ? Yan is</cell><cell>Location</cell><cell>(2, 4)</cell><cell>Header</cell><cell>Ques</cell></row><row><cell>Source</cell><cell>Longest-String</cell><cell></cell><cell>Descrip</cell><cell>""</cell><cell></cell></row><row><cell>Score</cell><cell>0.6</cell><cell></cell><cell cols="2">Plain Cell</cell><cell></cell></row><row><cell cols="2">Retrieved Cell</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table In</head><label>In</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>-Passage</cell><cell>Total</cell><cell>In-Table</cell><cell>In-Passage</cell><cell>Total</cell></row><row><cell></cell><cell>EM/F1</cell><cell>EM/F1</cell><cell>EM/F1</cell><cell>EM/F1</cell><cell>EM/F1</cell><cell>EM/F1</cell></row><row><cell>Table-Only</cell><cell>14.7/19.1</cell><cell>2.4/4.5</cell><cell>8.4/12.1</cell><cell>14.2/18.8</cell><cell>2.6/4.7</cell><cell>8.3/11.7</cell></row><row><cell>Passage-Only</cell><cell>9.2/13.5</cell><cell>26.1/32.4</cell><cell>19.5/25.1</cell><cell>8.9/13.8</cell><cell>25.5/32.0</cell><cell>19.1/25.0</cell></row><row><cell>HYBRIDER (BERT-base-uncased, ? =0.7)</cell><cell>51.2/58.6</cell><cell>39.6/46.4</cell><cell cols="2">42.9/50.0 50.9/58.6</cell><cell>37.4/45.7</cell><cell>41.8/49.5</cell></row><row><cell>HYBRIDER (BERT-base-uncased, ? =0.8)</cell><cell>51.3/58.4</cell><cell>40.1/47.6</cell><cell cols="2">43.5/50.6 51.7/59.1</cell><cell>37.8/46.0</cell><cell>42.2/49.9</cell></row><row><cell>HYBRIDER (BERT-base-uncased, ? =0.9)</cell><cell>51.5/58.6</cell><cell>40.5/47.9</cell><cell cols="2">43.7/50.9 52.1/59.3</cell><cell>38.1/46.3</cell><cell>42.5/50.2</cell></row><row><cell cols="2">HYBRIDER (BERT-large-uncased, ? =0.8) 54.3/61.4</cell><cell>39.1/45.7</cell><cell cols="2">44.0/50.7 56.2/63.3</cell><cell>37.5/44.4</cell><cell>43.8/50.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 5 :</head><label>5</label><figDesc>Experimental results of different models, In-Table refersto the subset of questions which have their answers in the table, In-Passage refers to the subset of questions which have their answer in a certain passage.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">) Ranking model: As the "retriever cells" contain many noises, we leverage a ranker model to predict the "correct" linked cells for the next stage. Specifically, this model takes each cell c along with its neighboring N c (cells in the same row) and</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/huggingface/ transformers</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>The authors would like to thank the anonymous reviewers and area chairs for their thoughtful comments. We would like to thank Amazon AWS Machine Learning Research Award for sponsoring the computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic parsing on freebase from question-answer pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Methods for exploring and mining tables on wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Sekhar Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanapon</forename><surname>Noraset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGKDD Workshop on Interactive Data Exploration and Analytics</title>
		<meeting>the ACM SIGKDD Workshop on Interactive Data Exploration and Analytics</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="18" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reading wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1870" to="1879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding dataset design choices for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4026" to="4032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Tabfact: A largescale dataset for table-based fact verification. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Don&apos;t take the easy way out: Ensemble based methods for avoiding known dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.03683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pradeep</forename><surname>Dasigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Sagun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Ugur Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05179</idno>
		<title level="m">Searchqa: A new q&amp;a dataset augmented with context from a search engine</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1601" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How much reading does reading comprehension require? a critical investigation of popular benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5010" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scaling semantic parsers with on-the-fly ontology matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1545" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Natural questions: a benchmark for question answering research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennimaria</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivia</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danielle</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compositional semantic parsing on semi-structured tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pullnet: Open domain question answering with iterative retrieval on knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tania</forename><surname>Bedrax-Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2380" to="2390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Open domain question answering using early fusion of knowledge bases and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Mazaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4231" to="4242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="641" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Towards ai-complete question answering: A set of prerequisite toy tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.05698</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving question answering over incomplete kbs with knowledgeaware reader</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4258" to="4264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04436</idno>
		<title level="m">Sqlnet: Generating structured queries from natural language without reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The value of semantic parse labeling for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jina</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingning</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanelle</forename><surname>Roman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3911" to="3921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational reasoning for question answering with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00103</idno>
		<title level="m">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
