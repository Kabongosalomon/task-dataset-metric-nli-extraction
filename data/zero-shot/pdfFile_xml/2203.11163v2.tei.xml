<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
							<email>w32zhong@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
							<email>jheng-hong.yang@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Xie</surname></persName>
							<email>yuqing.xie@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
							<email>jimmylin@uwaterloo.ca</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Cheriton</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Evaluating Token-Level and Passage-Level Dense Retrieval Models for Math Information Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>With the recent success of dense retrieval methods based on bi-encoders, studies have applied this approach to various interesting downstream retrieval tasks with good efficiency and in-domain effectiveness. Recently, we have also seen the presence of dense retrieval models in Math Information Retrieval (MIR) tasks, but the most effective systems remain classic retrieval methods that consider hand-crafted structure features. In this work, we try to combine the best of both worlds: a well-defined structure search method for effective formula search and efficient bi-encoder dense retrieval models to capture contextual similarities.</p><p>Specifically, we have evaluated two representative bi-encoder models for token-level and passage-level dense retrieval on recent MIR tasks. Our results show that bi-encoder models are highly complementary to existing structure search methods, and we are able to advance the state-of-the-art on MIR datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Math Information Retrieval (MIR) is a special information retrieval domain that deals with heterogeneous data. The core task in this field is to retrieve relevant information from documents that contain math formulas. As digitized math content (mostly in L A T E X markup or MathML format) becomes readily available nowadays, being able to index and retrieve formulas (or equations) in those documents effectively is possibly one of the hard nuts left to be cracked before we can search freely for scientific documents, educational materials, and other math content.</p><p>The need to measure similarities for highly structured formulas with special semantic properties and to model their connections to the surrounding text have a few interesting consequences: (1) Heuristic scores like the term-frequency factor in tf-idf scoring variants become less relevant in for-mula similarity assessment because symbols in a math formula can be interchangeable and similarity may depend on expression structure rather than frequency of co-occurrence. (2) At the same time, the same math content can be expressed differently, e.g., {1, 2, ...} and N + represent the same concept but they are made of totally different tokens. We also need to capture similar math expressions with different structure due to math transformations, e.g., 1 + 1</p><p>x and 1+x x . These have made structure search approaches alone suboptimal. (3) Many existing methods score math and text separately because they are of different modalities; however, failing to catch cross references between text and math will penalize retrieval effectiveness. For example, a top effective math-aware search engine adopting traditional ad-hoc search techniques <ref type="bibr" target="#b5">(Fraser et al., 2018;</ref><ref type="bibr" target="#b25">Ng et al., 2020</ref> tunes a hyperparameter to weight text and formulas in two separate passes, which provides little awareness of the connections between formulas and their surrounding text. The aforementioned challenges have put limitations on further advances in this field.</p><p>On the other hand, recent bi-encoder dense retrieval models <ref type="bibr" target="#b12">(Karpukhin et al., 2020;</ref><ref type="bibr" target="#b31">Santhanam et al., 2021;</ref><ref type="bibr" target="#b10">Hofst?tter et al., 2021;</ref><ref type="bibr" target="#b4">Formal et al., 2021;</ref><ref type="bibr" target="#b7">Gao and Callan, 2021)</ref> have been shown to be highly effective for in-domain retrieval while remaining efficient for large corpora in practice. Compared to traditional retrieval methods, these models use dual deep encoders, usually built on top of a Transformer encoder architecture <ref type="bibr" target="#b35">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b3">Devlin et al., 2019)</ref>, to encode query and document passages separately and eventually output contextual embeddings. Similarity scores can be efficiently computed given these embeddings, which limits costly neural inference to indexing time. The effectiveness of these models can be attributed to the encoder's ability to capture contextual connections or even high-level semantics without the necessity for exact lexical match-ing. This very complementary benefit compared to more rigorous structure search methods motivates us to investigate whether dense retrieval models can improve MIR results when combined with existing structure search methods. We summarize the contributions of this work as follows:</p><p>? We have performed a fair effectiveness comparison of a token-level and a passage-level dense retrieval baseline in the MIR domain. To our knowledge, this is the first time that a DPR model has been evaluated in this domain.</p><p>? We have successfully combined dense retrievers with a structure search system and have been able to achieve new state-of-the-art effectiveness in recent MIR datasets.</p><p>? A comprehensive list of dense retrievers and strong baselines for major MIR datasets are covered and compared. We believe our well-trained models and data pipeline 1 can serve as a stepping stone for future research in this domain, which suffers from a scarcity of resources.</p><p>2 Background and Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classic and Structure Search</head><p>Research on math information retrieval started with the DLMF project from NIST decades ago <ref type="bibr" target="#b22">(Miller and Youssef, 2003)</ref>. Naturally, early studies <ref type="bibr" target="#b22">(Miller and Youssef, 2003;</ref><ref type="bibr" target="#b37">Youssef, 2005)</ref> directly converted math symbols to textualized tokens (e.g., "+" will be converted to "plus") so they can be easily retrieved with existing IR systems. Later, a line of studies <ref type="bibr" target="#b9">(Hijikata et al., 2009;</ref><ref type="bibr" target="#b33">Sojka and L??ka, 2011;</ref><ref type="bibr" target="#b16">Lin et al., 2014;</ref><ref type="bibr" target="#b40">Zanibbi et al., 2015;</ref><ref type="bibr" target="#b15">Kristianto et al., 2016;</ref><ref type="bibr" target="#b5">Fraser et al., 2018)</ref> utilizing full-text search engines additionally introduced various intermediate tree representations to extract features that capture more structure information.</p><p>The MathDowsers system <ref type="bibr" target="#b5">(Fraser et al., 2018;</ref><ref type="bibr" target="#b25">Ng et al., 2020</ref><ref type="bibr" target="#b23">Ng, 2021)</ref> stands out in retrieval effectiveness by the incorporation of a mature full-text search engine and a curated list of over 5 types of features extracted from the Symbol Layout Tree (SLT) representation <ref type="bibr" target="#b39">(Zanibbi and Blostein, 2012)</ref>. Other features like the leafroot paths extracted from the Operator Trees or representational MathML DOMs are also popular 1 Our model checkpoints and source code are made publicly available: https://github.com/approach0/ math-dense-retrievers/tree/emnlp2022 among researchers <ref type="bibr" target="#b9">(Hijikata et al., 2009;</ref><ref type="bibr" target="#b36">Yokoi and Aizawa, 2009;</ref><ref type="bibr" target="#b42">Zhong, 2015;</ref><ref type="bibr" target="#b43">Zhong and Fang, 2016)</ref>; these features are invariant to operand position mutation (e.g., due to commutativity) and require less storage. More strict and top-down approaches <ref type="bibr" target="#b14">(Kohlhase et al., 2012;</ref><ref type="bibr" target="#b32">Schellenberg et al., 2012;</ref><ref type="bibr" target="#b41">Zanibbi et al., 2016b;</ref><ref type="bibr" target="#b46">Zhong and Zanibbi, 2019;</ref><ref type="bibr" target="#b18">Mansouri et al., 2020)</ref> have also been proposed by evaluating well-defined math formula structure similarity or edit distance, resulting in higher precision in top-ranked results generally. Furthermore, <ref type="bibr" target="#b45">Zhong et al. (2020)</ref> have shown that a top-down structure search method can be accelerated to achieve practically efficient first-stage retrieval as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data-Driven Methods</head><p>More recently, data-driven approaches that incorporate word embeddings <ref type="bibr" target="#b6">(Gao et al., 2017;</ref><ref type="bibr" target="#b20">Mansouri et al., 2019)</ref>, GNNs <ref type="bibr" target="#b34">(Song and Chen, 2021)</ref>, or Transformer models <ref type="bibr">Reusch et al., 2021a,b)</ref> have also been proposed for the MIR domain. By observing token co-occurrence and structure features during training, these models can discover synonyms or high-level semantic similarities, making them a good enhancement to strict structure matching. However, previous Transformer-based retrievers in this domain <ref type="bibr" target="#b19">(Mansouri et al., 2021a;</ref><ref type="bibr">Reusch et al., 2021a,b)</ref> either only evaluate partial collections due to the adoption of expensive cross encoders, or cover only a token-level bi-encoder retriever, i.e., using the ColBERT model <ref type="bibr" target="#b13">(Khattab and Zaharia, 2020;</ref><ref type="bibr" target="#b31">Santhanam et al., 2021)</ref>. The effectiveness of a finetuned bi-encoder Transformer retriever for passagelevel semantic similarity remains unknown.</p><p>In this work, we examine the DPR model <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref> as a passage-level dense retriever baseline for the MIR domain. We also fine-tune a ColBERT model <ref type="bibr" target="#b13">(Khattab and Zaharia, 2020</ref>) that greatly outperforms the same type of models previously described in this domain. Furthermore, previous efforts <ref type="bibr" target="#b20">(Mansouri et al., 2019;</ref> to consider structure features using data-driven models have achieved good levels of effectiveness; we will follow this path and evaluate the combination of structure-matching methods and dense retrieval.</p><p>Finally, some previous effective cross-encoder math retrieval runs <ref type="bibr">(Reusch et al., 2021b)</ref> are based on further-pretrained backbone models in this do-main. However, this domain-adaptive pretraining (DAPT) <ref type="bibr">(Gururangan et al., 2020)</ref> shows inconsistent benefits to downstream tasks <ref type="bibr" target="#b48">(Zhu et al., 2021)</ref>. In this work, we wish to investigate and compare different bi-encoder backbones on downstream retrieval effectiveness in a fair manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dense Retrieval Models</head><p>DPR In the Dense Passage Retriever (DPR) architecture <ref type="bibr" target="#b12">(Karpukhin et al., 2020)</ref>, a Transformer encoder E(?) is applied to the query or passage: the output embedding corresponding to the <ref type="bibr">[CLS]</ref> token is used to calculate a similarity score. To facilitate retrieval efficiency, a simple dot product is used:</p><formula xml:id="formula_0">S(q, p) = E(q) T ? E(p)<label>(1)</label></formula><p>where S(q, p) represents the similarity between a query q and a passage p.</p><p>During training, a pretrained model is used as the initial encoder state, and the encoder is optimized through the objective of a contrastive loss consisting of a query and a pair of positive and negative passages, p + and p ? . A common practice in training a batch of queries {q i } B 1 is to utilize passages of other training instances from the batch as additional in-batch negatives in the loss function:</p><formula xml:id="formula_1">L (i) (q i , p + i , p ? i,1 , ..., p ? i,2B?1 ) = ? log exp S(q i , p + i ) exp S(q i , p + i ) + 2B?1 j=1 exp S(q i , p ? i,j )<label>(2)</label></formula><p>ColBERT Instead of using a single passage-level embedding, the ColBERT model <ref type="bibr" target="#b13">(Khattab and Zaharia, 2020;</ref><ref type="bibr" target="#b31">Santhanam et al., 2021)</ref> preserves all output embeddings for the similarity calculation.</p><p>Since each Transformer encoder is pretrained using the MLM objective <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>, the model provides fine-grained contextualized semantics for individual tokens. Given a query token sequence q = q 0 , q 1 , ...q l and a passage token sequence p = d 1 , d 2 , ...d n , ColBERT uses either dot product or L2 distance of normalized embeddings for computing the tokenlevel similarity score s(q i , d j ). During scoring, it locates the highest-scoring token of a passage d j for each query token q i (i.e., the MaxSim operator), and a summation is taken over these partial scores as the overall similarity between query q and passage p:</p><formula xml:id="formula_2">S(q, p) = i?[E(q)] max j?[E(p)] s(q i , d j ).</formula><p>( <ref type="formula">3)</ref> Similar to the DPR model, given a triple of query and a contrastive passage pair, i.e., (q, p + , p ? ), the ColBERT model optimizes a pairwise softmax cross-entropy loss. Because ColBERT uses all passage token embeddings, it applies a linear pooling layer on top of its backbone encoder to obtain smaller fixed-size (d = 128 by default) embedding outputs for more efficient score computation or token-level indexing.</p><p>In addition, the model prepends two different special tokens, [Q] or [D], to distinguish the encoding of a query or a passage. In practice, the authors also demonstrate improved effectiveness via query augmentation by rewriting [PAD] query tokens to [MASK] tokens before query encoding.</p><p>In end-to-end retrieval, however, ColBERT typically relies on two-stage query processing for efficiency: (1) A candidate set of tokens is retrieved using highly efficient approximate nearest neighbors (ANN) search techniques (e.g., <ref type="bibr" target="#b11">J?gou et al., 2011)</ref>. (2) Then, the passages containing these tokens need to be located. (3) Finally, the candidate passages are then sent to the GPU for fast matrix multiplication to calculate token similarities for each query and passage in the candidate pool. Due to the candidate selection pipeline, this process is regarded as an approximate version of retrieval for the similarity search specified by Eq. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Fusing Dense and Structure Signals</head><p>Although  have performed structure mask pretraining for better matching formula substructures, their method is still based on additional structure embeddings generated from a different system. However, we argue that a dense retrieval model may excel at adding fuzziness and recall to math retrieval without being constrained to require a structure match in candidates. Given that previous math retrieval systems <ref type="bibr" target="#b45">(Zhong et al., 2020</ref> have already incorporated effective formula structure matching, we wish to combine existing well-defined structure similarity search systems with more fuzzy and higher-level semantic search capabilities from dense retrieval models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Evaluations in this paper are conducted on two recent MIR tasks:</p><p>NTCIR-12 Wiki-Formula <ref type="bibr" target="#b38">(Zanibbi et al., 2016a)</ref> A formula-only retrieval task made from mathrelated pages in Wikipedia. Both queries and documents are isolated formulas encoded using L A T E X. We consider all 20 concrete queries (no wildcards for formula variables) and index all (around 591,000) formulas as documents in this task. Judgment ratings are provided on a scale of 0 to 3. For each judged formula, the ratings are mapped to fully relevant (? 2), partially relevant (? 1), or irrelevant (= 0).</p><p>ARQMath-2 (main task) <ref type="bibr" target="#b21">(Mansouri et al., 2021b</ref>) A CLEF answer retrieval task for math-related questions. The collection includes roughly 1 million questions, containing 28 million formulas extracted from the MSE (Math StackExchange) website. <ref type="bibr">2</ref> There are 100 question posts sampled from MSE, where 71 of these questions are sufficiently evaluated (an average of 450 answers per topic are assessed by human experts). The official evaluation measurements in ARQMath are prime-versions of NDCG, MAP, and Precision at 10. They differ from the original metrics in that unjudged documents from the ranked lists are removed before evaluation. Relevance levels include High (= 3), Medium (= 2), Low (= 1), and Irrelevant (= 0). High and Medium relevance are collapsed for binary evaluation metrics.</p><p>We use the official evaluation metrics and protocols for both tasks. Each run contains a ranked list of 1000 documents per query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pretraining Configurations</head><p>We consider three types of pretrained Transformer backbones for downstream math retrieval tasks.</p><p>BERT <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref> A Transformer encoder pretrained using MLM and NSP objectives on a large corpus comprising the Toronto Book Corpus and English Wikipedia.</p><p>SciBERT <ref type="bibr" target="#b0">(Beltagy et al., 2019)</ref> A further pretrained Transformer encoder built on the BERT base model using 1.14M scientific papers with additional vocabularies for scientific content.</p><p>Example: Inequality between norm 1, norm 2 and norm ? of matrices:</p><formula xml:id="formula_3">A 2 ? A 1 A ?</formula><p>Output: Inequality between norm 1, norm 2 and norm &lt;in-fty&gt; of matrices: &lt;Vert&gt; &lt;A&gt; &lt;Vert&gt; &lt;subscript&gt; &lt;2&gt; &lt;le&gt; &lt;root&gt; &lt;{&gt; &lt;Vert&gt; &lt;A&gt; &lt;Vert&gt; &lt;subscript&gt; &lt;1&gt; &lt;Vert&gt; &lt;A&gt; &lt;Vert&gt; &lt;subscript&gt; &lt;infty&gt; &lt;}&gt; <ref type="figure">Figure 1</ref>: Example of pre-tokenized math content containing L A T E X markup. The output has all meaningful or syntactic L A T E X tokens preserved.</p><p>Our further-pretrained BERTs We further pretrain the BERT base model on 1.69M math-related documents composed of texts and math formulas using the MLM and NSP objectives proposed by <ref type="bibr" target="#b3">Devlin et al. (2019)</ref>. Specifically, we crawl MSE and the Art of Problem Solving 3 websites. Out of 9M sentences from these documents, we extracted 2.2M sentence pairs for training. All L A T E X markup are pre-tokenized using the PyA0 toolkit , which unifies semantically identical tokens (e.g., \frac and \dfrac), adding 539 new tokens into the vocabulary space. An example of the pre-tokenizing process can be found in <ref type="figure">Figure 1</ref>. We treat these L A T E X tokens as regular text during training. Our own backbone is trained on eight A100 GPUs with a batch size of 240 for 3 and 7 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fine-Tuning Configurations</head><p>On top of different backbones, we fine-tune our bi-encoder models using the ARQMath collection as training data (we use Q&amp;A posts prior to the year 2018). Given a query q, we sample a positive passage p + from accepted answers, duplicate questions, or any answer posts to the query receiving more than 7 upvotes. A random answer passage related to the same tags is treated as a hard negative sample p ? . We obtain 607K (q, p + , p ? ) triplets for training dense retrieval models. We use the AdamW optimizer <ref type="bibr" target="#b17">(Loshchilov and Hutter, 2017)</ref> in all our experiments, with a weight decay of 0.01, and a learning rate of 1 ? 10 ?6 for ColBERT and 3 ? 10 ?6 for DPR. Following <ref type="bibr">Reusch et al. (2021b)</ref>, we set the maximum number of input tokens to 512.</p><p>DPR models trained for 1 epoch To validate the effectiveness of our pretrained backbones, we design a comparative experiment where DPR models on different backbones are trained for the same number of steps (?550K iterations, approximately one epoch) with a batch size of 15. The goal of these conditions is to quickly compare the effectiveness of different backbones. Fully-trained models To maximize effectiveness, we fine-tune our DPR and ColBERT models on our backbone that has been further pretrained for 7 epochs. We fine-tune DPR for 10 epochs with a batch size of 36, and ColBERT with a batch size of 30 for 3 epochs, both on A6000 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Structure Search Fusion</head><p>The best way to add structure similarity awareness to dense retrieval models remains an important and open problem. In this work, we make the first attempt to simply merge dense retrieval results with results generated from a structure search system, Approach0 <ref type="bibr" target="#b46">(Zhong and Zanibbi, 2019;</ref><ref type="bibr" target="#b45">Zhong et al., 2020)</ref>. Approach0 takes a top-down approach to evaluate formula similarities, and thus it is very complementary to more fuzzy semantic retrieval.</p><p>We evaluate search fusion against the NTCIR-12 and ARQMath-2 tasks. Based on structure search results, which are generated by Approach0 and tuned on a different dataset (i.e., ARQMath-1), we perform one of two alternatives: (1) rerank the baseline using inference scores from DPR or ColBERT;</p><p>(2) linearly combine scores from the baseline and from DPR or ColBERT. In the second case, we perform 5-fold cross validation to tune a weight ? ? {0.1, ..., 0.9}. The final fusion score S f is interpolated by a convex combination:</p><formula xml:id="formula_4">S f = ? ? S d + (1 ? ?) ? S a ,</formula><p>where S d , S a are the scores from the dense retrievers and the structure search, respectively. Original scores are rescaled using min-max normalization, and when a document is missing from the other source during fusion, we set its score to zero.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Baselines</head><p>For the NTCIR-12 dataset, we compare our scores to the only Transformer retriever reported on this dataset, i.e., MathBERT ), a BERT model with specialized structure-aware further pretraining. However, their results should be regarded as an ensemble run because they are generated from reranking a highly effective run produced by Tangent-CFT <ref type="bibr" target="#b20">(Mansouri et al., 2019)</ref>.</p><p>For the ARQMath-2 dataset, we select other biencoder Transformer runs submitted or reported on the ARQMath-2 main task <ref type="bibr" target="#b21">(Mansouri et al., 2021b)</ref>. This includes ColBERT runs based on different backbones from the TU_DBS team <ref type="bibr">(Reusch et al., 2021b)</ref>, using the weights of the original BERTbase, SciBERT <ref type="bibr" target="#b0">(Beltagy et al., 2019)</ref>, and a Col-ARQBERT pretrained from scratch on the ARQ-Math corpus. Furthermore, two additional effective bi-encoder models-CompuBERT <ref type="bibr" target="#b26">(Novotn? et al., 2021)</ref> from MIRMU and FormulaEmb <ref type="bibr" target="#b1">(Dadure et al., 2021)</ref>-are also compared. The former uses averaged token embeddings of SentenceBERT finetuned by minimizing the cosine distance of questions to their accepted or high-ranking answers, while the latter uses pretrained Transformer embeddings directly for similarity computations.</p><p>In our fusion results, we compare top-effective existing systems. For the NTCIR-12 dataset, we include: MCAT <ref type="bibr" target="#b15">(Kristianto et al., 2016</ref>)an expensive MIR system that takes on average over 25 seconds per query to run; the Tangent-S system <ref type="bibr" target="#b2">(Davila and Zanibbi, 2017)</ref> using lowgranularity structure node pairs; and its successor Tangent-CFT <ref type="bibr" target="#b20">(Mansouri et al., 2019)</ref> based on FastText embeddings of local structures from the SLT representation; a GNN model for formula retrieval <ref type="bibr" target="#b34">(Song and Chen, 2021)</ref>; and finally, Math-BERT . However, the two most effective ensemble runs, TanAPP <ref type="bibr" target="#b20">(Mansouri et al., 2019)</ref> and MathAPP , are excluded because their linear fusion weights are tuned directly on the complete NTCIR-12 dataset.</p><p>For the ARQMath-2 dataset, we include the most effective systems for comparison: the Math-Dowsers primary system <ref type="bibr" target="#b5">(Fraser et al., 2018;</ref><ref type="bibr" target="#b25">Ng et al., 2020</ref><ref type="bibr" target="#b23">Ng, 2021)</ref> and the up-to-date Ap-proach0 system. Additionally, two cross-encoder dense retrievers are included: the TU_DBS primary retriever based on ALBERT <ref type="bibr" target="#b29">(Reusch et al., 2021a)</ref> and QASim <ref type="bibr" target="#b19">(Mansouri et al., 2021a)</ref>, which combines two Transformers, for question-question and question-answer similarity assessment. We also consider ensemble systems including the most effective run (WIBC) from the MIRMU team <ref type="bibr" target="#b26">(Novotn? et al., 2021)</ref> and the official tfidf and tf-idf+Tangent-S baselines provided in the ARQMath-2 main task. The tf-idf+Tangent-S baseline is an unweighted average fusion between the results produced by the Terrier system <ref type="bibr" target="#b27">(Ounis et al., 2005)</ref> and a structure-search system, Tangent-S <ref type="bibr" target="#b2">(Davila and Zanibbi, 2017)</ref>. In the Terrier pass, L A T E X strings are directly used for retrieval.  <ref type="formula" target="#formula_0">(1)-(4)</ref> show DPR models fine-tuned for one epoch, starting from different pretrained backbones. Our fully-trained passage-level and token-level models (DPR and ColBERT, respectively), rows <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref>, are compared with existing Transformer models in rows (5)-(10). */** denotes that the compared row performs weaker than the bottom row in each block, i.e., the 1epoch fine-tuned and 7-epoch further-pretrained BERT in row (4) or our fully-pretrained ColBERT model in row (12), at p &lt; 0.05/0.01 level using the two-tailed pairwise t-test. Underlined scores are not involved in any test of significance due to unavailable run files.  Results are divided by different topic categories (Calculation, Concept, or Proof), semantic dependencies (Text, Formula, or Both), and different difficulty levels <ref type="bibr">(Low, Medium, and High)</ref>. Note that the y-axes of all plots have the same scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overall Comparisons</head><p>Evaluation results for Transformer-based dense models are shown in <ref type="table" target="#tab_0">Table 1</ref>. Across both formulaonly retrieval (NTCIR-12) and math-aware full-text retrieval (ARQMath-2), our pretrained backbones can generally boost downstream DPR retrieval effectiveness compared to DPR models based on vanilla BERT, row (1), or SciBERT, row (2). This is presumably because we have further pretrained on more domain-specific data (unlike SciBERT, which also includes scientific text like biomedical articles) with a much larger batch size, i.e., 240 compared to SciBERT's 32 batch size. According to rows (2)-(4) in <ref type="table" target="#tab_0">Table 1</ref>, with pretraining only for 3 epochs, our model reaches a similar level of effectiveness as SciBERT; and more pretraining results in better downstream effectiveness. Our fully-trained ColBERT model, row <ref type="formula" target="#formula_0">(12)</ref>, achieves the best scores among other Transformer models. Compared to other ColBERT variants from rows (6)-(8) submitted by the TU_DBS team, we also achieve higher scores. Our DPR model, row (11), is generally more effective than previous bi-encoder systems, so it can be considered a cost-effective alternative to ColBERT, since the latter requires a much bigger index. For ARQMath-2, our DPR model requires a 5.3G index (at full precision), while our ColBERT model requires a 77G index (at half precision). However, our dense models are not on par with the MathBERT run on the NTCIR-12 dataset, row (5). This is because MathBERT reranks a highly effective run generated by Tangent-CFT; the latter is directly tuned on complete NTCIR-12 data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons on Different Topics</head><p>To further investigate the strengths and weaknesses of different architectures for math-aware retrieval, we break down results by different types of topics, e.g., computation, concept, or proof. Topic categories are labeled by the ARQMath-2 task organizers <ref type="bibr" target="#b21">(Mansouri et al., 2021b)</ref>.</p><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the DPR model, compared to itself, is good at text retrieval but bad at formula retrieval, while ColBERT is the opposite. The cross encoder (from left to right, the 3rd plot), on the other hand, handles all types of dependencies equally well, and it shows sufficient understanding for easy math question retrieval and proofrelated topics. On the other hand, structure search <ref type="table">Table 2</ref>: Comparison of effectiveness on the NTCIR-12 Wiki-Formula dataset. We combine a structure search system (Approach0) with our fully-trained DPR and ColBERT models. End-to-end fusion weights are tuned via cross-validation. */** denotes that the compared row performs weaker than the bottom row (i.e., Approach0 + DPR in end-to-end fusion) at p &lt; 0.05/0.01 level using the two-tailed pairwise t-test. Underlined scores are not involved in any test of significance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>Part. BPref Full BPref (the rightmost plot) excels at the calculation category and formula-dependent retrieval, with most categories performing even better than the cross encoder. This demonstrates that matching formula structure is still crucial for effective math-aware search, especially for formula-heavy content such as the calculation category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous systems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Fusion Results</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, even a cross encoder (without special structure pretraining) can fall short for formula retrieval compared to the structure search approach. Nevertheless, we want to learn if dense retrieval can be combined with structure search to further advance structure search effectiveness. Our fusion results are summarized in <ref type="table">Table 2 and  Table 3</ref>. On the NTCIR-12 Wiki-Formula dataset <ref type="table">(Table 2)</ref>, comparing rows (1)-(5), our linear fusion runs in rows (9)-(10) outperform others in fully relevant BPref scores. This shows we can generate a good ranking for highly relevant formulas when linearly combining end-to-end dense retrieval and structure search. The formula-only reranking in rows (7)-(8) is not beneficial, but on the other hand, end-to-end fusion in rows (9)-(10) is helpful because dense retrieval can improve recall when structure matching is too strict (more discussion below). <ref type="table">Table 3</ref>: Results from the most effective runs of previous systems in ARQMath-2 compared to our method for combining a structure search model (Approach0) with our fully-trained DPR and ColBERT models. End-to-end fusion weights are tuned via cross-validation. */** denotes that the compared row performs weaker than the bottom row (i.e., Approach0 + ColBERT in end-to-end fusion) at p &lt; 0.05/0.01 level using the two-tailed pairwise t-test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Runs</head><p>NDCG' MAP' P'@10 BPref</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Previous systems</head><p>(1) MathDowsers <ref type="bibr">Primary (2018;</ref><ref type="bibr" target="#b23">2021)</ref>  On the ARQMath dataset, comparing rows (7)-(11) in <ref type="table">Table 3</ref> and rows <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref> in <ref type="table" target="#tab_0">Table 1</ref>, we see that although the structure search baseline produced by Approach0 alone is generally more effective than dense retrieval models, both DPR and ColBERT can still boost the baseline results. With the assistance of structure search, we are also able to outperform cross-encoder models shown in row (2) and row (3) in <ref type="table">Table 3</ref>. These cross encoders require costly inference over every candidate pair. In fact, due to the impractical inference times of cross encoders for the ARQMath dataset, the TU_DBS team had to limit their candidate pool prior to indexing. Similarly, the DPRL QASim run adopts a smaller TinyBERT model to practically compute similarities for all candidate pairs in a limited set.</p><p>Interestingly, across two datasets, reranking is not helpful in general, other than a precision boost in rows (8)-(9), <ref type="table">Table 3</ref>. This is because the dense rerankers are prone to false positives at formula retrieval compared to structure search, and this is especially the case when a dense retriever is used to rerank a highly effective formula retriever baseline. We report extra experiments to support this argument in Section 5. This indicates that the dense retrievers are only complementary to the structure search approach in a way that helps recall rather than reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Given that linear fusion is able to produce such good results, a natural question to ask is whether other fusion methods can lead to even better results. Therefore, we compare popular fusion methods 4 on the ARQMath-2 datasets and our results are summarized in <ref type="table" target="#tab_2">Table 4</ref>. In all experiments, we directly choose the best fusion parameters tuned on the ARQMath-2 dataset to obtain an optimistic bound for each method. <ref type="table" target="#tab_2">Table 4</ref> shows that linear interpolation is sufficient to generate "good enough" results that are not significantly worse (sometimes better) than other popular fusion methods. We further investigate the reasons why structure search and dense retrieval are highly complementary but not so in the reranking case. After probing a number of queries where fusion runs achieve much better results, we find that the structure con- straint imposed on candidates by Approach0 can fail completely when relevant documents do not share any common substructure in math formulas, especially if the query is formula-centered, while dense retrieval has the capacity to find these relevant documents by matching contextual semantics. On the other hand, structure search is helpful to dense retrieval in cases where an obviously relevant document is found by matching a candidate formula perfectly. We illustrate the above statement using the topic where the precision metric has the largest increase after fusion. Specifically, topic A.287, which gains the most when the fusion run is compared against the Approach0 baseline (P@10 changes from 0 to 0.8), fails for Approach0 because structure match does not occur in relevant documents and the query is formula-centered. On the other hand, when compared to the ColBERT run, the fusion run in topic A.219 shows the most gain in precision (P@10 increases from 0.1 to 0.5). <ref type="figure" target="#fig_3">Figure 3</ref> shows the ranks of top-10 fusion results and their original positions in topic A.219. By further inspecting in detail, we discover that structure search prevents false positives in dense retrieval. Specifically, the top-3 dense retrieval hits in topic A.219 contain binomial coefficient notation, e.g., but is not equivalent mathematically. They are ruled out or get lowered in rank in the top-10 final results because their counterparts in structure search are missing (e.g., rank 1st and 2nd results from ColBERT run) or out of sight (e.g., rank 3rd result from ColBERT run), and those ColBERT hits paired with a structure hit in the Approach0 pass stand out.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Rapid progress in dense retrieval models using deep neural networks has greatly influenced many IR tasks. In this paper, we provide a thorough evaluation of both token-level and passage-level biencoder models in the math information retrieval domain. Our DPR and ColBERT models adapted to this domain in both pretraining and fine-tuning are made publicly accessible to provide stepping stones for future research. Our study also highlights the importance of combining structure search with dense retrieval models for better math-aware search. We show that bi-encoder dense retrieval models alone can be less effective than cross encoders, but when combined with strong structure search methods, they can further improve stateof-the-art effectiveness. With the huge modeling capacity of dense retrieval models, we believe it is worth exploring other directions for improvements so that we can unleash the potential of deep models in this domain, for example, to better identify similarities in mathematically transformed expressions with different structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>What our evaluations suggest in this work is to build end-to-end retrievers by combining strict structure search and dense retrieval for a highly effective math-aware search. We are aware of two limitations: First, an ensemble of two different end-to-end retrieval systems imposes engineering challenges; the benefit of supporting math-aware search may be offset by the overhead of adding multiple software stacks. Second, it is unclear how to highlight matching in the case of DPR; and in the case of ColBERT, it demands larger storage (see Section 4.1) and requires intensive GPU resources to perform the MaxSim operation over the embeddings of all candidate tokens (see Section 2.3).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The MAP scores produced by our fully-trained DPR, ColBERT, a cross encoder represented by the TU_DBS primary run, and the structure matching retriever Approach0, all evaluated on the ARQMath-2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Retrieved document ranks of the Approach0 + ColBERT fusion run (topic A.219, cut off at 10) and their positions in the original runs. The x-axis corresponds to the ranks of retrieved documents in the fusion run, the y-axis corresponds to the ranks of retrieved documents in the original runs, and each point represents one document.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Effectiveness comparisons of bi-encoder Transformers. Rows</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Other fusion methods evaluated using the most competitive Approach0 + ColBERT model combination on the ARQMath-2 dataset. */** denotes that the compared row performs significantly weaker than the linear fusion at p &lt; 0.05/0.01 level using the twotailed pairwise t-test. ISR and RRF stand for Inverse Square Rank and Reciprocal Rank Fusion, respectively.</figDesc><table><row><cell>Fusion</cell><cell cols="4">NDCG' MAP' P'@10 BPref</cell></row><row><cell>Borda Count</cell><cell>0.443</cell><cell>0.213</cell><cell>0.280</cell><cell>0.197</cell></row><row><cell>CombSUM</cell><cell cols="2">0.411** 0.213</cell><cell>0.296</cell><cell>0.216</cell></row><row><cell>ISR</cell><cell cols="2">0.433** 0.203</cell><cell>0.263</cell><cell>0.191</cell></row><row><cell>log-ISR</cell><cell cols="2">0.432** 0.202</cell><cell>0.263</cell><cell>0.189</cell></row><row><cell cols="2">RRF (k = 60) 0.449</cell><cell>0.221</cell><cell>0.284</cell><cell>0.200</cell></row><row><cell>Linear</cell><cell>0.449</cell><cell>0.217</cell><cell>0.279</cell><cell>0.204</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://math.stackexchange.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://artofproblemsolving.com/community</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We use the polyfuse tool: https://github.com/ rmit-ir/polyfuse</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council (NSERC) of Canada. Computational resources were provided in part by Compute Ontario and Compute Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.10676</idno>
		<title level="m">Sci-BERT: A pretrained language model for scientific text</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">BERT-based embedding model for formula retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pankaj</forename><surname>Dadure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Pakray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivaji</forename><surname>Bandyopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Layout and semantics: Combining representations for mathematical formula search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3077136.3080748</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10086</idno>
		<title level="m">SPLADE v2: Sparse lexical and expansion model for information retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Choosing math features for BM25 ranking with Tangent-L</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Tompa</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3209280.3209527</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In DocEng</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Preliminary exploration of formula embedding for mathematical information retrieval: Can mathematical formulae be em</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoren</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuoyu</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05154</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08253</idno>
		<title level="m">Condenser: A pre-training architecture for dense retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ana</forename><surname>Suchin Gururangan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Marasovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.10964</idno>
		<title level="m">2020. Don&apos;t stop pretraining: Adapt language models to domains and tasks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Search mathematical formulas by mathematical formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinori</forename><surname>Hijikata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shogo</forename><surname>Nishida</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/content/pdf/10.1007/978-3-642-02556-3_46.pdf</idno>
	</analytic>
	<monogr>
		<title level="m">SHI (Symposium on Human Interface)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficiently teaching an effective dense retriever with balanced topic aware sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462891</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching in one billion vectors: Re-rank with source coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Amsaleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ledell</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04906</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ColBERT: Efficient and effective passage search via contextualized late interaction over BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3397271.3401075</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MathWebSearch 0.5: Scaling an open formula search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bogdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corneliu-Claudiu</forename><surname>Matican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prodescu</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-642-31374-5_23</idno>
	</analytic>
	<monogr>
		<title level="m">CICM</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">MCAT math retrieval system for NTCIR-12 MathIR task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Yoko</forename><surname>Kristianto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NTCIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A mathematics retrieval system for formulae in layout presentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingnan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhong</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/pdf/10.1145/2600428.2609611</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DPRL systems in the CLEF 2020 ARQMath lab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DPRL systems in the CLEF 2021 ARQMath lab: Sentence-BERT for answer retrieval, learning-to-rank for formula retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tangent-CFT: An embedding model for mathematical formulas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaurya</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3341981.3344235</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Overview of ARQMath-2 (2021): Second CLEF lab on answer retrieval for questions on math (working notes version)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mansouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><forename type="middle">W</forename><surname>Oard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Agarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Technical aspects of the digital library of mathematical functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdou</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Youssef</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1023/A:1022967814992</idno>
	</analytic>
	<monogr>
		<title level="m">AMAI</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dowsing for math answers: Exploring MathCQA with a math-aware search engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Ki</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
		<respStmt>
			<orgName>University of Waterloo</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dowsing for answers to math questions: Ongoing viability of traditional MathIR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Ki</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Besat</forename><surname>Kassaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Tompa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dowsing for math answers with Tangent-L</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Ki</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dallas</forename><forename type="middle">J</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Besat</forename><surname>Kassaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Labahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mirette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Marzouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tompa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-85251-1_16</idno>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ensembling ten math information retrieval systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?t</forename><surname>Novotn?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>?tef?nik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D?vid</forename><surname>Lupt?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Geletka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Zelina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Terrier information retrieval platform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gianni</forename><surname>Amati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilis</forename><surname>Plachouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Macdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Johnson</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-31865-1_37</idno>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">MathBERT: A pre-trained model for mathematical formula understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcai</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00377</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An ALBERT-based similarity measure for mathematical answer retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Reusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maik</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3404835.3463023</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anja</forename><surname>Reusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maik</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>2021b. TU_DBS in the ARQMath lab 2021, CLEF. In CLEF</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Col-BERTv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01488</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Layout-based substitution tree indexing and retrieval for mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schellenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="DOI">https:/www.spiedigitallibrary.org/conference-proceedings-of-spie/8297/82970I/Layout-based-substitution-tree-indexing-and-retrieval-for-mathematical-expressions/10.1117/12.912502.short?SSO=1</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>In DRR</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The art of mathematics retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Sojka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>L??ka</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/2034691.2034703</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>In DocEng</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Searching for mathematical formulas based on graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-81097-9_11</idno>
	</analytic>
	<monogr>
		<title level="m">CICM</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An approach to similarity search for mathematical expressions using MathML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Yokoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DML (Digital Mathematics Library)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Search of mathematical contents: Issues and methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdou</forename><surname>Youssef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IASSE</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">NTCIR-12 MathIR task overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Aizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Kohlhase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iadh</forename><surname>Ounis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>Topic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Davila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NTCIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognition and retrieval of mathematical expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothea</forename><surname>Blostein</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1007/s10032-011-0174-4</idno>
	</analytic>
	<monogr>
		<title level="m">IJDAR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The Tangent search engine: Improved similarity metrics and scalability for math formula search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Tompa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.06235</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multi-stage math formula search: Using appearance-based similarity metrics at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenny</forename><surname>Davila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Tompa</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/2911451.2911512</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A novel similarity-search method for mathematical content in LaTeX markup and its implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>University of Delaware</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">OPMES: A similarity search engine for mathematical content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-319-30671-1_79</idno>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">PyA0: A Python toolkit for accessible math-aware search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/abs/10.1145/3404835.3462794</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Accelerating substructure similarity search for formula retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaurya</forename><surname>Rohatgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-45439-5_47</idno>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Structural similarity search for formulas using leaf-root paths in operator subtrees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECIR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Approach Zero and Anserini at the CLEF-2021 ARQMath track: Applying substructure search and BM25 on operator tree path tokens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zanibbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">When does further pre-training MLM help? An empirical study on task-oriented dialog pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP Insights Workshop</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
