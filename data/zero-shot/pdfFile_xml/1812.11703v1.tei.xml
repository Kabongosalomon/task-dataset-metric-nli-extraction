<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<email>libo@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<email>wuwei@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
							<email>qiang.wang@nlpr.ia.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
							<email>fangyi.zhang@vipl.ict.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><forename type="middle">Xing</forename><surname>Nlpr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casia</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<email>yanjunjie@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">NLPR</orgName>
								<address>
									<region>CASIA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">VIPL</orgName>
								<orgName type="institution" key="instit2">ICT</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SiamRPN++: Evolution of Siamese Visual Tracking with Very Deep Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Siamese network based trackers formulate tracking as convolutional feature cross-correlation between a target template and a search region. However, Siamese trackers still have an accuracy gap compared with state-of-theart algorithms and they cannot take advantage of features from deep networks, such as ResNet-50 or deeper. In this work we prove the core reason comes from the lack of strict translation invariance. By comprehensive theoretical analysis and experimental validations, we break this restriction through a simple yet effective spatial aware sampling strategy and successfully train a ResNet-driven Siamese tracker with significant performance gain. Moreover, we propose a new model architecture to perform layer-wise and depthwise aggregations, which not only further improves the accuracy but also reduces the model size. We conduct extensive ablation studies to demonstrate the effectiveness of the proposed tracker, which obtains currently the best results on five large tracking benchmarks, including OTB2015, VOT2018, UAV123, LaSOT, and TrackingNet. Our model will be released to facilitate further researches. * The first three authors contributed equally. Work done at SenseTime. Project page: https://lb1100.github.io/SiamRPN++. Recently, the Siamese network based trackers <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b44">44]</ref> have drawn much attention in the community. These Siamese trackers formulate the visual object tracking problem as learning a general similarity map by cross-correlation between the feature representations learned for the target template and the search region. To ensure tracking efficiency, the offline learned Siamese similarity function is often fixed during the running time <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15]</ref>. The CFNet tracker [41] and DSiam tracker [11] update the tracking model via a running average template and a fast transformation module, respectively. The SiamRNN tracker [24] introduces the region proposal network [24] after the Siamese network and performs joint classification and regression for tracking. The DaSiamRPN tracker [52] further introduces a distractor-aware module and improves the discrimination power of the model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual object tracking has received increasing attention over the last decades and has remained a very active research direction. It has a large range of applications in diverse fields like visual surveillance <ref type="bibr" target="#b47">[47]</ref>, human-computer interactions <ref type="bibr" target="#b25">[26]</ref>, and augmented reality <ref type="bibr" target="#b48">[48]</ref>. Although much progress has been made recently, it has still been commonly recognized as a very challenging task due to numerous factors such as illumination variation, occlusion, and background clutters, to name a few <ref type="bibr" target="#b46">[46]</ref>.</p><p>Although the above Siamese trackers have obtained outstanding tracking performance, especially for the wellbalanced accuracy and speed, even the best performed Siamese trackers, such as SiamPRN, the accuracy still has a notable gap with the state-of-the-arts <ref type="bibr" target="#b4">[5]</ref> on tracking benchmarks like OTB2015 <ref type="bibr" target="#b46">[46]</ref>. We observe that all these trackers have built their network upon architecture similar to AlexNet <ref type="bibr" target="#b22">[23]</ref> and tried several times to train a Siamese tracker with more sophisticated architecture like ResNet <ref type="bibr" target="#b13">[14]</ref> yet with no performance gain. Inspired by this observation, we perform an analysis of existing Siamese trackers and find the core reason comes from the destroy of the strict translation invariance. Since the target may appear at any position in the search region, the learned feature representation for the target template should stay spatial invariant, and we further theoretically find that, among modern deep architectures, only the zero-padding variant of AlexNet satisfies this spatial invariance restriction.</p><p>To overcome this restriction and drive the Siamese tracker with more powerful deep architectures, through extensive experimental validations, we introduce a simple yet effective sampling strategy to break the spatial invariance restriction of the Siamese tracker. We successfully train a SiamRPN <ref type="bibr" target="#b23">[24]</ref> based tracker using the ResNet as a backbone network and obtain significant performance improvements. Benefiting from the ResNet architecture, we propose a layer-wise feature aggravation structure for the cross-correlation operation, which helps the tracker to predict the similarity map from features learned at multiple levels. By analyzing the Siamese network structure for cross-correlations, we find that its two network branches are highly imbalanced in terms of parameter number; therefore we further propose a depth-wise separable correlation structure which not only greatly reduces the parameter number in the target template branch, but also stabilizes the training procedure of the whole model. In addition, an interesting phenomena is observed that objects in the same categories have high response on the same channels while responses of the rest channels are suppressed. The orthogonal property may also improve the tracking performance.</p><p>To summarize, the main contributions of this work are listed below in fourfold:</p><p>? We provide a deep analysis of Siamese trackers and prove that when using deep networks the decrease in accuracy comes from the destroy of the strict translation invariance.</p><p>? We present a simple yet effective sampling strategy to break the spatial invariance restriction which successfully trains Siamese tracker driven by a ResNet architecture.</p><p>? We propose a layer wise feature aggregation structure for the cross-correlation operation, which helps the tracker to predict the similarity map from features learned at multiple levels.</p><p>? We propose a depth-wise separable correlation structure to enhance the cross-correlation to produce multiple similarity maps associated with different semantic meanings.</p><p>Based on the above theoretical analysis and technical contributions, we have developed a highly effective and efficient visual tracking model which establishs a new state-of-the-art in terms of tracking accuracy, while running efficiently at 35 FPS. The proposed tracker, referred as SiamRPN++, consistently obtains the best tracking results on five of the largest tracking benchmarks, including OTB2015 <ref type="bibr" target="#b46">[46]</ref>, VOT2018 <ref type="bibr" target="#b20">[21]</ref>, UAV123 <ref type="bibr" target="#b30">[31]</ref>, LaSOT <ref type="bibr" target="#b9">[10]</ref>, and TrackingNet <ref type="bibr" target="#b29">[30]</ref>. Furthermore, we propose a fast variant of our tracker using MobileNet <ref type="bibr" target="#b17">[18]</ref> backbone that maintains competitive performance, while running at 70 FPS. To facilitate further studies on the visual tracking direction, we will release the source code and trained models of the SiamRPN++ tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly introduce recent trackers, with a special focus on the Siamese network based trackers <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b0">1]</ref>. Besides, we also describe the recent developments of deep architectures.</p><p>Visual tracking has witnessed a rapid boost in the last decade due to the construction of new benchmark datasets <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref> and improved methodologies <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b49">49]</ref>. The standardized benchmarks <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b9">10]</ref> provide fair testbeds for comparisons with different algorithms. The annually held tracking challenges <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> are consistently pushing forward the tracking performance. With these advancements, many promising tracking algorithms have been proposed. The seminal work by Bolme et al. <ref type="bibr" target="#b2">[3]</ref> introduces the Convolution Theorem from the signal processing field into visual tracking and transforms the object template matching problem into a correlation operation in the frequency domain. Own to this transformation, the correlation filter based trackers gain not only highly efficient running speed, but also increase accuracy if proper features are used <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b5">6]</ref>. With the wide adoption of deep learning models in visual tracking, tracking algorithms based on correlation filter with deep feature representations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref> have obtained the state-of-the-art accuracy in popular tracking benchmarks <ref type="bibr" target="#b45">[45,</ref><ref type="bibr" target="#b46">46]</ref> and challenge <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Recently, the Siamese network based trackers have received significant attentions for their well-balanced tracking accuracy and efficiency <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b44">44]</ref>. These trackers formulate visual tracking as a crosscorrelation problem and are expected to better leverage the merits of deep networks from end-to-end learning. In order to produce a similarity map from cross-correlation of the two branches, they train a Y-shaped neural network that joins two network branches, one for the object template and the other for the search region. Additionally, these two branches can remain fixed during the tracking phase <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">52]</ref> or updated online to adapt the appearance changes of the target <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b11">12]</ref>. The currently stateof-the-art Siamese trackers <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b52">52]</ref> enhance the tracking performance by a region proposal network after the Siamese network and produce very promising results. However, on the OTB benchmark <ref type="bibr" target="#b46">[46]</ref>, their tracking accuracy still leaves a relatively large gap with state-of-the-art deep trackers like ECO <ref type="bibr" target="#b4">[5]</ref> and MDNet <ref type="bibr" target="#b31">[32]</ref>.</p><p>With the proposal of modern deep architecture AlexNet by Alex et al. <ref type="bibr" target="#b22">[23]</ref> in 2012, the studies of the network architectures are rapidly growing and many sophisticated deep architectures are proposed, such as VGGNet <ref type="bibr" target="#b37">[37]</ref>, GoogleNet <ref type="bibr" target="#b38">[38]</ref>, ResNet <ref type="bibr" target="#b13">[14]</ref> and MobileNet <ref type="bibr" target="#b17">[18]</ref>. These deep architectures not only provide deeper understanding on the design of neural networks, but also push forwards the state-of-the-arts of many computer vision tasks like object detection <ref type="bibr" target="#b32">[33]</ref>, image segmentation <ref type="bibr" target="#b3">[4]</ref>, and human pose estimation <ref type="bibr" target="#b39">[39]</ref>. In deep visual trackers, the network architecture usually contains no more than five constitutional layers tailored from AlexNet or VGGNet. This phenomenon is explained that shallow features mostly contribute to the accurate localization of the object <ref type="bibr" target="#b33">[34]</ref>. In this work, we argue that the performance of Siamese trackers can significantly get boosted using deeper models if the model is properly trained with the whole Siamese network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Siamese Tracking with Very Deep Networks</head><p>The most important finding of this work is that the performance of the Siamese network based tracking algorithm can be significantly boosted if it is armed with much deeper networks. However, simply training a Siamese tracker by directly using deeper networks like ResNet does not obtain the expected performance improvement. We find the underlying reason largely involves the intrinsic restrictions of the Siamese trackers, Therefore, before the introduction of the proposed SiamRPN++ model, we first give a deeper analysis on the Siamese networks for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis on Siamese Networks for Tracking</head><p>The Siamese network based tracking algorithms <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b0">1]</ref> formulate visual tracking as a cross-correlation problem and learn a tracking similarity map from deep models with a Siamese network structure, one branch for learning the feature presentation of the target, and the other one for the search area. The target patch is usually given in the first frame of the sequence and can be viewed as an exemplar z. The goal is to find the most similar patch (instance) from following frame x in a semantic embedding space ?(?):</p><formula xml:id="formula_0">f (z, x) = ?(z) * ?(x) + b,<label>(1)</label></formula><p>where b is used to model the offset of the similarity value. This simple matching function naturally implies two intrinsic restrictions in designing a Siamese tracker.</p><p>? The contracting part and the feature extractor used in Siamese trackers have an intrinsic restriction for strict</p><formula xml:id="formula_1">translation invariance, f (z, x[ ? j ]) = f (z, x)[ ? j ], where [ ? j ]</formula><p>is the translation shift sub window operator, which ensures the efficient training and inference. ? The contracting part has an intrinsic restriction for structure symmetry, i.e. f (z, x ) = f (x , z), which is appropriate for the similarity learning.</p><p>After detailed analysis, we find the core reason for preventing Siamese tracker using deep network is related to these two aspects. Concretely speaking, one reason is that padding in deep networks will destroy the strict translation invariance. The other one is that RPN requires asymmetrical features for classification and regression. We will introduce spatial aware sampling strategy to overcome the first problem, and discuss the second problem in Sect. 3.4. Strict translation invariance only exists in no padding network such as modified AlexNet <ref type="bibr" target="#b0">[1]</ref>. Previous Siamese based Networks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b52">52]</ref> are designed to be shallow to satisfy this restriction. However, if the employed networks are replaced by modern networks like ResNet or MobileNet, padding is inevitable to make the network going deeper, which destroys the strict translation invariance restriction. Our hypothesis is that the violation of this restriction will lead to a spatial bias.</p><p>We test our hypothesis by simulation experiments on a network with padding. Shift is defined as the max range of translation generated by a uniform distribution in data augmentation. Our simulation experiments are performed as follows. First, targets are placed in the center with different shift ranges (0, 16 and 32) in three sepreate training experiments. After convergence, we aggregate the heatmaps generated on test dataset and then visualize the results in <ref type="figure">Fig.  1</ref>. In the first simulation with zero shift, the probabilities on the border area are degraded to zero. It shows that a strong center bias is learned despite of the appearances of test targets. The other two simulations show that increasing shift ranges will gradually prevent model collapse into this trivial solution. The quantitative results illustrate that the aggregated heatmap of 32-shift is closer to the location distribution of test objects. It proves that the spatial aware sampling strategy effectively alleviate the break of strict translation invariance property caused by the networks with padding.</p><p>To avoid putting a strong center bias on objects, we train SiamRPN with a ResNet-50 backbone by the spatial aware sampling strategy. As shown in <ref type="figure">Fig. 2</ref>, the performance with zero shift reduced to 0.14 on VOT2018, a suitable shift (?64 pixels) is vital for training a deep Siamese tracker.  <ref type="table" target="#tab_2">conv2_1  conv2_2  conv2_3  conv3_1  conv3_2  conv3_3  conv4_1  conv4_2   Search   conv1  pool1  conv2_1  conv2_2  conv2_3  conv3_1  conv3_2  conv3_3  conv4_1  conv4_2   Target   conv4_3  conv4_4  conv4_5  conv4_6  conv5_1  conv5_2  conv5_3   conv4_3  conv4_4  conv4_5  conv4_6  conv5_1  conv5_2</ref> conv5_3 1 <ref type="figure">Figure 3</ref>. Illustration of our proposed framework. Given a target template and search region, the network ouputs a dense prediction by fusion the outputs from multiple Siamese Region Proposal (SiamRPN) blocks. Each SiamRPN block is shown on right.</p><formula xml:id="formula_2">Siamese RPN Siamese RPN Siamese RPN BBox Regression CLS F l (z) F l (x) DW-Corr_1 DW-Corr_2 S l B l Box</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ResNet-driven Siamese Tracking</head><p>Based on the above analyses, the influence of center bias can be eliminated. Once we eliminate the learning bias to the center location, any off-the-shelf networks (e.g., Mo-bileNet, ResNet) can be utilized to perform visual tracking after domain adaptation. Moreover, we can adaptively construct the network topology and unveil the performance of deep network for visual tracking.</p><p>In this subsection, we will discuss how to transfer a deep network into our tracking algorithms. In particular, we conduct our experiments mainly focusing on ResNet-50 <ref type="bibr" target="#b13">[14]</ref>. The original ResNet has a large stride of 32 pixels, which is not suitable for dense Siamese network prediction. As shown in <ref type="figure">Fig.3</ref>, we reduce the effective strides at the last two block from 16 pixels and 32 pixels to 8 pixels by modifying the conv4 and conv5 block to have unit spatial stride, and also increase its receptive field by dilated convolutions <ref type="bibr" target="#b26">[27]</ref>. An extra 1 ? 1 convolution layer is appended to each of block outputs to reduce the channel to 256.</p><p>Since the paddings of all layers are kept, the spatial size of the template feature increases to 15, which imposes a heavy computational burden on the correlation module. Thus we crop the center 7 ? 7 regions <ref type="bibr" target="#b41">[41]</ref> as the template feature where each feature cell can still capture the entire target region.</p><p>Following <ref type="bibr" target="#b23">[24]</ref>, we use a combination of cross correlation layers and fully convolutional layers to assemble a head module for calculating classification scores (denoted by S) and bounding box regressor (denoted by B). The Siamese RPN blocks are denoted by P.</p><p>Furthermore, we find that carefully fine-tuning ResNet will boost the performance. By setting learning rate of ResNet extractor with 10 times smaller than RPN parts, the feature representation can be more suitable for tracking tasks. Different from traditional Siamese approaches, the parameters of the deep network are jointly trained in an end-to-end fashion. To the best of our knowledge, we are the first to achieve an end-to-end learning on a deep Siamese Network (&gt; 20 layers) for visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Layer-wise Aggregation</head><p>After utilizing deep network like ResNet-50, aggregating different deep layers becomes possible. Intuitively, visual tracking requires rich representations that span levels from low to high, scales from small to large, and resolutions from fine to coarse. Even with the depth of features in a convolutional network, a layer in isolation is not enough: compounding and aggregating these representations improve inference of recognition and localization.</p><p>In the previous works which only use shallow networks like AlexNet, multi-level features cannot provide very different representations. However, different layers in ResNet are much more meaningful considering that the receptive field varies a lot. Features from earlier layers will mainly focus on low level information such as color, shape, are essential for localization, while lacking of semantic information; Features from latter layers have rich semantic information that can be beneficial during some challenge scenarios like motion blur, huge deformation. The use of this rich hierarchical information is hypothesized to help tracking.</p><p>In our network, multi-branch features are extracted to collaboratively infer the target localization. As for ResNet-50, we explore multi-level features extracted from the last three residual block for our layer-wise aggregation. We refer these outputs as F 3 (z), F 4 (z), and F 5 (z), respectively. As shown in <ref type="figure">Fig. 3</ref>, the outputs of conv3, conv4, conv5 are fed into three Siamese RPN module individually.</p><p>Since the output sizes of the three RPN modules have the same spatial resolution, weighted sum is adopted directly on the RPN output. A weighted-fusion layer combines all the outputs.  The combination weights are separated for classification and regression since their domains are different. The weight is end-to-end optimized offline together with the network. In contrast to previous works, our approach does not explicitly combine convolutional features, but learn classifiers and regressions separately. Note that with the depth of the backbone network significantly increased, we can achieve substantial gains from the sufficient diversity of visual-semantic hierarchies.</p><formula xml:id="formula_3">S all = 5 l=3 ? i * S l , B all = 5 l=3 ? i * B l .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Depthwise Cross Correlation</head><p>The cross correlation module is the core operation to embed two branches information. SiamFC <ref type="bibr" target="#b0">[1]</ref> utilizes a Cross-Correlation layer to obtain a single channel response map for target localization. In SiamRPN <ref type="bibr" target="#b23">[24]</ref>, Cross-Correlation is extended to embed much higher level information such as anchors, by adding a huge convolutional layer to scale the channels (UP-Xcorr). The heavy up-channel module makes seriously imbalance of parameter distribution (i.e. the RPN module contains 20M parameters while the feature extractor only contains 4M parameters in <ref type="bibr" target="#b23">[24]</ref>), which makes the training optimization hard in SiamRPN.</p><p>In this subsection, we present a lightweight cross correlation layer, named Depthwise Cross Correlation (DW-148th 222th 226th <ref type="figure">Figure 5</ref>. Channels of depthwise correlation output in conv4. There are totally 256 channels in conv4, however, only few of them have high response during tracking. Therefore we choose 148th, 222th, 226th channels as demonstration, which are 2nd, 3rd, 4th rows in the figure. The first row contains six corresponding search regions from OTB dataset <ref type="bibr" target="#b46">[46]</ref>. Different channels represent different semantics, the 148th channel has high response on cars, while has low response on persons and faces. The 222th and 226th channel have high response on persons and faces, respectively.</p><p>XCorr), to achieve efficient information association. The DW-XCorr layer contains 10 times fewer parameters than the UP-XCorr used in SiamRPN while the performance is on par with it.</p><p>To achieve this, a conv-bn block is adopted to adjust features from each residual blocks to suit tracking task. Crucially, the bounding box prediction and anchor based classification both are asymmetrical, which is different from SiamFC (See Sect. 3.1). In order to encode the difference, the template branch and search branch pass two non-shared convolutional layers. Then two feature maps with the same number of channels do the correlation operation channel by channel. Another conv-bn-relu block is appended to fuse different channel outputs. Finally, the last convolution layer for the output of classification or regression is appended.</p><p>By replacing cross-correlation to depthwise correlation, we can greatly reduce the computational cost and the memory usage. In this way, the numbers of parameters on the template and the search branches are balanced, resulting the training procedure more stable.</p><p>Furthermore, an interesting phenomena is illustrated in <ref type="figure">Fig.5</ref>. The objects in the same category have high response on same channels (car in 148th channel, person in 222th channel, and face in 226th channel), while responses of the rest channels are suppressed. This property can be comprehended as the channel-wise features produced by the depthwise cross correlation are nearly orthogonal and each channel represents some semantic information. We also analyze the heatmaps when using the up-channel cross correlation and the reponse maps are less interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training Dataset and Evaluation</head><p>Training. The backbone network of our architecture <ref type="bibr" target="#b13">[14]</ref> is pre-trained on ImageNet <ref type="bibr" target="#b36">[36]</ref> for image labeling, which has proven to be a very good initialization to other tasks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b26">27]</ref>. We train the network on the training sets of COCO <ref type="bibr" target="#b24">[25]</ref>, ImageNet DET <ref type="bibr" target="#b36">[36]</ref>, ImageNet VID, and YouTube-BoundingBoxes Dataset <ref type="bibr" target="#b34">[35]</ref> and to learn a generic notion of how to measure the similarities between general objects for visual tracking. In both training and testing, we use single scale images with 127 pixels for template patches and 255 pixels for searching regions. Evaluation. We focus on the short-term single object tracking on OTB2015 <ref type="bibr" target="#b46">[46]</ref>, VOT2018 <ref type="bibr" target="#b20">[21]</ref> and UAV123 <ref type="bibr" target="#b30">[31]</ref>. We use VOT2018-LT <ref type="bibr" target="#b20">[21]</ref> to evaulate the long-term setting. In the long-term tracking, the object may leave the field of view or become fully occluded for a long period, which are more challenging than short-term tracking. We also analyze the generalization of our method on LaSOT <ref type="bibr" target="#b9">[10]</ref> and Track-ingNet <ref type="bibr" target="#b29">[30]</ref>, two of the recent largest benchmarks for single object tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Network Architecture. In experiments, we follow <ref type="bibr" target="#b52">[52]</ref> for the training and inference settings. We attach two sibling convolutional layers to the stride-reduced ResNet-50 (Sect. 3.2) to perform proposal classification and bounding box regression with 5 anchors. Three randomly initialized 1 ? 1 convolutional layers are attached to conv3, conv4, conv5 for reducing the feature dimension to 256. Optimization. SiamRPN++ is trained with stochastic gradient descent (SGD). We use synchronized SGD over 8 GPUs with a total of 128 pairs per minibatch (16 pairs per GPU), which takes 12 hours to converge. We use a warmup learning rate of 0.001 for first 5 epoches to train the RPN braches. For the last 15 epoches, the whole network is end-to-end trained with learning rate exponentially decayed from 0.005 to 0.0005. Weight decay of 0.0005 and momentum of 0.9 are used. The training loss is the sum of classification loss and the standard smooth L 1 loss for regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Experiments</head><p>Backbone Architecture. The choice of feature extractor is crucial as the number of parameters and types of layers directly affect memory, speed, and performance of the tracker. We compare different network architectures for the visual tracking. <ref type="figure">Fig. 6</ref> shows the performance of using AlexNet, ResNet-18, ResNet-34, ResNet-50, and MobileNet-v2 as backbones. We report performance by Area Under Curve (AUC) of success plot on OTB2015 with respect to the top1 accuracy on ImageNet. We observe that our SiamRPN++ can benefit from deeper ConvNets.  <ref type="table" target="#tab_2">Table 1</ref> also illustrates that by replacing AlexNet to ResNet-50, the performance improves a lot on VOT2018 dataset. Besides, our experiments shows that finetuning the backbone part is critical, which yields a great improvement on tracking performance. Layer-wise Feature Aggregation. To investigate the impact of layer-wise feature aggregation, first we train three variants with single RPN on ResNet-50. We empirically found that conv4 alone can achieve a competitive performance with 0.374 in EAO, while deeper layer and shallower layer perform with 4% drops. Through combining two branches, conv4 and conv5 gains improvement, however no improvement is observed on the other two combinations. Even though, the robustness has increased 10%, which is the key vulnerability of our tracker. It means that our tracker still has room for improvement. After aggregating all three layers, both accuracy and robustness steadily improve, with gains between 3.1% and 1.3% for VOT and OTB. In total, layer-wise feature aggregation yields a 0.414 EAO score on VOT2018, which is 4.0% higher than that of the single layer baseline. Depthwise Correlation. We compare the original Up-Channel Cross Correlation layer with the proposed Depthwise Cross Correlation layer. As shown in the  <ref type="table">Table 2</ref>. Comparison with the state-of-the-art in terms of expected average overlap (EAO), robustness (failure rate), and accuracy on the VOT2018 benchmark. We compare with the top-10 trackers and our baseline DaSiamRPN in the competition. Our tracker obtains a significant relative gain of 6.4% in EAO, compared to the top-ranked method (LADCF). demonstrates the importance of depthwise correlation. This is partly beacause a balanced parameter distribution of the two branches makes the learning process more stable, and converges better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with the state-of-the-art</head><p>OTB-2015 Dataset. The standardized OTB benchmark <ref type="bibr" target="#b46">[46]</ref> provides a fair testbed on robustness. The Siamese based tracker formulate the tracking as one-shot detection task without any online update, thus resulting in inferior performance on this no-reset setting benchmark. However, we identify the limited representation from the shallow network as the primary obstacle preventing Siamese based tracker from surpassing top-performing methods, such as C-COT variants <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref>. We compare our SiamRPN++ tracker on the OTB2015 with the state-of-the-art trackers. <ref type="figure" target="#fig_3">Fig. 7</ref> shows that our SiamRPN++ tracker produces leading result in overlap success. Compared with the recent DaSiamRPN <ref type="bibr" target="#b52">[52]</ref>, our SiamRPN++ improves 3.8% in overlap and 3.4% in precision from the considerably increased depth. Representations extracted from deep ConvNets are less sensitive to illumination and background clutter. And to the best of our knowledge, this is the first time that Siamese tracker can obtain the comparable performance with the state-of-the-art tracker on OTB2015 dataset.</p><p>VOT2018 Dataset. We test our SiamRPN++ tracker on the lastest VOT-2018 dataset <ref type="bibr" target="#b20">[21]</ref> in comparison with 10 state-of-the-art methods. The VOT-2018 public dataset is one of the most recent datasets for evaluating online modelfree single object trackers, and includes 60 public sequences with different challenging factors. Following the evalua- tion protocol of VOT-2018, we adopt the Expected Average Overlap (EAO), Accuracy(A) and Robustness(R) and no-reset-based Average Overlap(AO) to compare different trackers. The detailed comparisons are reported in <ref type="table">Table 2</ref>.</p><p>From <ref type="table">Table 2</ref>, we observe that the proposed SiamRPN++ method achieves the top-ranked performance on EAO, A and AO criteria. Especially, our SiamRPN++ tracker outperforms all existing trackers, including the VOT2018 challenge winner. Compared with the best tracker in the VOT2018 challenge (LADCF <ref type="bibr" target="#b20">[21]</ref>), the proposed method achieves a performance gain of 2.5%. In addition, our tracker achieves a substantial improvement over the challenge winner (MFT <ref type="bibr" target="#b20">[21]</ref>), with a gain of 9.5% in accuracy.</p><p>In comparison with the baseline tracker DaSiamRPN, our approach yields substantial gains of 10.3% on robustness, which is the common vulnerability of the Siamese Network based tracker against correlation filters method. Even though, due to the lack of adaption to the template, the robustness still has a gap with the state-of-art correlation filters methods <ref type="bibr" target="#b1">[2]</ref> which relies on the online updating.</p><p>The One Pass Evaluation (OPE) is also adopted to evaluate trackers and the AO values are reported to demonstrate their performance. From the last row in <ref type="table">Table 2</ref>, we can observe that our method achieves comparable performance compared to the DLSTpp <ref type="bibr" target="#b20">[21]</ref> and improves the DaSiamRPN <ref type="bibr" target="#b52">[52]</ref> method by an absolute gain of 10.0%. Accuracy vs. Speed. In <ref type="figure">Fig. 9</ref>, we visualize the EAO on VOT2018 with respect to the Frames-Per-Second (FPS). The reported speed is evaluated on a machine with an NVIDIA Titan Xp GPU, other results are provided by the VOT2018 official results. From the plot, our SiamRPN++ achieves best performance, while still running at realtime speed <ref type="bibr">(35 FPS)</ref>. It is worth noting that two of our variants achieve nearly the same accuracy as SiamRPN++, while running at more than 70 FPS, which makes these two variants highly competitive.  VOT2018 Long-term Dataset. In the latest VOT2018 challenge, a long-term experiment are newly introduced. It is composed of 35 long sequences, where targets may leave the field of view or become fully occluded for a long period. The performance measures are precision, recall and a combined F-score. We report all these metrics compared with the state-of-the-art trackers on VOT2018-LT. As shown in the <ref type="figure" target="#fig_5">Fig. 10</ref>, after equipping our tracker with the long term strategy, SiamRPN++ obtains 2.2% gain from DaSiam LT, and outperforms the best tracker by 1.9% in Fscore. The powerful feature extracted by ResNet improves both TP and TR by 2% absolutely from our baseline DaSi-amRPN. Meanwhile, the long term version of SiamRPN++ is still able to run at 21 FPS, which is nearly 8 times faster than MBMD <ref type="bibr" target="#b20">[21]</ref>, the winner of VOT2018-LT.</p><p>UAV123 Dataset. UAV123 dataset includes 123 sequences with average sequence length of 915 frames. Besides the recent trackers in <ref type="bibr" target="#b28">[29]</ref>, ECO <ref type="bibr" target="#b4">[5]</ref>, ECO-HC <ref type="bibr" target="#b4">[5]</ref>, DaSiamRPN <ref type="bibr" target="#b52">[52]</ref>, SiamRPN <ref type="bibr" target="#b23">[24]</ref> are added on comparison. <ref type="figure">Fig. 11</ref> illustrates the precision and success plots of the compared trackers. Specifically, our tracker achieves a success score of 0.613, which outperforms DaSiamRPN (0.586) and ECO (0.525) with a large margin.   LaSOT Dataset. To further validate the proposed framework on a larger and more challenging dataset, we conduct experiments on LaSOT <ref type="bibr" target="#b9">[10]</ref>. The LaSOT dataset provides a large-scale, high-quality dense annotations with 1,400 videos in total and 280 videos in the testing set. <ref type="figure" target="#fig_0">Fig. 12</ref> reports the overall performances of our SiamRPN++ tracker on LaSOT testing set. Without bells and whistles, our SiamRPN++ model is sufficient to achieve state-of-the-art AUC score of 49.6%. Specifically, SiamRPN++ increases the normalized distance precision and AUC relatively by 23.7% and 24.9% over MDNet <ref type="bibr" target="#b31">[32]</ref>, which is the best tracker reported in the original paper.</p><p>TrackingNet Dataset. The recently released Track-ingNet <ref type="bibr" target="#b29">[30]</ref> provides a large amount of data to assess trackers in the wild. We evaluate SiamRPN++ on its test set with 511 videos. Following <ref type="bibr" target="#b29">[30]</ref>, we use three metrics success (AUC), precision (P) and normalized precision (P norm ) for evaluation. <ref type="table" target="#tab_6">Table 3</ref> demonstrates the comparison results to trackers with top AUC scores, showing that SiamRPN++ achieves the best results on all three metrics. In specific, SiamRPN++ obtains the AUC score of 73.3%, P score of 69.4% and P norm score of 80.0%, outperforming the second best tracker DaSiamRPN <ref type="bibr" target="#b52">[52]</ref> with AUC score of 63.8%, P score of 59.1% and P norm score of 73.4% by 9.5%, 10.3% and 6.6%, respectively. In summary, it is important to note that all these consistent results show the generalization ability of SiamRPN++.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we have presented a unified framework, referred as SiamRPN++, to end-to-end train a deep Siamese network for visual tracking. We show theoretical and empirical evidence that how to train a deep network on Siamese tracker. Our network is composed of a multi-layer aggregation module which assembles the hierarchy of connections to aggregate different levels of representation and a depthwise correlation layer which allows our network to reduce computation cost and redundant parameters while also leading to better convergence. Using SiamRPN++, we obtained state-of-the-art results on the VOT2018 in real-time, showing the effectiveness of SiamRPN++. SiamRPN++ also acheived state-of-the-art results on large datasets like La-SOT and TrackingNet showing its generalizability.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .Figure 2 .</head><label>12</label><figDesc>Visualization of prior probabili ties of positive samples when using different random translations. The distributions become more uniform after random translations within ?32 pixels. The impacts of the random translation on VOT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Illustrations of different cross correlation layers. (a)Cross Correlation (XCorr) layer predicts a single channel similarity map between target template and search patches in SiamFC<ref type="bibr" target="#b0">[1]</ref>. (b) Up-Channel Cross Correlation (UP-XCorr) layer outputs a multi-channel correlation features by cascading a heavy convolutional layer with several independent XCorr layers in SiamRPN<ref type="bibr" target="#b23">[24]</ref>. (c) Depth-wise Cross Correlation (DW-XCorr) layer predicts multi-channel correlation features between a template and search patches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Success and precision plots show a comparison of our tracker with state-of-the-art trackers on the OTB2015 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Expected averaged overlap performance on VOT2018.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Long-term tracking performance. The average tracking precision-recall curves (left), the corresponding F-score curves (right). Tracker labels are sorted according to the F-score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Evaluation results of trackers on UAV123. Evaluation results of trackers on LaSOT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Ablation study of the proposed tracker on VOT2018 and OTB2015. L3, L4, L5 represent conv3,conv4,conv5, respectively. Finetune represents whether the backbone is trained offline. Up/DW means Up channel correlation and depthwise correlation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Top1 error vs AUC on OTB2015</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Res50</cell></row><row><cell></cell><cell>0.69</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Res34</cell><cell></cell><cell></cell></row><row><cell>AUC</cell><cell>0.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Res18</cell><cell cols="2">MobileNetv2</cell><cell></cell></row><row><cell></cell><cell>0.67</cell><cell cols="2">AlexNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.66</cell><cell>56</cell><cell>57</cell><cell>58</cell><cell>66</cell><cell>68 Top 1 error (%) 70</cell><cell>72</cell><cell>74</cell><cell>76</cell><cell>78</cell></row><row><cell cols="11">Figure 6. The Top-1 accuracy on ImageNet vs. Expected Average</cell></row><row><cell cols="7">Overlap (EAO) scores on OTB2015.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">BackBone L3 L4 L5 Finetune Corr VOT2018 OTB2015</cell></row><row><cell cols="3">AlexNet</cell><cell></cell><cell></cell><cell></cell><cell cols="2">UP DW 0.355 0.332</cell><cell></cell><cell>0.658 0.666</cell></row><row><cell cols="4">ResNet-50</cell><cell></cell><cell></cell><cell>UP UP</cell><cell>0.371 0.390</cell><cell></cell><cell>0.664 0.684</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DW 0.331</cell><cell></cell><cell>0.669</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DW 0.374</cell><cell></cell><cell>0.678</cell></row><row><cell cols="4">ResNet-50</cell><cell></cell><cell></cell><cell cols="2">DW 0.320 DW 0.346</cell><cell></cell><cell>0.646 0.677</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DW 0.336</cell><cell></cell><cell>0.674</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DW 0.383</cell><cell></cell><cell>0.683</cell></row><row><cell cols="4">ResNet-50</cell><cell></cell><cell></cell><cell cols="2">DW 0.395 DW 0.414</cell><cell></cell><cell>0.673 0.696</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 ,</head><label>1</label><figDesc>the proposed depthwise correlation gains 2.3% improvement on VOT2018 and 0.8% improvement on OTB2015, which DLSTpp DaSiamRPN SA Siam R CPT DeepSTRCF DRT RCO UPDT SiamRPN MFT LADCF Ours</figDesc><table><row><cell>EAO ?</cell><cell>0.325</cell><cell>0.326</cell><cell>0.337</cell><cell>0.339</cell><cell>0.345</cell><cell>0.356 0.376 0.378</cell><cell>0.383 0.385 0.389 0.414</cell></row><row><cell>Accuracy ?</cell><cell>0.543</cell><cell>0.569</cell><cell>0.566</cell><cell>0.506</cell><cell>0.523</cell><cell>0.519 0.507 0.536</cell><cell>0.586 0.505 0.503 0.600</cell></row><row><cell cols="2">Robustness ? 0.224</cell><cell>0.337</cell><cell>0.258</cell><cell>0.239</cell><cell>0.215</cell><cell>0.201 0.155 0.184</cell><cell>0.276 0.140 0.159 0.234</cell></row><row><cell>AO ?</cell><cell>0.495</cell><cell>0.398</cell><cell>0.429</cell><cell>0.379</cell><cell>0.436</cell><cell>0.426 0.384 0.454</cell><cell>0.472 0.393 0.421 0.498</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 .</head><label>3</label><figDesc>State-of-the-art comparison on the TrackingNet test set in terms of success, precision, and normalized precision.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin Zajc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin Zajc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pfugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hand posture recognition using finger geometric feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="445" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Subaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">An in-depth analysis of visual tracking with siamese neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00569</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazzocchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<title level="m">IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Dcfnet: Discriminant correlation filters network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04057</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Do not lose the details: Reinforced representation learning for high performance visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multiple human tracking based on multi-view upper-body detection and discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Good features to track for visual slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visual tracking via spatially aligned correlation filters network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Robust visual tracking using joint scale-spatial correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Joint scale-spatial correlation tracking with adaptive rotation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
