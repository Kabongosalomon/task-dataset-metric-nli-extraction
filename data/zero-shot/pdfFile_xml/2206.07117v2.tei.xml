<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TRIHORN-NET: A MODEL FOR ACCURATE DEPTH-BASED 3D HAND POSE ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rezaei</surname></persName>
							<email>mohammad.rezaei@mavs.uta.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razieh</forename><surname>Rastgoo</surname></persName>
							<email>rrastgoo@semnan.ac.ir</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Texas at Arlington</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Semnan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TRIHORN-NET: A MODEL FOR ACCURATE DEPTH-BASED 3D HAND POSE ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*Corresponding Author Vassilis Athitsos University of Texas at Arlington</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Hand Pose Estimation</term>
					<term>Depth Image</term>
					<term>Accuracy</term>
					<term>2D joints</term>
					<term>Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D hand pose estimation methods have made significant progress recently. However, estimation accuracy is often far from sufficient for specific real-world applications, and thus there is significant room for improvement. This paper proposes TriHorn-Net, a novel model that uses specific innovations to improve hand pose estimation accuracy on depth images. The first innovation is decomposition of the 3D hand pose estimation into the estimation of 2D joint locations in the depth image space (UV), and the estimation of their corresponding depths aided by two complementary attention maps. This decomposition prevents depth estimation, which is a more difficult task, from interfering with the UV estimations at both the prediction and feature levels. The second innovation is PixDropout, which is, to the best of our knowledge, the first appearance-based data augmentation method for hand depth images. Experimental results demonstrate that the proposed model outperforms the state-of-the-art methods on three public benchmark datasets. Our implementation is available at https://github.com/mrezaei92/TriHorn-Net.</p><p>Hands are crucial in allowing humans to interact with the world around them. Accurate hand pose estimation has many applications in areas such as human computer interaction (HCI), augmented reality (AR), virtual reality (VR) and gesture recognition <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>. As commodity depth cameras become more accurate and affordable and, as a result, more widely used, significant advancements have been made in depth-based 3D hand pose estimation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> and hand segmentation <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>. However, 3D hand pose estimation remains a very challenging task due to the large degree of variation in hand appearance, heavy self-occlusion, noise, high dimensionality and self-similarity between hand parts <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b12">13]</ref>.</p><p>With the advancement of deep neural networks (DNNs) [?], DNN-based hand pose estimation techniques rapidly displaced the previous methods such as <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b48">49]</ref> and have achieved impressive results. These methods can be broadly categorized into two groups: 1) regression-based methods and 2) detection-based methods. Regression-based methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref> encode the hand depth image into a single global feature which is in turn used to directly estimate the hand joints. Detection-based methods adopt a dense-prediction approach, where they utilize hierarchical features to compute pixel-wise predictions for each joint. Detection-based methods generally tend to outperform regression-based methods [2] because regression-based methods use a single global feature for estimation, which cannot fully retain fine-grained spatial information required for accurate mapping into 3D hand poses.</p><p>Despite the superior performance of detection-based methods, they still suffer from several drawbacks. Moon et al. [13]   achieve a high accuracy by using 3D CNNs, but their method comes at a heavy computational and memory cost. A new class of detection-based methods has recently emerged that adopts an approach based on dense pixel or point offset prediction, whereby they densely estimates all the pixels'(or points') offsets to joints and compute joint positions by a weighted average over all the corresponding offset values. Despite their high performance, <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5]</ref> use non-learnable information aggregation operations such as argmax operation or mean-shift estimation to compute joint coordinates from the heatmap or offset vector fields. However, the information aggregation operation is treated as a post-processing arXiv:2206.07117v2 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>step and is not incorporated into the training phase, causing a gap between training and inference. <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b33">34]</ref> require complex post-processing operations, such as taking neighboring points, causing inevitable quantization errors and rendering the pipeline not end-to-end differentiable. JGR-P2O <ref type="bibr" target="#b26">[27]</ref> partly solves these issues by predicting pixel-wise offsets and a weight map to compute the joint positions using the weighted average, but it still suffers from the common issue among this class of methods that the estimations are unstable when depth values near the target joint are heavily missing, leading to a performance degradation.</p><p>To tackle the aforementioned issues, we propose a novel model, that we call TriHorn-Net, for 3D hand pose estimation. TriHorn-Net consists of an encoder network that encodes the input hand depth image into a high-resolution feature volume, and three separate branches that take the hand feature volume as the input and together estimate the 3D hand pose. The first two branches compute two per-joint attention maps that are fused subsequently. The two attention maps are complementary in the sense that one guides the network's attention towards the pixels where the joints occur, and the other guides the network's attention towards non-joint pixels that can potentially give the network useful clues for estimating the corresponding joint depths. The attention maps computed by the UV branch are explicitly encouraged to focus on joint pixels by applying 2D supervision to the heatmaps resulted from passing them through a spatial softmax layer <ref type="bibr" target="#b2">[3]</ref>. This approach can be viewed as the typical detection-based approach based on dense pixel-wise joint predictions. The attention maps computed by the attention enhancement branch are learned under no constraints, allowing them to freely focus on hand pixels most relevant to the estimation of joint depths. The depth branch develops pixel-wise feature vectors that contain depth information of the joints. The proposed model uses the fused per-joint attention maps as guidance to pool features from relevant pixels for each joint. After the relevant features are pooled for each joint, a weight-sharing linear layer is used to estimate the corresponding depth value.</p><p>We also propose PixDropout, a simple yet effective appearance-based data augmentation function for depth-based hand pose estimation methods. This function performs augmentation on a given sample by uniformly sampling a fraction of the pixels on the hand surface and turning them into a background pixel (replaces their value with a constant background value). We show empirically that PixDropout leads to a performance improvement not only in the proposed method but also in a regression-based method.</p><p>The proposed model is end-to-end differentiable and does not include any post-processing step or data pre-processing such as converting the depth map into point clouds <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44]</ref> or voxelized volume <ref type="bibr" target="#b12">[13]</ref>. We conduct the evaluation of the proposed model on three publicly available datasets, namely ICVL <ref type="bibr" target="#b15">[16]</ref>, MSRA <ref type="bibr" target="#b14">[15]</ref> and NYU <ref type="bibr" target="#b38">[39]</ref>, which are challenging benchmarks commonly used for evaluation of 3D hand pose estimation methods. The results demonstrate that the proposed model outperforms the state-of-the-art methods on all these benchmarks.</p><p>In summary, our contributions are as follows:</p><p>? We propose a novel neural network architecture, TriHorn-Net, which enables accurate 3D hand pose estimation.</p><p>? We propose a novel formulation for effective decomposition of the hand pose estimation into the estimation of the 2D joint locations and their depths.</p><p>? We propose PixDropout, which is, to the best of our knowledge, the first appearance-based data augmentation function for depth-based hand pose estimation methods.</p><p>? We conduct extensive experiments to demonstrate that the proposed method outperforms the state-of-the-art methods. Our implementation is available at https://github.com/mrezaei92/TriHorn-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Hand Pose Estimation</head><p>Hand pose estimation has been a long-standing problem in Computer Vision. Before the widespread use of deep learning techniques, many approaches relied on hand-crafted features, optimization methods, and distance metrics. Athitsos et al. <ref type="bibr" target="#b46">[47]</ref> used edge maps and Chamfer matching to perform 3D hand pose estimation. Other approaches used optimization methods such as Particle Swarm Optimization (PSO) <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>. After the rise of deep learning, DNN-based methods quickly displaced the traditional methods. The two most common input data modalities for DNN-based methods are: 1) RGB images and 2) depth images. While DNN-based 3D hand pose estimation on RGB images is a relatively new field of research, it has attracted a lot of attention recently <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b55">55]</ref>. However, as our method is depth-based, we focus our attention here on other depth-based methods.</p><p>Depth-based hand pose estimation methods have significantly advanced in the last decade. These methods can be classified into three categories: generative methods <ref type="bibr" target="#b62">[62,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b41">42]</ref>, discriminative methods <ref type="bibr" target="#b61">[61,</ref><ref type="bibr" target="#b64">64]</ref>, and hybrid methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b65">65]</ref>. Oberweger et al. <ref type="bibr" target="#b18">[19]</ref> used a CNN to estimate the hand pose represented by PCA coefficients.</p><p>Instead of performing in the 2.5D space, several methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b12">13]</ref> converted 2.5D depth images into 3D voxels and adopted 3D CNNs to estimate the 3D hand pose. Fang et al. <ref type="bibr" target="#b26">[27]</ref> propose an approach based on graph CNNs to compute pixel offsets to the joints and use weighted average to compute the hand joint locations. Another line of research has recently emerged, that utilizes the latest advancements of point cloud processing, by converting depth images into point clouds and using a point cloud processing network to perform hand pose estimation <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>TriHorn-Net is inspired by the methods based on dense pixel-wise prediction, but it differs from them in some important aspects. It offers a novel formulation for hand pose estimation, which is based on the decomposition of hand pose estimation into estimating 2D joint locations and their depth values. While it takes advantage of the typical pixel-wise prediction approach for estimating the 2D joint locations, it breaks from the standard approach in the sense that it estimates pixel-wise feature vectors (as opposed to predictions) and uses a weight-sharing layer (as opposed to dedicated layers) for estimating the joint's depth values. Extensive experiments demonstrate the effectiveness of the proposed formulation.</p><p>The proposed model is similar to A2J <ref type="bibr" target="#b32">[33]</ref> in that it uses different branches for estimating joints' image coordinates and their depth values. However, it adopts a fundamentally different approach for estimating the joint positions. A2J <ref type="bibr" target="#b32">[33]</ref> relies on a fixed number of regularly spaced points placed in depth image space, which are called anchors, in order to predict joint UVD offsets, whereas the proposed method performs a pixel-wise likelihood estimation for computing joints' image coordinates UV, and uses the UV estimations to guide the estimation of the joint depth values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Data Augmentation</head><p>Data augmentation methods aim at increasing the amount and diversity of the training data by randomly creating novel and realistic-looking data samples. In recent years, significant progress has been made on data augmentation methods for vision <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, NLP <ref type="bibr" target="#b11">[12]</ref> and speech <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. In the image domain, novel samples are created by applying a set of transformations to an available sample. These transformations can be broadly categorized into two groups: 1) geometric transformations and 2) appearance transformations. There has been a wide range of appearance transformations proposed recently for performing data augmentation on RGB images, such as color jitter, histogram equalization and contrast adjustment. However, most of them are not applicable to depth images due to the different nature of the depth image, limiting the set of data augmentation functions used by depth-based hand pose estimation methods mostly to geometric transformations such as random rotation, translation and scaling.</p><p>In this paper, we propose PixDropout, a simple yet effective appearance transformation applicable as a data augmentation function to the depth images. It is strongly inspired by Dropout <ref type="bibr" target="#b6">[7]</ref>. While Dropout <ref type="bibr" target="#b6">[7]</ref> randomly selects and drops neurons in the layers of a neural network, the proposed PixDropout randomly drops some fraction of pixels on the hand surface in the input depth image. PixDropout is most similar to the RGB image augmentation methods proposed in <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, where they randomly sample and then mask out rectangular regions of the input image to simulate occlusion for an image recognition task. However, in contrast to <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, PixDropout applies no spatial priors(e.g., it samples individual pixels rather than rectangular regions). Empirical results show that despite its simplicity, PixDropout is an effective data augmentation function for depth-based hand pose estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The task of 3D hand pose estimation is defined as follows: given an input depth image D I ? R H?W , the task is to estimate the 3D location of a set of pre-defined hand joints P ? R J?3 in the camera coordinate system. H, W denote the height and width of the input depth image respectively. J denotes the total number of joints to be estimated. In this section, the proposed model is laid out in details.</p><p>As illustrated in <ref type="figure">Figure 1</ref>, TriHorn-Net consists of two stages. In the first stage, the input depth image is run through the encoder network f . The encoder extracts and combines low-level and high-level features of the hand and outputs a high resolution feature volume, which is passed on to three separate branches. The UV branch, computes a per-joint attention map, where each map is focused on pixels where the corresponding joint occurs. This behavior is explicitly enforced by the application of 2D supervision to the heatmaps computed by passing the attention maps through a special softmax layer <ref type="bibr" target="#b2">[3]</ref>. The second branch, called the attention enhancement branch, also computes a per-joint attention map but does so under no constraints, allowing it to freely learn to detect the hand pixels most important for estimating the joint depth values under different scenarios. This attention map enhances the attention map computed by the UV branch through a fusion operation, which is performed by a linear interpolation controlled by per-joint learnable parameters. As a result, the fused attention maps attend to not only the joint pixels but also the hand pixels that do not belong to joints but contain useful information for estimating the joint depth values. The fused attention map is then used as <ref type="figure">Figure 1</ref>: An overview of the TriHorn-Net architecture. It consists of an encoder that encodes the hand depth image into a high resolution feature volume, which serves as the input to three separate branches. The UV branch and the attention enhancement branch compute two per-joint attention maps Att uv and Att enh respectively. Att uv is focused on joint pixels, whereas Att enh attention map has the flexibility to shift the network's attention to hand pixels that are most relevant for joint depth estimation. Att uv and Att enh are fused via a linear interpolation controlled by per-joint learned parameters ? j . For estimating the joints' depths, the network uses the fused attention maps Att f used to pool features from the depth feature map D computed by the depth branch. The pooled feature vector for each joint is input to a weight-sharing linear layer to estimate its depth value.</p><p>guidance for pooling features from the depth feature map computed by the depth branch. Finally, a weight-sharing linear layer is used to estimate the joint depth values from the feature vectors computed for each joint.</p><p>The feature pooling is conducted through a dot-product operation. This type of feature pooling followed by an estimation layer shared across all the joints is adopted from Transformer networks <ref type="bibr" target="#b68">[68]</ref>. This approach breaks with the standard approach in hand pose estimation methods, where they use dedicated layers for estimating the location of each joint. We show in the sec 4.3 that TriHorn-Net derives its power from the proposed estimation formulation, namely the decomposition of the hand pose estimation into the estimation of 2D joint locations and the estimation of their corresponding depth values aided by two separate attention maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>The encoder is defined as a non-linear mapping from the input depth image to the output feature volume f : R H?W ? R c?h?w , where h, w and c denote the height, width and the number of the channels of the output feature volume respectively. While any off-the-shelf network architecture can be used as this non-linear mapping, we empirically find that, to maximize accuracy, the encoder network should have a high capability of extracting and fusing features at different scales. This is because the hand orientation, the arrangement of the fingers, and the relationships of adjacent joints are among the many cues that are best recognized at different scales in the depth image. We show in the sec 4.3 that the proposed model is robust to the choice of the encoder network architecture as long as the above-mentioned requirement of extracting and fusing features at different scales is met. We use an Hourglass network <ref type="bibr" target="#b35">[36]</ref> as the encoder. It uses skip-connections and repeated bottom-up, top-down processing to extract and consolidate features across different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">UV Branch</head><p>This branch takes as input the output feature volume from the encoder and computes a per-joint attention map Att uv ? R J?h?w . We use Att j uv ? R h?w to refer to the attention map corresponding to the j th joint. Att j uv is explicitly encouraged to focus on pixels where the j th joint occurs by applying 2D supervision to it. To this end, the attention map Att j uv is first normalized by a spatial softmax layer <ref type="bibr" target="#b2">[3]</ref> to obtain the corresponding heatmap H 2D j = ?(Att j uv ) as follows:</p><formula xml:id="formula_0">H 2D j (x, y) = exp(Att j uv (x, y)) ui,vi?? exp(Att j uv (u i , v i ))<label>(1)</label></formula><p>In the above, ? denotes the spatial softmax layer. The heatmap H 2D j represents the likelihood of the j th joint occurring at each pixel location. ? represents the spacial domain of the attention map Att j uv . The 2D location of the j th joint is computed through an integration operation similar to <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b2">3]</ref>, as follows:</p><formula xml:id="formula_1">(? j ,V j ) = ui vi (u i , v i )H 2D j (u i , v i )<label>(2)</label></formula><p>In the above, (? j ,V j ) represents the estimated coordinates of the j th joint in the depth image space. The supervision is applied to all attention maps Att j uv for j ? 1, 2, ..., J by minimizing the mean L1 distance defined as:</p><formula xml:id="formula_2">uv = 1 2J J j=1 |? j ? U j | + |V j ? V j |<label>(3)</label></formula><p>In the above, (U j , V j ) represents the ground-truth 2D location of the j th joint. Note that (? j ,V j ) is also used to report the estimated coordinates of the j th joint in the depth image space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Enhancement Branch</head><p>This branch is aimed at computing a more flexible attention map to enhance the attention maps computed by the UV branch Att uv towards facilitating the estimation of the joint depth values. Specifically, it takes as input the output feature volume from the encoder and computes a per-joint attention map Att enh ? R J?h?w . Att j enh ? R h?w denotes the attention map corresponding to the j th joint. In contrast to Att uv , no external constraint (supervision) is applied to this attention map, allowing it to freely learn which hand pixels are the most relevant ones for estimating the depth value for each joint under different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Depth Branch</head><p>Contrary to the common practice of computing pixel-wise depth offset or prediction, the proposed model computes dense pixel-wise depth feature vectors. Specifically, this branch takes as input the output feature volume from the encoder and produces a dense depth feature map D ? R D?h?w . D represents the depth feature vector dimension, which is set D = 64 in our experiments. The depth feature vector at the special location (x, y) in the depth feature map D, denoted by D(x, y) ? R D , is developed such that it contains information about the depth value of the joints gathered from the input depth image pixels included in the receptive field of the special location (x, y) in the depth feature volume. The final feature vector used for each joint to estimate its depth value is obtained using a weighted average computed over all the depth feature vectors, where the weight to each depth feature vector is assigned using the corresponding fused attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Attention Fusion</head><p>The two attention maps Att j uv and Att j enh are complementary in the sense that Att j uv shifts the network attention to the pixels where the j th joint occurs, while Att j enh helps the network pay attention to the non-joint pixels that might contain useful information for estimating the depth value of the j th joint. These two attention maps are fused as follows:</p><formula xml:id="formula_3">Att j f used = ?(? j Att j uv + (1 ? ? j )Att j enh )<label>(4)</label></formula><p>Here, ? j ? [0, 1] denotes the learned parameter that controls the contribution of each attention map to the fused attention map Att j f used . The proposed model uses Att j f used as guidance to pool features from the pixels that contain the most relevant information with respect to the depth of the j th joint. in order to guide the subsequent feature pooling for depth value estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Depth Value Estimation</head><p>The depth value for the j th joint is estimated from the feature vector obtained by pooling features from the pixels that contain the most relevant information about its depth, which is guided by Att j f used as follows:</p><formula xml:id="formula_4">F j = Att j f used ? D = x y</formula><p>Att j f used (x, y)D(x, y) Where F j ? R D denotes the pooled feature vector for the j th joint. The depth value for the j th joint, denoted byZ j , is then estimated using a single linear layer as follows:</p><formula xml:id="formula_6">Z j = F j W + b<label>(6)</label></formula><p>Where W ? R D and b ? R denote the weights of the linear layer. Note that this linear layer is shared across all the joints. This not only improves the parameter efficiency but also encourages ensemble-like behavior in the depth feature vectors, as each has to gather the depth information of all joints. This type of feature pooling using attention followed by a shared layer is inspired from the mechanism employed in Transformer networks <ref type="bibr" target="#b68">[68]</ref>.</p><p>The depth value estimation for the joints is supervised by the following loss term:</p><formula xml:id="formula_7">d = 1 J J j=1 |Z j ? Z j |<label>(7)</label></formula><p>Where Z j refers to the ground-truth depth value for the j th joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">End-to-End Training</head><p>The proposed model is end-to-end differentiable and is trained by minimizing the loss function that comprises the two loss terms discussed in the previous sections, which is formulated as:</p><formula xml:id="formula_8">L = uv + ? d<label>(8)</label></formula><p>where ? is a weighting factor to balance uv and d . We set ? = 1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">PixDropout</head><p>The proposed data augmentation function PixDropout is defined as a function T ? : R H?W ? R H?W , where the parameter ? ? [0, 1] controls the intensity of the augmentation. In the first step, we uniformly sample a probability ? from the range [0, ?]. In the second step, we uniformly sample a set of the pixels Q from the hand surface. Each pixel is selected with a probability of ?. The augmented depth imageD I = T ? (D I ) is computed by dropping the selected pixels as follows:D</p><formula xml:id="formula_9">I (p) = C if p ? Q D I (p) otherwise<label>(9)</label></formula><p>Where p denotes an arbitrary pixel in the depth image. C represents the constant value assigned to the background (non-hand) pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>The pre-processing method for preparing the input depth image includes first cropping the hand area from a depth image similar to <ref type="bibr" target="#b40">[41]</ref>, and then resizing it to a fixed size of 128x128. The depth values are normalized to <ref type="bibr">[-1, 1]</ref>. In  order to maximize accuracy, the encoder's output feature volume needs to be of high spatial dimension. To strike a balance between the computational complexity and performance, we set it to be half of that of the input depth image. We use Adam <ref type="bibr" target="#b66">[66]</ref> optimizer with a cosine learning rate decay schedule <ref type="bibr" target="#b30">[31]</ref> for training. The initial learning rate and the weight decay are set to be 10 ?3 and 10 ?5 respectively. For data augmentation, we use geometric transformations including in-plane rotation ([-180, 180] degree), 3D scaling ([0.9, 1,1]), and 3D translation ([-8, 8] mm), as well as the proposed PixDropout as the appearance transformation. We set ? = 0.15 in all the experiments. We trained the model for 40 epochs on ICVL, 40 epochs on NYU and 60 epochs on MSRA. All experiments are implemented by PyTorch framework <ref type="bibr" target="#b29">[30]</ref> and conducted on a single server with one NVIDIA 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Datasets and Evaluation Metrics</head><p>ICVL Dataset. The ICVL dataset <ref type="bibr" target="#b15">[16]</ref> provides 22K and 1.6K depth frames for training and testing, respectively. The ground-truth for each frame contains J = 16 joints, including one joint for the palm and three joints for each finger. We do not use the additional 300k augmented frames (which are obtained with in-plane rotations of the original training frames) included in this dataset.</p><p>MSRA Dataset. The MSRA dataset <ref type="bibr" target="#b14">[15]</ref> contains more than 76K frames captured from 9 subjects. Each subject contains 17 hand gestures and each hand gesture has about 500 frames with segmented hand depth image. Each frame is provided with a ground-truth of J = 21 joints, including one joint for the wrist and four joints for each finger. Following the protocol used by <ref type="bibr" target="#b14">[15]</ref>, we evaluate the proposed method on this dataset with the leave-one-subject-out cross-validation strategy.</p><p>NYU Dataset. The NYU dataset <ref type="bibr" target="#b38">[39]</ref> is captured from three different views with Microsoft Kinect sensor. Each view contains 72K training 8K testing depth images. Following the common protocol, we only use the first view with a subset of J = 14 joints out of total of 36 annotated joints provided for both the training and testing.</p><p>Evaluation metrics. We use the two most commonly used metrics for evaluation of 3D hand pose estimation: the mean distance error (in mm) and the success rate. The mean distance error measures the average Euclidean distance between the estimated and the ground-truth coordinates computed across all the joints and over the entire testing set. The success rate is defined as the fraction of the frames for which the mean distance error is less than a certain distance threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Impact of using complementary attention maps. We study the impact of employing two complementary attention maps in the proposed model. Specifically, we examine the performance of the model in three cases with respect to the attention map used for depth feature pooling. The first case only uses the attention map computed by the UV branch and removes the attention enhancement branch from the network. The second case only uses Att enh , which is computed by the attention enhancement branch. The third case corresponds to the proposed approach based on fusing the two complementary attention maps. As can be seen in <ref type="table" target="#tab_1">Table 2</ref>, using a second freely learned attention map enhancing the attention map computed by the UV branch leads to the best performing case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Different Approaches for Attention Fusion.</head><p>We study the effectiveness of the proposed attention fusion approach in our model. We implement the proposed model using three different approaches for attention fusion: 1) concatenation 2) summation and 3) the proposed strategy. For concatenation, the two attention maps are first concatenated and then passed through a number of convolutional layers to obtain the fused attention map. For the second experiment, the two attention maps are simply fused by the element-wise addition. The proposed strategy is <ref type="table">Table 3</ref>: Comparison of different choices for the encoder network architecture on ICVL <ref type="bibr" target="#b15">[16]</ref>. #Params indicates the number of the model parameters Encoder Architecture Error (mm) #Params HRnet <ref type="bibr" target="#b69">[69]</ref> 5.91 7.22M ResDeconv <ref type="bibr" target="#b70">[70]</ref> 6.04 27.24M Hourglass <ref type="bibr" target="#b35">[36]</ref> 5.73 7.81M an extension of the summation approach, where the contribution of each attention map to the fused attention map is controlled by a learned parameter. As can be seen in <ref type="table" target="#tab_2">Table 4</ref>, the proposed strategy performs best.</p><p>Impact of Different Encoder Network Architectures. We analyse the impact of different encoder network architectures on the model performance. Specifically, we use three representative architectures: 1) Hourglass network <ref type="bibr" target="#b35">[36]</ref>, 2) HRNet <ref type="bibr" target="#b69">[69]</ref> and 3) ResNetDeconv <ref type="bibr" target="#b70">[70]</ref>. Although through different mechanisms, Hourglass network and HRNet both directly transfer the low-level features extracted in the early layers to deeper layers via direct skip-connections. On the other hand, ResNetDeconv down-samples the input to a low resolution feature map and then up-samples it back by a deconvolution head. As can be seen in <ref type="table">Table 3</ref>, the Hourglass network and HRNet both achieve the state-of-the-art results. Although ResNetDeconv does not have the same ability to extract and fuse features at different scales, it still performs strongly compared to the existing methods. These observations demonstrate that the proposed model is robust to the choice of the encoder network architecture and derives its superior performance from our novel formulation of pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effectiveness of PixDropout</head><p>To verify the effectiveness of the proposed data augmentation function PixDropout, we conduct two independent comparisons. We compare the performance of the proposed model with and without PixDropout. We also repeat this comparison using a regression-based method. Specifically, we use a ResNet-50 network <ref type="bibr" target="#b63">[63]</ref>, with its last fully connected layer replaced by 2 fully connected layers to estimate the hand pose. As can be seen in <ref type="table" target="#tab_0">Table 1</ref>, PixDropout leads to a performance improvement not only in the proposed model but also in a model of different nature, demonstrating its effectiveness as an augmentation function for depth-based hand pose estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparison with the State-of-the-Art Methods</head><p>We compare the proposed model with the state-of-the-art methods including both dense detection-based methods and regression-based methods. These methods include model-based method (DeepModel) <ref type="bibr" target="#b60">[60]</ref>, DeepPrior <ref type="bibr" target="#b18">[19]</ref>, improved DeepPrior (DeepPrior++) <ref type="bibr" target="#b40">[41]</ref>, region ensemble network (Ren-4x6x6 <ref type="bibr" target="#b21">[22]</ref>, Ren-9x6x6 <ref type="bibr" target="#b22">[23]</ref>), Pose-Ren <ref type="bibr" target="#b23">[24]</ref>, Generalized-Feedback <ref type="bibr" target="#b44">[45]</ref>, dense regression network (DenseReg) <ref type="bibr" target="#b36">[37]</ref>, A2J <ref type="bibr" target="#b32">[33]</ref>, CrossInfoNet <ref type="bibr" target="#b31">[32]</ref> and JGR-P2O    <ref type="bibr" target="#b60">[60]</ref> 11.56 DeepPrior <ref type="bibr" target="#b18">[19]</ref> 10.4 DeepPrior++ <ref type="bibr" target="#b40">[41]</ref> 8.1 REN-4x6x6 <ref type="bibr" target="#b21">[22]</ref> 7.63 REN-9x6x6 <ref type="bibr" target="#b22">[23]</ref> 7.31 DenseReg <ref type="bibr" target="#b36">[37]</ref> 7.3 SHPR-Net <ref type="bibr" target="#b20">[21]</ref> 7.22 HandPointNet <ref type="bibr" target="#b33">[34]</ref> 6.94 Pose-REN <ref type="bibr" target="#b23">[24]</ref> 6.79 CrossInfoNet <ref type="bibr" target="#b31">[32]</ref> 6.73 NARHT <ref type="bibr" target="#b45">[46]</ref> 6.47 A2J <ref type="bibr" target="#b32">[33]</ref> 6.46 Point-to-Point <ref type="bibr" target="#b34">[35]</ref> 6.3 V2V-PoseNet <ref type="bibr" target="#b12">[13]</ref> 6.28 JGR-P2O <ref type="bibr" target="#b26">[27]</ref> 6.02 HandFoldingNet <ref type="bibr" target="#b43">[44]</ref> 5.95 Ours 5.73</p><p>Methods Error DeepPrior <ref type="bibr" target="#b18">[19]</ref> 19.73 DeepModel <ref type="bibr" target="#b60">[60]</ref> 17.04 3DCNN <ref type="bibr" target="#b19">[20]</ref> 14.1 REN-4x6x6 <ref type="bibr" target="#b21">[22]</ref> 13.39 REN-9x6x6 <ref type="bibr" target="#b22">[23]</ref> 12.69 DeepPrior++ <ref type="bibr" target="#b40">[41]</ref> 12.24 Pose-REN <ref type="bibr" target="#b23">[24]</ref> 11.81 Generalized-Feedback <ref type="bibr" target="#b44">[45]</ref> 10.89 SHPR-Net <ref type="bibr" target="#b20">[21]</ref> 10.78 HandPointNet <ref type="bibr" target="#b33">[34]</ref> 10.54 DenseReg <ref type="bibr" target="#b36">[37]</ref> 10.2 CrossInfoNet <ref type="bibr" target="#b31">[32]</ref> 10.08 NARHT <ref type="bibr" target="#b45">[46]</ref> 9.8 Point-to-Point <ref type="bibr" target="#b34">[35]</ref> 9.1 A2J <ref type="bibr" target="#b32">[33]</ref> 8.61 HandFoldingNet <ref type="bibr" target="#b43">[44]</ref> 8.58 V2V-PoseNet <ref type="bibr" target="#b12">[13]</ref> 8.42 JGR-P2O <ref type="bibr" target="#b26">[27]</ref> 8.29 Ours 7.68</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Error REN-9x6x6 <ref type="bibr" target="#b22">[23]</ref> 9.79 3DCNN <ref type="bibr" target="#b19">[20]</ref> 9.58 DeepPrior++ <ref type="bibr" target="#b40">[41]</ref> 9.5 Pose-REN <ref type="bibr" target="#b23">[24]</ref> 8.65 HandPointNet <ref type="bibr" target="#b33">[34]</ref> 8.5 CrossInfoNet <ref type="bibr" target="#b31">[32]</ref> 7.86 SHPR-Net <ref type="bibr" target="#b20">[21]</ref> 7.76 Point-to-Point <ref type="bibr" target="#b34">[35]</ref> 7.7 V2V-PoseNet <ref type="bibr" target="#b12">[13]</ref> 7.59 JGR-P2O <ref type="bibr" target="#b26">[27]</ref> 7.55 NARHT <ref type="bibr" target="#b45">[46]</ref> 7.55 HandFoldingNet <ref type="bibr" target="#b43">[44]</ref> 7.34 DenseReg <ref type="bibr" target="#b36">[37]</ref> 7.23 Ours 7.13 <ref type="bibr" target="#b26">[27]</ref>, 3DCNN <ref type="bibr" target="#b19">[20]</ref>, SHPR-Net <ref type="bibr" target="#b20">[21]</ref>, HandPointNet <ref type="bibr" target="#b33">[34]</ref>, Point-to-Point <ref type="bibr" target="#b34">[35]</ref>, NARHT <ref type="bibr" target="#b45">[46]</ref>, HandFoldingNet <ref type="bibr" target="#b43">[44]</ref> and V2V <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure" target="#fig_3">Figure 4</ref> and <ref type="figure" target="#fig_2">Figure 3</ref> respectively show the success rate and per-joint mean error (mm) on the ICVL, NYU and MSRA datasets. <ref type="table" target="#tab_3">Table 5</ref> summarizes the performance based on the mean distance error on the three datasets.</p><p>The results show that the proposed method significantly outperforms the state-of-the-art methods on all of these three benchmark datasets, achieving a mean distance error of 5.73 mm, 7.68 mm and 7.13 mm on ICVL, NYU and MSRA respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we proposed TriHorn-Net, a novel and powerful neural network for 3D hand pose estimation from a single depth image. It achieves improved accuracy in hand pose estimation by introducing a novel formulation to decompose the 3D hand pose estimation into the estimation of 2D joint location in the image coordinate space, and the estimation of their corresponding depth values, which is guided by an attention map resulted from the fusion of two complementary attention maps computed by two separate branches. Experimental results on three challenging benchmarks demonstrate that, despite having a simple architecture and requiring no optimization approaches at test time, the proposed network outperforms the state-of-the-art methods. We also proposed a simple data augmentation method for depth-based hand pose estimation methods and presented empirical results demonstrating its effectiveness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>shows some qualitative examples of how the two attention maps Att j uv and Att j enh play the complementary role in forming the final fused attention map Att j f used</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative examples of how the two attention maps complement each other to guide feature pooling for depth estimation. Each row shows an example including the input depth image, the Att j uv and Att j enh attention maps computed by the first and second branches respectively, and the resulting fused attention map Att j f used . The red dot in the input depth image marks the ground-truth joint location for which the attention maps are computed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Comparison with the state-of-the-art methods on ICVL<ref type="bibr" target="#b15">[16]</ref> (Left), NYU<ref type="bibr" target="#b38">[39]</ref> (Middle), and MSRA<ref type="bibr" target="#b14">[15]</ref> (Right) datasets. The per-joint mean error is used for comparison (R: root, T: tip).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-art methods on ICVL<ref type="bibr" target="#b15">[16]</ref> (Left), NYU<ref type="bibr" target="#b38">[39]</ref> (Middle), and MSRA<ref type="bibr" target="#b14">[15]</ref> (Right) datasets. Success rates over different error thresholds is used for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The performance of the models with and without (w/o) PixDropout as augmentation function on ICVL<ref type="bibr" target="#b15">[16]</ref>.</figDesc><table><row><cell cols="2">The numbers indicate the mean distance error (mm)</cell><cell></cell></row><row><cell>Model</cell><cell>with PixDropout</cell><cell>w/o PixDropout</cell></row><row><cell>TriHorn-Net</cell><cell>5.73</cell><cell>5.90</cell></row><row><cell>ResNet-50</cell><cell>7.71</cell><cell>7.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Impact of using different attention maps for depth feature pooling on the performance on ICVL<ref type="bibr" target="#b15">[16]</ref> </figDesc><table><row><cell>Attention Map</cell><cell>Error (mm)</cell></row><row><cell>Att uv</cell><cell>5.91</cell></row><row><cell>Att enh</cell><cell>6.03</cell></row><row><cell>Fused Attention</cell><cell>5.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Comparison of different attention fusion approaches on ICVL [16]</cell></row><row><cell>Approach</cell><cell>Error (mm)</cell></row><row><cell>Concatenation</cell><cell>5.98</cell></row><row><cell>Summation</cell><cell>5.84</cell></row><row><cell>Ours</cell><cell>5.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">Comparison with the state-of-the-art methods on ICVL [16] (Left), NYU [39] (Middle), and MSRA [15]</cell></row><row><cell cols="2">(Right). "Error" indicates the mean distance error in (mm)</cell></row><row><cell>Methods</cell><cell>Error</cell></row><row><cell>DeepModel</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sign Language Production: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition Workshops</title>
		<meeting>Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3451" to="3461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depth-based 3d hand pose estimation: From current achievements to future goals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mu Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2636" to="2645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hand pose estimation via latent 2.5 d heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The European Conference On Computer Vision (ECCV)</title>
		<meeting>Of The European Conference On Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="118" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HandVoxNet: Deep Voxel-Based Network for 3D Hand Shape and Pose Estimation from a Single Depth Map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elhayek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7113" to="7122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">SRN: Stacked Regression Network for Real-time 3D Hand Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Point-to-pose voting based hand pose estimation using residual permutation equivariant layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11927" to="11936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal Of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Others Deep speech: Scaling up end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5567</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501.</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Qanet: Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09541.</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sign language recognition: a deep survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems With Applications</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Opening the black box: Hierarchical sampling optimization for estimating human hand pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE International Conference On Computer Vision</title>
		<meeting>Of The IEEE International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3325" to="3333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mapping unlabeled real data for label austerity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Poier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Schinagl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference On Applications Of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1393" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hands deep in deep learning for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.06807</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for efficient and robust hand pose estimation from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1991" to="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Shpr-net: Deep semantic hand pose regression from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="43425" to="43439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Region ensemble network: Improving convolutional network for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference On Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4512" to="4516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Region ensemble network: Towards good practices for deep 3D hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Of Visual Communication And Image Representation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="404" to="414" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pose guided structured region ensemble network for cascaded hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">395</biblScope>
			<biblScope unit="page" from="138" to="149" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hand sign language recognition using multi-view hand skeleton. Expert Systems With Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastgoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">150</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">So-handnet: Self-organizing network for 3d hand pose estimation with semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE International Conference On Computer Vision</title>
		<meeting>Of The IEEE International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6961" to="6970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">JGR-P2O: joint graph reasoning based pixel-to-offset prediction network for 3D hand pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference On Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="120" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The AAAI Conference On Artificial Intelligence</title>
		<meeting>Of The AAAI Conference On Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="13001" to="13008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">&amp; Others Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sgdr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-task information sharing based hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crossinfonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9896" to="9905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>A2j</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE International Conference On Computer Vision</title>
		<meeting>Of The IEEE International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hand pointnet: 3d hand pose estimation using point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8417" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Point-to-point regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The European Conference On Computer Vision (ECCV)</title>
		<meeting>Of The European Conference On Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference On Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dense 3d regression for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Augmented skeleton space transfer for depth-based hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8330" to="8339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Real-time continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Handseg: An automatically labeled dataset for hand segmentation from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malireddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Conference On Computer And Robot Vision (CRV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepprior++: Improving fast and accurate 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE International Conference On Computer Vision Workshops</title>
		<meeting>Of The IEEE International Conference On Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="585" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Capturing hands in action using discriminative salient points and physics simulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ballan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aponte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="page" from="172" to="193" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Embodied hands: Modeling and capturing hands and bodies together</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">HandFoldingNet: A 3D Hand Pose Estimation Network Using Multiscale-Feature Guided Folding of a 2D Hand Skeleton</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE/CVF International Conference On Computer Vision</title>
		<meeting>Of The IEEE/CVF International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11260" to="11269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generalized feedback loop for joint hand-object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Pattern Analysis And Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1898" to="1912" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hand-Transformer</surname></persName>
		</author>
		<title level="m">non-autoregressive structured modeling for 3D hand pose estimation. European Conference On Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Estimating 3D hand pose from a cluttered image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference On Computer Vision And Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">432</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">&amp; Others Accurate, robust, and flexible real-time hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rhemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Leichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The 33rd Annual ACM Conference On Human Factors In Computing Systems</title>
		<meeting>Of The 33rd Annual ACM Conference On Human Factors In Computing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3633" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Efficient model-based 3D tracking of hand articulations using Kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Oikonomidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kyriazis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyros</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bmvc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d hand pose from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE International Conference On Computer Vision</title>
		<meeting>Of The IEEE International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards accurate alignment in real-time 3d hand-mesh reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE/CVF International Conference On Computer Vision</title>
		<meeting>Of The IEEE/CVF International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11698" to="11707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Habermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Habibie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5346" to="5355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10833" to="10842" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Pushing the envelope for rgb-based dense 3d hand pose estimation via neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1067" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Weakly supervised 3d hand pose estimation via biomechanical constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="211" to="228" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVII 16</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="752" to="768" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VII 16</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deephandmesh: A weakly-supervised deep encoder-decoder framework for high-fidelity hand mesh modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shiratori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference On Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="440" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Hand Image Understanding via Deep Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE/CVF International Conference On Computer Vision</title>
		<meeting>Of The IEEE/CVF International Conference On Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11281" to="11292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Online generative model personalization for hand tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tkach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Remelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Model-based deep hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06854</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Hand pose estimation and hand shape classification using multi-layered randomized decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>K?ra?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akarun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference On Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="852" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning an efficient model of hand shape variation from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</title>
		<meeting>Of The IEEE Conference On Computer Vision And Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2540" to="2548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Identity mappings in deep residual networks. European Conference On Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Parsing the hand in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Multimedia</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1241" to="1253" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">&amp; Others Efficient and precise interactive hand tracking through joint, continuous optimization of pose and correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bordeaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Corish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Keskin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv Preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The European Conference On Computer Vision (ECCV)</title>
		<meeting>Of The European Conference On Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances In Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Others Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions On Pattern Analysis And Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="3349" to="3364" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Of The European Conference On Computer Vision (ECCV)</title>
		<meeting>Of The European Conference On Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
