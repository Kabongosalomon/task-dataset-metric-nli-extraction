<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Deep Hough Transform for Semantic Line Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Bin</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE Deep Hough Transform for Semantic Line Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Semantic line detection</term>
					<term>Hough transform</term>
					<term>CNN</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We focus on a fundamental task of detecting meaningful line structures, a.k.a., semantic line, in natural scenes. Many previous methods regard this problem as a special case of object detection and adjust existing object detectors for semantic line detection. However, these methods neglect the inherent characteristics of lines, leading to sub-optimal performance. Lines enjoy much simpler geometric property than complex objects and thus can be compactly parameterized by a few arguments. To better exploit the property of lines, in this paper, we incorporate the classical Hough transform technique into deeply learned representations and propose a one-shot end-to-end learning framework for line detection. By parameterizing lines with slopes and biases, we perform Hough transform to translate deep representations into the parametric domain, in which we perform line detection. Specifically, we aggregate features along candidate lines on the feature map plane and then assign the aggregated features to corresponding locations in the parametric domain. Consequently, the problem of detecting semantic lines in the spatial domain is transformed into spotting individual points in the parametric domain, making the post-processing steps, i.e., non-maximal suppression, more efficient. Furthermore, our method makes it easy to extract contextual line features that are critical for accurate line detection. In addition to the proposed method, we design an evaluation metric to assess the quality of line detection and construct a large scale dataset for the line detection task. Experimental results on our proposed dataset and another public dataset demonstrate the advantages of our method over previous state-of-the-art alternatives. The dataset and source code is available at https://mmcheng.net/dhtline/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>D Etecting line structures from digital images has a long history in computer vision. The organization of line structures is an early yet essential step to transform the visual signal into useful intermediate concepts for visual interpretation <ref type="bibr" target="#b1">[2]</ref>. Though many techniques have been proposed to detect salient objects <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> and areas <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, little work has been made for detecting outstanding/structurerevealing line structures. A recent study <ref type="bibr" target="#b10">[11]</ref> was proposed to detect outstanding straight line(s), referred to as "semantic line", that outlines the conceptual structure of natural images. Identifying these semantic lines is of crucial importance for computer graphics and vision applications, such as photographic composition <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, structure-preserving image processing <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, image aesthetic <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, lane detection <ref type="bibr" target="#b19">[20]</ref>, and artistic creation <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. As demonstrated in <ref type="figure">Fig. 1</ref>, Liu et al. <ref type="bibr" target="#b11">[12]</ref> proposed to crop images according to the golden ratio by using 'prominent line'. Detecting these 'semantic lines' can help to produce images that are visually pleasing in the photographic composition. The Hough transform <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> is one representative method for line detection, which was first proposed to detect straight lines in bubble chamber photographs <ref type="bibr" target="#b26">[27]</ref>. Since its simplicity and efficiency, HT is employed to detect lines in digital images <ref type="bibr" target="#b24">[25]</ref>, and further extended by <ref type="bibr" target="#b25">[26]</ref> to detect other regular shapes like circles and rectangles. The key idea of the Hough transform is to vote evidence from the image domain to the parametric domain, and then detect shapes in the parametric domain by identifying local-maximal responses. In the case of line detection, a line in the image domain can be represented by its parameters, e.g., slope, and offset in the parametric space. Hough transform collects evidence along with a line in an image and accumulates evidence to a single point in the parameter space. Consequently, line detection in the image domain is converted to the problem of detecting peak responses in the parametric domain. Classical Hough transform based line detectors <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> usually detect continuous straight edges while neglecting the semantics in line structures. Moreover, these methods are sensitive to light changes and occlusion. Therefore, the results are often noisy and contain irrelevant lines <ref type="bibr" target="#b31">[32]</ref>, as shown in <ref type="figure">Fig. 1(d)</ref>.</p><p>Convolutional Neural Networks (CNNs) have achieved remarkable success in a wide range of computer vision tasks. Several recent studies <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b33">[34]</ref> have proposed CNN-based methods for line detection. Concretely, they regard line detection as a special case of object detection and employ existing object detectors e.g., faster R-CNN <ref type="bibr" target="#b34">[35]</ref> or CornerNet <ref type="bibr" target="#b35">[36]</ref>, for line detection. Limited by the ROI pooling and non-maximal suppression of lines, both <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b33">[34]</ref> are less efficient in terms of running time. Moreover, ROI pooling <ref type="bibr" target="#b36">[37]</ref> aggregates features along with a single line, while many recent studies reveal that richer context information is critical to many tasks, e.g., video classification <ref type="bibr" target="#b37">[38]</ref> and semantic segmentation <ref type="bibr" target="#b38">[39]</ref>. This point will be validated in Sec. 6.6, in which we experimentally verify that only aggregating features along a single line will produces sub-optimal results.</p><p>Incorporate powerful CNNs to Hough transform is a arXiv:2003.04676v4 [cs.CV] 1 May 2021 <ref type="figure">Fig. 1</ref>. Example pictures from <ref type="bibr" target="#b11">[12]</ref> reveal that semantic lines may help in the photographic composition. (a): a photo was taken with an arbitrary pose. (b): a photo fits the golden ratio principle <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b32">[33]</ref> which is obtained by the method described in <ref type="bibr" target="#b11">[12]</ref> using so-called 'prominent lines' in the image. promising direction for semantic line detection. A simple way of combining CNN with Hough transform is performing edge detection with a CNN-based edge detector <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref> and then apply standard Hough transform to the edge maps. However, the two components have diverse optimization targets, leading to sub-optimal results, as evidenced by our experiments. In this paper, we propose to incorporate CNN with Hough transform into an end-to-end manner so that each component in our proposed method shares the same optimization target. Our method first extracts pixel-wise representations with a CNN-based encoder and then performs Hough transform on the deep representations to convert representations from feature space into parametric space. Then the global line detection problem is converted to simply detecting peak response in the transformed features, making the problem much simpler. For example, the time-consuming non-maximal suppression (NMS) can be simply replaced by calculating the centroids of connected areas in the parametric space, making our method very efficient that can detect lines in real-time. Moreover, in the detection stage, we use several convolutional layers on top of the transformed features to aggregate context-aware features of nearby lines. Consequently, the final decision is made upon not only features of a single line, but also information about lines nearby. As shown in <ref type="figure">Fig. 1</ref>(c), our method detects clean, meaningful and outstanding lines, that are helpful to photographic composition. To better evaluate line detection methods, we introduce a principled metric to assess the agreement of a detected line w.r.t. its corresponding ground-truth line. Although <ref type="bibr" target="#b10">[11]</ref> has proposed an evaluation metric that uses intersection areas to measure the similarity between a pair of lines, this measurement may lead to ambiguous and misleading results. And at last, we collect a large scale dataset with 6,500 carefully annotated images for semantic line detection. The new dataset, namely NKL (short for NanKai Lines), contains images of diverse scenes, and the scale is much larger than the existing SEL <ref type="bibr" target="#b10">[11]</ref> dataset in both terms of images and annotated lines.</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula><p>The contributions of this paper are summarized below:</p><p>? We proposed an end-to-end framework for incorporating the feature learning capacity of CNN with Hough transform, resulting in an efficient real-time solution for semantic line detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>To facilitate the research of semantic line detection, we construct a new dataset with 6,500 images, which is larger and more diverse than a previous SEL dataset <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We introduce a principled metric that measures the similarity between two lines. Compared with the previous IOU based metric <ref type="bibr" target="#b10">[11]</ref>, our metric has straightforward interpretation and simplicity in implementation, as detailed in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Evaluation results on an open benchmark demonstrate that our method outperforms prior arts with a significant margin. A preliminary version of this work was presented in <ref type="bibr" target="#b0">[1]</ref>. In this extended work, we introduce three major improvements:</p><p>? We propose a novel "edge-guided refinement" module to adjust line positions and obtain better detection performance with the help of accurate edge information. This part is detailed in Sec. 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We introduce a new large-scale dataset for semantic line detection, as presented in Sec. 5. The new dataset, namely NKL (short for NanKai Lines), contains 6,500 images in total, and each image is annotated by multiple skilled annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We employ the maximal bipartite graph matching <ref type="bibr" target="#b41">[42]</ref> to match ground-truth and detected lines during evaluation (Sec. 6.1). The matching procedure removes redundant true positives so that each ground-truth line is associated with at most one detected line and vice versa. The rest of this paper is organized as follows: Sec. 2 summarizes the related works. Sec. 3 elaborates the proposed Deep Hough transform method. Sec. 4 describes the proposed evaluating metric, which is used to assess the similarity between a pair of lines. Sec. 5 introduces our newly constructed dataset. Sec. 6 presents experimental details and report comparison results. Sec. 7 makes a conclusion remark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>The research of line detection in digital images dates back to the very early stage of computer vision research. Here, we first brief the evolution of Hough transform <ref type="bibr" target="#b24">[25]</ref> (HT), one of the most fundamental tools, for line detection. Then we introduce several recent CNN based methods for line detection. At last, we summarize the methods and datasets for semantic line detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hough transform</head><p>The Hough transform (HT) was firstly proposed in <ref type="bibr" target="#b26">[27]</ref> for machine analysis of bubble chamber photographs. It parametrizes straight lines with slope-offset, leading to an unbounded transform space (since the slope can be infinity). <ref type="bibr" target="#b24">[25]</ref> extended HT by using angle-radius rather than slope-offset parameters, and is conceptually similar to twodimensional Radom transform <ref type="bibr" target="#b42">[43]</ref>. Then Ballard et al. <ref type="bibr" target="#b25">[26]</ref> generalized the idea of HT to localize arbitrary shapes, e.g., ellipses and circles, from digital images. For example, by parameterizing with angle and radius, line detection can be performed by voting edge evidence and finding peak response in the finite parametric space. Typically, with the edge detectors such as Canny <ref type="bibr" target="#b43">[44]</ref> and Sobel <ref type="bibr" target="#b44">[45]</ref>, the detected lines are the maximal local response points in the transformed parametric space. The core idea of HT is used in two recent works which parameterize the outputs of CNNs with offsets and orientations to predict surface meshes <ref type="bibr" target="#b45">[46]</ref> or convex decomposition <ref type="bibr" target="#b46">[47]</ref> of 3D shapes.</p><p>Despite the success of HT on line detection, it suffers from high computational costs and unstable performance. To accelerate the voting of HT, Nahum et al. <ref type="bibr" target="#b30">[31]</ref> proposed the "probabilistic Hough transform" to randomly pick sample points from a line, while <ref type="bibr" target="#b47">[48]</ref> using the gradient direction of images to decide the voting points. Meanwhile, the work of <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b48">[49]</ref> employed kernel-based Hough transform to perform hough voting by using the elliptical-Gaussian kernel on collinear pixels to boost the original HT. Besides, John et al. <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> partitioned the input image into hierarchical image patches, and then applied HT independently to these patches. Illingworth et al. <ref type="bibr" target="#b49">[50]</ref> use a coarse-to-fine accumulation and search strategy to identify significant peaks in the Hough parametric spaces. <ref type="bibr" target="#b50">[51]</ref> tackled line detection within a regularized framework, to suppress the effect of noise and clutter corresponding to nonlinear image features. The Hough voting scheme is also used in many other tasks such as detecting centroid of 3D shapes in point cloud <ref type="bibr" target="#b51">[52]</ref> and finding image correspondence <ref type="bibr" target="#b52">[53]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Line Segments Detection</head><p>Though its robustness and parallelism, Hough transform cannot be directly used for line segments detection, since it cannot determine the endpoints of line segments. Probabilistic Hough transform <ref type="bibr" target="#b30">[31]</ref> uses random sampling in the voting scheme, and reconstructs line segments by localizing the sample locations. But this method still prefers long straight lines. In addition to Hough transform, many other studies have been developed to detect line segments. Burns et al. <ref type="bibr" target="#b1">[2]</ref> used the edge orientation as the guide for line segments extraction. The main advantage is that the orientation of the gradients can help to discover low-contrast lines and endpoints. Etemadi et al. <ref type="bibr" target="#b53">[54]</ref> established a chain from the given edge map and extracted line segments and orientations by walking over these chains. Chan et al. <ref type="bibr" target="#b54">[55]</ref> used a quantified edge orientation to search and merge short line segments. Gioi et al. <ref type="bibr" target="#b55">[56]</ref> proposed a linear-time line segment detector (LSD) without tuning parameters, and is used by many subsequent studies <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b56">[57]</ref>, <ref type="bibr" target="#b57">[58]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CNN based Line Detection.</head><p>Recently, CNNs have brought remarkable improvements in computer vision tasks, and also be applied to line detection. These methods either focus on straight line detection, e.g., semantic line detection <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b58">[59]</ref>, or line segments detection, e.g., wireframe parsing <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>. Lee et al. <ref type="bibr" target="#b10">[11]</ref> followed the two-branch pipeline of faster-RCNN <ref type="bibr" target="#b34">[35]</ref> and proposed a straight line detection framework to find the meaningful semantic straight line in an image. One branch verifies the existence of a line and the other branch further refines the position of the line by regression. Zhang et al. <ref type="bibr" target="#b33">[34]</ref> adopted the conception of CornerNet <ref type="bibr" target="#b35">[36]</ref> to extract line segments as a pair of key points in indoor scenes. Huang et al. <ref type="bibr" target="#b59">[60]</ref> proposed a two-head network to predict lines and junction points for wireframe parsing. This is extended in <ref type="bibr" target="#b60">[61]</ref> by adding a line proposal sub-network. Zhou et al. <ref type="bibr" target="#b60">[61]</ref> proposed an end-to-end architecture to perform accurate line segments detection in wireframe parsing.</p><p>All these methods extract line-wise feature vectors by LoI pooling that aggregate deep features solely along each line, leading to inadequate context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Semantic Line Detection</head><p>The meaningful straight line which helps photographic composition was firstly discussed in <ref type="bibr" target="#b10">[11]</ref>, and named as "semantic line". <ref type="bibr" target="#b10">[11]</ref> regarded semantic line detection as a special case of object detection. It first extracts CNN representations of line proposals using LoI pooling, which bilinearly interpolates the features along the entire straight line. Then the line representations are verified by a classifier and a regressor, similar to Faster-RCNN <ref type="bibr" target="#b36">[37]</ref>. The line proposals are all unique lines in an image. The metric of the intersection of union (IoU) of two straight lines is proposed in <ref type="bibr" target="#b10">[11]</ref> to evaluate the similarity of two straight lines in an image. This metric may produce ambiguous definitions in some scenarios, as will be mentioned in Sec. 4. Besides, Lee et al. <ref type="bibr" target="#b10">[11]</ref> collected a semantic line detection dataset which contains about 1,700 outdoor images, and most of them are natural landscape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>In this section, we give the details of the proposed deep Hough transform for semantic line detection. Our proposed method mainly contains four components: 1) a CNN encoder that extracts pixel-wise deep representations; 2) the deep Hough transform (DHT) that converts the deep representations from the spatial domain to the parametric domain; 3) a line detector that is responsible for detecting lines in the parametric space, and 4) a reverse Hough transform (RHT) component that converts the detected lines back to image space. All these components are integrated in an end-to-end framework that performs forward inference and backward training within a single step. The pipeline is illustrated in <ref type="figure">Fig. 2</ref>, and the detailed architecture is shown in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Line Parameterization and Reverse</head><p>As shown in <ref type="figure">Fig. 3</ref>, given a 2D image I H?W ? R H?W , we set the origin to the center of the image. In the 2D plane, a straight line l can be parameterized by two parameters: an orientation parameter ? l ? [0, ?) representing the angle between l and the x-axis and a distance parameter r l , indicating the distance between l and the origin. Obviously</p><formula xml:id="formula_1">? l ? I, r l ? [? ? W 2 + H 2 /2, ? W 2 + H 2 /2].</formula><p>Given any line l on I, we can parameterize it with the above formulations, and also we can perform a reverse mapping to translate any valid (r, ?) pair to a line instance. We define the line-to-parameters and the inverse mapping as:</p><formula xml:id="formula_2">r l , ? l ? P (l), l ? P ?1 (r l , ? l ).<label>(1)</label></formula><p>Obviously, both P and P ?1 are bijective mappings. In practice, r and ? are quantized to discrete bins to be processed by computer programs. Suppose the quantization interval for r and ? are ?r and ??, respectively. Then the quantization can be formulated as below:</p><formula xml:id="formula_3">r l = r l ?r ,? l = ? l ?? ,<label>(2)</label></formula><p>wherer l and? l are the quantized line parameters. The number of quantization levels, denoted with ? and R, are:</p><formula xml:id="formula_4">? = ? ?? , R = ? W 2 + H 2 ?r ,<label>(3)</label></formula><p>as shown in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Transformation with Deep Hough Transform</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Deep Hough transform.</head><p>Given an input image I, we first extract deep CNN features X ? R C?H?W with the encoder network, where C indicates the number of channels and H and W are the spatial size. Afterward, the deep Hough transform (DHT) takes X as input and produces the transformed features, Y ? R C???R . The size of transformed features, ?, R, is determined by the quantization intervals, as described in Eq. <ref type="formula" target="#formula_4">(3)</ref>. As shown in <ref type="figure" target="#fig_2">Fig. 4(a)</ref>, given an arbitrary line l on the image, we aggregate features of all pixels along l, to (? l ,r l ) in the parametric space Y :</p><formula xml:id="formula_5">Y(? l ,r l ) = i?l X(i),<label>(4)</label></formula><p>where i is the positional index.? l andr l are determined by the parameters of line l, according to Eq. (1), and then quantized into discrete grids, according to Eq. (2). Given the number of quantization levels ? and R, we have ? ? R unique line candidates. Then the DHT is applied to all these candidate lines and their respective features are aggregated to the corresponding position in Y. It is worth noting that DHT is order-agnostic in both the feature space and the parametric space, making it highly parallelizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multi-scale DHT with FPN.</head><p>Our proposed DHT could be easily applied to arbitrary spatial features. We use the feature pyramid network (FPN) <ref type="bibr" target="#b62">[63]</ref> as our encoder. FPN can help to extract multi-scale and rich semantic features.</p><p>Specifically, the FPN outputs 4 feature maps X 1 , X 2 , X 3 , X 4 and their respective resolutions are 1/4, 1/8, 1/16, 1/16 of the input resolution. Then each feature map is transformed by a DHT module independently, as shown in <ref type="figure">Fig. 2</ref>. Since these feature maps are in different resolutions, the transformed features Y 1 , Y 2 , Y 3 , Y 4 also have different sizes, because we use the same quantization interval in all stages (see Eq. (3) for details). To fuse transformed features together, we interpolate Y 2 , Y 3 , Y 4 to the size of Y 1 , and then fuse them by concatenation. </p><formula xml:id="formula_6">+ + H W ? R X Y (? l ,r l ) (a) W H ? R DHT RHT feature space parametric space (b)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Line Detection in the Parametric Space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Context-aware line detector.</head><p>After the deep Hough transform (DHT), features are translated to the parametric space where grid location (?, r) corresponds to features along an entire line l = P ?1 (?, r) in the feature space. An important reason to transform the features into the parametric space is that the line structures could be more compactly represented. As shown in <ref type="figure" target="#fig_2">Fig. 4(b)</ref>, lines nearby a specific line l are translated to surrounding points near (? l , r l ). Consequently, features of nearby lines can be efficiently aggregated using convolutional layers in the parametric space.</p><p>In each stage of the FPN, we use two 3 ? 3 convolutional layers to aggregate contextual line features. Then we interpolate features to match the resolution of features from different stages, as illustrated in <ref type="figure">Fig. 2</ref>, and concatenate the interpolated features together. Finally, a 1 ? 1 convolutional layer is applied to the concatenated feature maps to produce pointwise predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Loss function.</head><p>Since the prediction is directly produced in the parametric space, we calculate the loss in the same space as well. For a training image I, the ground-truth lines are first converted into the parametric space with the standard Hough transform. Then to help converging faster, we smooth and expand the ground-truth with a Gaussian kernel. Similar tricks have been used in many other tasks like crowed counting <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref> and road segmentation <ref type="bibr" target="#b65">[66]</ref>. Formally, let G be the binary ground-truth map in the parametric space, G i,j = 1 indicates there is a line located at i, j in the parametric space. The expanded ground-truth map i?</p><formula xml:id="formula_7">G = G K,</formula><p>where K is a 5 ? 5 Gaussian kernel and denotes the convolution operation. An example pair of smoothed ground-truth and the predicted map is shown in <ref type="figure">Fig. 2</ref>.</p><p>In the end, we compute the cross-entropy between the smoothed ground-truth and the predicted map in the parametric space:</p><formula xml:id="formula_8">L = ? i ? i ? log(P i ) + (1 ?? i ) ? log(1 ? P i )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reverse Mapping</head><p>Our detector produces predictions in the parametric space representing the probability of the existence of lines. The predicted map is then binarized with a threshold (e.g., 0.01).</p><p>Then we find each connected area and calculate respective centroids. These centroids are regarded as the parameters of detected lines. At last, all lines are mapped back to the image space with P ?1 (?), as formulated in Eq. (1). We refer to the "mapping back" step as "Reverse Mapping of Hough Transform (RHT)", as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Edge-guided Line Refinement</head><p>Semantic lines are outstanding structures that separate different regions in a scene. Therefore, edges may serve as indicators for semantic lines. We propose to refine the detection results by aligning line positions using edge information. First, we compute an edge map E using HED <ref type="bibr" target="#b40">[41]</ref>. Afterward, given a detected line l, the edge density of l is defined as the average edge response along l:</p><formula xml:id="formula_9">?(l) = i?l E i |l| ,<label>(6)</label></formula><p>where |l| is the number of pixels on l. For the sake of stability, we widen l by 1 pixel on both sides (totally the width is 3) when dealing with Eq. (6). Let L be a set of lines that are close to l. These lines are obtained by moving the end-points of l by ? r pixels clockwise and anti-clockwise. Since there are two end-points and each one has ? r + 1 possible locations, the size of the set is ||L|| = (? r + 1) 2 . Then the refinement can be achieved by finding the optimal line l * from L that holds the highest edge density:</p><formula xml:id="formula_10">l * = arg max l?L ?(l).<label>(7)</label></formula><p>The performance of "edge-guided line refinement" with different ? r is recorded in Sec. 6.6.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">THE PROPOSED EVALUATION METRIC</head><p>In this section, we elaborate on the proposed evaluation metric that measures the agreement, or alternatively, the similarity between the two lines in an image. Firstly, we review several widely used metrics in the computer vision community and then explain why these existing metrics are not proper for our task. Finally, we introduce our newly proposed metric, which measures the agreement between two lines considering both Euclidean distance and angular distance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Review of Existing Metrics</head><p>The intersection over union (IOU) is widely used in object detection, semantic segmentation and many other tasks to measure the agreement between detected bounding boxes (segments) w.r.t the ground-truth. Lee et al. <ref type="bibr" target="#b10">[11]</ref> adopt the original IOU into line detection, and propose the line-based IOU to evaluate the quality of detected lines. Concretely, the similarity between the two lines is measured by the intersection areas of lines divided by the image area. Take <ref type="figure">Fig. 5(a)</ref> as an example, the similarity between line m and n is IOU(m, n) = area(red)/area(I).</p><p>However, we argue that this IOU-based metric is improper and may lead to unreasonable or ambiguous results under specific circumstances. As illustrated in <ref type="figure">Fig. 5(a)</ref>, two pairs of lines (m, n, and p, q) with similar structures could have very different IOU scores. In <ref type="figure">Fig. 5(b)</ref>, even humans cannot determine which areas (red or blue) should be used as intersection areas in line based IOU.</p><p>There are other metrics, e.g., the Earth Mover's Distance (EMD) <ref type="bibr" target="#b66">[67]</ref> and the Chamfer distance (CD) <ref type="bibr" target="#b67">[68]</ref>, that can be used to measure line similarities. However, these metrics require to rasterize the lines into pixels and then calculate pixel-wise distances, which is less efficient.</p><p>To remedy the deficiencies, we propose a simple yet effective metric that measures the similarity of two lines in the parametric space. Our proposed metric is much more efficient than EMD and CD. Quantitative comparisons in Sec. 6.4 demonstrate that our proposed metric presents very similar results to EMD and CD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">The Proposed Metric</head><p>Our proposed metric, termed EA-score, considers both Euclidean distance and Angular distance between a pair of lines. Let l i , l j be a pair of lines to be measured, the angular distance S ? is defined according to the angle between two lines:</p><formula xml:id="formula_11">S ? = 1 ? ?(l i , l j ) ?/2 ,<label>(8)</label></formula><p>where ?(l i , l j ) is the angle between l i and l j . The Euclidean distance is defined as:</p><formula xml:id="formula_12">S d = 1 ? D(l i , l j ),<label>(9)</label></formula><p>where D(l i , l j ) is the Euclidean distance between midpoints of l i and l j . Note that we normalize the image into a unit square before calculating D(l i , l j ). Examples of S d and S ? can be found in <ref type="figure">Fig. 5</ref>(c) and <ref type="figure">Fig. 5(d)</ref>. Finally, our proposed EA-score is:</p><formula xml:id="formula_13">S = (S ? ? S d ) 2 .<label>(10)</label></formula><p>Eq. (10) is squared to make it more sensitive and discriminative when the values are high. Several example line pairs and corresponding EA-scores are demonstrated in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">NKL: A SEMANTIC LINE DETECTION DATASET</head><p>To the best of our knowledge, there is only one dataset, SEL <ref type="bibr" target="#b10">[11]</ref>, specifically for semantic line detection. SEL contains 1,715 images of which 175 images for testing and others for training. To fulfill the gap between large CNN-based models and the scale of the existing dataset, we collect a new dataset for semantic line detection.</p><p>The new dataset, namely NKL (short for NanKai Lines) contains 6,500 images that present richer diversity in terms of both scenes and the number of lines. Each image of NKL is annotated by multiple skilled human annotators to ensure the annotation quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Data Collection and Annotation</head><p>All the images of NKL are crawled from the internet using specific keywords such as sea, grassland et al. After copyright checking, we carefully filter out images with at least one semantic line. Since the annotation of semantic lines is subjective and depends on annotators, each image is first annotated by 3 knowledgeable human annotators and verified by others. A line is regarded as positive only if all of the 3 annotators are consistent. Then the inconsistent lines are reviewed by two other annotators. In a word, for each line, there are at least 3 and at most 5 annotators, and a line is regarded as positive only if the line is marked as positive by more than 3 annotators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Dataset Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Number of images and semantic lines</head><p>There are totally 13,148 semantic lines in NKL and 2,791 semantic lines in SEL <ref type="bibr" target="#b10">[11]</ref> dataset over all images. Tab. 1 summarizes the number of images and lines of the two datasets, respectively. <ref type="figure" target="#fig_5">Fig. 8</ref> summarizes the histogram of the per-image number of lines in NKL and SEL <ref type="bibr" target="#b10">[11]</ref> datasets. More than half (67%, 4,356/6,500) of the images in NKL dataset contain more than 1 semantic line, while the percentage of SEL is only 45.5%. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Diversity Analysis</head><p>To analyze the diversity of SEL and NKL datasets, we feed all the images into a ResNet50 <ref type="bibr" target="#b68">[69]</ref> network that is pretrained on the Place365 <ref type="bibr" target="#b69">[70]</ref>, and then collect the outputs as category labels. The results are presented in <ref type="figure">Fig. 9</ref>.</p><p>There are totally 365 categories in Place365 <ref type="bibr" target="#b69">[70]</ref> dataset, among which we got 167 unique category labels on SEL dataset and 327 on NKL. Besides, as shown in <ref type="figure">Fig. 9</ref>, scene labels on NKL dataset are more fairly distributed compared to SEL dataset. For example, in SEL dataset, top-3 populated categories (sky, field, desert) make up more than a quarter of the total. While in NKL, top-3 makes up less than one-fifth of the total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we introduce the implementation details of our system, and report experimental results compared with existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation Details</head><p>Our system is implemented with the PyTorch <ref type="bibr" target="#b70">[71]</ref> framework, and a Jittor <ref type="bibr" target="#b71">[72]</ref> implementation is also available. Since the proposed deep Hough transform (DHT) is highly parallelizable, we implement DHT with native CUDA programming, and all other parts are implemented based on framework level Python API. We use a single RTX 2080 Ti GPU for all experiments.  <ref type="figure">Fig. 9</ref>. Category distribution of SEL (a) and NKL (b) datasets. Category labels are obtained through a Places365 pretrained model. There are 327 (totally 365) scene labels presented in NKL dataset, in contrast to 167 in SEL dataset. The labels of NKL are also more fairly distributed compared to that of SEL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NKL (b)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Network architectures.</head><p>We use two representative network architectures, ResNet50 <ref type="bibr" target="#b68">[69]</ref> and VGGNet16 <ref type="bibr" target="#b72">[73]</ref>, as our backbone and the FPN <ref type="bibr" target="#b62">[63]</ref> to extract multi-scale deep representations. For the ResNet network, following the common practice in previous works <ref type="bibr" target="#b73">[74]</ref>, <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, the dilated convolution <ref type="bibr" target="#b76">[77]</ref> is used in the last layer to increase the resolution of feature maps. The common used batch normalization <ref type="bibr" target="#b77">[78]</ref> is also adopted in the network. The dilated convolutions and normalization can be tuned by <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b79">[80]</ref> in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Hyper-parameters.</head><p>The size of the Gaussian kernel used in Sec. 3.3.2 is 5 ? 5. All images are resized to (400, 400) and then wrapped into a mini-batch of 8. We train all models for 30 epochs using Adam optimizer <ref type="bibr" target="#b80">[81]</ref> without weight decay. The learning rate and momentum are set to 2 ? 10 ?4 and 0.9, respectively. The quantization intervals ??, ?r will be detailed in Sec. 6.3 and Eq. (12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Datasets and data augmentation.</head><p>Our experiments are conducted on the SEL <ref type="bibr" target="#b10">[11]</ref> dataset and our Proposed NKL dataset. The Statistics of the two datasets are detailed in Sec. 5. Following the setup in <ref type="bibr" target="#b10">[11]</ref>, we use only left-right flip data augmentation in all our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Evaluation Protocol</head><p>We measure the quality of detection lines in terms of precision, recall and F-measure. The first step is to match the detected lines and ground-truth lines.</p><p>Let P and G be the sets of predicted lines and groundtruth lines, respectively. p i and g j are individual predicted and ground-truth line. We first match the lines in P and G based on bipartite matching. Suppose G = {V, E} be a bipartite graph 1 . The vertice set V can be divided into two disjoint and independent sets, in our case, P and G:</p><formula xml:id="formula_14">V = P ? G P ? G = ?.</formula><p>1. https://en.wikipedia.org/wiki/Bipartite graph Each edge in E denotes the similarity between a pair of lines under a certain similarity measure. Apart from the proposed EA-score, we also use two other popular metrics: the earth mover's distance (EMD) <ref type="bibr" target="#b66">[67]</ref> and the Chamfer distance (CD) <ref type="bibr" target="#b67">[68]</ref>, as described in Sec. 4. Note that we normalize both EMD and chamfer distance by their maximal possible value to bound the value within [0, 1] (for both EMD and Chamfer distance, the maximal distance occurs when two lines shrinkage to two points on the opposite diagonals).</p><p>Given the graph G = {V, E}, a matching in a Bipartite Graph is a set of the edges chosen in such a way that no two edges share a common vertice. In our task, given the set of predicted lines P and the set of ground-truth lines G, we seek to find a matching so that each ground-truth line g i corresponds to no more than one detected line p j and vice versa. This problem, maximum matching of a bipartite graph, can be easily solved using the classical Hungarian method <ref type="bibr" target="#b41">[42]</ref> with polynomial time complexity.</p><p>After matching P and G, we can calculate true positive (TP), false positive (FP) and false negative (FN) accordingly. As illustrated in <ref type="figure" target="#fig_6">Fig. 10</ref>, predicted lines (p 1 , p 2 ) that are paired with ground-truth lines (g 2 , g 1 ) are considered as true positive. Predicted line (p 3 ) that is not matched with any ground-truth line is a false positive, and ground-truth line (g 3 ) without a corresponding predicted line is a false negative.</p><p>Finally, the Precision, Recall, and F-measure are:</p><formula xml:id="formula_15">P = T P T P + F P , R = T P T P + F N , F = 2P R P + R .<label>(11)</label></formula><p>We apply a series thresholds ? = 0.01, 0.02, ..., 0.99 to prediction &amp; ground-truth pairs. Accordingly, we derive a series of precision, recall, and F-measure scores. Finally, we evaluate the performance in terms of average precision, recall, and Fmeasure. We use EMD <ref type="bibr" target="#b66">[67]</ref>, CD <ref type="bibr" target="#b67">[68]</ref>, and our proposed EA metric for quantitative comparisons . In the ablation study, we only use EA metric for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Tuning the Quantization Intervals</head><p>The quantization intervals ?? and ?r in Eq. (2) are important factors to the performance and running efficiency. Larger  intervals lead to fewer quantization levels, i.e., ? and R, and the model will be faster. With smaller intervals, there will be more quantization levels, and the computational overhead is heavier.</p><formula xml:id="formula_16">p 1 p 2 p 3 p 4 g 1 g 2 g 3 g 1 g 2 g 3 p 1 p 2 p 3 p 4 g 1 g 2 g 3 p 1 p 2 p 3 p 4 FN TP TP FP FP ? ? ? (a) (b) (c) (d)</formula><p>We perform a coordinate descent on SEL <ref type="bibr" target="#b10">[11]</ref> dataset to find proper intervals that are computationally efficient and functionally effective. Note that we use the EA-score as line similarity measure since its simplicity. In the first round, we fix the angular quantization interval to ?? = ?/100 and then search for ?r, the results are shown in <ref type="figure">Fig. 11(a)</ref>. According to <ref type="figure">Fig. 11(a)</ref>, the performance first rises slowly and then drops down with the decrease of ?r, and the turning point is near ?r = ? 2. In the second round, we fix ?r = ? 2 and train with different ??. Similar to <ref type="figure">Fig. 11(a)</ref>, the results in <ref type="figure">Fig. 11(b)</ref> demonstrate that the performance first increases smoothly with the drop of ??, and then quickly decreases with vibration. Therefore, the turning point ?? = ?/100 is a proper choice for angular quantization.</p><p>In summary, we use ?? = ?/100 and ?r = ? 2 in quantization, and corresponding quantization levels are:</p><formula xml:id="formula_17">? = 100, R = W 2 + H 2 2 ,<label>(12)</label></formula><p>where H, W are the size of feature maps to be transformed in DHT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Quantitative Comparisons</head><p>We compare our proposed method with the SLNet <ref type="bibr" target="#b10">[11]</ref> and the classical Hough line detection <ref type="bibr" target="#b24">[25]</ref> with HED <ref type="bibr" target="#b40">[41]</ref> as the edge detector. Note that we train the HED edge detector on the SEL <ref type="bibr" target="#b10">[11]</ref> training set using the line annotations as edge ground-truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Results on SEL dataset</head><p>Tab. 2 summarizes the results on the SEL dataset <ref type="bibr" target="#b10">[11]</ref>. With either VGG16 or ResNet50 as the backbone, Our proposed method consistently outperforms SLNet and HT+HED with a considerable margin. In addition to Tab. 2, we plot the Fmeasure v.s. threshold and the precision v.s. recall curves. <ref type="figure" target="#fig_8">Fig. 12</ref> reveals that our method achieves higher F-measure than others under a wide range of thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Results on the NKL dataset</head><p>We report the performance of our newly constructed NKL dataset. Since SLNet <ref type="bibr" target="#b10">[11]</ref> did not release the training code, we only compare our method with HED+HT. As shown in Tab. 2, our proposed method outperforms the baseline method (HED edge detector + Hough transform) with a clear margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">Runtime efficiency.</head><p>In this section, we benchmark the runtime of different methods including SLNet <ref type="bibr" target="#b10">[11]</ref> with various iteration steps, classical Hough transform and our proposed method. Both SLNet <ref type="bibr" target="#b10">[11]</ref> and HT require HED <ref type="bibr" target="#b40">[41]</ref> edge detector as a preprocessing step. The non-maximal suppression (NMS) in SLNet requires edge maps as guidance, and the classical  Hough transform takes an edge map as input. Moreover, SLNet uses a refining network to enhance the results iteratively. Therefore, the inference speed is related to the iteration steps. In contrast, our method produces output results with a single forward pass, and the NMS is as simple as computing the centroids of each connected area in the parametric space. Results in Tab. 3 illustrate that our method is significantly faster than all other competitors with a very considerable margin. Even with only 1 iteration step, SLNet is still slower than our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Qualitative Comparisons</head><p>Here we give several example results of our proposed method along with SNLet and HED+HT. As shown in <ref type="figure">Fig. 13</ref>, compared with other methods, our results are more compatible with the ground-truth as well as human cognition. In addition to the results in <ref type="figure">Fig. 13</ref>, we provide all the detection results of our method and SLNet in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 3</head><p>Quantitative speed comparisons. Our method (without ER) is much faster than the other two competitors in network forward. Furthermore, our method doesn't require any extra-process e.g., edge detection. As a result, our method can run at 49 FPS, which is remarkably higher than the other two methods.  <ref type="bibr" target="#b10">[11]</ref> and classical Hough transform <ref type="bibr" target="#b24">[25]</ref>, our results are more consistent with the ground-truth. At last, we combine all the components to form our final full method, which achieves the best performance among all other combinations. Experimental results in this section clearly demonstrate that each component of our proposed method contributes to the success of our method. . First, we test the performance of DHT+ER using different ? r . The ? r parameter controls the size of the searching space in ER (L in Eq. <ref type="formula" target="#formula_10">(7)</ref>). This experiment is conducted on the SEL dataset using the ResNet50 backbone. Results in Tab. 5 tells that the performance first increases and then gets saturated with the growth of ? r . Since the peak performance occurs when ? r = 5, we set ? r = 5 for better performance. After setting ? r to 5, we compare the performance of our method with and without ER, using different backbones and datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we proposed a simple yet effective method for semantic line detection in natural scenes. By incorporating the strong learning ability of CNNs into classical Hough transform, our method is able to capture complex textures and rich contextual semantics of lines. To better assess the similarity between a pair of lines, we designed a new evaluation metric considering both Euclidean distance and angular distance between lines. Besides, a new dataset for semantic line detection was constructed to fulfill the gap between the scale of existing datasets and the complexity of modern CNN models. Both quantitative and qualitative results revealed that our method significantly outperforms previous arts in terms of both detection quality and speed.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(c): Our detection results are clean and comprise only a few meaningful lines that are potentially helpful in the photographic composition. (d): Line detection results by the classical line detection algorithms often focus on fine detailed straight edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>The pipeline of our proposed method. DHT is short for the proposed Deep Hough Transform, and RHT represents the Reverse Hough Transform. CTX means the context-aware line detector which contains multiple convolutional layers. A line can be parameterized by bias r l and slope ? l .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>(a): Features along a line in the feature space (blue, left) are accumulated to a point (r l ,? l ) in the parametric space (red, right). (b): Illustration of the proposed context-aware feature aggregation. Features of nearby lines in the feature space (left) are translated into neighbor points in the parametric space (right). In the parametric space, a simple 3 ? 3 convolutional operation can easily capture contextual information for the central line (orange). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 . 9 Fig. 6 .</head><label>596</label><figDesc>(a): Two pairs of lines with similar relative position could have very different IOU scores. (b): Even humans cannot determine which area (blue or red) should be considered as the intersection in the IOU-based metric [11]. (c) and (d): Our proposed metric considers both Euclidean distance and angular distance between a pair of lines, resulting in consistent and reasonable scores. Best viewed in color. Example lines with various EA-scores (S in Eq. (10)). The larger the EA-score is, the more similar the lines are.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Example images and annotations (yellow lines) of NKL. Images of NKL present diverse scenes and rich line annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Histogram chart of number of lines. Lines of our dataset are more fairly distributed compared to SEL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Illustration of the bipartite graph matching in evaluation. (a) An example image with 3 ground-truth lines (g 1 , g 2 , g 3 ) and 4 predictions (p 1 , p 2 , p 3 , p 4 ). (b) the corresponding bipartite graph. The edge between a pair of nodes represents the similarity (S in Eq. (10)) between lines. (c) after maximum matching of a bipartite graph, each node in a subgraph is connected with no more than 1 node from the other subgraph. (d) true positive (TP), false positive (FP) and false negative (FN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 Fig. 11 .</head><label>211</label><figDesc>Left: performance under different distance quantization intervals ?r with a fixed angular quantization interval ?? = ?/100. Larger ?r indicates less quantization levels R. Right: performance under different angular quantization intervals ?? with a fixed distance quantization interval ?r = ? 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 12 .</head><label>12</label><figDesc>Left: F-measure under various thresholds. Right: The precision-recall curve. Out method outperforms SLNet<ref type="bibr" target="#b10">[11]</ref> and classical Hough transform<ref type="bibr" target="#b24">[25]</ref> with a considerable margin. Moreover, even with 10 rounds of location refinement, SLNet still presents inferior performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 14 .</head><label>14</label><figDesc>Detection results of our method on the NKL dataset. Our method produces results that are visually compatible with human perception.6.6.1 Components in DHTWe first ablate components of "deep Hough transform". Specifically, they are: (a) the Deep Hough transform (DHT) module detailed in Sec. 3.2; (b) the multi-scale (MS) DHT architecture described in Sec. 3.2.2; (c) the context-aware (CTX) line detector proposed in Sec. 3.3.1. Experimental results are shown in Tab. 4.We first construct a baseline model with plain ResNet50 and DHT module. Then we verify the effectiveness of the multi-scale (MS) strategy and context-aware line detector (CTX), individually. We separately append MS and CTX to the baseline model and then evaluate their performance, respectively. Results in Tab. 4 indicate that both MS and CTX can improve the performance of the baseline model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Number of images and lines in SEL<ref type="bibr" target="#b10">[11]</ref> and NKL.</figDesc><table><row><cell>Dataset</cell><cell>Total #images, #lines</cell><cell>Training #images, #lines</cell><cell>Evaluation #images, #lines</cell></row><row><cell>SEL [11]</cell><cell>1,715, 2,791</cell><cell>1,541, 2,493</cell><cell>175, 298</cell></row><row><cell>NKL (Ours)</cell><cell>6,500, 13,148</cell><cell>5,200, 10,498</cell><cell>1,300, 2,650</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Quantitative comparisons on the SEL<ref type="bibr" target="#b10">[11]</ref> and NKL dataset. On SEL<ref type="bibr" target="#b10">[11]</ref> dataset, our method (without ER) significantly outperforms other competitors in terms of average F-measure. 'CD,' 'EMD,' and 'EA' are different evaluation metrics described in Sec. 4.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>Avg. P</cell><cell>CD Avg. R</cell><cell>Avg. F</cell><cell>Avg. P</cell><cell>EMD Avg. R</cell><cell>Avg. F</cell><cell>Avg. P</cell><cell>EA Avg. R</cell><cell>Avg. F</cell></row><row><cell></cell><cell>HED [41] + HT [25]</cell><cell>0.491</cell><cell>0.578</cell><cell>0.531</cell><cell>0.461</cell><cell>0.543</cell><cell>0.498</cell><cell>0.356</cell><cell>0.420</cell><cell>0.385</cell></row><row><cell></cell><cell>SLNet-iter1 [11]</cell><cell>0.740</cell><cell>0.905</cell><cell>0.812</cell><cell>0.723</cell><cell>0.888</cell><cell>0.797</cell><cell>0.654</cell><cell>0.803</cell><cell>0.721</cell></row><row><cell>SEL [11]</cell><cell>SLNet-iter5 [11] SLNet-iter10 [11]</cell><cell>0.826 0.858</cell><cell>0.841 0.821</cell><cell>0.834 0.839</cell><cell>0.810 0.840</cell><cell>0.824 0.804</cell><cell>0.817 0.822</cell><cell>0.735 0.762</cell><cell>0.747 0.729</cell><cell>0.741 0.745</cell></row><row><cell></cell><cell>Ours (VGG16)</cell><cell>0.841</cell><cell>0.835</cell><cell>0.838</cell><cell>0.830</cell><cell>0.824</cell><cell>0.827</cell><cell>0.756</cell><cell>0.774</cell><cell>0.765</cell></row><row><cell></cell><cell>Ours (ResNet50)</cell><cell>0.886</cell><cell>0.815</cell><cell>0.849</cell><cell>0.878</cell><cell>0.807</cell><cell>0.841</cell><cell>0.819</cell><cell>0.755</cell><cell>0.786</cell></row><row><cell></cell><cell>HED [41] + HT [25]</cell><cell>0.301</cell><cell>0.878</cell><cell>0.448</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.213</cell><cell>0.622</cell><cell>0.318</cell></row><row><cell>NKL</cell><cell>Ours (VGG16)</cell><cell>0.750</cell><cell>0.864</cell><cell>0.803</cell><cell>0.726</cell><cell>0.837</cell><cell>0.778</cell><cell>0.659</cell><cell>0.759</cell><cell>0.706</cell></row><row><cell></cell><cell>Ours (ResNet50)</cell><cell>0.766</cell><cell>0.864</cell><cell>0.812</cell><cell>0.743</cell><cell>0.839</cell><cell>0.789</cell><cell>0.679</cell><cell>0.766</cell><cell>0.719</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Ablation study for each component. MS indicates DHTs with multi-scale features as described in Sec. 3.2.2, and CTX means context-aware aggregation as described in Sec. 3.3.1.</figDesc><table><row><cell>DHT</cell><cell>MS</cell><cell>CTX</cell><cell>F-measure</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.664</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.758</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.771</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.786</cell></row><row><cell cols="2">6.6.2 Edge-guided Refinement</cell><cell></cell><cell></cell></row><row><cell cols="4">Here we ablate the "Edge-guided Refinement" module (ab-</cell></row><row><cell>breviated as ER)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Performance DHT+ER with different ?r. Models are trained/tested on the SEL dataset using the Resnet50 backbone. ?r = 0 represents with vanilla DHT method without ER.</figDesc><table><row><cell>?r</cell><cell>Precision</cell><cell>Recall</cell><cell>F-measure</cell></row><row><cell>0</cell><cell>0.8190</cell><cell>0.7530</cell><cell>0.7861</cell></row><row><cell>1</cell><cell>0.8199</cell><cell>0.7561</cell><cell>0.7866</cell></row><row><cell>3</cell><cell>0.8208</cell><cell>0.7569</cell><cell>0.7874</cell></row><row><cell>5</cell><cell>0.8214</cell><cell>0.7574</cell><cell>0.7880</cell></row><row><cell>7</cell><cell>0.8213</cell><cell>0.7573</cell><cell>0.7878</cell></row><row><cell>9</cell><cell>0.8212</cell><cell>0.7571</cell><cell>0.7877</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 6</head><label>6</label><figDesc>Performance with and without ER (?r = 5) using different backbones and datasets.Results in Tab. 6 clearly demonstrate that edge-guided refinement can effectively improve detection results regardless of backbone architectures and datasets.</figDesc><table><row><cell>Dataset</cell><cell>Arch</cell><cell>Edge</cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>F@0.95</cell></row><row><cell></cell><cell>VGG16</cell><cell></cell><cell>0.756</cell><cell>0.774</cell><cell>0.765</cell><cell>0.380</cell></row><row><cell>SEL [11]</cell><cell>VGG16 Resnet50</cell><cell></cell><cell>0.758 0.819</cell><cell>0.777 0.753</cell><cell>0.770 0.786</cell><cell>0.439 0.420</cell></row><row><cell></cell><cell>Resnet50</cell><cell></cell><cell>0.821</cell><cell>0.757</cell><cell>0.788</cell><cell>0.461</cell></row><row><cell></cell><cell>VGG16</cell><cell></cell><cell>0.659</cell><cell>0.759</cell><cell>0.706</cell><cell>0.434</cell></row><row><cell>NKL</cell><cell>VGG16 Resnet50</cell><cell></cell><cell>0.664 0.679</cell><cell>0.765 0.766</cell><cell>0.711 0.719</cell><cell>0.472 0.459</cell></row><row><cell></cell><cell>Resnet50</cell><cell></cell><cell>0.684</cell><cell>0.771</cell><cell>0.725</cell><cell>0.486</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep hough transform for semantic line detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="750" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extracting straight lines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Riseman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="425" to="455" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimizing the Fmeasure for threshold-free salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019-10" />
			<biblScope unit="page" from="8849" to="8857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="815" to="828" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Highly efficient salient object detection with 100k parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Salient object detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="117" to="150" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Revisiting video saliency prediction in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="220" to="237" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="569" to="582" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">S4net: Single stage salient-instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="204" />
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semantic line detection and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3229" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimizing photo composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Graph. Forum</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="469" to="478" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The photographer&apos;s eye: composition and design for better digital photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Structure-preserving neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Rosin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="909" to="920" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Patchnet: a patch-based image representation for interactive librarydriven image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pac-net: pairwise aesthetic comparison network for image aesthetic assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Image Process</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2491" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Property-specific aesthetic assessment with unsupervised aesthetic property discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="114" to="349" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Photo aesthetics ranking network with attributes and content adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="662" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Composition-preserving deep photo aesthetics assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spinnet: Spinning convolutional network for lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="428" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Photography: the art of composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krages</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Simon and Schuster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inverse image editing: Recovering a semantic editing history from a beforeand-after image pair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sketch2photo: Internet image montage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">What and where: A context-based recommendation system for object insertion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="79" to="93" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Use of the hough transformation to detect lines and curves in pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
	<note>Sri International Menlo Park Ca Artificial Intelligence Center</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Generating the hough transform to detect arbitary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ballard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Method and means for recognizing complex patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Hough</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
	<note>uS Patent 3,069,654</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time line detection through an improved hough transform voting scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="299" to="314" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical line extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Yacoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Jolion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings-Vision, Image and Signal Processing</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">142</biblScope>
			<biblScope unit="page" from="7" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hierarchical approach to line extraction based on the hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Princen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, graphics, and image processing</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="57" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A probabilistic hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kiryati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="316" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Edlines: A real-time line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1633" to="1642" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Art and Design in Photoshop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caplin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Elsevier/Focal</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PPGnet: Learning point-pair graph for line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Richer convolutional features for edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1939" to="1946" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1395" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">?ber die bestimmung von funktionen durch ihre integralwerte l?ngs gewisser mannigfaltigkeiten</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Radon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classic papers in modern diagnostic radiology</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">An isotropic 3x3 image gradient operator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sobel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Presentation at Stanford A.I. Project</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Bsp-net: Generating compact meshes via binary space partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cvxnet: Learnable convex decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yazdani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Finding picture edges through collinearity of feature points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>O&amp;apos;gorman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Clowes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Computers</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="449" to="456" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Real-time detection of planar regions in unorganized point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Limberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2043" to="2053" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The adaptive hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="690" to="698" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Line detection in images through regularized hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Karl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="582" to="591" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hyperpixel flow: Semantic correspondence with multi-layer neural features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Robust segmentation of edge data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Etemadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing and its Applications. IET</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="311" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Line detection algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Yip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="126" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Lsd: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Von Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="732" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Edcircles: A real-time circle detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn. Let</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="725" to="740" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Automatic fastener classification and defect detection in vision-based railway inspection systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T Instrum. Meas</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="877" to="888" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Comparison of semantic segmentation approaches for horizon/sky line detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Campr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>?adik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on neural networks</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4436" to="4443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="962" to="971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Holistically-attracted wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2788" to="2797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Context-aware crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning spatial awareness to improve crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6152" to="6161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Vecroad: Point-based iterative graph exploration for road graphs extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Q</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The earth mover&apos;s distance as a metric for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="121" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Distance transformations in digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borgefors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, graphics, and image processing</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="344" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inform. Process. Syst</title>
		<imprint>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Jittor: a novel deep learning framework with meta-operators and unified graph execution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">222103</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Representative batch normalization with feature calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Global2local: Efficient structure search for video action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
