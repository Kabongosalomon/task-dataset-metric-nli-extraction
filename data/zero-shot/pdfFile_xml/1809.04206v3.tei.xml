<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Pattern Attention for Multivariate Time Series Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-09-18">18 Sep 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun-Yao</forename><surname>Shih</surname></persName>
							<email>shunyaoshih@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<email>hungyilee@ntu.edu.tw</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Shih</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Pattern Attention for Multivariate Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-09-18">18 Sep 2019</date>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Forecasting of multivariate time series data, for instance the prediction of electricity consumption, solar power production, and polyphonic piano pieces, has numerous valuable applications. However, complex and non-linear interdependencies between time steps and series complicate this task. To obtain accurate prediction, it is crucial to model long-term dependency in time series data, which can be achieved by recurrent neural networks (RNNs) with an attention mechanism. The typical attention mechanism reviews the information at each previous time step and selects relevant information to help generate the outputs; however, it fails to capture temporal patterns across multiple time steps. In this paper, we propose using a set of filters to extract time-invariant temporal patterns, similar to transforming time series data into its "frequency domain". Then we propose a novel attention mechanism to select relevant time series, and use its frequency domain information for multivariate forecasting. We apply the proposed model on several real-world tasks and achieve state-of-the-art performance in almost all of cases. Our source code is available at https://github.com/gantheory/TPA-LSTM.</p><p>* indicates equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref> <p>Historical prices of crude oil, gasoline, and lumber. Units are omitted and scales are normalized for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In everyday life, time series data are everywhere. We observe evolving variables generated from sensors over discrete time steps and organize them into time series data. For example, household electricity consumption, road occupancy rate, currency exchange rate, solar power production, and even music notes can all be seen as time series data. In most cases, the collected data are often multivariate time series (MTS) data, such as the electricity consumption of multiple clients, which are tracked by the local power company. There can exist complex dynamic interdependencies between different series that are significant but difficult to capture and analyze.</p><p>Analysts often seek to forecast the future based on historical data. The better the interdependencies among different series are modeled, the more accurate the forecasting can be. For instance, as shown in <ref type="figure">Figure 1</ref> 1 , the price of crude oil heavily influences the price of gasoline, but has a smaller influence on the price of lumber. Thus, given the realization that gasoline is produced from crude oil and lumber is not, we can use the price of crude oil to predict the price of gasoline.</p><p>In machine learning, we want the model to automatically learn such interdependencies from data. Machine learning has been applied to time series analysis for both classification and forecasting [G. <ref type="bibr" target="#b24">Zhang and</ref><ref type="bibr">Hu(1998), Zhang(2003)</ref>, <ref type="bibr" target="#b15">Lai et al.(2018)</ref> <ref type="bibr">Lai, Chang, Yang, and Liu, Qin et al.(2017)</ref> <ref type="bibr">Qin, Song, Cheng, Cheng, Jiang, and Cottrell]</ref>. In classification, the machine learns to assign a label to a time series, for instance evaluating a patient's diagnostic categories by reading values from medical sensors. In forecasting, the machine predicts future time series based on past observed data. For example, precipitation in the next days, weeks, or months can be forecast according to historical measurements. The further ahead we attempt to forecast, the harder it is.</p><p>When it comes to MTS forecasting using deep learning, recurrent neural networks (RNNs) [David E. <ref type="bibr" target="#b5">Rumelhart and Williams(1986)</ref>, J. <ref type="bibr">Werbos(1990)</ref>, <ref type="bibr" target="#b6">Elman(1990)</ref>] are often used. However, one disadvantage in using RNNs in time series analysis is their weakness on managing long-term dependencies, for instance yearly patterns in a daily recorded sequence <ref type="bibr" target="#b14">[Kyunghyun Cho and Bengio(2014)</ref>]. The attention mechanism <ref type="bibr" target="#b16">[Luong et al.(2015)</ref>Luong, Pham, and Manning, <ref type="bibr" target="#b1">Bahdanau et al.(2015)</ref> <ref type="bibr">Bahdanau, Cho, and Bengio]</ref>, originally utilized in encoder-decoder <ref type="bibr" target="#b21">[Sutskever et al.(2014)</ref>Sutskever, Vinyals, and Le] networks, somewhat alleviates this problem, and thus boosts the effectiveness of RNN <ref type="bibr" target="#b15">[Lai et al.(2018)</ref> <ref type="bibr">Lai, Chang, Yang, and Liu]</ref>.</p><p>In this paper, we propose the temporal pattern attention, a new attention mechanism for MTS forecasting, where we use the term "temporal pattern" to refer to any time-invariant pattern across multiple time steps. The typical attention mechanism identifies the time steps relevant to the prediction, and extracts the information from these time steps, which poses obvious limitations for MTS prediction. Consider the example in <ref type="figure">Figure 1</ref>. To predict the value of gasoline, the machine must learn to focus on "crude oil" and ignore "lumber". In our temporal pattern attention, instead of selecting the relevant time steps as in the typical attention mechanism, the machine learns to select the relevant time series.</p><p>In addition, time series data often entails noticeable periodic temporal patterns, which are critical for prediction. However, the periodic patterns spanning multiple time steps are difficult for the typical attention mechanism to identify, as it usually focuses only on a few time steps. In temporal pattern attention, we introduce a convolutional neural network (CNN) <ref type="bibr" target="#b16">[LeCun and Bengio(1995)</ref>, A. <ref type="bibr" target="#b0">Krizhevsky and Hinton(2012)</ref>] to extract temporal pattern information from each individual variable.</p><p>The main contributions of this paper are summarized as follows:</p><p>-We introduce a new attention concept in which we select the relevant variables as opposed to the relevant time steps. The method is simple and general to apply on RNN. -We use toy examples to verify that our attention mechanism enables the model to extract temporal patterns and focus on different time steps for different time series. -Attested by experimental results on real-world data ranging from periodic and partially linear to non-periodic and non-linear tasks, we show that the proposed attention mechanism achieves state-of-the-art results across multiple datasets. -The learned CNN filters in our attention mechanism demonstrate interesting and interpretable behavior.</p><p>The remainder of this paper is organized as follows. In Section 2 we review related work and in Section 3 we describe background knowledge. Then, in Section 4 we describe the proposed attention mechanism. Next, we present and analyze our attention mechanism on toy examples in Section 5, and on MTS and polyphonic music dataset in Section 6. Finally, we conclude in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The most well-known model for linear univariate time series forecasting is the autoregressive integrated moving average (ARIMA) <ref type="bibr" target="#b8">[G. E. Box and Ljung(2015)</ref>], which encompasses other autoregressive time series models, including autoregression (AR), moving average (MA), and autoregressive moving average (ARMA). Additionally, linear support vector regression (SVR) <ref type="bibr" target="#b1">[Cao and Tay(2003)</ref>, <ref type="bibr" target="#b13">Kim(2003)</ref>] treats the forecasting problem as a typical regression problem with time-varying parameters. However, these models are mostly limited to linear univariate time series and do not scale well to MTS. To forecast MTS data, vector autoregression (VAR), a generalization of AR-based models, was proposed. VAR is probably the most well-known model in MTS forecasting. Nevertheless, neither AR-based nor VAR-based models capture non-linearity. For that reason, substantial effort has been put into non-linear models for time series forecasting based on kernel methods <ref type="bibr" target="#b2">[Chen et al.(2008)</ref>Chen, Wang, and Harris], ensembles <ref type="bibr">[Bouchachia and Bouchachia(2008)</ref>], Gaussian processes <ref type="bibr" target="#b7">[Frigola and Rasmussen(2014)</ref>] or regime switching <ref type="bibr" target="#b22">[Tong and Lim(2009)</ref>]. Still, these approaches apply predetermined non-linearities and may fail to recognize different forms of non-linearity for different MTS.</p><p>Recently, deep neural networks have received a great amount of attention due to their ability to capture non-linear interdependencies. Long shortterm memory (LSTM) <ref type="bibr">[Hochreiter and Schmidhuber(1997)</ref>], a variant of recurrent neural network, has shown promising results in several NLP tasks and has also been employed for MTS forecasting. Work in this area began with using naive RNN [J. <ref type="bibr" target="#b10">Connor and Martin(1991)</ref>], improved with hybrid models that combined ARIMA and multilayer perceptrons [G. <ref type="bibr" target="#b24">Zhang and</ref><ref type="bibr">Hu(1998), Zhang(2003)</ref>, <ref type="bibr" target="#b11">Jain and Kumar(2007)</ref>], and then most recently progressed to dynamic Boltzmann machines with RNN <ref type="bibr" target="#b4">[Dasgupta and Osogami(2017)</ref>]. Although these models can be applied to MTS, they mainly target univariate or bivariate time series.</p><p>To the best of our knowledge, the long-and short-term time-series network (LSTNet) <ref type="bibr" target="#b15">[Lai et al.(2018)</ref>Lai, Chang, Yang, and Liu] is the first model designed specifically for MTS forecasting with up to hundreds of time series. LSTNet uses CNNs to capture short-term patterns, and LSTM or GRU for memorizing relatively long-term patterns. In practice, however, LSTM and GRU cannot memorize very long-term interdependencies due to training instability and the gradient vanishing problem. To address this, LSTNet adds either a recurrent-skip layer or a typical attention mechanism. Also part of the overall model is traditional autoregression, which helps to mitigate the scale insensitivity of neural networks. Nonetheless, LSTNet has three major shortcomings when compared to our proposed attention mechanism: (1) the skip length of the recurrent-skip layer must be manually tuned in order to match the period of the data, whereas our proposed approach learns the periodic patterns by itself; (2) the LSTNet-Skip model is specifically designed for MTS data with periodic patterns, whereas our proposed model, as shown in our experiments, is simple and adaptable to various datasets, even non-periodic <ref type="figure">Fig. 2</ref> Proposed attention mechanism. ht represents the hidden state of the RNN at time step t. There are k 1-D CNN filters with length w, shown as different colors of rectangles. Then, each filter convolves over m features of hidden states and produces a matrix H C with m rows and k columns. Next, the scoring function calculates a weight for each row of H C by comparing with the current hidden state ht. Last but not least, the weights are normalized and the rows of H C is weighted summed by their corresponding weights to generate Vt. Finally, we concatenate Vt, ht and perform matrix multiplication to generate h t , which is used to create the final forecast value. ones; and (3) the attention layer in LSTNet-Attn model selects a relevant hidden state as in typical attention mechanism, whereas our proposed attention mechanism selects relevant time series which is a more suitable mechanism for MTS data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>In this section, we briefly describe two essential modules related to our proposed model: the RNN module, and the typical attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Recurrent Neural Networks</head><p>Given a sequence of information {x 1 , x 2 , . . . , x t }, where x i ? R n , an RNN generally defines a recurrent function, F , and calculates h t ? R m for each time step, t, as</p><formula xml:id="formula_0">h t = F (h t?1 , x t )<label>(1)</label></formula><p>where the implementation of function F depends on what kind of RNN cell is used. Long short-term memory (LSTM) <ref type="bibr">[Hochreiter and Schmidhuber(1997)</ref>] cells are widely used, which have a slightly different recurrent function:</p><formula xml:id="formula_1">h t , c t = F (h t?1 , c t?1 , x t ),<label>(2)</label></formula><p>which is defined by the following equations:</p><formula xml:id="formula_2">i t = sigmoid(W xi x t + W hi h t?1 ) (3) f t = sigmoid(W x f x t + W h f h t?1 ) (4) o t = sigmoid(W xo x t + W ho h t?1 ) (5) c t = f t c t?1 + i t tanh(W xg x t + W hg h t?1 ) (6) h t = o t tanh(c t )<label>(7)</label></formula><p>where</p><formula xml:id="formula_3">i t , f t , o t , and c t ? R m , W xi , W x f , W xo and W xg ? R m?n , W hi , W h f ,</formula><p>W ho and W hg ? R m?m , and denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Typical Attention Mechanism</head><p>In the typical attention mechanism <ref type="bibr" target="#b16">[Luong et al.(2015)</ref>Luong, Pham, and Manning, <ref type="bibr" target="#b1">Bahdanau et al.(2015)Bahdanau, Cho, and Bengio]</ref> in an RNN, given the previous states H = {h 1 , h 2 , . . . , h t?1 }, a context vector v t is extracted from the previous states. v t is a weighted sum of each column h i in H, which represents the information relevant to the current time step. v t is further integrated with the present state h t to yield the prediction. Assume a scoring function f : R m ?R m ? R which computes the relevance between its input vectors. Formally, we have the following formula to calculate the context vector v t :</p><formula xml:id="formula_4">? i = exp(f (h i , h t )) t?1 j=1 exp(f (h j , h t )) (8) v t = t?1 i=1 ? i h i .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Temporal Pattern Attention</head><p>While previous work focuses mainly on changing the network architecture of the attention-based models via different settings to improve performance on various tasks, we believe there is a critical defect in applying typical attention mechanisms on RNN for MTS forecasting. The typical attention mechanism selects information relevant to the current time step, and the context vector v t is the weighted sum of the column vectors of previous RNN hidden states, H = {h 1 , h 2 , . . . , h t?1 }. This design lends itself to tasks in which each time step contains a single piece of information, for example, an NLP task in which each time step corresponds to a single word. If there are multiple variables in each time step, it fails to ignore variables which are noisy in terms of forecasting utility. Moreover, since the typical attention mechanism averages the information across multiple time steps, it fails to detect temporal patterns useful for forecasting.</p><p>The overview of the proposed model is shown in <ref type="figure">Figure 2</ref>. In the proposed approach, given previous RNN hidden states H ? R m?(t?1) , the proposed attention mechanism basically attends to its row vectors. The attention weights on rows select those variables that are helpful for forecasting. Since the context vector v t is now the weighted sum of the row vectors containing the information across multiple time steps, it captures temporal information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Problem Formulation</head><p>In MTS forecasting, given an MTS, X = {x 1 , x 2 , . . . , x t?1 }, where x i ? R n represents the observed value at time i, the task is to predict the value of x t?1+? , where ? is a fixed horizon with respect to different tasks. We denote the corresponding prediction as y t?1+? , and the ground-truth value as? t?1+? = x t?1+? . Moreover, for every task, we use only {x t?w , x t?w+1 , . . . , x t?1 } to predict x t?1+? , where w is the window size. This is a common practice <ref type="bibr" target="#b15">[Lai et al.(2018)Lai, Chang, Yang, and</ref><ref type="bibr">Liu, Qin et al.(2017)</ref>Qin, Song, Cheng, Cheng, Jiang, and Cottrell], because the assumption is that there is no useful information before the window and the input is thus fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Temporal Pattern Detection using CNN</head><p>CNN's success lies in no small part to its ability to capture various important signal patterns; as such we use a CNN to enhance the learning ability of the model by applying CNN filters on the row vectors of H. Specifically, we have k filters C i ? R 1?T , where T is the maximum length we are paying attention to. If unspecified, we assume T = w. Convolutional operations yield H C ? R n?k where H C i,j represents the convolutional value of the i-th row vector and the j-th filter. Formally, this operation is given by</p><formula xml:id="formula_5">H C i,j = w l=1 H i,(t?w?1+l) ? C j,T ?w+l .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Proposed Attention Mechanism</head><p>We calculate v t as a weighted sum of row vectors of H C . Defined below is the scoring function f :</p><formula xml:id="formula_6">R k ? R m ? R to evaluate relevance: f (H C i , h t ) = (H C i ) W a h t ,<label>(11)</label></formula><p>where H C i is the i-th row of H C , and W a ? R k?m . The attention weight ? i is obtained as</p><formula xml:id="formula_7">? i = sigmoid(f (H C i , h t )).<label>(12)</label></formula><p>Note that we use the sigmoid activation function instead of softmax, as we expect more than one variable to be useful for forecasting. Completing the process, the row vectors of H C are weighted by ? i to obtain the context vector</p><formula xml:id="formula_8">v t ? R k , v t = n i=1 ? i H C i .<label>(13)</label></formula><p>Then we integrate v t and h t to yield the final prediction</p><formula xml:id="formula_9">h t = W h h t + W v v t ,<label>(14)</label></formula><formula xml:id="formula_10">y t?1+? = W h h t ,<label>(15)</label></formula><formula xml:id="formula_11">where h t , h t ? R m , W h ? R m?m , W v ? R m?k , and W h ? R n?m and y t?1+? ? R n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis of Proposed Attention on Toy Examples</head><p>In order to elaborate the failure of traditional attention mechanisms and the influence of interdependencies, we study the performance of different attention mechanisms on two types of artificially constructed toy examples.</p><p>In the first type of toy examples, the t-th time step of the i-th time series is defined as sin( 2?it 64 ), that is, each time series is a sine wave with different periods. Notice that any two time series are mutually independent in the first type, so there are no interdependency. The second type of toy examples adds interdependencies to the first type by mixing time series, and thus the t-th time step of the i-th time series is formulated as:</p><formula xml:id="formula_12">sin( 2?it 64 ) + 1 D ? 1 D j=1,j =i sin( 2?jt 64 ),<label>(16)</label></formula><p>where D is the number of time series. Both types of toy examples are visualized in <ref type="figure" target="#fig_0">Fig. 3</ref> for D = 6. All models in the following analyses are trained with window size w = 64, horizon ? = 1, and similar amount of parameters. In this setup, each of our toy examples consists of 64 samples. Each time series in the first sample comprises values of Eq. 16 from t = 0 to 63, and we can shift one time step to get the second sample with values from t = 1 to 64. For the last sample, we use values from t = 63 to 126 as the input series correspondingly. Note that values from t = 64 to 127 are equal to those from t = 0 to 63. We trained the models for 200 epochs on two types of toy examples for D = {1, 6, 11, . . . , 56} and record mean absolute loss in training. There is no validation and testing data because the intent of this section is to demonstrate the greater capability of our attention over typical attention to fit MTS data not the generalizability of our attention. The results are shown in <ref type="figure" target="#fig_1">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Failure of traditional attention mechanisms</head><p>Intuitively, for the first toy example, the model can accurately predict the next value by memorizing the value that appears exactly one period before. However, we know that different time series have different periods, which means to have a good prediction, the model should be able to look back different numbers of time steps for different series. From this point, it is clear that the failure of traditional attention mechanisms comes from extracting only one previous time step while discounting the information in other time steps. On the other hand, our attention mechanism attends on the features extracted from row vectors of RNN hidden states by CNN filters, which enables the model to select relevant information across multiple time steps.</p><p>The aforementioned explanation is verified by the left plot in <ref type="figure" target="#fig_1">Figure 4</ref>, where we observe that the performance of the LSTM with Luong attention is poor when D 1, compared to the others. Notice that all models have similar amount of parameters, which implies that the LSTM without attention has a larger hidden size when compared to the LSTM with Luong attention. Consequently, the LSTM without attention outperforms the LSTM with Luong attention when D 1, because the larger hidden size helps the model to make prediction while the Luong attention is nearly useless. On the contrary, our attention is useful, so the LSTM with our attention is better than the LSTM without attention on average, even though its hidden size is smaller. Also, removing the CNN from our attention, which results in the same model as the "Sigmoid -W/o CNN" cell in <ref type="table">Table 4</ref>, does not affect the performance, which implies that our feature-wise attention is indispensable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Influence of interdependencies</head><p>When there are interdependencies in MTS data, it is desirable to leverage the interdependencies to further improve forecasting accuracy. The right plot in <ref type="figure" target="#fig_1">Figure 4</ref> shows that both the LSTM with Luong attention and the LSTM without attention do not benefit from the added interdependencies, since the loss values remain the same. On the other hand, the loss of the LSTM with the proposed attention is lower when there are interdependencies, which suggests that our attention successfully utilized the interdependencies to facilitate MTS forecasting. Again, removing the CNN from our attention does not affect the performance in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Analysis</head><p>In this section, we first describe the datasets upon which we conducted our experiments. Next, we present our experimental results and a visualization of the prediction against LSTNet. Then, we discuss the ablation study. Finally, we analyze in what sense the CNN filters resemble the bases in DFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>To evaluate the effectiveness and generalization ability of the proposed attention mechanism, we used two dissimilar types of datasets: typical MTS datasets and polyphonic music datasets.</p><p>The typical MTS datasets are published by <ref type="bibr" target="#b15">[Lai et al.(2018)</ref>Lai, Chang, Yang, and Liu]; there are four datasets: These datasets are real-world data that contains both linear and non-linear interdependencies. Moreover, the Solar Energy, Traffic, and Electricity datasets exhibit strong periodic patterns indicating daily or weekly human activities.</p><p>According to the authors of LSTNet, each time series in all datasets have been split into training (60%), validation (20%), and testing set (20%) in chronological order.</p><p>In contrast, the polyphonic music datasets introduced below are much more complicated, in the sense that no apparent linearity or repetitive patterns exist:</p><p>-MuseData [Nicolas <ref type="bibr" target="#b18">Boulanger-Lewandowski and Vincent(2012)</ref>]: a collection of musical pieces from various classical music composers in MIDI format.</p><p>-LPD-5-Cleansed <ref type="bibr" target="#b9">[Hao-Wen Dong and Yang(2018)</ref>, <ref type="bibr" target="#b20">Raffel(2016)</ref>]: 21, 425 multi-track piano-rolls that contain drums, piano, guitar, bass, and strings.</p><p>To train models on these datasets, we consider each played note as 1 and 0 otherwise (i.e., a musical rest), and set one beat as one time step as shown in <ref type="table" target="#tab_0">Table 1</ref>. Given the played notes of 4 bars consisting of 16 beats, the task is to predict whether each pitch at the next time step is played or not. For training, validation, and testing sets, we follow the original MuseData separation, which is divided into 524 training pieces, 135 validation pieces, and 124 testing pieces. LPD-5-Cleansed, however, was not split in previous work <ref type="bibr" target="#b9">[Hao-Wen Dong and Yang(2018)</ref>, <ref type="bibr" target="#b20">Raffel(2016)</ref>]; thus we randomly split it into training (80%), validation (10%), and testing (10%) sets. The size of LPD-5-Cleansed dataset is much larger than others, so we decided to use a smaller validation and testing set. The main difference between typical MTS datasets and polyphonic music datasets is that scalars in typical MTS datasets are continuous but scalars in polyphonic music datasets are discrete (either 0 or 1). The statistics of both the typical MTS datasets and polyphonic music datasets are summarized in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Methods for Comparison</head><p>We compared the proposed model with the following methods on the typical MTS datasets:</p><p>-AR: standard autoregression model. However, as both traditional baseline methods and LSTNet are ill-suited to polyphonic music datasets due to their non-linearity and the lack of periodicity, we use LSTM and LSTM with Luong attention as the baseline models to evaluate the proposed model on polyphonic music datasets:</p><p>-LSTM: RNN cells as introduced in Section 3.</p><p>-LSTM with Luong attention: LSTM with an attention mechanism scoring function of which f <ref type="bibr" target="#b16">Luong et al.(2015)</ref>Luong, Pham, and Manning].</p><formula xml:id="formula_13">(h i , h t ) = (h i ) W h t , where W ? R m?m [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Model Setup and Parameter Settings</head><p>For all experiments, we used LSTM units in our RNN models, and fixed the number of CNN filters at 32. Also, inspired by LSTNet, we included an autoregression component in our model when training and testing on typical MTS datasets.</p><p>For typical MTS datasets, we conducted a grid search over tunable parameters as done with LSTNet. Specifically, on Solar Energy, Traffic, and Electricity, the range for window size w was {24, 48, 96, 120, 144, 168}, the range for the number of hidden units m was {25, 45, 70}, and the range for the step of the exponential learning rate decay with a rate of 0.995 was {200, 300, 500, 1000}. On Exchange Rate, these three parameters were {30, 60}, {6, 12}, and {120, 200}, respectively. Two types of data normalization were also viewed as part of the grid search: one normalized each time series by the maximum value in itself, and the other normalized every time series by the maximum value over the whole dataset. Lastly, we used the absolute loss function and Adam with a 10 ?3 learning rate on Solar Energy, Traffic, and Electricity, and a 3 ? 10 ?3 learning rate on Exchange Rate. For AR, LRidge, LSVR and GP, we followed the parameter settings as reported in the LSTNet paper <ref type="bibr" target="#b15">[Lai et al.(2018)Lai, Chang, Yang, and Liu]</ref>. For SETAR, we searched the embedding dimension over <ref type="bibr">{24,48,96,120,144,168}</ref> for Solar Energy, Traffic, and Electricity, and fixed the embedding dimension to 30 for Exchange Rate. The two different setups between our method and LSTNet are (1)we have two data normalization methods to choose from, whereas LSTNet only used the first type of data normalization; and (2) the grid search over the window size w is different.</p><p>For models used for the polyphonic music datasets, including the baselines and proposed models in the following subsections, we used 3 layers for all RNNs, as done in <ref type="bibr" target="#b3">[Chuan and Herremans(2018)</ref>], and fixed the trainable parameters to around 5 ? 10 6 by adjusting the number of LSTM units to fairly compare different models. In addition, we used the Adam optimizer with a 10 ?5 learning rate and a cross entropy loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluation Metrics</head><p>On typical MTS datasets, since we compared the proposed model with LST-Net, we followed the same evaluation metrics: RAE, RSE and CORR. The first metric is the relative absolute error (RAE), which is defined as  <ref type="table">Table 2</ref> Results on typical MTS datasets using RAE, RSE and CORR as metrics. Best performance in boldface; second best performance is underlined. We report the mean and standard deviation of our model in ten runs. All numbers besides the results of our model is referenced from the paper of LSTNet <ref type="bibr" target="#b15">[Lai et al.(2018)</ref>Lai, Chang, Yang, and Liu].  The next metric is the root relative squared error (RSE):</p><formula xml:id="formula_14">RAE = t1 t=t0 n i=1 |(y t,i ?? t,i )| t1 t=t0 n i=1 |? t,i ?? t0:t1,1:n | .<label>(17</label></formula><formula xml:id="formula_15">RSE = t1 t=t0 n i=1 (y t,i ?? t,i ) 2 t1 t=t0 n i=1 (? t,i ?? t0:t1,1:n ) 2 ,<label>(18)</label></formula><p>and finally the third metric is the empirical correlation coefficient (CORR):</p><formula xml:id="formula_16">CORR = 1 n n i=1 t1 t=t0 (y t,i ? y t0:t1,i )(? t,i ?? t0:t1,i ) t1 t=t0 (y t,i ? y t0:t1,i ) 2 t1 t=t0 (? t,i ?? t0:t1,i ) 2 ,<label>(19)</label></formula><p>where y,? is defined in Section 4.1,? t , ?t ? [t 0 , t 1 ] is the ground-truth value of the testing data, and y denotes the mean of set y. RAE and RSE both disregards data scale and is a normalized version of the mean absolute error (MAE) and the root mean square error (RMSE), respectively. For RAE and RSE, the lower the better, whereas for CORR, the higher the better. To decide which model is better on polyphonic music datasets, we use validation loss (negative log-likelihood), precision, recall, and F1 score as measurements which are widely used in work on polyphonic music generation [Nicolas <ref type="bibr" target="#b18">Boulanger-Lewandowski and Vincent(2012)</ref>, <ref type="bibr" target="#b3">Chuan and Herremans(2018)</ref>].  <ref type="table">Table 3</ref> Precision, recall, and F1 score of different models on polyphonic music datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results on Typical MTS Datasets</head><p>On typical MTS datasets, we chose the best model on the validation set using RAE/RSE/CORR as the metric for the testing set. The numerical results are tabulated in <ref type="table">Table 2</ref>, where the metric of the first two tables are RAE, followed by two tables of RSE metric, and ended by another two tables using CORR metric. Both tables show that the proposed model outperforms almost all other methods on all datasets, horizons, and metrics. Also, our models are able to deal with a wide range of dataset size, from the smallest 534 KB Exchange Rate dataset to the largest 172 MB Solar Energy dataset. In these results, the proposed model consistently demonstrates its superiority for MTS forecasting.</p><p>In the comparison to LSTNet-Skip and LSTNet-Attn, the previous stateof-the-art methods, the proposed model exhibits superior performance, especially on Traffic and Electricity, which contain the largest amount of time series. Moreover, on Exchange Rate, where no repetitive pattern exists, the proposed model is still the best overall; the performance of LSTNet-Skip and LSTNet-Attn fall behind traditional methods, including AR, LRidge, LSVR, GP, and SETAR. In <ref type="figure" target="#fig_4">Figure 5</ref> we also visualize and compare the prediction of the proposed model and LSTNet-Skip.</p><p>In summary, the proposed model achieves state-of-the-art performance on both periodic and non-periodic MTS datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Results on Polyphonic Music Datasets</head><p>In this subsection, to further verify the efficacy and generalization ability of the proposed model to discrete data, we describe experiments conducted on polyphonic music datasets; the results are shown in <ref type="figure" target="#fig_5">Figure 6</ref> and <ref type="table">Table 3</ref>. We compared three RNN models: LSTM, LSTM with Luong attention, and LSTM with the proposed attention mechanism. <ref type="figure" target="#fig_5">Figure 6</ref> shows the validation loss across training epochs, and in <ref type="table">Table 3</ref>, we use the models with the lowest validation loss to calculate precision, recall, and F1 score on the testing set.</p><p>From the results, we first verify our claim that the typical attention mechanism does not work on such tasks, as under similar hyperparameters and  trainable weights, LSTM and the proposed model outperform such attention mechanisms. In addition, the proposed model also learns more effectively compared to LSTM throughout the learning process and yields better performance in terms of precision, recall, and F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.7">Analysis of CNN Filters</head><p>DFT is a variant of the Fourier transform (FT) which handles equally-spaced samples of a signal in time. In the field of time series analysis, there is a wide body of work that utilizes FT or DFT to reveal important characteristics in time series [N.E. <ref type="bibr" target="#b17">Huang and</ref><ref type="bibr">Liu(1998), Bloomfield(1976)</ref>]. In our case, since the MTS data is also equally-spaced and discrete, we could apply DFT to analyze it. However, in MTS data, there is more than one time series, so we naturally average the magnitude of the frequency components of every time series, and arrive at a single frequency domain representation. We denote this the average discrete Fourier transform (avg-DFT  <ref type="table">Table 4</ref> Ablation Study. Evaluation metric for Solar Energy, Traffic, and Electricity is RSE, and negative log-likelihood for MuseData. We report the mean and standard deviation in ten runs. On each corpus, bold text represents the best and underlined text represents second best. <ref type="figure" target="#fig_4">Figure 5</ref>, which is verified by the avg-DFT of the Traffic dataset shown in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>Since we expect our CNN filters to learn temporal MTS patterns, the prevailing frequency components in the average CNN filters should be similar to that of the training MTS data. Hence, we also apply avg-DFT on the k = 32 CNN filters that are trained on Traffic with a 3-hour horizon; in <ref type="figure" target="#fig_6">Figure 7</ref> we plot the result alongside with the avg-DFT of every window of Traffic dataset. Impressively, the two curves reach peaks at the same periods most of the time, which implies that the learned CNN filters resemble bases in DFT. At the 24, 12, 8, and 6-hour periods, not only is the magnitude of the Traffic dataset at its peak, but the magnitudaaie of CNN filters also tops out. Moreover, in <ref type="figure" target="#fig_7">Figure 8</ref>, we show that different CNN filters behave differently. Some specialize at capturing long-term (24-hour) temporal patterns, while others are good at recognizing short-term (8-hour) temporal patterns. As a whole, we suggest that the proposed CNN filters play the role of bases in DFT. As demonstrated in the work by <ref type="bibr" target="#b20">[Rippel et al.(2015)</ref>Rippel, Snoek, and Adams], such a "frequency domain" serves as a powerful representation for CNN to use in training and modeling. Thus, LSTM relies on the frequency-domain information extracted by the proposed attention mechanism to accurately forecast the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Ablation Study</head><p>In order to verify that the above improvement comes from each added component rather than a specific set of hyperparameters, we conducted an ablation study on the Solar Energy, Traffic, Electricity, and MuseData datasets. There were two main settings: one controlling how we attend to hidden states, H, of RNN and the other controlling how we integrate the scoring function f into the proposed model, or even disable the function. First, in the proposed method, we let the model attend to values of various filters on each position (H C i ); we can also consider attending to values of the same filters at various positions ((H C ) i ) or row vectors of H (H i ). These three different approaches correspond to the column headers in <ref type="table">Table 4</ref>: "Position", "Filter", and "Without CNN". Second, whereas in the typical attention mechanism, softmax is usually used on the output value of scoring function f to extract the most relevant information, we use sigmoid as our activation function. Therefore, we compare these two different functions. Another possible structure for forecasting is to concatenate all previous hidden states and let the model automatically learn which values are important. Taking these two groups of settings into consideration, we trained models with all combinations of possible structures on these four datasets.</p><p>The MuseData results show that the model with sigmoid activation and attention on H C i (position) is clearly the best, which suggests that the proposed model is reasonably effective for forecasting. No matter which proposed component is removed from the model, performance drops. For example, using softmax instead of sigmoid raises the negative log-likelihood from 0.04882 to 0.04923; we obtain a even worse model with a negative log-likelihood of 0.4979 if we do not use CNN filters. In addition, we note no significant improvement between the proposed model and that model using softmax on the first three datasets in <ref type="table">Table 4</ref>: Solar Energy, Traffic, and Electricity. This is not surprising, given our motivation for using sigmoid, as explained in Section 4.3. Originally, we expected CNN filters to find basic patterns and expected the sigmoid function to help the model to combine these patterns into one that helps. However, due to the strongly periodic nature of these three datasets, it is possible that using a small number of basic patterns is sufficient for good prediction. Overall, however, the proposed model is more general and yields stable and competitive results across different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we focus on MTS forecasting and propose a novel temporal pattern attention mechanism which removes the limitation of typical attention mechanisms on such tasks. We allow the attention dimension to be featurewise in order for the model learn interdependencies among multiple variables not only within the same time step but also across all previous times and series. Our experiments on both toy examples and real-world datasets strongly support this idea and show that the proposed model achieves state-of-the-art results. In addition, the visualization of filters also verifies our motivation in a more understandable way to human beings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3</head><label>3</label><figDesc>Visualization of the first type of toy examples without interdependencies (left) and the second type of toy examples with interdependencies (right) for D = 6, which means that there are 6 time series in each example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4</head><label>4</label><figDesc>Mean absolute loss and the range of standard deviation in log 10 of the first type of toy examples without interdependencies (left) and the second type of toy examples with interdependencies (right), both in ten runs. The baseline indicates the loss if all predicted values are zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>-</head><label></label><figDesc>Solar Energy 2 : the solar power production data from photovoltaic plants in Alabama State in 2006. -Traffic 3 : two years (2015-2016) of data provided by the California Department of Transportation that describes the road occupancy rate (between 0 and 1) on San Francisco Bay area freeways. -Electricity 4 : a record of the electricity consumption of 321 clients in kWh. -Exchange Rate: the exchange rates of eight foreign countries (Australia, British, Canada, China, Japan, New Zealand, Singapore, and Switzerland) from 1990 to 2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>-</head><label></label><figDesc>LRidge: VAR model with L2-regularization: the most popular model for MTS forecasting. -LSVR: VAR model with SVR objective function [V. Vapnik(1997)]. -GP: Gaussian process model [Frigola-Alcade(2015), S. Roberts and Aigrain(2011)]. -SETAR: Self-exciting threshold autoregression model, a classical univariate non-linear model [Tong and Lim(2009)]. -LSTNet-Skip: LSTNet with recurrent-skip layer. -LSTNet-Attn: LSTNet with attention layer. AR, LRidge, LSVR, GP and SETAR are traditional baseline methods, whereas LSTNet-Skip and LSTNet-Attn are state-of-the-art methods based on deep neural networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Prediction results for proposed model and LSTNet-Skip on Traffic testing set with 3-hour horizon. Proposed model clearly yields better forecasts around the flat line after the peak and in the valley.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Validation loss under different training epochs on MuseData (left), and LPD-5-Cleansed (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>Magnitude comparison of (1) DFT of CNN filters trained on Traffic with a 3-hour horizon, and (2) every window of the Traffic dataset. To make the figure more intuitive, the unit of the horizontal axis is the period.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8</head><label>8</label><figDesc>Two different CNN filters trained on Traffic with a 3-hour horizon, which detect different periods of temporal patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Statistics of all datasets, where L is the length of the time series, D is the number of time series, S is the sampling spacing, and B is size of the dataset in bytes. MuseData and LPD-5-Cleansed both have various-length time series since the length of music pieces varies.</figDesc><table><row><cell>Dataset</cell><cell>L</cell><cell>D</cell><cell>S</cell><cell>B</cell></row><row><cell>Solar Energy</cell><cell>52,560</cell><cell>137</cell><cell>10 minutes</cell><cell>172 M</cell></row><row><cell>Traffic</cell><cell>17,544</cell><cell>862</cell><cell>1 hour</cell><cell>130 M</cell></row><row><cell>Electricity</cell><cell>26,304</cell><cell>321</cell><cell>1 hour</cell><cell>91 M</cell></row><row><cell>Exchange Rate</cell><cell>7,588</cell><cell>8</cell><cell>1 day</cell><cell>534 K</cell></row><row><cell>MuseData</cell><cell>216-102,552</cell><cell>128</cell><cell>1 beat</cell><cell>4.9 M</cell></row><row><cell>LPD-5-Cleansed</cell><cell>1,072-1,917,952</cell><cell>128</cell><cell>1 beat</cell><cell>1.7 G</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). The single frequencydomain representation reveals the prevailing frequency components of the MTS data. For instance, it is reasonable to assume a notable 24-hour oscillation in</figDesc><table><row><cell>Dataset</cell><cell cols="3">Solar Energy (Horizon = 24)</cell><cell cols="3">Traffic (Horizon = 24)</cell></row><row><cell></cell><cell>Position</cell><cell>Filter</cell><cell>W/o CNN</cell><cell>Position</cell><cell>Filter</cell><cell>W/o CNN</cell></row><row><cell>Softmax</cell><cell>0.4397</cell><cell>0.4414</cell><cell>0.4502</cell><cell>0.4696</cell><cell>0.4832</cell><cell>0.4810</cell></row><row><cell></cell><cell>? 0.0089</cell><cell>? 0.0093</cell><cell>? 0.0099</cell><cell>? 0.0062</cell><cell>? 0.0109</cell><cell>? 0.0083</cell></row><row><cell>Sigmoid</cell><cell>0.4389</cell><cell>0.4598</cell><cell>0.4639</cell><cell>0.4765</cell><cell>0.4785</cell><cell>0.4803</cell></row><row><cell></cell><cell>? 0.0084</cell><cell>? 0.0011</cell><cell>? 0.0101</cell><cell>? 0.0068</cell><cell>? 0.0069</cell><cell>? 0.0104</cell></row><row><cell>Concat</cell><cell>0.4431</cell><cell>0.4454</cell><cell>0.4851</cell><cell>0.4812</cell><cell>0.4783</cell><cell>0.4779</cell></row><row><cell></cell><cell>? 0.0100</cell><cell>? 0.0093</cell><cell>? 0.0049</cell><cell>? 0.0082</cell><cell>? 0.0077</cell><cell>? 0.0073</cell></row><row><cell>Dataset</cell><cell cols="3">Electricity (Horizon = 24)</cell><cell></cell><cell>MuseData</cell><cell></cell></row><row><cell></cell><cell>Position</cell><cell>Filter</cell><cell>W/o CNN</cell><cell>Position</cell><cell>Filter</cell><cell>W/o CNN</cell></row><row><cell>Softmax</cell><cell>0.0997</cell><cell>0.1007</cell><cell>0.1010</cell><cell>0.04923</cell><cell>0.04929</cell><cell>0.04951</cell></row><row><cell></cell><cell>? 0.0012</cell><cell>? 0.0013</cell><cell>? 0.0011</cell><cell>? 0.0037</cell><cell>? 0.0031</cell><cell>? 0.0041</cell></row><row><cell>Sigmoid</cell><cell>0.1006</cell><cell>0.1022</cell><cell>0.1013</cell><cell>0.04882</cell><cell>0.04958</cell><cell>0.04979</cell></row><row><cell></cell><cell>? 0.0015</cell><cell>? 0.0009</cell><cell>? 0.0011</cell><cell>? 0.0031</cell><cell>? 0.0028</cell><cell>? 0.0027</cell></row><row><cell>Concat</cell><cell>0.1021</cell><cell>0.1065</cell><cell>0.1012</cell><cell>0.05163</cell><cell>0.05179</cell><cell>0.05112</cell></row><row><cell></cell><cell>? 0.0017</cell><cell>? 0.0029</cell><cell>? 0.0008</cell><cell>? 0.0040</cell><cell>? 0.0036</cell><cell>? 0.0027</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Source: https://www.eia.gov and https://www.investing.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://www.nrel.gov/grid/solar-power-data.html 3 http://pems.dot.ca.gov 4 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Krizhevsky IS, Hinton GE (2012) Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems pp</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support vector machine with adaptive parameters in financial time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bahdanau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Nonlinear Dynamics and Synchronization Cao and Tay</title>
		<editor>Bouchachia A, Bouchachia S</editor>
		<meeting>the 1st International Workshop on Nonlinear Dynamics and Synchronization Cao and Tay<address><addrLine>Bloomfield</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley Bouchachia and Bouchachia</publisher>
			<date type="published" when="1976" />
			<biblScope unit="page" from="1506" to="1518" />
		</imprint>
	</monogr>
	<note>Neural machine translation by jointly learning to align and translate</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Narxbased nonlinear system identification using orthogonal least squares basis hunting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Control Systems</title>
		<imprint>
			<biblScope unit="page" from="78" to="84" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modeling temporal tonal relations in polyphonic music through deep networks with a novel image-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Herremans ; Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Herremans</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16679" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Nonlinear dynamic Boltzmann machines for time-series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osogami</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Osogami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning representations by backpropagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geh</forename><surname>Williams ; David E Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature pp</title>
		<imprint>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Elman JL (1990) Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Cognitive science pp</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="179" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Integrated pre-processing for Bayesian nonlinear system identification with Gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rasmussen</forename><surname>Frigola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Frigola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="552" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Forecasting with artificial neural networks: The state of the art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; G E</forename><surname>Frigola-Alcade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gcr G M</forename><surname>Box</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jenkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gm ; G</forename><surname>Ljung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bep</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Frigola-Alcade R (2015) Bayesian time series learning with Gaussian processes</title>
		<imprint>
			<publisher>John Wiley &amp; Sons G. Zhang and Hu</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="35" to="62" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge G. E. Box and Ljung</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>Time series analysis: forecasting and control</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment Hochreiter and Schmidhuber(1997). Hochreiter S, Schmidhuber J (1997) Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang ; Hao-Wen Dong Lcy Wen-Yi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recurrent networks and NARMA modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems pp</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="301" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hybrid neural network models for hydrologic time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar ;</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="585" to="592" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">JWerbos P (1990) Backpropagation through time: what it does and how to do it</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE pp</title>
		<meeting>the IEEE pp</meeting>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="1550" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Kim KJ (2003) Financial time series forecasting using support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Kyunghyun Cho DB Bart Van Merrienboer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno>arXiv:14091259</idno>
	</analytic>
	<monogr>
		<title level="m">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio ; Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y ; Networks</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
	<note>Convolutional networks for images, speech, and time series</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The empirical mode decomposition and Hilbert spectrum for nonlinear and nonstationary time series analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Ne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slmwhsqznyct Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc Roy Soc London A</title>
		<imprint>
			<biblScope unit="volume">454</biblScope>
			<biblScope unit="page" from="903" to="995" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Nicolas Boulanger-Lewandowski YB, Vincent P (2012) Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A dual-stage attention-based recurrent neural network for time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qin</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3172077.3172254" />
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;17</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2627" to="2633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Raffel C (2016) Learning-based methods for comparing sequences, with applications to audio-to-MIDI alignment and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spectral representations for convolutional neural networks. NIPS pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2449" to="2457" />
		</imprint>
	</monogr>
	<note type="report_type">PhD Thesis Rippel et</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; S</forename><surname>Aigrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mesrng M</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Osborne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S ;</forename><surname>Aigrain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems pp</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
	<note>Gaussian processes for time-series modelling</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">In: Exploration Of A Nonlinear World: An Appreciation of Howell Tong&apos;s Contributions to Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lim ;</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>World Scientific</publisher>
			<biblScope unit="page" from="9" to="56" />
		</imprint>
	</monogr>
	<note>Threshold autoregression, limit cycles and cyclical data</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Support vector method for function approximation, regression estimation, and signal processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik ; V Vapnik Asea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S E</forename><surname>Golowich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems pp</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="281" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time series forecasting using a hybrid ARIMA and neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Zhang ; Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neurocomputing pp</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="159" to="175" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
