<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sahithya</forename><surname>Ravi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Chinchure</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute for AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<email>rjliao@ece.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
							<email>vshwartz@cs.ubc.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">University of British Columbia</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Vector Institute for AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">VLC-BERT: Visual Question Answering with Contextualized Commonsense Knowledge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There has been a growing interest in solving Visual Question Answering (VQA) tasks that require the model to reason beyond the content present in the image. In this work, we focus on questions that require commonsense reasoning. In contrast to previous methods which inject knowledge from static knowledge bases, we investigate the incorporation of contextualized knowledge using Commonsense Transformer (COMET), an existing knowledge model trained on human-curated knowledge bases. We propose a method to generate, select, and encode external commonsense knowledge alongside visual and textual cues in a new pre-trained Vision-Language-Commonsense transformer model, VLC-BERT. Through our evaluation on the knowledge-intensive OK-VQA and A-OKVQA datasets, we show that VLC-BERT is capable of outperforming existing models that utilize static knowledge bases. Furthermore, through a detailed analysis, we explain which questions benefit, and which don't, from contextualized commonsense knowledge from COMET. Code: https://github.com/aditya10/VLC-BERT</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent progress in multimodal vision-language learning has been fueled by large-scale annotated datasets for Visual Question Answering (VQA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49]</ref>, in which models are presented with questions about an image. To answer questions correctly, models are required to perform scene understanding and learn meaningful connections between the two modalities. In recent years, transformer-based vision and language (VL) models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b43">44]</ref>, pre-trained on large-scale multimodal corpora, have reached impressive accuracies on standard VQA datasets.</p><p>VQA often necessitates not only visual comprehension of the scene depicted by the image (e.g., "A plate with meat, potatoes and bread") but also making inferences about plau-* Denotes equal contribution <ref type="figure">Figure 1</ref>: OK-VQA <ref type="bibr" target="#b28">[29]</ref>: Where might one buy this? sible stories behind the image (e.g., "The plate is likely found at a restaurant"). Humans make such inferences based on prior experience and commonsense knowledge (e.g., "This is likely a lunch or dinner at a restaurant, people may be enjoying themselves..."). Most existing methods rely on world knowledge implicitly encoded by language models, which often lacks in both accuracy and coverage <ref type="bibr" target="#b31">[32]</ref>. This is primarily due to the fact that commonsense knowledge is extremely broad, and frequently assumed. Commonsense knowledge learned from text suffers from reporting bias <ref type="bibr" target="#b10">[11]</ref>: over-representation of exceptional facts (e.g., "people die in accidents") in text corpora, at the expense of rarely discussed trivial facts known to everyone (e.g., "people eat").</p><p>Several visual question answering benchmarks were proposed, in which the questions require either factual <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b44">45]</ref> or commonsense knowledge <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">49]</ref> beyond the visual scene comprehension. This prompted the development of neurosymbolic methods combining transformer-based representations with knowledge bases (KBs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref>. However, retrieving relevant facts directly from a KB is challenging due to lack of coverage, and because KB facts are only appropriate in certain contexts.</p><p>In this work, we propose VLC-BERT (Vision-Language-Commonsense BERT), a model designed to incorporate contextualized commonsense knowledge into a Vision-Language transformer built on VL-BERT <ref type="bibr" target="#b40">[41]</ref>. As an alternative to the retrieval paradigm often used in knowledgebased VQA, our model generates contextualized commonsense inferences on the question phrase combined with image object tags using COMET <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, a language model trained on commonsense knowledge graphs. We augment sentence transformers <ref type="bibr" target="#b30">[31]</ref> to rank, filter and embed the commonsense inferences. We incorporate the filtered inferences into VLC-BERT using an attention-driven fusion mechanism that learns to focus on the most important inferences for each question. Commonsense knowledge may not be necessary for answering every question, as some questions are either purely visual, factual, or straight-forward. To eliminate injecting noisy knowledge in such cases, we employ weak supervision to help us discriminate between situations when commonsense knowledge may or may not be valuable.</p><p>Our evaluations on the challenging OK-VQA <ref type="bibr" target="#b28">[29]</ref> and A-OKVQA <ref type="bibr" target="#b35">[36]</ref> datasets confirm that leveraging commonsense is consistently useful for knowledge-intensive visual question answering tasks. We analyze the successful predictions and show how the commonsense inferences help answering difficult questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision-Language Transformer Models</head><p>Pre-trained Vision-Language models based on BERT <ref type="bibr" target="#b7">[8]</ref> have shown impressive performances on downstream multimodal tasks such as Visual Question Answering. ViL-BERT <ref type="bibr" target="#b24">[25]</ref> and LXMERT <ref type="bibr" target="#b41">[42]</ref> use a two-stream architecture to first encode language and vision modalities independently, and then apply a cross-modality encoder to align textual and visual tokens. VL-BERT <ref type="bibr" target="#b40">[41]</ref>, OSCAR <ref type="bibr" target="#b21">[22]</ref> and OSCAR+ <ref type="bibr" target="#b49">[50]</ref> use a single-stream architecture to directly learn inter-modality interactions. Large-scale pretraining is commonly done using the Conceptual Captions <ref type="bibr" target="#b37">[38]</ref> dataset, with objectives that are designed to encourage interaction between modalities, such as predicting masked tokens or image regions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42]</ref>, and using contrastive loss between modalities <ref type="bibr" target="#b21">[22]</ref>. As a result, such models inherently capture some commonsense knowledge through their pre-training regime. While these models perform impressively on downstream tasks such as VQA <ref type="bibr" target="#b0">[1]</ref>, they typically perform worse on questions requiring reasoning about knowledge beyond the image content or involving multiple reasoning hops. In our work, we introduce VLC-BERT, a multimodal transformer model based on VL-BERT that explicitly incorporates external knowledge to alleviate this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge-based Visual Question Answering</head><p>In recent years, several VQA datasets were designed specifically to require reasoning about external knowledge beyond the image, whether using factual and web information (FVQA <ref type="bibr" target="#b44">[45]</ref>, WebQA <ref type="bibr" target="#b4">[5]</ref>, a provided text passage (VLQA <ref type="bibr" target="#b33">[34]</ref>), commonsense-driven reasoning (VCR <ref type="bibr" target="#b48">[49]</ref>), or external commonsense knowledge (OK-VQA <ref type="bibr" target="#b28">[29]</ref>, A-OKVQA <ref type="bibr" target="#b35">[36]</ref>). This motivated a line of work on knowledge-enhanced VL transformer models. External knowledge is typically retrieved from a structured knowledge base like ConceptNet <ref type="bibr" target="#b39">[40]</ref>, in the form of a subgraph, and integrated into the VL transformer as an additional input <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b46">47]</ref>. Alternative sources of knowledge include image captions <ref type="bibr" target="#b32">[33]</ref>, Google Search results <ref type="bibr" target="#b25">[26]</ref>, and textual and visual knowledge from Wikipedia, and Google Images <ref type="bibr" target="#b46">[47]</ref>. In contrast to most of the preceding work, PICa <ref type="bibr" target="#b47">[48]</ref> and Knowledge Augmented Transformer (KAT) <ref type="bibr" target="#b12">[13]</ref> attempt to use GPT-3 <ref type="bibr" target="#b2">[3]</ref> in a few-shot setting on the VQA task, by building prompts containing the caption and object tags generated using the image, followed by the question statement, asking the model to produce an answer. In our proposed model, we focus on a specific subset of the knowledge-intensive datasets that require commonsense knowledge. Our approach, that uses COMET <ref type="bibr" target="#b14">[15]</ref>, for incorporating commonsense knowledge is distinctly different, far simpler and more cost-effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Knowledge incorporation in NLP</head><p>Structured large-scale knowledge bases (KBs) like Con-ceptNet <ref type="bibr" target="#b39">[40]</ref> and ATOMIC <ref type="bibr" target="#b34">[35]</ref> are widely used in NLP tasks to provide additional commonsense knowledge to models. ConceptNet contains 3.4M assertions focusing on concept and entity relations (such as RelatedTo, Synonym, IsA, MadeOf). ATOMIC contains 1.33M triplets focusing on event-centric social commonsense about causes, effects, mental states of the event participants. Several approaches were proposed for incorporating symbolic knowledge from these KBs into downstream NLP tasks such as encoding subgraphs of relevant knowledge <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b22">23]</ref> and pre-training on commonsense knowledge bases or tasks <ref type="bibr" target="#b50">[51]</ref>. Despite the performance improvements, incorporating knowledge directly from KBs suffers from two limitations: lack of coverage and lack of consideration for context. Commonsense Transformer, COMET <ref type="bibr" target="#b14">[15]</ref>, attempts to alleviate these issues by fine-tuning pre-trained language models on KBs. COMET can generate inferences for the various KB relations dynamically for new inputs. It has been successfully used for generating knowledge in language tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. Inspired by the success of these models, we chose to use COMET <ref type="bibr" target="#b14">[15]</ref> to generate relevant contextual expansions rather than directly retrieving knowledge from KBs. To the best of our knowledge, we are the first to incorporate commonsense knowledge using COMET in VQA tasks. Newer COMET variants <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b45">46]</ref> are less applicable to OK-VQA and A-OKVQA as they focus more on event commonsense than entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We briefly outline the overall architecture of our model and then delve deeper into its individual components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YOLOv5</head><p>Object tags dog, chair... </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a) Overall architecture</head><formula xml:id="formula_0">C 1 ...C n C 1 ...C K Sentence construction O dog, chair</formula><p>The purpose of the umbrellas with dog and chair</p><p>You are likely to find umbrella at store Umbrellas is made of umbrella head ... <ref type="figure">Figure 2</ref>: Architecture of VLC-BERT: Given an image, VLC-BERT generates commonsense inferences for the questionobject phrase using COMET. These inferences are relevance ranked, and top ones are selected and fed along with image regions into a VL-Transformer in order to produce an answer. We utilize semantic similarity between Q and C to select the final K inferences that go into VLC-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) Knowledge generation and selection</head><p>ure 2a illustrates the VLC-BERT pipeline. Given an image with corresponding image regions I precomputed using Fast RCNN <ref type="bibr" target="#b9">[10]</ref> and a question Q related to the image, we generate commonsense inferences C on the events and entities in the question phrase and two object tags O, and select the set of commonsense inferences which is the most useful for answering the question,</p><formula xml:id="formula_1">C = {C 1 , C 2 , ..., C k } ( ?3.1).</formula><p>Finally, we embed Q, I and C, as input to VLC-BERT and train it to predict an answer A to Q ( ?3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Structured knowledge generation and selection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Knowledge Generation</head><p>To generate commonsense knowledge, we employ the most recent version of COMET <ref type="bibr" target="#b14">[15]</ref> initialized using BART <ref type="bibr" target="#b18">[19]</ref> in a zero-shot setting. COMET is trained to complete 50 relation types from both ConceptNet <ref type="bibr" target="#b39">[40]</ref> (such as AtLocation, Madeof) and ATOMIC <ref type="bibr" target="#b34">[35]</ref> (such as xNeed, xWants), thus capturing concept as well as event oriented knowledge. We generate inferences based on 30 relation types most relevant to our work and supported by COMET. <ref type="bibr" target="#b0">1</ref> Consider the example shown in <ref type="figure">Figure 2b</ref>. For the given question, "What is the purpose of the umbrella?" we first process each question using AllenNLP's constituency parser <ref type="bibr" target="#b16">[17]</ref> and convert it into a declarative sentence, since COMET was mainly trained on declarative sentences. In the example shown, "What is the purpose of the umbrella?" is rephrased as "The purpose of the umbrellas is". We then adopt a stateof-the-art object detection model, YOLOv5 <ref type="bibr" target="#b15">[16]</ref>, to translate the corresponding image into object tags that COMET can understand. We select the top two most confident object tags and combine it with the question phrase to obtain a question-object(QO) phrase, "The purpose of the umbrella <ref type="bibr" target="#b0">1</ref> We include the full list of relation types in the supplementary material.</p><p>is, with dog and chair". We restrict the number of the object tags used in COMET's input to two because the addition of multiple tags make the inferences more conflated and noisy.</p><p>In this manner, we can obtain inferences that can provide additional knowledge about both the visual and language inputs to VLC-BERT. We use beam search to decode the top 5 inferences for each relation type, ranked according to the model's confidence. Overall, we get 30 ? 5 = 150 inferences for each input phrase. Finally, we convert each inference to a sentence in natural language using relation-specific templates as defined in <ref type="bibr" target="#b6">[7]</ref>. In the shown example, the assertion &lt; umbrella, Located At, store &gt; is expressed as "You are likely to find umbrella at store". In order to remove redundant sentences of the same relation type, we measure the lexical overlap by measuring the percentage of common words between two given sentences. We exclude the sentences which have more than 70% overlap with previously constructed sentences of the same relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Knowledge Selection</head><p>Due to the high cost of computation, and the noise associated with feeding such a large number of text tokens, feeding up to 150 COMET inferences into the VL Transformer model is impractical. In order to rank and select the inferences, we employ semantic search based on sentence transformers (SBERT) <ref type="bibr" target="#b30">[31]</ref>, which are pre-trained on tasks that retrieve candidate answers to a search query. In this method, the question and the inferences are embedded into the same vector space using SBERT <ref type="bibr" target="#b30">[31]</ref> and cosine similarity between the question and the inference embeddings is used to rank the inferences. We prune the set of inference sentences C by picking K = 5 inferences which are expected to be the most useful for answering the question Q.</p><p>Augmented-SBERT. We augment the SBERT used for semantic search by starting with a pre-trained SBERT model and continuing to train it for 2 epochs on questioninference instances from the training set of our datasets. To achieve this, we label the inferences for each question with similarity scores based on the proportion of overlap with the human-annotated answers. Since SBERT is trained on corpora that are distinct from our task, the augmentation ensures that the model understands the nature of queryinference pairings in our tasks. The augmented SBERT especially helps with narrowing down the right relations to the question. For instance, the question in shown in <ref type="figure">Figure 2b</ref> benefits most from the relations that talk about what the umbrella (UsedFor) is used for or capable of (CapableOf.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">VLC-BERT</head><p>We use a single-stream multimodal transformer encoder, VL-BERT <ref type="bibr" target="#b40">[41]</ref>, as the basis of VLC-BERT. VL-BERT is pre-trained on large-scale vision-language and languageonly datasets with a goal of aligning the visual and linguistic features and building robust multimodal representations for downstream tasks. It is trained on the vision-language Conceptual Captions dataset <ref type="bibr" target="#b37">[38]</ref>, to predict regions-of-interests (RoIs) from language cues, and on the language-only Book-Corpus <ref type="bibr" target="#b51">[52]</ref> and English Wikipedia corpora, with a masked language modeling objective. <ref type="figure">Figure 3</ref> shows the VLC-BERT Transformer architecture. In the following paragraphs, we share how the input sequence is constructed and how the predicted answer is selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Inputs</head><p>Like VL-BERT, VLC-BERT accepts word token embeddings for language inputs and RoI token embeddings from the image for vision inputs. The architecture of VLC-BERT Transformer is shown in <ref type="figure">Figure 3</ref>. We use the <ref type="bibr">[CLS]</ref> in the beginning of the sequence, <ref type="bibr">[END]</ref> to mark the end of the sequence, and the separator token <ref type="bibr">[SEP]</ref> between different inputs. We feed the question Q as a sequence of word tokens and the image regions I as sequences of RoIs. A <ref type="bibr">[MASK]</ref> token is used to represent the unknown answer. In addition, we introduce a commonsense fusion token, F , to the input sequence, to incorporate our commonsense inferences.</p><p>A straightforward way to leverage the commonsense inferences C = {C 1 , C 2 , ..., C k } is to embed each word token in every inference sentence as an input token. However, this would lead to a very long input sequence, where the majority of inputs consist of inferences, thus potentially drawing the model's attention away from the other inputs.</p><p>To overcome the challenge, we summarize the information contained in each inference sentence C i into a single token representation ? C i , by embedding the inference using SBERT <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_2">? C i = SBERT(C i )<label>(1)</label></formula><p>Next, in order to obtain a fused representation of the k commonsense inferences, we attend to the corresponding SBERT embeddings, [ ? C i ... ? C k ] against the SBERT embedding of the question, ? Q = SBERT(Q). The intuition behind this approach is that the model learns to assign a higher score to the most important inference to the question. The key (K A ), query (Q A ) and value (V A ) are assigned as shown below,</p><formula xml:id="formula_3">K A = ? Q (2) Q A , V A = append([ ? C i ... ? C k ], ? Q) (3) ? F = MHA(K A , Q A , V A )<label>(4)</label></formula><p>where MHA is the standard multi-head attention <ref type="bibr" target="#b43">[44]</ref>, that delivers a single vector incorporating all relevant commonsense knowledge required to answer the question. Note that we append the question embedding ? Q to list of commonsense inference embeddings for Q and V because there may be cases where none of the inferences are useful to answer the question. In such a case, the model may choose to ignore the inferences by attending to the question embedding ? Q instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weak Supervision.</head><p>In order to train the MHA block effectively, we employ weak supervision on the attention weights. For a small subset of the questions in the training set, we obtain label attention weights by following these steps: (1) we initialize a vector? of length k + 1 where all values are 0.05, (2) for each C i , if C i contains a word in the ground-truth answer list, then we set the? i to 0.8, (3) if none of the C inferences contain answer words, we assign a weight of 0.8 to? k+1 so that the question has the largest weight, and (4) we normalize? so that its values sum up to 1. We then apply cross-entropy loss between the predicted attention weights from MHA and our label attention weights?, and sum this with the answer prediction loss.</p><p>Finally, a positional encoding is added to all input tokens following the method described in VL-BERT. In addition, a different segment type encoding is applied to the four segments in the input sequence: the question segment, the commonsense segment, the masked answer segment, and the image region segment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Answer Selection</head><p>We use the encoded <ref type="bibr">[MASK]</ref> token to represent the answer, thereby making VQA a masked language modelling task  <ref type="figure">Figure 3</ref>: VLC-BERT Transformer is a single-stream Transformer that can attend across language, vision, and commonsense representations. We use the MHA block to fuse commonsense inferences into a useful commonsense representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VLC-BERT Transformer</head><p>with visual cues. To predict the final answer, we apply a classifier over the entire answer vocabulary, as done in VL-BERT. During training, we follow VL-BERT and use a cross-entropy loss over picking the correct answer from an answer vocabulary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Datasets</head><p>We perform experiments on the OK-VQA <ref type="bibr" target="#b28">[29]</ref> and A-OKVQA <ref type="bibr" target="#b35">[36]</ref> datasets. In order to utilize the existing VL-BERT model effectively, we pre-train VLC-BERT on the larger VQA 2.0 <ref type="bibr" target="#b11">[12]</ref>.</p><p>OK-VQA. In the Outside-Knowledge VQA dataset, questions require external knowledge in addition to the information in the images. A-OKVQA. A-OKVQA <ref type="bibr" target="#b35">[36]</ref> is the augmented successor to OK-VQA and consists of 25K questions that require a combination of commonsense, visual, and physical knowledge.</p><p>In contrast to other knowledge-based visual question answering datasets, the questions in A-OKVQA are conceptually diverse, involving knowledge that is not contained in the image, and cannot be resolved by a simple knowledge base query. A-OKVQA is split into training, validation, and test sets based on images used from the COCO 2017 <ref type="bibr" target="#b23">[24]</ref> dataset. Moreover, all questions in the dataset have human annotated direct answers as well as multiple-choice options, but we focus on the direct answers. The A-OKVQA test set is blind, requiring us to submit to the leaderboard to obtain a test accuracy.</p><formula xml:id="formula_4">VQA 2.0.</formula><p>The Visual Question Answering (v2.0) dataset contains 1.1 million crowdsourced questions about 204,721 images from the COCO dataset <ref type="bibr" target="#b23">[24]</ref>. Each question is annotated with 10 ground truth answers obtained using Amazon Mechanical Turk. A majority of the questions in this dataset do not require external commonsense knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluation Metric</head><p>Both datasets use the same accuracy-based evaluation metric. Each question has a set of 10 ground truth answers provided by different annotators. Accuracy is calculated as the percentage of predicted answers that were proposed by at least 3 human annotators: acc = min( # humans gave the answer 3 , 1). <ref type="bibr" target="#b1">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>The implementation of our model builds on VL-BERT <ref type="bibr" target="#b40">[41]</ref>. To that end, we follow the fine-tuning steps provided in the official codebase of the VL-BERT model for VQA 2.0, and modify it to support the OK-VQA and A-OKVQA datasets. We maintain the recommended hyperparameter values, and train the BERT BASE size of the model, with a <ref type="table">Table 1</ref>: Accuracy of our model against other models for OK-VQA and A-OKVQA datasets. Our model improves upon existing knowledge base based models due to the contextualized commonsense inferences from COMET, which is trained on ConceptNet and ATOMIC. We compare favourably against the highlighted models that utilize external knowledge bases. Note: P.T. stands for Pre-Training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Knowledge hidden feature dimension of 768. The model is trained for 20 epochs on the OK-VQA and A-OKVQA datasets. For all models, we use a batch size of 16 and gradient accumulation step size of 4. We train the models presented in the main result thrice and report the average test accuracy on the OK-VQA dataset, and the best (leaderboard) test accuracy on the A-OKVQA dataset. Answer Vocabulary. Due to the large number of unique answers to questions in visual question answering datasets, it is infeasible to use all answers in the answer vocabulary. For the OK-VQA dataset, following KRISP <ref type="bibr" target="#b27">[28]</ref>, we build an answer vocabulary of 2,249 answers by selecting all answers in the training set that appear at least 10 times. This answer vocabulary ignores the empty space answer, and includes an &lt;UNK&gt; answer token. During training, if a ground truth answer is not present in the answer vocabulary, we assign it to the (&lt;UNK&gt; ) token. For the A-OKVQA dataset, we use the answer dictionary that is already provided in the dataset <ref type="bibr" target="#b35">[36]</ref>. VQA Pre-Training (VQA P.T) Following the idea that pre-training is beneficial for Transformer models, we initialize VLC-BERT with weights obtained after fine-tuning VL-BERT on the VQA 2.0 dataset for 5 epochs. Note that KRISP <ref type="bibr" target="#b27">[28]</ref> benefits from pre-training on the VQA 2.0 dataset, and PICa <ref type="bibr" target="#b47">[48]</ref> and KAT <ref type="bibr" target="#b13">[14]</ref> utilize GPT-3, a large-scale pre-trained model, for external commonsense. Furthermore, because OK-VQA and A-OKVQA are significantly smaller than VQA 2.0, this initialization favourably benefits the training process and gives us a stronger baseline to work with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>In this section, we focus on evaluating VLC-BERT on the OK-VQA and A-OKVQA datasets and comparing against existing state-of-the-art models for VQA with external commonsense knowledge. <ref type="table">Table 1</ref> highlights our performance improvements on the test set for OK-VQA and A-OKVQA against other models. Later in this section, we ablate on the components of our model. <ref type="table">Table 1</ref> specifies which knowledge sources each model leverages. In the top section, we consider models that utilize knowledge bases such as ConceptNet and Wikipedia, as well as models that utilize web search APIs to obtain external knowledge. VLC-BERT incorporates COMET, which is trained on ConceptNet and ATOMIC, and we compare favourably against these models. Notably, VLC-BERT achieves an accuracy of 43.14 on OK-VQA, outperforming KRISP (Wikipedia + ConceptNet + VQA P.T.) by over 4 points, and MAVEx (Wikipedia + ConceptNet + Google Images) by about 2 points. While our model clearly outperforms previous methods that use knowledge bases, it does not outperform models with large-scale pretraining and large number of parameters such as GPT-3 <ref type="bibr" target="#b2">[3]</ref> and GPV2 <ref type="bibr" target="#b17">[18]</ref>, which incorporate implicit commmonsense knowledge and require extensive resources to train. However, on OK-VQA, we achieve very similar results to PICa-Base <ref type="bibr" target="#b47">[48]</ref>, despite not having access to GPT-3. We expect that the use of a large pre-trained model like GPT-3 can further boost the performance of VLC-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Ablation Tests</head><p>We perform comprehensive ablations on the validation set of the A-OKVQA dataset, as represented in <ref type="table" target="#tab_4">Table 2</ref>. <ref type="bibr" target="#b2">3</ref> VQA P.T. We begin by training A-OKVQA on the baseline VL-BERT model without VQA pre-training. This gives us a score of 36.24. Next, obtain a new baseline for our model with VQA pre-training, where we then initialize VLC-BERT with pre-trained weights on the VQA 2.0 dataset, and further train it on the A-OKVQA dataset. This results in a score of 43.46, over 7 points better, highlighting the impact of pre-training with a large-scale dataset. This model is a strong baseline for our VQA tasks.</p><p>Comm. Inference Representation. In the full model, we use SBERT to summarize each commonsense inference into a single vector, and use the multi-head attention block to capture useful information from the list of inference vectors. To test the effectiveness of our commonsense inference representation method, we first ablate SBERT, i.e., we incorporate all inferences as an additional text input for VLC-BERT, feeding them token-by-token. This results in an accuracy score of 43.44, which is slightly lower than our baseline with VQA pre-training. Next, we use SBERT to summarize inferences, and feed the SBERT embeddings directly into VLC-BERT with only a linear projection layer rather than the MHA block. This variant performs worse than the model with the MHA block by 1.25 points. <ref type="bibr" target="#b2">3</ref> We present additional ablations in supplementary material Sec 2.3 Augmented SBERT. In order to familiarize SBERT with our question-inference pairs, we fine-tune SBERT on the training set of A-OKVQA and OK-VQA (Sec 3.1.2). We perform an ablation by evaluating our model on SBERT that has never been exposed to the question-inference-pairs. This results in a drop of 0.85 points in accuracy, which shows that our augmentation of SBERT is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Commonsense subsets</head><p>Questions in OK-VQA and A-OKVQA datasets are diverse and require commonsense reasoning, visual understanding, as well as factual knowledge. While COMET can generate contextualized commonsense knowledge, it does not help with questions that require scene understanding (e.g., "What is to the left of the computer?"), factual knowledge (e.g., "Where was this food invented?"), or text/symbol recognition (e.g., "What does this sign say?"). Moreover, averaging results on the entirety of OK-VQA and A-OKVQA obfuscates the improvements brought about to a subset of questions that truly require commonsense knowledge. We propose subsets to assess the performance of our model on questions that are more likely to require external commonsense knowledge. We obtain the subsets by eliminating questions that are mostly factual or visual, and hence do not require commonsense, following these conditions:</p><formula xml:id="formula_5">(1) factual:</formula><p>The question or answer contains named entities (e.g., "USA"); (2) numerical: The answers contain numbers or number words (e.g., "twenty") or the question has date or time words (e.g., "century"); (3) visual: The question contains directional words (e.g., "left of") and words referring to symbols (e.g., "mascot").</p><p>In <ref type="table" target="#tab_5">Table 3</ref>, we show that VLC-BERT with COMET performs 3 points better on the A-OKVQA subset, and maintains an 0.8 point improvement on the OK-VQA subset. This substantiates our claim that utilizing our COMET pipeline substantially increases VLC-BERT's ability to answer questions that require external knowledge.  <ref type="formula">)</ref> and (c) are from OK-VQA. We observe that the weakly supervised attention layer in VLC-BERT accurately picks useful commonsense inferences. In (c), we observe how object tags are useful to guide COMET to produce contextualized knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Attention Analysis</head><p>In this section, we show qualitative examples to demonstrate questions where VLC-BERT benefits from contextualized commonsense knowledge from COMET. We also show the corresponding attention weights, to show the effectiveness of the proposed weakly-supervised attention mechanism. <ref type="figure" target="#fig_1">Fig 4a shows</ref> an example from A-OKVQA, where COMET's inferences on the question and the object tags, weighted by the attention score, results in the correct answer. <ref type="figure" target="#fig_1">Fig 4b shows</ref> an example from OK-VQA where VLC-BERT COMET exhibits higher attention towards the fire despite the object tags missing the fireplace. This is an example where deriving inferences from the question phrase is equally important as doing so with the object tags. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We presented Vision-Language-Commonsense BERT (VLC-BERT) for external knowledge-driven VQA tasks. VLC-BERT outperforms previous models based on knowledge bases on the OK-VQA and A-OKVQA datasets by incorporating contextualized commonsense knowledge from COMET and combining it with visual and linguistic inputs. Through our evaluation, we show the effectiveness of our knowledge generation, selection, and incorporation strategies, and the positive impact of VQA pre-training.</p><p>Our analysis of VLC-BERT highlighted a few limitations of our model and the datasets we evaluate on. First, some questions require a deeper understanding and linking of multiple entities and events in the image, that object tags lack, for deriving relevant commonsense inferences. Second, condensing the commonsense inferences using SBERT and MHA leads to a compressed representation that may cause the model to lose some information. Finally, our model is limited by COMET, and the knowledge bases it is trained on, as we observe that large-scale models like GPT-3 outperform it.</p><p>We view our work as a first step in analyzing the potential of generative commonsense incorporation, and exploring approaches to decide when commonsense is needed. In the future, our goal is to work towards creating a version of COMET that can utilize image context concerning multiple entities and events. We also plan to investigate the potential of multi-hop reasoning with COMET to bridge the question and image-based expansions closer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Attention analysis: (a) is from A-OKVQA, and (b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 4c shows that inferences on the object tag kite drove the model to answer correctly. The supplementary material includes additional examples of improvements and failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Science and Technology; and Weather and Climate. OK-VQA only contains open-ended questions with five human-provided answers. Since OK-VQA does not have a validation set, we dedicate 1,000 of the 9,009 training questions for validation.</figDesc><table><row><cell>The dataset is composed of</cell></row><row><cell>14,031 images and 14,055 questions, and the crowsourced</cell></row><row><cell>questions are divided into ten knowledge categories:</cell></row><row><cell>Vehicles and Transportation; Brands, Companies and</cell></row><row><cell>Products; Objects, Materials and Clothing; Sports and</cell></row><row><cell>Recreation; Cooking and Food; Geography, History,</cell></row><row><cell>Language and Culture; People and Everyday Life, Plants</cell></row><row><cell>and Animals;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation of various components in VLC-BERT, evaluated on the A-OKVQA validation set. We observe that all the components of our model play a critical role in empirical performance.</figDesc><table><row><cell cols="5">VQA P.T. Aug. SBERT SBERT Attn. Val</cell></row><row><cell></cell><cell cols="2">VQA Pre-training</cell><cell></cell><cell></cell></row><row><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>36.24</cell></row><row><cell>?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>43.46</cell></row><row><cell></cell><cell cols="3">Comm. Inference Representation</cell><cell></cell></row><row><cell>?</cell><cell>?</cell><cell>-</cell><cell>-</cell><cell>43.44</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>-</cell><cell>43.64</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">? 44.95</cell></row><row><cell></cell><cell cols="2">Augmentation of SBERT</cell><cell></cell><cell></cell></row><row><cell>?</cell><cell>-</cell><cell>?</cell><cell cols="2">? 44.10</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">? 44.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Evaluation on the subsets of OK-VQA test (OK s ) and A-OKVQA validation (A-OK s ) sets, where factual, numerical and visual questions are pruned. The performance gain observed on the subsets shows a better picture of where external commonsense is effective. Method OK OK s A-OK A-OK s Base 42.29 47.4 43.46 46.52 w/ COMET 43.14 48.21 44.95 49.53</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following the same evaluation, each of the 5 answers in OK-VQA is used twice</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">www.vectorinstitute.ai/#partners</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Acknowledgments</head><p>This work was funded, in part, by the Vector Institute for AI, Canada CIFAR AI Chair, NSERC CRC, NSERC DG and Accelerator Grants, and a research gift from AI2. Hardware resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute 4 . Additional hardware support was provided by John R. Evans Leaders Fund CFI grant and Compute Canada under the Resource Allocation Competition award. Finally, we sincerely thank Prof. Giuseppe Carenini for valuable feedback and discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">COMET: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">57th Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">It&apos;s not rocket science : Interpreting figurative language in narratives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuhin</forename><surname>Chakrabarty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Webqa: Multihop and multimodal qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingshan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mridu</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisami</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Visual Dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khushi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avi</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deshraj</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Commonsense knowledge mining from pretrained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1173" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Conceptbert: Concept-aware representation for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Gard?res</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Ziaeefard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddy</forename><surname>Baptiste Abeloos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L?cu?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FINDINGS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reporting bias and knowledge acquisition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Automated Knowledge Base Construction</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Kat: A knowledge augmented transformer for vision-and-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">KAT: A knowledge augmented transformer for vision-and-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangke</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2022-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Jocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Stoken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirka</forename><surname>Borovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghye</forename><surname>Nanocode012</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacong</forename><surname>Taoxie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorna</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Abhiram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jebastin</forename><surname>Nadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Laughing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Skalski</surname></persName>
		</author>
		<title level="m">ultralytics/yolov5: v6.1 -TensorRT, TensorFlow Edge TPU and OpenVINO Export and Inference</title>
		<editor>Wang, Adam Hogan, Cristi Fati, Lorenzo Mammana, AlexWang1900, Deep Patel, Ding Yiwei, Felix You, Jan Hajek, Laurentiu Diaconu, and Mai Thanh Minh</editor>
		<imprint>
			<date type="published" when="2022-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Extending a parser to distant domains using a few dozen partially annotated examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hopkins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Webly supervised concept expansion for general purpose vision models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amita</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Boosting visual question answering with context-aware knowledge aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">VisualBERT: A simple and performant baseline for vision and language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Oscar: Objectsemantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kagnet: Knowledge-aware graph networks for commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyue</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Computer Vision -ECCV 2014</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly-supervised visual-retriever-reader for knowledgebased question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyay</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Like hiking? you probably enjoy nature: Persona-grounded dialog with commonsense expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Bodhisattwa Prasad Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Jhamtani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="9194" to="9206" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">KRISP: Integrating implicit and symbolic knowledge for open-domain knowledge-based vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14111" to="14121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">OK-VQA: A visual question answering benchmark requiring external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualcomet: Reasoning about the dynamic context of a still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A primer in BERTology: What we know about how BERT works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Image captioning for effective use of language models in knowledge-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ander</forename><surname>Salaberria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Azkune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oier</forename><surname>Lopez De Lacalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visuo-linguistic question answering (VLQA) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shailaja Keyur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Sampat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitta</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Lebras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">A-okvqa: A benchmark for visual question answering using world knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Marino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Mottaghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cycle-consistency for robust visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meet</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6642" to="6651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised commonsense question answering with self-talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vered</forename><surname>Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2020-11" />
			<biblScope unit="page" from="4615" to="4629" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">LXMERT: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hy-poGen: Hyperbole generation with commonsense and counterfactual knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><forename type="middle">Krishna</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics: EMNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1583" to="1593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Fvqa: Fact-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2413" to="2427" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Symbolic knowledge distillation: from general language models to commonsense models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandrasekhar</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Ashish Sabharwal, and Roozbeh Mottaghi. Multi-Modal Answer Validation for Knowledge-based VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">An empirical study of gpt-3 for few-shot knowledge-based vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving question answering by commonsense-based pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanjun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCF International Conference on Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="16" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
