<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ShotgunWSD: An unsupervised algorithm for global word sense disambiguation inspired by DNA sequencing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Radu Tudor Ionescu</roleName><forename type="first">Andrei</forename><forename type="middle">M</forename><surname>Butnaru</surname></persName>
							<email>butnaruandreimadalin@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florentina</forename><surname>Hristea</surname></persName>
							<email>fhristea@fmi.unibuc.ro</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Bucharest</orgName>
								<address>
									<addrLine>14 Academiei</addrLine>
									<settlement>Bucharest</settlement>
									<country key="RO">Romania</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ShotgunWSD: An unsupervised algorithm for global word sense disambiguation inspired by DNA sequencing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel unsupervised algorithm for word sense disambiguation (WSD) at the document level. Our algorithm is inspired by a widely-used approach in the field of genetics for whole genome sequencing, known as the Shotgun sequencing technique. The proposed WSD algorithm is based on three main steps. First, a brute-force WSD algorithm is applied to short context windows (up to 10 words) selected from the document in order to generate a short list of likely sense configurations for each window. In the second step, these local sense configurations are assembled into longer composite configurations based on suffix and prefix matching. The resulted configurations are ranked by their length, and the sense of each word is chosen based on a voting scheme that considers only the top k configurations in which the word appears. We compare our algorithm with other state-of-the-art unsupervised WSD algorithms and demonstrate better performance, sometimes by a very large margin. We also show that our algorithm can yield better performance than the Most Common Sense (MCS) baseline on one data set. Moreover, our algorithm has a very small number of parameters, is robust to parameter tuning, and, unlike other bioinspired methods, it gives a deterministic solution (it does not involve random choices).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel unsupervised algorithm for word sense disambiguation (WSD) at the document level. Our algorithm is inspired by a widely-used approach in the field of genetics for whole genome sequencing, known as the Shotgun sequencing technique. The proposed WSD algorithm is based on three main steps. First, a brute-force WSD algorithm is applied to short context windows (up to 10 words) selected from the document in order to generate a short list of likely sense configurations for each window. In the second step, these local sense configurations are assembled into longer composite configurations based on suffix and prefix matching. The resulted configurations are ranked by their length, and the sense of each word is chosen based on a voting scheme that considers only the top k configurations in which the word appears. We compare our algorithm with other state-of-the-art unsupervised WSD algorithms and demonstrate better performance, sometimes by a very large margin. We also show that our algorithm can yield better performance than the Most Common Sense (MCS) baseline on one data set. Moreover, our algorithm has a very small number of parameters, is robust to parameter tuning, and, unlike other bioinspired methods, it gives a deterministic solution (it does not involve random choices).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word Sense Disambiguation (WSD), the task of identifying which sense of a word is used in a given context, is a core NLP problem, having the potential to improve many applications such as machine translation <ref type="bibr">(Carpuat and Wu, 2007)</ref>, text summarization <ref type="bibr" target="#b22">(Plaza et al., 2011)</ref>, information retrieval <ref type="bibr">(Chifu and Ionescu, 2012;</ref><ref type="bibr" target="#b8">Chifu et al., 2014)</ref> or sentiment analysis <ref type="bibr">(Sumanth and Inkpen, 2015)</ref>. Most of the existing WSD algorithms <ref type="bibr" target="#b0">(Agirre and Edmonds, 2006;</ref><ref type="bibr" target="#b19">Navigli, 2009)</ref> are commonly classified into supervised, unsupervised, and knowledge-based techniques, but hybrid approaches have also been proposed in the literature <ref type="bibr" target="#b8">(Hristea et al., 2008)</ref>. The main disadvantage of supervised methods (that have led to the best disambiguation results) is that they require a large amount of annotated data which is difficult to obtain. Hence, over the last few years, many researchers have concentrated on developping unsupervised learning approaches <ref type="bibr" target="#b24">(Schwab et al., 2012;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a;</ref><ref type="bibr" target="#b25">Schwab et al., 2013b;</ref><ref type="bibr" target="#b7">Chen et al., 2014;</ref><ref type="bibr">Bhingardive et al., 2015)</ref>. In this paper, we introduce a novel WSD algorithm, termed ShotgunWSD 1 , that stems from the Shotgun genome sequencing technique <ref type="bibr" target="#b1">(Anderson, 1981;</ref><ref type="bibr">Istrail et al., 2004)</ref>. Our WSD algorithm is also unsupervised, but it requires knowledge in the form of WordNet <ref type="bibr" target="#b18">(Miller, 1995;</ref><ref type="bibr">Fellbaum, 1998)</ref> synsets and relations as well. Thus, our algorithm can be regarded as a hybrid approach.</p><p>WSD algorithms can perform WSD at the local or at the global level. A local WSD algorithm, such as the extended Lesk measure <ref type="bibr" target="#b15">(Lesk, 1986;</ref><ref type="bibr">Banerjee and Pedersen, 2002;</ref><ref type="bibr">Banerjee and Pedersen, 2003)</ref>, is designed to assign the appropriate sense, from an existing sense inventory, for a target word in a given context window of a few words. For instance, for the word "sense" in the context "You have a good sense of humor.", the sense that corresponds to the natural ability rather than the meaning of a word or the sensation should be chosen by a WSD algorithm. Rather more generally, a global WSD approach aims to choose the appropriate sense for each ambiguous word in a text document. The straightforward solution is the exhaustive evaluation of all sense combinations (configurations) <ref type="bibr">(Patwardhan et al., 2003)</ref>, but the time complexity is exponential with respect to the number of words in the text, as also noted by <ref type="bibr" target="#b24">Schwab et al. (2012)</ref>, <ref type="bibr" target="#b25">Schwab et al. (2013a)</ref>. Indeed, the brute-force (BF) solution quickly becomes impractical for windows of more than a few words. Hence, several approximation methods have been proposed for the global WSD task in order to overcome the exponentional growth of the search space <ref type="bibr" target="#b24">(Schwab et al., 2012;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a)</ref>. Our algorithm is designed to perform global WSD by combining multiple local sense configurations that are obtained using BF search, thus avoiding BF search on the whole text. A local WSD algorithm is employed to build the local sense configurations. We alternatively use two methods at this step, namely the extended <ref type="bibr">Lesk measure (Banerjee and Pedersen, 2002;</ref><ref type="bibr">Banerjee and Pedersen, 2003)</ref> and an approach based on deriving sense embeddings from word embeddings <ref type="bibr" target="#b4">(Bengio et al., 2003;</ref><ref type="bibr">Collobert and Weston, 2008;</ref><ref type="bibr">Mikolov et al., 2013)</ref>. Both local WSD approaches are based on WordNet synsets and relations.</p><p>Our global WSD algorithm can be briefly described in a few steps. In the first step, context windows of a fixed length n are selected from the document, and for each context window the top scoring sense configurations constructed by BF search are kept for the second step. The retained sense configurations are merged based on suffix and prefix matching. The configurations obtained so far are ranked by their length (the longer, the better), and the sense of each word is given by a majority vote on the top k configurations that cover the respective word. Compared to other state-of-the-art bio-inspired methods <ref type="bibr" target="#b24">(Schwab et al., 2012;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a)</ref>, our algorithm has less parameters. Different from the other methods, these parameters (n and k) can be intuitively tuned with respect to the WSD task. As we select a single context window at every possible location in a text, our algorithm becomes deterministic, obtaining the same global configuration for a given set of parameters and input document. Thus, our algorithm is not affected by random chance, unlike stochastic algorithms such as Ant Colony Optimization <ref type="bibr" target="#b14">(Lafourcade and Guinand, 2010;</ref><ref type="bibr" target="#b24">Schwab et al., 2012;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a)</ref>.</p><p>We have conducted experiments on SemEval 2007 <ref type="bibr">(Navigli et al., 2007)</ref>, <ref type="bibr">Senseval-2 (Edmonds and Cotton, 2001)</ref> and Senseval-3 <ref type="bibr" target="#b17">(Mihalcea et al., 2004)</ref> data sets in order to compare ShotgunWSD with three state-of-the-art approaches <ref type="bibr" target="#b25">(Schwab et al., 2013a;</ref><ref type="bibr" target="#b7">Chen et al., 2014;</ref><ref type="bibr">Bhingardive et al., 2015)</ref> along with the Most Common Sense (MCS) baseline 2 , which is considered as the strongest baseline in WSD <ref type="bibr" target="#b0">(Agirre and Edmonds, 2006)</ref>. The empirical results show that our algorithm compares favorably to these approaches.</p><p>The rest of this paper is organized as follows. Related work on unsupervised WSD algorithms is presented in Section 2. The ShotgunWSD algorithm is described in Section 3. The experiments are given in Section 4. Finally, we draw our conclusions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There is a broad range of methods designed to perform WSD <ref type="bibr" target="#b0">(Agirre and Edmonds, 2006;</ref><ref type="bibr" target="#b19">Navigli, 2009;</ref><ref type="bibr" target="#b27">Vidhu Bhala and Abirami, 2014)</ref>. The most accurate techniques are supervised <ref type="bibr" target="#b11">(Iacobacci et al., 2016)</ref>, but they require annotated training data which is not always available. In order to overcome this limitation, some researchers have proposed various unsupervised or knowledgde-based WSD methods <ref type="bibr">(Banerjee and Pedersen, 2002;</ref><ref type="bibr">Banerjee and Pedersen, 2003;</ref><ref type="bibr" target="#b24">Schwab et al., 2012;</ref><ref type="bibr" target="#b20">Nguyen and Ock, 2013;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a;</ref><ref type="bibr" target="#b25">Schwab et al., 2013b;</ref><ref type="bibr" target="#b7">Chen et al., 2014;</ref><ref type="bibr" target="#b0">Agirre et al., 2014;</ref><ref type="bibr">Bhingardive et al., 2015)</ref>. Since our approach is unsupervised and based on Word-Net <ref type="bibr" target="#b18">(Miller, 1995;</ref><ref type="bibr">Fellbaum, 1998)</ref>, our focus is to present related work in the same direction. Banerjee and Pedersen (2002) extend the gloss overlap algorithm of <ref type="bibr" target="#b15">Lesk (1986)</ref> by using WordNet relations. <ref type="bibr">Patwardhan et al. (2003)</ref> proposed a bruteforce algorithm for global WSD by employing the extended <ref type="bibr">Lesk measure (Banerjee and Pedersen, 2002;</ref><ref type="bibr">Banerjee and Pedersen, 2003)</ref> to compute the semantic relatedness among senses in a given text. However, their BF approach is not suitable for whole text documents, because of the high computational time. More recently, <ref type="bibr" target="#b24">Schwab et al. (2012)</ref> have proposed and compared three stochastic algorithms for global WSD as alternatives to BF search, namely a Genetic Algorithm, Simulated Annealing, and Ant Colony Optimization. Among these, the authors <ref type="bibr" target="#b24">(Schwab et al., 2012;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a)</ref> have found that the Ant Colony Algorithm yields better results than the other two.</p><p>Recently, word embeddings have been used for WSD <ref type="bibr" target="#b7">(Chen et al., 2014;</ref><ref type="bibr">Bhingardive et al., 2015;</ref><ref type="bibr" target="#b11">Iacobacci et al., 2016)</ref>. Word embeddings are well known in the NLP community <ref type="bibr" target="#b4">(Bengio et al., 2003;</ref><ref type="bibr">Collobert and Weston, 2008)</ref>, but they have recenlty become more popular due to the work of <ref type="bibr">Mikolov et al. (2013)</ref> which introduced the word2vec framework that allows to efficiently build vector representations from words. <ref type="bibr" target="#b7">Chen et al. (2014)</ref> present a unified model for joint word sense representation and disambiguation. They use the Skip-gram model to learn word vectors. On the other hand, Bhingardive et al. <ref type="formula">(2015)</ref> use pre-trained word vectors to build sense embeddings by averaging the word vectors produced for each sense of a target word. As their goal is to find an approximation for the MCS baseline, they try to find the sense embedding that is closest to the embedding of the target word. <ref type="bibr" target="#b11">Iacobacci et al. (2016)</ref> propose different methods through which word embeddings can be leveraged in a supervised WSD system architecture. Remarkably, they find that a WSD approach based on word embeddings alone can provide significant performance improvements over a state-of-the-art WSD system that uses standard WSD features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ShotgunWSD</head><p>As also noted by <ref type="bibr" target="#b24">Schwab et al. (2012)</ref>, bruteforce WSD algorithms based on semantic relatedness <ref type="bibr">(Patwardhan et al., 2003)</ref> are not practical for whole text disambiguation due to their exponential time complexity. In this section, we describe a novel WSD algorithm that aims to avoid this computational issue. Our algorithm is inspired by the Shotgun genome sequencing technique <ref type="bibr" target="#b1">(Anderson, 1981)</ref> which is used in genetics research to obtain long DNA strands. For example, <ref type="bibr">Istrail et al. (2004)</ref> have used this technique to assemble the human genome. Before a long DNA strand can be read, Shotgun sequencing needs to create multiple copies of the respective strand. Next, DNA is randomly broken down into many small segments called reads (usually between 30 and 400 nucleotides long), by adding a restriction enzyme into the chemical solution containing the DNA. The reads can then be sequenced using Next-Generation Sequencing techonlogy <ref type="bibr" target="#b28">(Voelkerding et al., 2009)</ref>, for example by using an Illumina (Solexa) machine <ref type="bibr" target="#b5">(Bennett, 2004)</ref>. In genome assembly, the low quality reads are usually eliminated (Patel and Jain, 2012) and the whole genome is reconstructed by assembling the remaining reads. One strategy is to merge two or more reads in order to obtain longer DNA segments, if they have a significant overlap of matching nucleotides. Because of reading errors or mutations, the overlap is usually measured using a distance measure, e.g. edit distance <ref type="bibr" target="#b16">(Levenshtein, 1966)</ref>. If a backbone DNA sequence is available, the reads are aligned to the backbone DNA before assembly, in order to find their approximate position in the DNA that needs to be reconstructed.</p><p>We next present how we adapt the Shotgun sequencing technique for the task of global WSD. We will make a few observations along the way that will lead to a simplified method, namely Shot-gunWSD, which is formally presented in Algorithm 1. We use the following notations in Algorithm 1. An array (or an ordered set of elements) is denoted by X = (x 1 , x 2 , ...., x m ) and the length of X is denoted by |X| = m. Arrays are considered to be indexed starting from position 1, thus</p><formula xml:id="formula_0">X[i] = x i , ?i ? {1, 2, ...m}.</formula><p>Our goal is to find a configuration of senses G for the whole document D, that matches the ground-truth configuration produced by human annotators. A configuration of senses is simply obtained by assigning a sense to each word in a text. In this work, the senses are selected from WordNet <ref type="bibr" target="#b18">(Miller, 1995;</ref><ref type="bibr">Fellbaum, 1998)</ref>, according to steps 7-8 of Algorithm 1. Naturally, we will consider that the sense configuration of the whole document corresponds to the long DNA strand that needs to be sequenced. In this context, sense configurations of short context windows (less than 10 words) will correspond to the short DNA reads. A crucial difference here is that we know the location of the context windows in the whole document from the very beginning, so our task will be much easier compared to Shotgun sequencing (we do not need to use a backbone solution for the alignment of short sense configurations). At every possible location in the text document (step Algorithm 1: ShotgunWSD Algorithm 1 Input: 2 D = (w1, w2, ..., wm) -a document of m words denoted by wi, i ? {1, 2, ..., m}; 3 n -the length of the context windows (1 &lt; n &lt; m); 4 k -the number of sense configurations considered for the voting scheme (k &gt; 0); 5 Initialization: 6 c ? 20; 7 for i ? {1, 2, ..., m} do 8 Sw i ? the set of WordNet synsets of wi; 12), we select a window of n words. The window length n is an external parameter of our algorithm that can be tuned for optimal results. For each context window we will compute all possible sense configurations (steps 14-15). A score is assigned to each sense configuration by using the semantic relatedness between word senses (steps 16-19), as described by <ref type="bibr">Patwardhan et al. (2003)</ref>.</p><formula xml:id="formula_1">9 S ? ?; 10 G ? (0, 0, ...., 0), such that |G| = m; 11 Computation: 12 for i ? {1, 2, ..., m ? n + 1} do 13 Ci ? ?; 14 while did not generate all sense configurations do 15 C ? a new configuration (sw i , sw i+1 , ..., sw i+n?1 ), sw j ? Sw j , ?j ? {i, ..., i + n ? 1}, such that C / ? Ci; 16 r ? 0; 17 for p ? {1, 2, ..., n ? 1}</formula><p>We alternatively employ two measures to compute the semantic relatedness, one is the extended Lesk measure (Banerjee and <ref type="bibr">Pedersen, 2002;</ref><ref type="bibr">Banerjee and Pedersen, 2003)</ref> and the other is a simple approach based on deriving sense embeddings from word embeddings <ref type="bibr">(Mikolov et al., 2013)</ref>. Both methods are described in Section 3.1. We will keep the top scoring sense configurations (step 21)</p><p>for the assembly phase (steps 23-39). In step 21, we use an internal parameter c in order to determine exactly how many sense configurations are kept per context window. Another important remark is that we assume that the BF algorithm used to obtain sense configurations for short windows does not produce output errors, so it is not necessary to use a distance measure in order to find overlaps for merging configurations. We simply check if the suffix of a former configuration coincides with the prefix of a latter configuration in order to join them together (steps 29-33). The length l of the suffix and the prefix that get overlapped needs to be greater then zero, so at least one sense choice needs to coincide. We gradually consider shorter and shorter suffix and prefix lengths starting with l = min{4, n ? 1} (step 23). Sense configurations are assembled in order to obtain longer configurations (step 34), until none of the resulted configurations can be further merged together. When merging, the relatedness score of the resulting configuration needs to be recomputed (steps 36-38), but we can take advantage of some of the previously computed scores (step 35). Longer configurations indicate that there is an agreement (regarding the chosen senses) that spans across a longer piece of text. In other words, longer configurations are more likely to provide correct sense choices, since they inherently embed a higher degree of agreement among senses. After the configuration assembly phase, we start assigning the sense to each word in the document (step 40). Based on the assumption that longer configurations provide better information, we build a ranked list of sense configurations for each word in the document (step 42). Naturally, for a given word, we only consider the configurations that contain the respective word (step 41).</p><p>Finally, the sense of each word is given by a majority vote on the top k configurations from its ranked list (steps 43-44). The number of sense configurations k is an external parameter of our approach, and it can be tuned for optimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Relatedness</head><p>For a sense configuration of n words, we compute the semantic relatedness between each pair of selected senses. We use two different approaches for computing the relatedness score and both of them are based on WordNet semantic relations. In this context, we essentially need to compute the se-mantic relatedness of two WordNet synsets. For each synset we build a disambiguation vocabulary by extracting words from the WordNet lexical knowledge base, as follows. Starting from the synset itself, we first include all the synonyms that form the respective synset along with the content words of the gloss (examples included). We also include into the disambiguation vocabulary words indicated by specific WordNet semantic relations that depend on the part-of-speech of the word. More precisely, we have considered hyponyms and meronyms for nouns. For adjectives, we have considered similar synsets, antonyms, attributes, pertainyms and related (see also) synsets. For verbs, we have considered troponyms, hypernyms, entailments and outcomes. Finally, for adverbs, we have considered antonyms, pertainyms and topics. These choices have been made because previous studies <ref type="bibr">(Banerjee and Pedersen, 2003;</ref><ref type="bibr" target="#b8">Hristea et al., 2008)</ref> have come to the conclusion that using these specific relations for each part-of-speech seems to provide useful information in the WSD process. The disambiguation vocabulary generated by the WordNet feature selection described so far needs to be further processed in order to obtain the final vocabulary.</p><p>The first processing step is to eliminate the stopwords. The remaining words are stemmed using the Porter stemmer algorithm <ref type="bibr" target="#b23">(Porter, 1980)</ref>. The resulted stems represent the final set of features that we use to compute the relatedness score between two synsets. The two measures that we employ for computing the relatedness score are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Extended Lesk Measure</head><p>The original Lesk algorithm <ref type="bibr" target="#b15">(Lesk, 1986)</ref> only considers one word overlaps among the glosses of a target word and those that surround it in a given context. Banerjee and Pedersen (2002) note that this is a significant limitation because dictionary glosses tend to be fairly short and they fail to provide sufficient information to make fine grained distinctions required for WSD. Therefore, Banerjee and Pedersen <ref type="formula">(2003)</ref> introduce a measure that takes as input two WordNet synsets and returns a numeric value that quantifies their degree of semantic relatedness by taking into consideration the glosses of related WordNet synsets as well. Moreover, when comparing two glosses, the extended Lesk measure considers overlaps of multiple consecutive words, based on the assumption that the longer the phrase, the more representative it is for the relatedness of the two synsets. Given two input glosses, the longest overlap between them is detected and then replaced with a unique marker in each of the two glosses. The resulted glosses are then again checked for overlaps, and this process continues until there are no more overlaps. The lengths of the detected overlaps are squared and added together to obtain the score for the given pair of glosses. Depeding on the WordNet relations used for each part-of-speech, several pairs of glosses are compared and summed up together to obtain the final relatedness score. However, if the two words do not belong to the same part-ofspeech, we only use their WordNet glosses and examples. Further details regarding this approach are provided by Banerjee and Pedersen <ref type="formula">(2003)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Sense Embeddings</head><p>A simple approach based on word embeddings is employed to measure the semantic relatedness of two synsets. Word embeddings <ref type="bibr" target="#b4">(Bengio et al., 2003;</ref><ref type="bibr">Collobert and Weston, 2008;</ref><ref type="bibr">Mikolov et al., 2013)</ref> represent each word as a low-dimensional real valued vector, such that related words reside in close vicinity in the generated space. We have used the pre-trained word embeddings computed by the word2vec toolkit (Mikolov et al., 2013) on the Google News data set using the Skip-gram model. The pre-trained model contains 300-dimensional vectors for 3 million words and phrases. The relatedness score between two synsets is computed as follows. For each word in the disambiguation vocabulary that represents a synset, we compute its word embedding vector. Thus, we obtain a cluster of word embedding vectors for each given synset. Sense embeddings are then obtained by computing the centroid of each cluster as the median of all the word embeddings in the respective cluster. We can naturally assume that some of the words in the cluster may actually be outliers. Thus, we believe that using the (geometric) median instead of the mean is more appropriate, as the mean is largely influenced by outliers. Finally, the semantic relatedness of two synsets is simply given by the cosine similarity between their cluster centroids.</p><p>It is important to note that an approach based on the mean of word vectors to construct sense embeddings is used by <ref type="bibr">Bhingardive et al. (2015)</ref>, but with a slightly different purpose than ours, namely to determine which synset better fits a target word, assuming that this synset should correspond to the most common sense of the respective word. As such, they completely disregard the context of the target word. Different from their approach, we are trying to find how related two synsets of distinct words that appear in the same context window are. Furthermore, the empirical results presented in Section 4 show that our approach yields better performance than the MCS estimation method of <ref type="bibr">Bhingardive et al. (2015)</ref>, thus putting a greater gap between the two methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data Sets</head><p>We compare our global WSD algorithm with several state-of-the-art unsupervised WSD methods using the same test data as in the works presenting them.</p><p>We first compare ShotgunWSD with two stateof-the-art approaches <ref type="bibr" target="#b25">(Schwab et al., 2013a;</ref><ref type="bibr" target="#b7">Chen et al., 2014)</ref> and the MCS baseline, on the SemEval 2007 coarse-grained English all-words task <ref type="bibr">(Navigli et al., 2007)</ref>. The SemEval 2007 coarse-grained English all-words data set 3 is composed of 5 documents that contain 2269 ambiguous words (1108 nouns, 591 verbs, 362 adjectives, 208 adverbs) altogether. We also compare our approach with the MCS estimation method of <ref type="bibr">Bhingardive et al. (2015)</ref>, the MCS baseline and the extended Lesk algorithm <ref type="bibr" target="#b26">(Torres and Gelbukh, 2009</ref>) on the Senseval-2 English all-words <ref type="bibr">(Edmonds and Cotton, 2001)</ref> and the Senseval-3 English all-words <ref type="bibr" target="#b17">(Mihalcea et al., 2004)</ref> data sets. The Senseval-2 data set 4 is composed of 3 documents that contain 2473 ambiguous words (1136 nouns, 581 verbs, 457 adjectives, 299 adverbs), while the Senseval-3 data set 4 is composed of 3 documents that contain 2081 ambiguous words (951 nouns, 751 verbs, 364 adjectives, 15 adverbs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Tuning</head><p>As <ref type="bibr" target="#b25">Schwab et al. (2013a)</ref>, we tune our parameters on the first document of SemEval 2007. We first set the value of the internal parameter c to 20 without specifically tuning it. Using this value for c gives us a reasonable amount of configuration choices for the subsequent steps, without using too  <ref type="figure">Figure 1</ref>: The running times (in seconds) of Shot-gunWSD based on sense embeddings, on the first document of SemEval 2007, using various context window lengths n ? {4, 5, 6, 7, 8, 9}. The reported times were measured on a computer with Intel Core i7 3.4 GHz processor and 16 GB of RAM using a single Core. much space and time. For tuning the parameters n and k, we employ sense embeddings for computing the semantic relatedness score. We begin by tuning the length of the context windows n. It is important to note that the upper bound accuracy of ShotgunWSD is given by the brute-force algorithm that explores every possible configuration of senses. Intuitively, we will get closer and closer to this upper bound as we use longer and longer context windows. However, the main decision factor is the time, which grows exponentially with respect to the length of the windows. <ref type="figure">Figure 1</ref> illustrates the time required by our algorithm to disambiguate the first document in SemEval 2007, for increasing window lengths in the range 4-9. The algorithm runs in about 15 seconds for n = 4 and in about 1892 seconds for n = 9, so it becomes nearly 120 times slower from using context windows of length 4 to context windows of length 9. As the algorithm runs in a reasonable amount of time for n = 8 (187 seconds), we choose to use context windows of 8 words throughout the rest of the experiments.</p><p>The parameter k has almost no influence on the running time of the algorithm, so we tune this parameter with respect to the F 1 score obtained on the first document of SemEval 2007. We try out several values of k in the set {1, 3, 5, 10, 15, 20} and the results are shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The best F 1 score (83.42%) is obtained for k = 15. Hence, we choose to assign the final sense for each word using a majority vote based on the top 15 configurations.</p><p>To summarize, all the results of ShotgunWSD on SemEval 2007, Senseval-2 and Senseval-3 are reported using n = 8 and k = 15. We hereby note that better results in terms of accuracy can probably be obtained by trying out other values for these parameters on each data set. However, tuning the parameters on a single document from SemEval 2007 ensures that we avoid overfitting to a particular data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on SemEval 2007</head><p>We first conduct a comparative study on the Se-mEval 2007 coarse-grained English all-words task in order to evaluate our ShotgunWSD algorithm. As described in Section 3.1, we use two different approaches for computing the semantic relatedness scores, namely extended Lesk and sense embeddings. We compare our two variants of ShotgunWSD with several algorithms presented in <ref type="bibr" target="#b24">(Schwab et al., 2012;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a)</ref>, namely a Genetic Algorithm, Simulated Annealing, and Ant Colony Optimization. We also include in the comparison an approach based on sense embeddings <ref type="bibr" target="#b7">(Chen et al., 2014)</ref>. All the approaches comprised in the evaluation are unsupervised. We compare them with the MCS baseline which is based on human annotations. The F 1 Method F1 Score Most Common Sense 78.89% Genetic Algorithms <ref type="bibr" target="#b25">(Schwab et al., 2013a)</ref> 74.53% Simulated Annealing <ref type="bibr" target="#b25">(Schwab et al., 2013a)</ref> 75.18% Ant Colony <ref type="bibr" target="#b25">(Schwab et al., 2013a)</ref> 79.03% S2C Unsupervised <ref type="bibr" target="#b7">(Chen et al., 2014)</ref> 75.80% ShotgunWSD + Extended Lesk 79.15% ShotgunWSD + Sense Embeddings 79.68%  <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Among the state-of-the-art methods, it seems that the Ant Colony Optimization algorithm, based on a weighted voting scheme <ref type="bibr" target="#b25">(Schwab et al., 2013a)</ref>, is the only method able to surpass the MCS baseline. The unsupervised S2C approach gives lower results than the MCS baseline, but <ref type="bibr" target="#b7">Chen et al. (2014)</ref> report better results in a semi-supervised setting. Both variants of ShotgunWSD yield better results than the MCS baseline (78.89%) and the Ant Colony Optimization algorithm (79.03%). Indeed, we obtain an F 1 score of 79.15% when using the extended Lesk measure and an F 1 score of 79.68% when using sense embeddings. We can also point out that ShotgunWSD gives slightly better results when sense embeddings are used instead of the extended Lesk method. An important remark is that we have tuned the parameter k on the first document included in the test set, following the same evaluation procedure as <ref type="bibr" target="#b25">Schwab et al. (2013a)</ref>. Although this brings us to a fair comparison with <ref type="bibr" target="#b25">Schwab et al. (2013a)</ref>, it might also raise suspicions of overfitting the parameter k to the test set. Hence, we have tested all values of k in {1, 3, 5, 10, 15, 20} for Shotgun-WSD based on word embeddings, and we have always obtained results above 79%, with the top score of 79.77% for k = 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Senseval-2</head><p>We compare the two alternative forms of Shot-gunWSD with the MCS baseline, the MCS estimation method of <ref type="bibr">Bhingardive et al. (2015)</ref> and the extended Lesk measure (Torres and Gelbukh,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>F1 Score Most Common Sense 60.10% MCS Estimation <ref type="bibr">(Bhingardive et al., 2015)</ref> 52.34% Extended Lesk <ref type="bibr" target="#b26">(Torres and Gelbukh, 2009)</ref> 54.60% ShotgunWSD + Extended Lesk 55.78% ShotgunWSD + Sense Embeddings 57.55% 2009) on the Senseval-2 English all-words data set. As shown in <ref type="table" target="#tab_2">Table 2</ref>, the ShotgunWSD based on sense embeddings obtains an F 1 score that is almost 5% better than the F 1 score of Bhingardive et al. <ref type="formula">(2015)</ref>, while the ShotgunWSD based on extended Lesk gives an F 1 score that is around 1% better than the F 1 score reported by <ref type="bibr" target="#b26">Torres and Gelbukh (2009)</ref>. It is important to note that Torres and Gelbukh (2009) apply the extended Lesk measure by performing the brute-force search at the sentence level, hence it is not surprising that we are able obtain better results. However, our best ShotgunWSD approach (57.55%) is still under the MCS baseline (60.10%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on Senseval-3</head><p>Method F1 Score Most Common Sense 62.30% MCS Estimation <ref type="bibr">(Bhingardive et al., 2015)</ref> 43.28% Extended Lesk <ref type="bibr" target="#b26">(Torres and Gelbukh, 2009)</ref> 49.60% ShotgunWSD + Extended Lesk 57.89% ShotgunWSD + Sense Embeddings 59.82% We also compare the two variants of Shotgun-WSD with the MCS baseline, the MCS estimation method of <ref type="bibr">Bhingardive et al. (2015)</ref> and the extended Lesk measure <ref type="bibr" target="#b26">(Torres and Gelbukh, 2009)</ref> on the Senseval-3 English all-words data set. The F 1 scores are presented in <ref type="table" target="#tab_3">Table 3</ref>. The empirical results show that both ShotgunWSD variants give considerably better results compared to the MCS estimation method of <ref type="bibr">Bhingardive et al. (2015)</ref>. By using sense embeddings in a completely different way than Bhingardive et al. <ref type="formula">(2015)</ref>, we are able to report an F 1 score of 59.82%, which is much closer to the MCS baseline (62.30%). With an F 1 score of 57.89%, the ShotgunWSD based on the extend Lesk measure brings an improvement of 8% over the extended Lesk algorithm applied at the sentence level <ref type="bibr" target="#b26">(Torres and Gelbukh, 2009</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>Considering all the experiments, we can conclude that ShotgunWSD gives better results (around 1%) when sense embeddings are used instead of the extended Lesk method. On one of the data sets, ShotgunWSD yields better performance than the MCS baseline. It is important to underline that the strong MCS baseline cannot be used in practice, since human input is required to indicate which sense of a word is the most frequent in a given text (a word's dominant sense will vary across domains and text genres). Corpora used for the evaluation of WSD methods usually contain this kind of annotations, but the MCS baseline will not work outside the annotated data. Therefore, we consider important even slightly outperforming the MCS baseline. Overall, our algorithm compares favorably to other state-of-the-art unsupervised WSD methods <ref type="bibr" target="#b25">(Schwab et al., 2013a;</ref><ref type="bibr" target="#b7">Chen et al., 2014;</ref><ref type="bibr">Bhingardive et al., 2015)</ref> and to the extended <ref type="bibr">Lesk measure (Banerjee and Pedersen, 2002;</ref><ref type="bibr" target="#b26">Torres and Gelbukh, 2009)</ref>.</p><p>Regarding the performance of our algorithm, an interesting question that arises is how much does the assembly phase help. We look to investigate this further in future work, but we can carry out a small experiment to provide a quick answer to this question. We consider the ShotgunWSD variant based on sense embeddings without changing its parameters, and we remove the assembly phase completely. Therefore, the algorithm will no longer produce configurations of length greater than 8, as the parameter n is set to 8. We have evaluated this stub algorithm on SemEval 2007 and we have obtained a lower F 1 score (77.61%). This indicates that the assembly phase in Algorithm 1 boosts the performance by nearly 2%. More ex-periments are required to make sure that the performance boost is consistent across data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this paper, we have introduced a novel unsupervised global WSD algorithm inspired by the Shotgun genome sequencing technique <ref type="bibr" target="#b1">(Anderson, 1981)</ref>. Compared to other bio-inspired WSD methods <ref type="bibr" target="#b24">(Schwab et al., 2012;</ref><ref type="bibr" target="#b25">Schwab et al., 2013a)</ref>, our algorithm has only two parameters. Furthermore, our algorithm is deterministic, obtaining the same result for a given set of parameters and input document. The empirical results indicate that our algorithm can obtain better performance than other state-of-the-art unsupervised WSD methods <ref type="bibr" target="#b25">(Schwab et al., 2013a;</ref><ref type="bibr" target="#b7">Chen et al., 2014;</ref><ref type="bibr">Bhingardive et al., 2015)</ref>. Although the fact that ShotgunWSD is deterministic brings several advantages, it is also a key difference from our source of inspiration, Shotgun sequencing, which is a non-deterministic technique.</p><p>In future work, we aim to investigate if training sense embeddings instead of deriving them from pre-trained word embeddings could yield better accuracy. Another promising direction is to compute the semantic relatedness of sense configurations based on the sum of sense tuples instead of sense pairs. An approach to combine the two semantic relatedness approaches independently used by ShotgunWSD, namely the extended Lesk measure and sense embeddings, is also worth exploring in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The F 1 scores of ShotgunWSD based on sense embeddings on the first document of Se-mEval 2007, using different values for the parameter k ? {1, 3, 5, 10, 15, 20}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Cp[2], ..., Cp[np], Cq[l + 1], Cq[l + 2], ..., Cq[nq]); ? {1, 2, ..., np + nq ? l} do 37 for j ? {l + 1, l + 2, ..., nq} do Qj ? the top k configurations obtained by sorting the configurations in Qj by their length (descending); 43 psw j ? the predominant sense obtained by using a majority voting scheme on Qj; 44 G[j] ? psw j ;45 Output: 46 G = (psw 1 , psw 2 , ..., psw m ), psw i ? Sw i , ?i ? {1, 2, ..., m} -the global configuration of senses returned by the algorithm.</figDesc><table><row><cell></cell><cell>do</cell></row><row><cell>18</cell><cell>for q ? {p + 1, 2, ..., n} do</cell></row><row><cell>19</cell><cell>r ? r + relatedness(C[p], C[q]);</cell></row><row><cell>20</cell><cell>Ci ? Ci ? {(C, i, n, r)};</cell></row><row><cell>22</cell><cell>S ? S ? Ci;</cell></row><row><cell>35</cell><cell>rp?q ? rp;</cell></row><row><cell>36</cell><cell>for i</cell></row></table><note>21 Ci ? the top c configurations obtained by sorting the configurations in Ci by their relatedness score (descending);23 for l ? {min{4, n ? 1}, ..., 1} do24 for p ? {1, 2, ..., |S|} do25 (Cp, ip, np, rp) ? the p-th component of S;26 for q ? {1, 2, ..., |S|} do27 (Cq, iq, nq, rq) ? the q-th component of S;28 if iq ? ip &lt; np and ip = iq then29 t ? true;30 for x ? {1, ..., l} do31 if Cp[np ? l + x] = Cq[x] then32 t ? false;33 if t = true then34 Cp?q ? (Cp[1],38 rp?q ? rp?q + relatedness(Cp?q[i], Cq[j]);39 S ? S ? {(Cp?q, ip, np + nq ? l, rp?q)};40 for j ? {1, 2, ..., m} do41 Qj ? {(C, i, d, r) | (C, i, d, r) ? S, j ? {i, i + 1, ..., i + d ? 1}};42</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: The F 1 scores of various unsupervised</cell></row><row><cell>state-of-the-art WSD approaches, compared to the</cell></row><row><cell>F 1 scores of ShotgunWSD based on the extended</cell></row><row><cell>Lesk measue and ShotgunWSD based on sense</cell></row><row><cell>embeddings, on the SemEval 2007 coarse-grained</cell></row><row><cell>English all-words task. The results reported for</cell></row><row><cell>both ShotgunWSD variants are obtained for win-</cell></row><row><cell>dows of n = 8 words and a majority vote on the</cell></row><row><cell>top k = 15 configurations.</cell></row><row><cell>scores on SemEval 2007 are presented in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The F 1 scores of an unsupervised WSD approach and the extended Lesk mesure, compared to the F 1 scores of ShotgunWSD based on the extended Lesk measue and ShotgunWSD based on sense embeddings, on the Senseval-2 English all-words data set. The results reported for both ShotgunWSD approaches are obtained for windows of n = 8 words and a majority vote on the top k = 15 configurations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The F 1 scores of an unsupervised WSD approach and the extended Lesk mesure, compared to the F 1 scores of ShotgunWSD based on the extended Lesk measue and ShotgunWSD based on sense embeddings, on the Senseval-3 English all-words data set. The results reported for both ShotgunWSD approaches are obtained for windows of n = 8 words and a majority vote on the top k = 15 configurations.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our open source Java implementation of ShotgunWSD is freely available at http://ai.fmi.unibuc.ro/Home/Software.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Also known as the Most Frequent Sense baseline.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://nlp.cs.swarthmore.edu/semeval/tasks/index.php 4 http://web.eecs.umich.edu/?mihalcea/downloads.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank the reviewers for their helpful suggestions. We also thank Alexandru I. Tomescu from the University of Helsinki for insightful comments about the Shotgun sequencing technique.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random Walks for Knowledge-based Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Glenny</forename><surname>Edmonds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="84" />
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>Word Sense Disambiguation: Algorithms and Applications</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shotgun DNA sequencing using cloned DNase I-generated fragments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="3015" to="3027" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CICLing</title>
		<editor>Pedersen2002] Satanjeev Banerjee and Ted Pedersen</editor>
		<meeting>CICLing</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extended Gloss Overlaps As a Measure of Semantic Relatedness</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<editor>Pedersen2003] Satanjeev Banerjee and Ted Pedersen</editor>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="805" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Neural Probabilistic Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised Most Frequent Sense Detection using Word Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bennett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sudha Bhingardive, Dhirendra Singh, Rudramurthy V, Hanumant Harichandra Redkar, and Pushpak Bhattacharyya</title>
		<imprint>
			<date type="published" when="2004-06" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1238" to="1243" />
		</imprint>
	</monogr>
	<note>Proceedings of NAACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improving statistical machine translation using word sense disambiguation</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<editor>Carpuat and Wu2007] Marine Carpuat and Dekai Wu</editor>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Unified Model for Word Sense Representation and Disambiguation. Proceedings of EMNLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chifu and Ionescu2012] Adrian-Gabriel Chifu and Radu Tudor Ionescu</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="398" to="411" />
		</imprint>
	</monogr>
	<note>Word sense disambiguation to improve precision for ambiguous queries</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word sense discrimination in information retrieval: a spectral clustering-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chifu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. Proceedings of ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
	<note>Collobert and Weston2008] Ronan Collobert and Jason Weston</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<title level="m">SENSEVAL-2: Overview. Proceedings of SENSEVAL</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Performing word sense disambiguation at the border between unsupervised and knowledge-based techniques</title>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<editor>Florentina Hristea, Marius Popescu, and Monica Dumitrescu</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="67" to="86" />
			<date type="published" when="1998" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>WordNet: An Electronic Lexical Database</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ignacio Iacobacci, Mohammad Taher Pilehvar, and Roberto Navigli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Iacobacci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="897" to="907" />
		</imprint>
	</monogr>
	<note>Embeddings for Word Sense Disambiguation: An Evaluation Study</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorin</forename><surname>Istrail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliana</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">L</forename><surname>Florea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clark</forename><forename type="middle">M</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mobarry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lippert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagit</forename><surname>Walenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Shatkay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">R</forename><surname>Dew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">J</forename><surname>Flanigan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolanos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fasulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bjarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Halldorsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Hannenhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibu</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fu</forename><surname>Yooseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">R</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nusskern</surname></persName>
		</author>
		<editor>Bixiong Chris Shue, Xiangqun Holly Zheng, Fei Zhong, Arthur L. Delcher, Daniel H. Huson, Saul A. Kravitz, Laurent Mouchard, Knut Reinert, Karin A. Remington, Andrew G. Clark, Michael S. Waterman, Evan E. Eichler, Mark D</editor>
		<imprint/>
	</monogr>
	<note>Istrail et al.2004</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Whole Genome Shotgun Assembly and Comparison of Human Genome Assemblies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">W</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><forename type="middle">W</forename><surname>Hunkapiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Craig</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1916" to="1921" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Artificial Ants for Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Lafourcade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Guinand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Ants. From Collective Intelligence to Real-life Optimization and Beyond</title>
		<editor>N. Monmarch, F. Guinand, and P. Siarry</editor>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="455" to="492" />
		</imprint>
	</monogr>
	<note>Lafourcade and Guinand2010</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGDOC</title>
		<meeting>SIGDOC</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="24" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions and reverseals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cybernetics and Control Theory</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Senseval-3 English Lexical Sample Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Distributed Representations of Words and Phrases and their Compositionality. Proceedings of NIPS</title>
		<editor>Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean</editor>
		<imprint>
			<date type="published" when="2004-07" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
	<note>Proceedings of SENSEVAL-3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">WordNet: A Lexical Database for English</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coarse-grained English All-words Task. Proceedings of SemEval</title>
		<meeting><address><addrLine>Navigli, Kenneth C.</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1995-11" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
	<note>Litkowski, and Orin Hargraves</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Word sense disambiguation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Navigli</surname></persName>
		</author>
		<idno>10:1-10:69</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word sense disambiguation as a traveling salesman problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheol-Young</forename><surname>Kiem-Hieu Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="405" to="427" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Nguyen and Ock2013</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">NGS QC Toolkit: A Toolkit for Quality Control of Next Generation Sequencing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mukesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CICLing</title>
		<meeting>CICLing</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
	<note>Using Measures of Semantic Relatedness for Word Sense Disambiguation</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Studying the correlation between different word sense disambiguation methods and summarization effectiveness in biomedical texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="355" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">An algorithm for suffix stripping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">F</forename><surname>Porter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
			<publisher>Program</publisher>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ant Colony Algorithm for the Unsupervised Word Sense Disambiguation of Texts: Comparison and Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2012-12" />
			<biblScope unit="page" from="2389" to="2404" />
		</imprint>
	</monogr>
	<note>Andon Tchechmedjiev, and Herv? Blanchon</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Worst-case Complexity and Empirical Evaluation of Artificial Intelligence Methods for Unsupervised Word Sense Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of WASSA</title>
		<editor>Chiraag Sumanth and Diana Inkpen</editor>
		<meeting>WASSA</meeting>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="115" to="121" />
		</imprint>
	</monogr>
	<note>How much does word sense disambiguation help in sentiment analysis of micropost data</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comparing Similarity Measures for Original WSD Lesk Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gelbukh2009] Sulema</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelbukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in Computing Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="155" to="166" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Trends in word sense disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Vidhu Bhala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abirami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="159" to="171" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Vidhu Bhala and Abirami2014</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Next Generation Sequencing: From Basic Research to Diagnostics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voelkerding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Chemistry</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="41" to="47" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
