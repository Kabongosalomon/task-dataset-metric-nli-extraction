<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuchao</forename><surname>Sui</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
							<email>shaohua@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Geng</surname></persName>
							<email>gengxue@i2r.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research, A*STAR</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research, A*STAR</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Goh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of High Performance Computing</orgName>
								<address>
									<region>A*STAR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyuan</forename><surname>Zhu</surname></persName>
							<email>zhuh@i2r.a-star.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Infocomm Research, A*STAR</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CRAFT: Cross-Attentional Flow Transformer for Robust Optical Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optical flow estimation aims to find the 2D motion field by identifying corresponding pixels between two images. Despite the tremendous progress of deep learning-based optical flow methods, it remains a challenge to accurately estimate large displacements with motion blur. This is mainly because the correlation volume, the basis of pixel matching, is computed as the dot product of the convolutional features of the two images. The locality of convolutional features makes the computed correlations susceptible to various noises. On large displacements with motion blur, noisy correlations could cause severe errors in the estimated flow. To overcome this challenge, we propose a new architecture "CRoss-Attentional Flow Transformer" (CRAFT), aiming to revitalize the correlation volume computation. In CRAFT, a Semantic Smoothing Transformer layer transforms the features of one frame, making them more global and semantically stable. In addition, the dot-product correlations are replaced with transformer Cross-Frame Attention. This layer filters out feature noises through the Query and Key projections, and computes more accurate correlations. On Sintel (Final) and KITTI (foreground) benchmarks, CRAFT has achieved new state-of-the-art performance. Moreover, to test the robustness of different models on large motions, we designed an image shifting attack that shifts input images to generate large artificial motions. Under this attack, CRAFT performs much more robustly than two representative methods, RAFT and GMA. The code of CRAFT is is available at https://github.com/askerlee/craft.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Optical flow estimates pixel-wise 2D motions between two consecutive video frames by matching corresponding * Equal . pixels. It is a fundamental computer vision task with broad applications in action recognition <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38]</ref>, video segmentation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b45">46]</ref>, video frame interpolation <ref type="bibr" target="#b17">[18]</ref>, medical image registration <ref type="bibr" target="#b28">[29]</ref>, representation learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>, autonomous driving <ref type="bibr" target="#b26">[27]</ref>, and robot navigation <ref type="bibr" target="#b5">[6]</ref>. In recent years, deep learning based methods have advanced optical flow estimation tremendously <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>. Although newest methods are very accurate on benchmark data, under certain conditions, such as large displacements with motion blur <ref type="bibr" target="#b9">[10]</ref>, flow errors could still be large. It spurs us to dig deeper to identify the root causes.</p><p>Most of these methods perform optical flow estimation based on a correlation volume (also known as a cost volume), which stores the pairwise similarity between each pixel in Frame 1 and another in Frame 2. Given the correlation volume, subsequent modules try to match the two images, with an aim of maximizing the overall correlations between matched regions. The current paradigm computes the pairwise pixel similarity as the dot product of two convolutional feature vectors. Due to the locality and rigid weights of convolution, limited contextual information is incorporated into pixel features, and the computed correlations suffer from a high level of randomness, such that most of the high correlation values are spurious matches ( <ref type="figure">Figure  6</ref>). Noises in the correlations increase with noises in the input images, such as loss of texture, lighting variations and motion blur. Naturally, noisy correlations may lead to unsuccessful image matching and inaccurate output flow <ref type="figure" target="#fig_0">(Figure 1</ref>). This problem becomes more prominent when there are large displacements. Reducing noisy correlations can lead to substantial improvements of flow estimation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>Recent years have witnessed the widespread adoption of transformers for computer vision tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. An important advantage of Vision Transformers (ViTs) over convolution is that, transformer features better encode global context, by attending to pixels with dynamic weights based on their contents. For the optical flow task, useful information can propagate from clear areas to blurry areas, or from non-occluded areas to occluded areas <ref type="bibr" target="#b18">[19]</ref>, to improve the flow estimation of the latter. A recent study <ref type="bibr" target="#b29">[30]</ref> suggests that, ViTs are low-pass filters that do spatial smoothing of feature maps. Intuitively, after transformer self-attention, similar feature vectors take weighted sums of each other, smoothing out irregularities and high-frequency noises.</p><p>Inspired by the feature denoising property of ViTs, we propose "CRoss-Attentional Flow Transformer" (CRAFT), a novel architecture for optical flow estimation. With two novel components, CRAFT revitalizes the computation of the correlation volume. First, a semantic smoothing transformer layer fuses the features of one image, making them more global and semantically smoother. Second, a crossframe attention layer replaces the dot-product operator for correlation computation. It provides an additional level of feature filtering through the Query and Key projections, so that the computed correlations are more accurate.</p><p>We performed extensive evaluations of CRAFT on common optical flow benchmarks. On Sintel (Final) and KITTI (foreground) benchmarks, CRAFT has achieved new stateof-the-art (SOTA) performance. In addition, to test the robustness of different models on large motions, we designed an image shifting attack that shifts input images to generate large artificial motions. As the motion magnitude increases, CRAFT performs robustly, while two representative methods, RAFT and GMA, deteriorate severely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>FlowNet <ref type="bibr" target="#b7">[8]</ref> is a pioneering work that uses deep neural networks to do end-to-end optical flow learning. It inspires a series of deep learning methods, such as FlowNet2.0 <ref type="bibr" target="#b13">[14]</ref>, DCFlow <ref type="bibr" target="#b42">[43]</ref>, SpyNet <ref type="bibr" target="#b30">[31]</ref>, PWC-Net <ref type="bibr" target="#b36">[37]</ref>, MaskFlowNet <ref type="bibr" target="#b47">[48]</ref> LiteFlowNet3 <ref type="bibr" target="#b11">[12]</ref>, ScopeFlow <ref type="bibr" target="#b2">[3]</ref> and IRR <ref type="bibr" target="#b12">[13]</ref>. Most of these methods use a correlation volume as the basis of pixel matching.</p><p>RAFT <ref type="bibr" target="#b38">[39]</ref> is an important development of deep learning flow methods. By using multi-scale correlation volumes and iterative flow refinement, RAFT achieves good performance, and is the precursor of a few successive works, such as GMA <ref type="bibr" target="#b18">[19]</ref>, RAFT-Stereo <ref type="bibr" target="#b21">[22]</ref> and CRAFT. GMA <ref type="bibr" target="#b18">[19]</ref> is among the first works to incorporate transformer into optical flow methods. In the motion regression stage (cf. <ref type="figure" target="#fig_1">Figure  2)</ref>, it uses self-attention to propagate motion features from non-occluded areas to occluded areas, and helps estimate more accurate flow of occluded areas. It complements with the improvements of CRAFT on correlation volumes.</p><p>All the aforementioned methods compute correlations using dot-product or cosine similarity of convolutional features. Within this paradigm, some works improve the efficiency of the correlation volume, such as VCN <ref type="bibr" target="#b44">[45]</ref> and DICL <ref type="bibr" target="#b40">[41]</ref>. Similar to our objective, Separable Flow <ref type="bibr" target="#b46">[47]</ref> aims to improve the accuracy of the correlation volume, by decomposing the 4D correlation volume into two 3D volumes, for the uand v-directional flow regression, respectively. Separable Flow essentially imposes stronger inductive biases to obtain more accurate correlations than RAFT, as well as more accurate flow <ref type="bibr" target="#b0">1</ref> . In contrast, CRAFT improves correlation computation by using contextualized frame features and reducing feature noises.</p><p>Optical flow training requires large, expensive annotated datasets. SelFlow <ref type="bibr" target="#b22">[23]</ref> and Autoflow <ref type="bibr" target="#b35">[36]</ref> are two selfsupervised methods that generate synthetic annotations. SMURF <ref type="bibr" target="#b33">[34]</ref> integrates a set of techniques to do selfsupervised learning on unannotated video frames and has achieved promising results. <ref type="figure" target="#fig_1">Figure 2</ref> presents the architecture of CRAFT. It inherits the influential flow estimation pipeline of RAFT <ref type="bibr" target="#b38">[39]</ref>. Our main contribution is to revitalize the correlation volume computation part (the dashed green rectangle) with two novel components: the Semantic Smoothing Transformer on Frame-2 features, and a Cross-Frame Attention Layer to compute the correlation volume. These two components help suppress spurious correlations in the correlation volume, as visualized in <ref type="figure">Figure 6</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The CRAFT Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Semantic Smoothing Transformer</head><p>Given two consecutive images -Frame 1 and Frame 2as input, the first step of the flow pipeline is to extract frame features using a convolutional feature network.</p><p>To enhance the frame features with better global context, the Semantic Smoothing Transformer (or simply SSTrans) is used to transform the Frame-2 features.</p><p>To better accommodate diverse features, we adopt the Expanded Attention proposed in <ref type="bibr" target="#b20">[21]</ref> as the SSTrans, instead of the commonly used Multi-Head Attention (MHA) <ref type="bibr" target="#b39">[40]</ref>. Expanded Attention is a type of Mixture-of-Experts <ref type="bibr" target="#b32">[33]</ref> with higher capacities, and has demonstrated advantages over MHA for image segmentation tasks.</p><p>An Expanded Attention (EA) layer consists of N modes (sub-transformers), computing N sets of features, which are aggregated into one set using dynamic mode attention <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_0">X (k) out = Transformer (k) (X),<label>(1)</label></formula><formula xml:id="formula_1">B (k) = Linear (k) (X (k) out ),<label>(2)</label></formula><p>with k ? {1, ? ? ? , N },</p><formula xml:id="formula_2">G = softmax B (1) , ? ? ? , B (N ) ,<label>(3)</label></formula><formula xml:id="formula_3">EA(X) =G ? X (1) out , ? ? ? , X (N ) out ,<label>(4)</label></formula><p>where B (k) are mode attention scores, and the mode attention probabilities G are softmax of all B (k) along the mode dimension. The output features EA(X) are a linear combi-nation of all mode features.</p><p>To better preserve original frame features, we add a weighted skip connection with a learnable weight w 1 :</p><formula xml:id="formula_5">SSTrans(X) = w 1 X + (1 ? w 1 ) EA(X),<label>(6)</label></formula><p>To impose spatial biases, we found conventional positional embeddings do not form meaningful biases, and use a relative position bias <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref> instead. The bias is a matrix B ? R (2r+1)?(2r+1) , added to the computed attention, where r is the radius specifying the local range of the bias.</p><p>Specifically, suppose the original attention matrix is reshaped to a 4-dimensional tensor A ? R H?W ?H?W , where H, W are the height and width of the frame feature maps. For each pixel at i, j, where i ? {1, ? ? ? , H}, j ? {1, ? ? ? , W }, A(i, j) is a matrix, specifying the attention weights between pixel (i, j) with all the pixels in the same frame. The relative position bias B is added to the neighborhood of radius r of pixel (i, j):</p><formula xml:id="formula_6">A (i, j, i + x, j + y) = A(i, j, i + x, j + y) + B(x, y), if |x| ? r, |y| ? r A(i, j, i + x, j + y). otherwise<label>(7)</label></formula><p>In our implementation, we choose the number of modes to be 4, and the radius r of the relative position bias to be 7. <ref type="figure" target="#fig_2">Figure 3</ref> visualizes the learned relative position bias of CRAFT trained on Sintel. Two interesting patterns are observed: 1. The minimum bias value is around ?2 located at (0, 0), which means that, when computing the new features of a pixel (i, j), this bias term will reduce the weight of its own features by 2. Without this term, the attention weight of pixel (i, j) to itself will probably dominate the weights to other pixels, as a feature vector is most similar to itself. This term reduces the proportions of the old features of a pixel in the combined output features, effectively encouraging inflow of new information from other pixels.</p><p>2. The largest weights are 2 ? 3 pixels 2 away from the center pixel, meaning that features of these surrounding pixels are most often used to supplement the features of the central pixel.</p><p>These two observations are confirmed in <ref type="figure" target="#fig_9">Figure 8</ref>, where each query draws new features from a nearby area. Setting the position bias to 0 leads to performance degradation. It is tempting to apply transformers on the features of both frames. However, in our experiments, doing so leads to performance drop. Our hypothesis is based on the common belief that image matching heavily relies on high-frequency (HF) features that are local and structural <ref type="bibr" target="#b14">[15]</ref>. Meanwhile, there are abundant HF noises that pollute informative features and hinder matching. SSTrans serves as a low-pass filter to suppress HF noises <ref type="bibr" target="#b29">[30]</ref>, but at the same time, may reduce HF features and enhance low-frequency (LF) features. Hence, the model learns to trade off between the LF and HF components in Frame 2 for matching with Frame 1. After applying SSTrans on both frames, both frames contain less HF and more LF components. Matching them may yield many spurious correlations and hurt flow accuracy. This intuition is confirmed in <ref type="figure" target="#fig_8">Figure 7</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Cross-Frame Attention for Correlation Volume</head><p>In the current paradigm, a correlation volume is the basis of cross-frame pixel matching. After the frame features f 1 ? R H?W ?D and f 2 ? R H?W ?D are computed, the correlation volume is computed as a 4D tensor C ? R H?W ?H?W (dashed green rectangle in <ref type="figure" target="#fig_1">Figure 2</ref>).</p><p>Traditionally, the correlation volume is computed as the pairwise dot-product between f 1 and f 2 <ref type="bibr" target="#b38">[39]</ref>:</p><formula xml:id="formula_7">C(i, j, m, n) = 1 ? D f 1 (i, j) ? f 2 (m, n).<label>(8)</label></formula><p>Conceptually, the correlation volume is essentially Cross Attention <ref type="bibr" target="#b39">[40]</ref> in transformers, without feature transformation by the Query and Key projections. The query/key projections can be viewed as feature filters that separate out most informative features for correlations. In addition, to capture diverse correlations, we could use multiple query and key projections, as with Expanded Attention (EA) <ref type="bibr" target="#b20">[21]</ref>. Similar multi-faceted correlations are pursued in VCN <ref type="bibr" target="#b44">[45]</ref> with multiple channels. These benefits motivate us to replace the dot-product with a simplified EA:</p><formula xml:id="formula_8">C k (i, j, m, n) = 1 ? D (f 1 (i, j)Q k ) ? K k f 2 (m, n), (9) C(i, j, m, n) = K k=1 softmax(C k (i, j, m, n))C k (i, j, m, n),<label>(10)</label></formula><p>where Q k , K k are the k-th query and key projections, respectively; C k (i, j, m, n) is the correlation computed with the k-th mode. The softmax operator is taken along the K modes, and aggregates the K correlations. The EA here is simplified by removing the value projection and the feedfoward network. The weights of Q k and K k are tied, as the correlation between two frames is symmetric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global correlation normalization</head><p>Sometimes extreme values may appear in the correlation volume, which may disrupt the pixel matching. To match a pixel, intuitively the relative orders of the correlations with candidate pixels are more important than absolute correlation values. In this light, we perform layer normalization <ref type="bibr" target="#b0">[1]</ref> on the whole correlation volume to stabilize correlations. Empirically, this leads to slightly improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our experiments consist of six parts:</p><p>1. Standard evaluation. We evaluate different methods on Sintel <ref type="bibr" target="#b3">[4]</ref> and KITTI <ref type="bibr" target="#b27">[28]</ref>. On the two public leaderboards, CRAFT has achieved the state-of-the-art performance on both Sintel (final pass) and KITTI (foreground regions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Error distribution wrt. motion magnitudes.</head><p>To study the model behavior when the motion becomes larger, we calculate the flow error distribution wrt. different magnitudes of motions. CRAFT is significantly more accurate than other methods on large motions, and performs equally well on small motions. <ref type="bibr" target="#b2">3</ref>. Ablation studies. To analyze the impact of different components in CRAFT, i.e., the Semantic Smoothing transformer, the Cross-Frame Attention and the GMA module, we remove each of them and evaluate the ablated models on the KITTI-2015 benchmark. All these components show importance to the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Image Shifting attack.</head><p>To test the robustness of models, we manually create large motions by shifting the first frames. At very large shifts, RAFT and GMA deteriorate severely. CRAFT is significantly more robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Visualization of correlation volumes. We visualize the correlations between a query point in Frame 1 and all pixels in Frame 2, to intuitively learn the differences between the correlation volumes computed by different models. CRAFT has the fewest spurious correlations compared with RAFT and GMA.</p><p>6. Visualization of semantic smoothing transformer attention. To gain an intuitive idea how a pixel draws information from surrounding pixels through the SS transformer, we visualize the self-attention between a query point and all pixels in Frame 2.</p><p>Training Loss Following RAFT <ref type="bibr" target="#b38">[39]</ref>, the loss function we adopt is a weighted multi-iteration l 1 loss.</p><p>Training Schedule We follow the same optical flow training procedure <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> of first pretraining the models on FlyingChairs ("C") <ref type="bibr" target="#b7">[8]</ref> for 120k iterations (batch size = 8), then on FlyingThings ("T") <ref type="bibr" target="#b25">[26]</ref> for another 120k iterations with (batch size = 6). For Sintel evaluation, we fine-tune all models on a combination of FlyingThings, Sintel ("S") <ref type="bibr" target="#b3">[4]</ref>, KITTI 2015 ("K") <ref type="bibr" target="#b27">[28]</ref> and HD1K ("H") <ref type="bibr" target="#b19">[20]</ref> for 120k iterations (batch size = 6). For KITTI evaluation, we fine-tune all models on KITTI 2015 for 50k iterations (batch size = 6). Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>, we adopt the one-cycle learning rate scheduler with the same learning rates, in which 5 percent of the iterations are used for warm-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>The main evaluation metric, also used by the Sintel leaderboard 3 , is the average end-point error (AEPE), which is the average pixelwise flow error, measured by number of pixels. The KITTI leaderboard <ref type="bibr" target="#b3">4</ref> uses the Fl-fg (%) and Fl-All (%) metrics, which refer to the percentage of outliers (pixels whose end-point error is &gt; 3 pixels or 5% of the ground truth flow magnitude), averaged over foreground regions and all pixels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Standard Evaluation</head><p>Seven recent methods are compared, most of which are selected from the top-performing methods on the Sintel and 3 http://sintel.is.tue.mpg.de/quant?metric_id= 0&amp;selected_pass=0 4 http : / / www . cvlibs . net / datasets / kitti / eval _ scene_flow.php?benchmark=flow KITTI leaderboards:</p><p>? RAFT <ref type="bibr" target="#b38">[39]</ref>: an important recent methods, and was previous SOTA before being surpassed by GMA.</p><p>? RAFT-A <ref type="bibr" target="#b35">[36]</ref> uses the synthesized AutoFlow dataset (instead of "C+T") to pretrain RAFT, followed by the standard fine-tuning steps.</p><p>? Perceiver-IO <ref type="bibr" target="#b15">[16]</ref> is a general architecture not specifically designed for optical flow estimation. It is pretrained on Autoflow, the same as RAFT-A. The performance on the test sets is not reported in their paper.</p><p>? RFPM <ref type="bibr" target="#b24">[25]</ref> replaces the downsampling layers of RAFT to improve the flow estimation on fine details. The performance under "C+T / Autoflow" training is not reported in their paper.</p><p>? Separable Flow <ref type="bibr" target="#b46">[47]</ref> decomposes the 4D correlation volume as two 3D volumes for the u and v directions.</p><p>? GMA <ref type="bibr" target="#b18">[19]</ref>: a recent method that enhances RAFT with a Global Motion Aggregation module to better estimate the motions of occluded pixels.</p><p>? CRAFT: with 4 modes in expanded attention layers. <ref type="table">Table 1</ref> summarizes the evaluation results of the seven methods on Sintel and KITTI. The results on the training sets (in parentheses, left side of the table) can hardly reflect how well the models generalize to new data, and are only listed for reference. The results on the test sets are evaluated on held-out data by the Sintel and KITTI servers and obtained from their leaderboards, and better reflect model performance. Although performing closely to the other methods on the training sets, CRAFT shows clear advantages on the test sets, and outperforms all other optical flow methods <ref type="bibr" target="#b4">5</ref> on Sintel (Final) and KITTI Fl-fg (i.e., fewest foreground outliers).</p><p>We argue that these two performance metrics (AEPE on Sintel Final pass, and Fl-fg on KITTI) has important practical implications. For real world performance, the results on Sintel (Final) are more indicative than on Sintel (Clean), as the final-pass images more closely resemble real world videos, with various lighting variation, shadows and motion blur. In addition, as the foreground objects in KITTI are usually cars, pedestrians, etc., which naturally are more important than the background. Hence, smaller pixel errors in foreground regions as measured by Fl-fg, probably imply greater practical benefits than smaller errors in background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Error Distribution wrt. Motion Magnitudes</head><p>To analyze the behavior of different models when facing varying magnitudes of motions, we divide the pixels into where not otherwise stated, as well as the Fl-fg and Fl-all metrics for the KITTI dataset, which are the percentages of optical flow outliers (pixels with significant flow errors), calculated on the foreground regions and all pixels, respectively. "C + T / Autoflow" refers to methods that are pretrained either on the combined Chairs and Things datasets, or on the Autoflow dataset <ref type="bibr" target="#b35">[36]</ref>. "S/K + H" refers to methods that are fine-tuned on the Sintel, KITTI and HD1K datasets. All results on Sintel (test) are generated with the "warm-start" strategy <ref type="bibr" target="#b38">[39]</ref>. ? Results are ranked as the top-1 (as of November 2021) on the two public leaderboards, which include many other methods not listed here. five subsets according to their groundtruth motion magnitudes, and evaluate the AEPE within each subset. As the validation/test splits of Sintel and KITTI are unavailable, the evaluation is done on the validation split of FlyingThings, Clean pass and Final pass, respectively. Three models, RAFT, GMA and CRAFT are evaluated. All the models are trained on "C+T". <ref type="table">Table 2</ref> presents the AEPE on different magnitudes of motions. When the motion is &lt; 20 pixels, CRAFT performs on par with GMA. On large motions that are &gt; 30 pixels, CRAFT makes 10?15% less AEPE than RAFT and GMA. CRAFT has three important components: the Semantic Smoothing transformer ("SS trans"), the Cross-Frame Attention ("CFA"), and the GMA module. To study their individual contributions, in each turn we remove one of them, train the ablated models with the standard schedule, and evaluate on the KITTI-2015 leaderboard. <ref type="table">Table 3</ref> shows that all the three components make important contributions to the overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image Shifting Attack</head><p>Typically, most pixels in standard benchmark images are with small motions, and large motions only appear in local areas. As a result, when the model makes big errors on large local motions, as these errors are local, they may  be easily corrected by considering the contextual small motions, so that the final flow may still be accurate. Thereby, the fragility on large motions is hidden under small AEPE.</p><p>To fully reveal the model robustness on large motions, we design an image shifting attack, i.e., create large motions by shifting one image along the u, v plane. Local corrections would hardly work on such image pairs, as all the pixels will have large displacements.</p><p>Specifically, we shift the first frame I 1 by (?u, ?v) towards the bottom right, getting a new image shift u,v (I 1 ). The new image is truncated at the original image boundary.</p><p>Suppose a model M estimates the flow F 0 accurately on the original image pairs: F 0 = M (I 1 , I 2 ) ? F gt , where F gt is the groundtruth flow. We test M on the shifted pairs and get new flow: F 1 = M (shift u,v (I 1 ), I 2 ). Then we unshift F 1 and get F 2 . If the model is robust against the shift, it can be proven that the following equation should hold:</p><formula xml:id="formula_10">F 2 ? shift u,v (F 0 )?(?u, ?v) ? shift u,v (F gt )?(?u, ?v).</formula><p>(11) <ref type="figure" target="#fig_5">Figure 4</ref> presents an example of the shifting attack. The two frames are from Slow Flow <ref type="bibr" target="#b16">[17]</ref>, a dataset with motion blur (flow magnitude=100, blur duration=3). After downsampling the original images from (1280, 720) to (640, 360), the first image is shifted by (220, 110). RAFT and GMA completely fail to estimate the flow, with huge AEPE. In contrast, CRAFT still yields accurate estimation. <ref type="figure" target="#fig_6">Figure 5</ref> presents the quantitative evaluations of RAFT, GMA and CRAFT under the shifting attack. The models are trained with "C+T+S+K+H", and evaluated on the training split of Sintel (Clean) and Sintel (Final), as well as on Slow Flow (flow magnitude=100, blur duration=3), under varying (?u, ?v). In our experiments, the horizontal shift ?u ? [100, 300], and the vertical shift ?v . = 1 2 ?u. When ?u ? 160, all models perform well with AEPE &lt; 8. When ?u goes beyond 160, RAFT and GMA quickly deteriorate; in contrast, CRAFT performs much more robustly with significantly smaller AEPE. Possibly due to motion blur, the AEPE of RAFT and GMA on Slow Flow is 80 ? 100 pixels larger than on Sintel, while the AEPE of CRAFT on Slow Flow is only 35 pixels larger, showing its robustness against motion blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Visualization of Correlation Volumes</head><p>The main reason that CRAFT performs more robustly is probably that the computed correlation volumes contains much fewer spurious correlations, thanks to the SS transformer and the cross-frame attention layer.</p><p>To gain an intuitive understanding of the differences between the correlation volumes computed by different models, we visualize the correlations between a query point in Frame 1 and all pixels in Frame 2. The query point is marked as a small red square in Frame 1 (projected to the small green square in Frame 2). It moves to the small red square in Frame 2. The dashed green rectangle is a 256 ? 256-pixel square centered at the query point, truncated at the image boundary. It encloses the field of view (FoV) of the model at the first iteration of flow estimation. Only correlations within the FoV are shown. <ref type="figure">Figure 6</ref> visualizes the correlation volumes on two frames from Sintel (Final), which is rendered with shadows and motion blur. Bright blobs in the heatmaps are high correlations, and those not at the groundtruth location (red square) are spurious and may be targets for mismatch. The correlation volumes 6 computed by RAFT and GMA contain many more spurious correlations than CRAFT. If removing the SS transformer (the cross-frame attention layer remains), CRAFT yields more noisy correlations, but they are still fewer than RAFT and GMA, suggesting that the cross-frame attention layer also helps denoising.</p><p>In addition, as stated in Section 3.1, we tested to apply the SS transformer to both Frame 1 and Frame 2 (referred to as "Double SSTrans"), and observed degraded performance. To shed light on why this happens, <ref type="figure" target="#fig_8">Figure 7</ref> visualizes the computed correlations with Double SSTrans. Compared with the standard "Single SSTrans", many more spurious correlations are observed. This may explain the degradation of the flow accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization of the Self-Attention of Semantic</head><p>Smoothing Transformer <ref type="figure" target="#fig_9">Figure 8</ref> visualizes the SS transformer self-attention weights on three queries in Frame 2. For each query (the small red square), its attention weights with all pixels in the   <ref type="figure">Figure 6</ref>. Heatmaps of the correlations between Frame 2 and a query point in Frame 1 (the small red square), on Sintel test set (Final pass). The small green square in Frame 2 indicates the original position of the query in Frame 1. As the images are blurry with coarser details, RAFT and GMA make many noisy correlations. In contrast, CRAFT has significantly fewer noisy correlations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single SSTrans No SSTrans</head><p>Double SSTrans same image are displayed as a heatmap. The highest attention areas are somewhere around the query points (at different relative directions). We guess that these areas may provide texture or contextual information absent at the queries. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We present a novel optical flow estimation method Cross-Attentional Flow Transformer (CRAFT). It revitalizes the computation of correlation volumes with two novel components: Semantic-Smoothing Transformer and Cross-Frame Attention. They help compute more accurate correlation volumes by spatially smoothing feature semantics and filtering out feature noises. CRAFT has achieved new state-of-the-art performance on a few metrics, and is especially robust on large displacements with motion blur. <ref type="table">Table 4</ref> presents the number of parameters and FLOPs of RAFT, GMA and CRAFT. The "Ratio" columns take RAFT as the base. FLOPs are measured while inference on Sintel images (1024x436 pixels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Size and FLOPs</head><p>Similar to the SS Transformer and the Cross-Frame Attention, the GMA module in CRAFT is implemented with Expanded Attention <ref type="bibr" target="#b20">[21]</ref>, which contains multiple modes. In <ref type="table">Table 4</ref>, two CRAFT models with different numbers of GMA modes m are presented. These two models have very similar overall performance ( <ref type="table">Table 6</ref>). When m = 2, the model size is only 7% and 19% larger than GMA and RAFT, respectively, and thus the performance gain is not to be explained away as merely having more parameters. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the GMA module dominates the total overhead of the three transformer modules. This drastic difference is because the GMA module is applied in every iteration of the iterative motion refinement <ref type="bibr" target="#b38">[39]</ref>, while the other two are only applied once. In this regard, for the FLOPs computation above, we fixed SS Trans and CFA to have 4 modes, and only varied the number of modes of the GMA module. SS Trans CFA GMA Remaining FLOPs (G) 66 6.6 317 405 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image Shifting as Augmentation</head><p>To explore how manually shifting some training images impacts the model performance, we take it as an extra augmentation, namely "ShiftAug", for the training of optical flow models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Mild ShiftAug</head><p>In the mild ShiftAug training, 10% of the training batches are shifted by (?u, ?v) sampled from two Laplacian distributions with scales 16 and 10 (the mean value of a Laplacian distribution is the scale 7 ), respectively. 7 https://en.wikipedia.org/wiki/Laplace distribution We trained two GMA models and two CRAFT m=2 models, with and without ShiftAug, respectively. They are denoted as GMA, GMA-shift, CRAFT and CRAFT-shift. <ref type="table">Table 6</ref> presents the performance scores of the two CRAFT m=2 models (with or without ShiftAug), along with the standard CRAFT m=4 (without ShiftAug), on Sintel and KITTI leaderboards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.1 Leaderboard Evaluation</head><p>Without ShiftAug, when m reduces from 4 to 2, the performance on large motions ("s40+" for Sintel, and "Fl-fg" for KITTI) degrades slightly. As expected, ShiftAug recovers the model performance on large motions on Sintel (Clean) and KITTI. However, it is surprising to see that with ShiftAug, the performance on large motions on Sintel (Final) degrades slightly.</p><p>Due to the restricted frequency of submissions to the leaderboards, we were unable to evaluate more model settings before the camera ready deadline, such as CRAFT m=4 and GMA trained with ShiftAug.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1.2 Performance under Image Shifting Attack</head><p>We evaluated the 4 models under the image shifting attack, to study whether ShiftAug makes models more robust against it. <ref type="figure" target="#fig_10">Figure 9</ref> presents the performance of of the four models, evaluated on the training split of Sintel (Clean) and Sintel (Final), under varying (?u, ?v). The horizontal shift ?u ? [100, 300], and the vertical shift ?v . = 1 2 ?u. It can be seen that, under the image shifting attack, GMA-shift and CRAFT-shift follow similar performance curves as GMA and CRAFT, respectively. On both Sintel (Clean) and Sintel (Final), CRAFT-shift yields significant smaller AEPE (20-50%) on very large shifts (? 200 pixels). However, on Sintel (Final), GMA-shift yields almost identical AEPE on very large shifts as GMA. Bewilderingly, on Sintel (Clean), the AEPE of GMA-shift becomes significantly higher on very large shifts than GMA. ShiftAug helps both GMA and CRAFT reduce AEPE on medium-to-large shifts (120-180 pixels). Without ShiftAug, the maximum AEPE in this range is 3.4 pixels.</p><p>Based on the observations above, we conclude that mild ShiftAug does not help make GMA more robust against very large image shifts. On the other hand, mild Shif-tAug already helps make CRAFT significantly more robust against very large image shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Aggressive ShiftAug against Image Shifting</head><p>In this section, we aim to test the effects of more aggressive ShiftAug, i.e., with a probability of 10%, shift frame 1 by (?x, ?y), where ?x is uniformly drawn from [?320, 320], and ?y is uniformly drawn from [?160, 160].  <ref type="table">Table 6</ref>. Additional results on Sintel and KITTI 2015 leaderboards. We report the average end-point error (AEPE) for Sintel, and the Fl-bg, Fl-fg and Fl-all metrics for KITTI, which are the percentages of optical flow outliers (pixels with significant flow errors), calculated on the foreground regions and all pixels, respectively. To show the performance on large motions, we present the AEPE on s10-40 and s40+, i.e., pixels whose velocities are within <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> pixels, and &gt; 40 pixels, respectively.  We fine-tuned CRAFT and GMA on Things models (pretrained on C+T) with such random shifting. Then we evaluated the two models on Chairs 8 with varying degree of shifting. GMA-shift becomes more robust against image shifting attack, but CRAFT-shift still outperforms GMA-shift with a large margin on larger shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Innate and Acquired Robustness</head><p>Based on the observations in Section B.1 and B.2, we hypothesize that there are two types of model robustness innate and acquired robustness. The latter is learned through augmentation, but stronger innate robustness of CRAFT makes it robust to variations beyond the training data. Hence, CRAFT is likely more robust against other unseen image variations as well, e.g., rotations, lighting variations and motion blur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Iterative Motion Refinement on Shifted</head><p>Slow-Flow Images <ref type="figure" target="#fig_0">Figure 10</ref> presents the flow fields estimated at different iterations by GMA and CRAFT (both without shift augmentation), respectively, on the same Slow Flow <ref type="bibr" target="#b16">[17]</ref> image pair as in <ref type="figure" target="#fig_5">Figure 4</ref> of the main text. It partially explains why the AEPE (average end-point error) of GMA is huge when the image shift is large ( <ref type="figure" target="#fig_6">Figure 5</ref> in the main text).</p><p>The flow field is estimated through iterative refinement of N = 12 iterations in both GMA and CRAFT. At the i + 1-th iteration, it uses the flow estimated at the i-th iteration as initialization, and attempts to estimate a more accurate flow field. This is effective when the flow errors are confined in very small areas, in which cases the model can correct the errors by considering the estimated motions of the surrounding pixels (which are largely accurate). How-   <ref type="figure" target="#fig_0">Figure 10</ref>. The iterative refinement of optical flow on the shifted Slow Flow image pair ( <ref type="figure" target="#fig_5">Figure 4</ref> in the main text), by GMA and CRAFT, respectively. As in the first iteration, GMA makes excessively huge errors, it is unable to recover with more iterations. CRAFT recovers from smaller initial errors and yields an accurate flow field eventually. ever, if large errors appear in broader areas, the model may fail to recover from the errors with more iterations. Therefore, a relatively small AEPE at the first iteration is crucial for achieving a small AEPE after all iterations. In <ref type="figure" target="#fig_0">Figure  10</ref>, the flow estimated by GMA at the first iteration has huge errors (AEPE = 253.8), and thus even with more iterations, GMA is unable to recover. In contrast, the flow estimated by CRAFT at the first iteration has a much smaller AEPE = 113.8, and CRAFT quickly corrects the errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization of Correlation Volumes on Slow Flow</head><p>Figures 11-13 present more visualizations of the correlation volume. <ref type="figure" target="#fig_0">Figure 11</ref> visualizes the correlation volume of RAFT, GMA and CRAFT on the shifted image pair from Slow Flow. This is the same example as in <ref type="figure" target="#fig_0">Figure 10</ref>, and <ref type="figure" target="#fig_5">Figure 4</ref> in the main text. It can be seen that, GMA has the most spurious high correlations, and CRAFT has the least. CRAFT ranked the 1st and 5th places on Sintel (Final pass), Sintel (Clean pass), respectively. As Sintel (Final pass) images contain more light variations, shadows, motion blurs, etc. that are common in real world, we argue that the performance on Sintel (Final) better reflects the performance of a model on real-world images. Evidence has been presented in the Sintel paper ( <ref type="figure" target="#fig_6">Figure 5</ref>, <ref type="bibr" target="#b3">[4]</ref>) that Sintel (Final) has similar image and motion statistics as other real-world datasets, including Lookalikes <ref type="bibr" target="#b3">[4]</ref> and Middlebury <ref type="bibr" target="#b1">[2]</ref>.</p><p>On the KITTI flow-2015 leaderboard, a few methods among the top are scene flow methods (marked with strikethrough text) that take two stereo pairs of images as input (cf. two monocular images of optical flow), and thus are not comparable with optical flow methods. Among the  top optical flow methods, CRAFT ranks 5th. In particular, it achieves the highest accuracy on foreground regions, measured as the smallest Fl-fg (percentage of flow outliers 9 in the foreground regions). On Fl-all (percentage of flow outliers in both foreground and background regions), MixSup <ref type="bibr" target="#b8">9</ref> Pixels whose end-point error is &gt; 3 pixels or 5% of the ground truth flow magnitude. ranks as the top-1 optical flow method, but its training and implementation details are missing for further analysis and comparison with CRAFT. Separable Flow <ref type="bibr" target="#b46">[47]</ref> and RFPM <ref type="bibr" target="#b24">[25]</ref> have significantly worse performance on foreground regions. RAFT-A uses Autoflow <ref type="bibr" target="#b35">[36]</ref> as the pretraining data, and thus not directly comparable with CRAFT. Autoflow explore a new way to synthesise training data, which is orthogonal to our method or other recent architectures. It is worth noting that, the foreground objects in KITTI are usually cars, pedestrians, etc., which naturally are more important than the background. Thus, smaller Fl-fg are probably more important for practical applications than smaller Fl-bg or Fl-all.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The optical flow fields estimated by RAFT, GMA and CRAFT on two frames from Sintel test set, in which a dragon is chasing a chicken. On the Clean pass, all the three methods perform similarly. On the Final pass, as the area enclosed in the red rectangle has large motions (80 ? 100 pixels) with motion blur, RAFT and GMA only identified part of the motions. Nonetheless, CRAFT still performs well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>CRAFT architecture. In the correlation volume computation part (the dashed green rectangle), two novel components are highlighted as boxes with red borders: the Semantic Smoothing Transformer fuses and smooths the Frame-2 features, and the Cross-Frame Attention layer computes the correlation volume. The GMA module at the bottom is a Global Motion Aggregation module<ref type="bibr" target="#b18">[19]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Learned relative positional bias with radius r = 7. Two interesting patterns can be observed, as detailed below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Frame 1</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Flows fields estimated by RAFT, GMA and CRAFT on two frames from the Slow Flow dataset. (?u, ?v) = (220, 110) pixels. RAFT and GMA failed with huge AEPE. CRAFT still yielded accurate estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>The AEPE of RAFT, GMA and CRAFT change differently with the magnitude of image shifts. (a)-(c) are on Sintel (Clean), Sintel (Final) and Slow Flow, respectively. The horizontal shift ?u change from 100 to 300, and the vertical shift ?v . = 1 2 ?u. When ?u goes beyond 160, RAFT and GMA quickly deteriorate, and CRAFT performs much more robustly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>The correlations between Frame 2 and a query point in Frame 1, on Sintel test set (Final pass). Images are cropped. The standard CRAFT setting ("Single SSTrans") has fewest noisy correlations. "Double SSTrans" yields many more noisy correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Heatmaps of the SS transformer self-attention, between a query point (a red rectangle) and all pixels in the same image. The most intense areas are where the query points pay the highest attention and draw features to enrich themselves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .</head><label>9</label><figDesc>The AEPE of RAFT, GMA and CRAFT change differently with the magnitude of image shifts. (a)-(c) are on Sintel (Clean), Sintel (Final) and Slow Flow, respectively. The horizontal shift ?u change from 100 to 300, and the vertical shift ?v . = 1 2 ?u. When ?u goes beyond 160, RAFT and GMA quickly deteriorate, and CRAFT performs much more robustly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figures 12 and 13</head><label>13</label><figDesc>visualize the correlation volumes with two different query points in Frame 1, on the rider's body, and on the horse's tail, respectively, on the original image pair from Slow Flow. Similar distributions of spurious high correlations are observed. Among the four models, CRAFT (with SS trans) always has the least spurious high correlations, showing that it is able to greatly suppress spurious correlations and compute a more accurate correlation volume, which may explain its robustness demonstrated in Figure 10.E. Screenshots of Sintel and KITTI LeaderboardsFigures 14-16 are the screenshots of the Sintel (Final), Sintel (Clean) and the KITTI-2015 optical flow leaderboards, which were taken near the CVPR'2022 submission deadline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Heatmaps of the correlation matrices between Frame 2 and a query point on the rider's body in a shifted Frame 1, on the Slow Flow dataset. At the presence of motion blur, CRAFT has significantly fewer noisy correlations than RAFT and GMA, showing its robustness. Removing SS transformer results in more noisy correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 .</head><label>12</label><figDesc>Heatmaps of the correlation matrices between Frame 2 and a query point on the rider's body in Frame 1, on Slow Flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 .</head><label>13</label><figDesc>Heatmaps of the correlation matrices between Frame 2 and a query point on the horse's tail in Frame 1, on the Slow Flow dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 .</head><label>14</label><figDesc>Screenshot of Sintel (Final) leaderboard, taken near the CVPR'2022 submission deadline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 .</head><label>15</label><figDesc>Screenshot of Sintel (Clean) leaderboard, taken near the CVPR'2022 submission deadline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 16 .</head><label>16</label><figDesc>Screenshot of KITT-2015 leaderboard, taken near the CVPR'2022 submission deadline. Five scene flow methods are marked with strikethrough text, as they are not comparable to optical flow methods. There remain the top 19 optical flow methods. "CRAFTnoca" and "CRAFT-nof2" are the ablated models of removing the Cross-Frame Attention, and the Semantic Smoothing Transformer from CRAFT, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>FLOPs (on Sintel images) of different components in CRAFTm=4. SS Trans, CFA and GMA all have 4 modes.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>AEPE on Chairs under image shifting attack.</figDesc><table><row><cell>?x (px)</cell><cell>0</cell><cell>240</cell><cell>280</cell><cell>320</cell><cell>360</cell><cell>400</cell></row><row><cell>GMA</cell><cell cols="6">1.19 120.2 270.2 362.7 427.9 478.1</cell></row><row><cell>CRAFT</cell><cell cols="6">1.11 10.8 51.8 138.4 236.5 336.9</cell></row><row><cell>GMA-shift</cell><cell cols="6">1.21 1.83 3.11 9.33 47.5 124.3</cell></row><row><cell cols="7">CRAFT-shift 1.13 1.59 1.74 2.15 6.73 32.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Unfortunately, we could not compare Separable Flow with CRAFT wrt. the correlation volume accuracy, as their source code is unavailable.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here "pixels" mean points in feature maps, which correspond to ?8 pixels in the input image.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">As of November 2021.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">All matrices have been normalized into [0, 1] to make sure the pattern differences are not caused by range discrepancy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">We intentionally chose Chairs for evaluation, which has a slight domain gap with Things (used for training), to see how the robustness generalizes.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported by A*STAR under its Career Development Fund (Grant Nos. C210812035 and C210112016), and its Human-Robot Collaborative AI for Advanced Manufacturing and Engineering programme (Grant No. A18A2b0046).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scopeflow: Dynamic scene scoping for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviram</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garrett</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<editor>Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi Sato, and Cordelia Schmid, editors, ECCV</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Enhancing optical-flow-based control by learning visual appearance cues for flying robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C H E</forename><surname>De Croon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">De</forename><surname>Wagter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="41" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Position information in transformers: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11090,2021.3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>Guney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<title level="m">Workshop results and summary</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ECCV Workshops</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">LiteFlowNet3: Resolving Correspondence Ambiguity for More Accurate Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative residual refinement for joint optical flow and occlusion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhwa</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonmoy</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margret</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Robust multi-sensor image alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Andrew Zisserman, Oriol Vinyals, and Jo?o Carreira. Perceiver IO: A general architecture for structured inputs &amp; outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<idno>arxiv:2107.14795</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Slow flow: Exploiting high-speed cameras for accurate and diverse optical flow reference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Janai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Super slomo: High quality estimation of multiple intermediate frames for video interpolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to estimate hidden motions with global motion aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The hci benchmark suite: Stereo and flow ground truth with uncertainties for urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Kondermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Krispin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Andrulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Gussefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Rahimimoghaddam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claus</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Jahne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Medical image segmentation using squeeze-and-expansion transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuchao</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rick</forename><surname>Goh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Raft-stereo: Multilevel recurrent field transforms for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lahav</forename><surname>Lipson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Selflow: Self-supervised learning of optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwin</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Detail preserving residual feature pyramid modules for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Libo</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Lang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10990</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint 3d estimation of vehicles and scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Heipke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ISPRS Workshop on Image Sequence Analysis (ISA)</title>
		<meeting>of the ISPRS Workshop on Image Sequence Analysis (ISA)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flowreg: Fast deformable unsupervised medical image registration using optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergiu</forename><surname>Mocanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">R</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">April</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning for Biomedical Imaging</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How do vision transformers work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namuk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songkuk</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2022</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the integration of optical flow and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatma</forename><surname>G?ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixtureof-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Smurf: Self-teaching multi-frame unsupervised raft with full-image warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayvaci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jonschkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Actor-centric relation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Autoflow: Learning a better training set for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">PWC-Net: CNNs for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Optical flow guided feature: A fast and robust motion representation for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanghui</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiser</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Displacement-invariant matching cost learning for accurate optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised representation learning from flow equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Accurate Optical Flow via Direct Cost Volume Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Self-supervised video object segmentation by motion grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charig</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hala</forename><surname>Lamdouar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Volumetric correspondence networks for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning to segment rigid motions from two frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengshan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1266" to="1275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Separable flow: Learning motion cost volumes for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feihu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><forename type="middle">J</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Victor Adrian Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Maskflownet: Asymmetric feature matching with learnable occlusion mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
