<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
							<email>yuangong@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
							<email>glass@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">MIT Computer Science</orgName>
								<orgName type="laboratory">Artificial Intelligence Laboratory</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AST: Audio Spectrogram Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: audio classification</term>
					<term>self-attention</term>
					<term>Transformer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the past decade, convolutional neural networks (CNNs)   have been widely adopted as the main building block for endto-end audio classification models, which aim to learn a direct mapping from audio spectrograms to corresponding labels. To better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN, forming a CNN-attention hybrid model. However, it is unclear whether the reliance on a CNN is necessary, and if neural networks purely based on attention are sufficient to obtain good performance in audio classification. In this paper, we answer the question by introducing the Audio Spectrogram Transformer (AST), the first convolution-free, purely attention-based model for audio classification. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the advent of deep neural networks, over the last decade audio classification research has moved from models based on hand-crafted features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> to end-to-end models that directly map audio spectrograms to corresponding labels <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Specifically, convolutional neural networks (CNNs) <ref type="bibr" target="#b5">[6]</ref> have been widely used to learn representations from raw spectrograms for end-to-end modeling, as the inductive biases inherent to CNNs such as spatial locality and translation equivariance are believed to be helpful. In order to better capture long-range global context, a recent trend is to add a self-attention mechanism on top of the CNN. Such CNN-attention hybrid models have achieved state-of-the-art (SOTA) results for many audio classification tasks such as audio event classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>, speech command recognition <ref type="bibr" target="#b8">[9]</ref>, and emotion recognition <ref type="bibr" target="#b9">[10]</ref>. However, motivated by the success of purely attention-based models in the vision domain <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, it is reasonable to ask whether a CNN is still essential for audio classification.</p><p>To answer the question, we introduce the Audio Spectrogram Transformer (AST), a convolution-free, purely attentionbased model that is directly applied to an audio spectrogram and can capture long-range global context even in the lowest layers. Additionally, we propose an approach for transferring knowledge from the Vision Transformer (ViT) <ref type="bibr" target="#b11">[12]</ref> pretrained on ImageNet <ref type="bibr" target="#b13">[14]</ref> to AST, which can significantly improve the performance. The advantages of AST are threefold. First, AST has superior performance: we evaluate AST on a variety of audio classification tasks and datasets including AudioSet <ref type="bibr" target="#b14">[15]</ref>, ESC-50 <ref type="bibr" target="#b15">[16]</ref> and Speech Commands <ref type="bibr" target="#b16">[17]</ref>. AST outperforms state-of-the-art systems on all these datasets. Second, AST naturally supports variable-length inputs and can be applied to different tasks without any change of architecture. Specifically, the Code at https://github.com/YuanGongND/ast.  <ref type="figure">Figure 1</ref>: The proposed audio spectrogram transformer (AST) architecture. The 2D audio spectrogram is split into a sequence of 16?16 patches with overlap, and then linearly projected to a sequence of 1-D patch embeddings. Each patch embedding is added with a learnable positional embedding. An additional classification token is prepended to the sequence. The output embedding is input to a Transformer, and the output of the classification token is used for classification with a linear layer.</p><p>models we use for all aforementioned tasks have the same architecture while the input lengths vary from 1 sec. (Speech Commands) to 10 sec. (AudioSet). In contrast, CNN-based models typically require architecture tuning to obtain optimal performance for different tasks. Third, comparing with SOTA CNNattention hybrid models, AST features a simpler architecture with fewer parameters, and converges faster during training. To the best of our knowledge, AST is the first purely attentionbased audio classification model. Related Work The proposed Audio Spectrogram Transformer, as the name suggests, is based on the Transformer architecture <ref type="bibr" target="#b17">[18]</ref>, which was originally proposed for natural language processing tasks. Recently, the Transformer has also been adapted for audio processing, but is typically used in conjunction with a CNN <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. In <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, the authors stack a Transformer on top of a CNN, while in <ref type="bibr" target="#b20">[21]</ref>, the authors combine a Transformer and a CNN in each model block. Other efforts combine CNNs with simpler attention modules <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. The proposed AST differs from these studies in that it is convolution-free and purely based on attention mechanisms. The closest work to ours is the Vision Transformer (ViT) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, which is a Transformer architecture for vision tasks. AST and ViT have similar architectures but ViT has only been applied to fixed-dimensional inputs (images) while AST can process variable-length audio inputs. In addition, we propose an approach to transfer knowledge from Ima-geNet pretrained ViT to AST. We also conduct extensive experiments to show the design choice of AST on audio tasks.  <ref type="figure">Figure 1</ref> illustrates the proposed Audio Spectrogram Transformer (AST) architecture. First, the input audio waveform of t seconds is converted into a sequence of 128-dimensional log Mel filterbank (fbank) features computed with a 25ms Hamming window every 10ms. This results in a 128 ? 100t spectrogram as input to the AST. We then split the spectrogram into a sequence of N 16?16 patches with an overlap of 6 in both time and frequency dimension, where N = 12 (100t ? 16)/10 is the number of patches and the effective input sequence length for the Transformer. We flatten each 16?16 patch to a 1D patch embedding of size 768 using a linear projection layer. We refer to this linear projection layer as the patch embedding layer. Since the Transformer architecture does not capture the input order information and the patch sequence is also not in temporal order, we add a trainable positional embedding (also of size 768) to each patch embedding to allow the model to capture the spatial structure of the 2D audio spectrogram.</p><p>Similar to <ref type="bibr" target="#b21">[22]</ref>, we append a [CLS] token at the beginning of the sequence. The resulting sequence is then input to the Transformer. A Transformer consists of several encoder and decoder layers. Since AST is designed for classification tasks, we only use the encoder of the Transformer. Intentionally, we use the original Transformer encoder <ref type="bibr" target="#b17">[18]</ref> architecture without modification. The advantages of this simple setup are 1) the standard Transformer architecture is easy to implement and reproduce as it is off-the-shelf in TensorFlow and PyTorch, and 2) we intend to apply transfer learning for AST, and a standard architecture makes transfer learning easier. Specifically, the Transformer encoder we use has an embedding dimension of 768, 12 layers, and 12 heads, which are the same as those in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11]</ref>. The Transformer encoder's output of the [CLS] token serves as the audio spectrogram representation. A linear layer with sigmoid activation maps the audio spectrogram representation to labels for classification.</p><p>Strictly speaking, the patch embedding layer can be viewed as a single convolution layer with a large kernel and stride size, and the projection layer in each Transformer block is equivalent to 1?1 convolution. However, the design is different from conventional CNNs that have multiple layers and small kernel and stride sizes. These Transformer models are usually referred to as convolution-free to distinguish them from CNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">ImageNet Pretraining</head><p>One disadvantage of the Transformer compared with CNNs is that the Transformer needs more data to train <ref type="bibr" target="#b10">[11]</ref>. In <ref type="bibr" target="#b10">[11]</ref>, the authors point out that the Transformer only starts to outperform CNNs when the amount of data is over 14 million for image classification tasks. However, audio datasets typically do not have such large amounts of data, which motivates us to apply cross-modality transfer learning to AST since images and audio spectrograms have similar formats. Transfer learning from vision tasks to audio tasks has been previously studied in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b7">8]</ref>, but only for CNN-based models, where ImageNet-pretrained CNN weights are used as initial CNN weights for audio classification training. In practice, it is computationally expensive to train a state-of-the-art vision model, but many commonly used architectures (e.g., ResNet <ref type="bibr" target="#b25">[26]</ref>, Ef-ficientNet <ref type="bibr" target="#b26">[27]</ref>) have off-the-shelf ImageNet-pretrained models for both TensorFlow and PyTorch, making transfer learning much easier. We also follow this regime by adapting an off-the-shelf pretrained Vision Transformer (ViT) to AST.</p><p>While ViT and AST have similar architectures (e.g., both use a standard Transformer, same patch size, same embedding size), they are not same. Therefore, a few modifications need to make for the adaptation. First, the input of ViT is a 3-channel image while the input to the AST is a single-channel spectrogram, we average the weights corresponding to each of the three input channels of the ViT patch embedding layer and use them as the weights of the AST patch embedding layer. This is equivalent to expanding a single-channel spectrogram to 3channels with the same content, but is computationally more efficient. We also normalize the input audio spectrogram so that the dataset mean and standard deviation are 0 and 0.5, respectively. Second, the input shape of ViT is fixed (either 224 ? 224 or 384 ? 384), which is different from a typical audio spectrogram. In addition, the length of an audio spectrogram can be variable. While the Transformer naturally supports variable input length and can be directly transferred from ViT to AST, the positional embedding needs to be carefully processed because it learns to encode the spatial information during the ImageNet training. We propose a cut and bi-linear interpolate method for positional embedding adaptation. For example, for a ViT that takes 384 ? 384 image input and uses a patch size of 16 ? 16, the number of patches and corresponding positional embedding is 24 ? 24 = 576 (ViT splits patches without overlap). An AST that takes 10-second audio input has 12 ? 100 patches, each patch needs a positional embedding. We therefore cut the first dimension and interpolate the second dimension of the 24 ? 24 ViT positional embedding to 12 ? 100 and use it as the positional embedding for the AST. We directly reuse the positional embedding for the [CLS] token. By doing this we are able to transfer the 2D spatial knowledge from a pretrained ViT to the AST even when the input shapes are different. Finally, since the classification task is essentially different, we abandon the last classification layer of the ViT and reinitialize a new one for AST. With this adaptation framework, the AST can use various pretrained ViT weights for initialization. In this work, we use pretrained weights of a data-efficient image Transformer (DeiT) <ref type="bibr" target="#b11">[12]</ref>, which is trained with CNN knowledge distillation, 384 ? 384 images, has 87M parameters, and achieves 85.2% top-1 accuracy on ImageNet 2012. During ImageNet training, DeiT has two [CLS] tokens; we average them as a single [CLS] token for audio training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>In this section, we focus on evaluating the AST on AudioSet (Section 3.1) as weakly-labeled audio event classification is one of the most challenging audio classification tasks. We present our primary AudioSet results and ablation study in Section 3.1.2 and Section 3.1.3, respectively. We then present our experiments on ESC-50 and Speech Commands V2 in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">AudioSet Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Dataset and Training Details</head><p>AudioSet <ref type="bibr" target="#b14">[15]</ref> is a collection of over 2 million 10-second audio clips excised from YouTube videos and labeled with the sounds that the clip contains from a set of 527 labels. The balanced training, full training, and evaluation set contains 22k, 2M, and 20k samples, respectively. For AudioSet experiments, we use the exact same training pipeline with <ref type="bibr" target="#b7">[8]</ref>. Specifically, we use ImageNet pretraining (as described in Section 2.2), balanced sampling (for full set experiments only), data augmenta- tion (including mixup <ref type="bibr" target="#b27">[28]</ref> with mixup ratio=0.5 and spectrogram masking <ref type="bibr" target="#b28">[29]</ref> with max time mask length of 192 frames and max frequency mask length of 48 bins), and model aggregation (including weight averaging <ref type="bibr" target="#b29">[30]</ref> and ensemble <ref type="bibr" target="#b30">[31]</ref>). We train the model with a batch size of 12, the Adam optimizer <ref type="bibr" target="#b31">[32]</ref>, and use binary cross-entropy loss. We conduct experiments on the official balanced and full training set and evaluate on the Au-dioSet evaluation set. For balanced set experiments, we use an initial learning rate of 5e-5 and train the model for 25 epochs, the learning rate is cut into half every 5 epoch after the 10th epoch. For full set experiments, we use an initial learning rate of 1e-5 and train the model for 5 epochs, the learning rate is cut into half every epoch after the 2nd epoch. We use the mean average precision (mAP) as our main evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">AudioSet Results</head><p>We repeat each experiment three times with the same setup but different random seeds and report the mean and standard deviation. When AST is trained with the full AudioSet, the mAP at the last epoch is 0.448?0.001. As in <ref type="bibr" target="#b7">[8]</ref>, we also use weight averaging <ref type="bibr" target="#b29">[30]</ref> and ensemble <ref type="bibr" target="#b30">[31]</ref> strategies to further improve the performance of AST. Specifically, for weight averaging, we average all weights of the model checkpoints from the first to the last epoch. The weight-averaged model achieves an mAP of 0.459?0.000, which is our best single model (weight averaging does not increase the model size). For ensemble, we evaluate two settings: 1) Ensemble-S: we run the experiment three times with the exact same setting, but with a different random seed. We then average the output of the last checkpoint model of each run. In this setting, the ensemble model achieves an mAP of 0.475; 2) Ensemble-M: we ensemble models trained with different settings, specifically, we ensemble the three models in Ensemble-S together with another three models trained with different patch split strategies (described in Section 3.1.3 and shown in <ref type="table" target="#tab_5">Table 5</ref>). In this setting, the ensemble model achieves an mAP of 0.485, this is our best full model on AudioSet. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the proposed AST outperforms the previous best system in <ref type="bibr" target="#b7">[8]</ref> in all settings. Note that we use the same training pipeline with <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b7">[8]</ref> also use ImageNet pretraining, so it is a fair comparison. In addition, we use fewer models (6) for our best ensemble models than <ref type="bibr" target="#b7">[8]</ref>  <ref type="bibr" target="#b9">(10)</ref>. Finally, it is worth mentioning that AST training converges quickly; AST only needs 5 training epochs, while in <ref type="bibr" target="#b7">[8]</ref>, the CNN-attention hybrid model is trained for 30 epochs.</p><p>We also conduct experiments with the balanced AudioSet (about 1% of the full set) to evaluate the performance of AST when the training data volume is smaller. For weight averaging, we average all weights of the model checkpoints of the   <ref type="table" target="#tab_3">(Table 3)</ref>, different positional embedding interpolation <ref type="table" target="#tab_4">(Table 4)</ref>, and different patch split strategies ( <ref type="table" target="#tab_5">Table 5</ref>). The single, Ensemble-S, and Ensemble-M model achieve 0.347?0.001, 0.363, and 0.378, respectively, all outperform the previous best system. This demonstrates that AST can work better than CNN-attention hybrid models even when the training set is relatively small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3.">Ablation Study</head><p>We conduct a series of ablation studies to illustrate the design choices for the AST. To save compute, we mainly conduct ablation studies with the balanced AudioSet. For all experiments, we use weight averaging but do not use ensembles.</p><p>Impact of ImageNet Pretraining. We compare ImageNet pretrained AST and randomly initialized AST. As shown in <ref type="table" target="#tab_2">Table 2</ref>, ImageNet pretrained AST noticeably outperforms randomly initialized AST for both balanced and full AudioSet experiments. The performance improvement of ImageNet pretraining is more significant when the training data volume is smaller, demonstrating that ImageNet pretraining can greatly reduce the demand for in-domain audio data for AST. We further study the impact of pretrained weights used. As shown in <ref type="table" target="#tab_3">Table 3</ref>, we compare the performance of AST models initialized with pretrained weights of ViT-Base, ViT-Large, and DeiT models. These models have similar architectures but are trained with different settings. We made the necessary architecture modifications for AST to reuse the weights. We find that AST using the weights of the DeiT model with distillation that performs best on ImageNet2012 also performs best on AudioSet.</p><p>Impact of Positional Embedding Adaptation. As mentioned in Section 2.2, we use a cut and bi-linear interpolation approach for positional embedding adaptation when transferring knowledge from the Vision Transformer to the AST. We compare it with a pretrained AST model with a randomly initialized positional embedding. As shown in <ref type="table" target="#tab_4">Table 4</ref>, we find reinitializing the positional embedding does not completely break the pretrained model as the model still performs better than a fully randomly reinitialized model, but it does lead to a noticeable performance drop compared with the proposed adaptation approach. This demonstrates the importance of transferring spatial   Impact of Patch Split Overlap. We compare the performance of models trained with different patch split overlap <ref type="bibr" target="#b12">[13]</ref>. As shown in <ref type="table" target="#tab_5">Table 5</ref>, the performance improves with the overlap size for both balanced and full set experiments. However, increasing the overlap also leads to longer patch sequence inputs to the Transformer, which will quadratically increase the computational overhead. Even with no patch split overlap, AST can still outperform the previous best system in <ref type="bibr" target="#b7">[8]</ref>.</p><p>Impact of Patch Shape and Size. As mentioned in Section 2.1, we split the audio spectrogram into 16 ? 16 square patches, so the input sequence to the Transformer cannot be in temporal order. We hope the positional embedding can learn to encode the 2D spatial information. An alternative way to split the patch is slicing the audio spectrogram into rectangular patches in the temporal order. We compare both methods in <ref type="table" target="#tab_6">Table 6</ref>, when the area of the patch is the same (256), using 128 ? 2 rectangle patches leads to better performance than using 16 ? 16 square patches when both models are trained from scratch. However, considering there is no 128 ? 2 patch based ImageNet pretrained models, using 16 ? 16 patches is still the current optimal solution. We also compare using patches with different sizes, smaller size patches lead to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results on ESC-50 and Speech Commands</head><p>The ESC-50 <ref type="bibr" target="#b15">[16]</ref> dataset consists of 2,000 5-second environmental audio recordings organized into 50 classes. The current best results on ESC-50 are 86.5% accuracy (trained from scratch, SOTA-S) <ref type="bibr" target="#b32">[33]</ref> and 94.7% accuracy (with AudioSet pretraining, SOTA-P) <ref type="bibr" target="#b6">[7]</ref>. We compare AST with the SOTA models in these two settings, specifically, we train an AST model with only ImageNet pretraining (AST-S) and an AST model with ImageNet and AudioSet pretraining (AST-P). We train both models with frequency/time masking <ref type="bibr" target="#b28">[29]</ref> data augmentation, a batch size of 48, and the Adam optimizer <ref type="bibr" target="#b31">[32]</ref> for 20 epochs. We use an initial learning rate of 1e-4 and 1e-5 for AST-S and AST-P, respectively, and decrease the learning rate with a factor of 0.85 every epoch after the 5th epoch. We follow the standard 5-fold cross-validation to evaluate our model, repeat each experiment three times, and report the mean and standard deviation. As shown in <ref type="table" target="#tab_7">Table 7</ref>, AST-S achieves 88.7?0.7 and AST-P achieves 95.6?0.4, both outperform SOTA models in the same setting. Of note, although ESC-50 has 1,600 training samples for each fold, AST still works well with such a small amount of data even without AudioSet pretraining. Speech Commands V2 <ref type="bibr" target="#b16">[17]</ref> is a dataset consists of 105,829 1-second recordings of 35 common speech commands. The training, validation, and test set contains 84,843, 9,981, and 11,005 samples, respectively. We focus on the 35-class classification task, the SOTA model on Speech Commands V2 (35class classification) without additional audio data pretraining is the time-channel separable convolutional neural network <ref type="bibr" target="#b33">[34]</ref>, which achieves 97.4% on the test set. In <ref type="bibr" target="#b34">[35]</ref>, a CNN model pretrained with additional 200 million YouTube audio achieves 97.7% on the test set. We also evaluate AST in these two settings. Specifically, we train an AST model with only ImageNet pretraining (AST-S) and an AST model with ImageNet and AudioSet pretraining (AST-P). We train both models with frequency and time masking <ref type="bibr" target="#b28">[29]</ref>, random noise, and mixup <ref type="bibr" target="#b27">[28]</ref> augmentation, a batch size of 128, and the Adam optimizer <ref type="bibr" target="#b31">[32]</ref>. We use an initial learning rate of 2.5e-4 and decrease the learning rate with a factor of 0.85 every epoch after the 5th epoch. We train the model for up to 20 epochs, and select the best model using the validation set, and report the accuracy on the test set. We repeat each experiment three times and report the mean and standard deviation. AST-S model achieves 98.11?0.05, outperforms the SOTA model in <ref type="bibr" target="#b8">[9]</ref>. In addition, we find AudioSet pretraining unnecessary for the speech command classification task as AST-S outperforms AST-P. To summarize, while the input audio length varies from 1 sec. (Speech Commands), 5 sec. (ESC-50) to 10 sec. (AudioSet) and content varies from speech (Speech Commands) to non-speech (Au-dioSet and ESC-50), we use a fixed AST architecture for all three benchmarks and achieve SOTA results on all of them. This indicates the potential for AST use as a generic audio classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>Over the last decade, CNNs have become a common model component for audio classification. In this work, we find CNNs are not indispensable, and introduce the Audio Spectrogram Transformer (AST), a convolution-free, purely attention-based model for audio classification which features a simple architecture and superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head><p>This work is partly supported by Signify.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of AST and previous methods on AudioSet.</figDesc><table><row><cell></cell><cell>Model</cell><cell>Balanced</cell><cell>Full</cell></row><row><cell></cell><cell>Architecture</cell><cell>mAP</cell><cell>mAP</cell></row><row><cell>Baseline [15]</cell><cell>CNN+MLP</cell><cell>-</cell><cell>0.314</cell></row><row><cell>PANNs [7]</cell><cell>CNN+Attention</cell><cell>0.278</cell><cell>0.439</cell></row><row><cell>PSLA [8] (Single)</cell><cell>CNN+Attention</cell><cell>0.319</cell><cell>0.444</cell></row><row><cell>PSLA (Ensemble-S)</cell><cell>CNN+Attention</cell><cell>0.345</cell><cell>0.464</cell></row><row><cell cols="2">PSLA (Ensemble-M) CNN+Attention</cell><cell>0.362</cell><cell>0.474</cell></row><row><cell>AST (Single)</cell><cell>Pure Attention</cell><cell>0.347 ? 0.001</cell><cell>0.459 ? 0.000</cell></row><row><cell>AST (Ensemble-S)</cell><cell>Pure Attention</cell><cell>0.363</cell><cell>0.475</cell></row><row><cell>AST (Ensemble-M)</cell><cell>Pure Attention</cell><cell>0.378</cell><cell>0.485</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance impact due to ImageNet pretraining. "Used" denotes the setting used by our optimal AST model.</figDesc><table><row><cell></cell><cell cols="2">Balanced Set Full Set</cell></row><row><cell>No Pretrain</cell><cell>0.148</cell><cell>0.366</cell></row><row><cell>ImageNet Pretrain (Used)</cell><cell>0.347</cell><cell>0.459</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of AST models initialized with different ViT weights on balanced AudioSet and corresponding ViT models' top-1 accuracy on ImageNet 2012. (* Model is trained without patch split overlap due to memory limitation.)</figDesc><table><row><cell></cell><cell cols="3"># Params ImageNet AudioSet</cell></row><row><cell>ViT Base [11]</cell><cell>86M</cell><cell>0.846</cell><cell>0.320</cell></row><row><cell>ViT Large [11]*</cell><cell>307M</cell><cell>0.851</cell><cell>0.330</cell></row><row><cell>DeiT w/o Distill [12]</cell><cell>86M</cell><cell>0.829</cell><cell>0.330</cell></row><row><cell>DeiT w/ Distill (Used)</cell><cell>87M</cell><cell>0.852</cell><cell>0.347</cell></row><row><cell cols="4">last 20 epochs. For Ensemble-S, we follow the same setting</cell></row><row><cell cols="4">used for the full AudioSet experiment; for Ensemble-M, we in-</cell></row><row><cell cols="4">clude 11 models trained with different random seeds (Table 1),</cell></row><row><cell cols="2">different pretrained weights</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Performance impact due to various positional embedding adaptation settings.</figDesc><table><row><cell></cell><cell>Balanced Set</cell></row><row><cell>Reinitialize</cell><cell>0.305</cell></row><row><cell>Nearest Neighbor Interpolation</cell><cell>0.346</cell></row><row><cell>Bilinear Interpolation (Used)</cell><cell>0.347</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance impact due to various patch overlap size.</figDesc><table><row><cell></cell><cell cols="3"># Patches Balanced Set Full Set</cell></row><row><cell>No Overlap</cell><cell>512</cell><cell>0.336</cell><cell>0.451</cell></row><row><cell>Overlap-2</cell><cell>657</cell><cell>0.342</cell><cell>0.456</cell></row><row><cell>Overlap-4</cell><cell>850</cell><cell>0.344</cell><cell>0.455</cell></row><row><cell>Overlap-6 (Used)</cell><cell>1212</cell><cell>0.347</cell><cell>0.459</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Performance impact due to various patch shape and size. All models are trained with no patch split overlap.</figDesc><table><row><cell></cell><cell cols="3"># Patches w/o Pretrain w/ Pretrain</cell></row><row><cell>128?2</cell><cell>512</cell><cell>0.154</cell><cell>-</cell></row><row><cell>16?16 (Used)</cell><cell>512</cell><cell>0.143</cell><cell>0.336</cell></row><row><cell>32?32</cell><cell>128</cell><cell>0.139</cell><cell>-</cell></row><row><cell cols="4">knowledge. Bi-linear interpolation and nearest-neighbor inter-</cell></row><row><cell cols="3">polation do not result in a big difference.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparing AST and SOTA models on ESC-50 and Speech Commands. "-S" and "-P" denotes model trained without and with additional audio data, respectively.</figDesc><table><row><cell></cell><cell>ESC-50</cell><cell>Speech Commands V2 (35 classes)</cell></row><row><cell>SOTA-S</cell><cell>86.5 [33]</cell><cell>97.4 [34]</cell></row><row><cell>SOTA-P</cell><cell>94.7 [7]</cell><cell>97.7 [35]</cell></row><row><cell>AST-S</cell><cell>88.7?0.7</cell><cell>98.11?0.05</cell></row><row><cell>AST-P</cell><cell>95.6?0.4</cell><cell>97.88?0.03</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Recent developments in openSMILE, the Munich open-source multimedia feature extractor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<editor>Multimedia</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mortillaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salamin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polychroniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Valente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a better representation of speech soundwaves using restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end learning for music audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Brueckner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Handbook of Brain Theory and Neural Networks</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
			<biblScope unit="page">1995</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">PANNs: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2880" to="2894" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PSLA: Improving audio event classification with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01243</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Streaming keyword spotting on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rybakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kononenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laurenzo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An attention pooling based representation learning method for speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Mcloughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-R</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Tokens-to-token ViT: Training vision transformers from scratch on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Audio Set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ESC: Dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolution augmented transformer for semisupervised sound event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Komatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Takeda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>DCASE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sound event detection of weakly labelled data with CNN-transformer and automatic threshold optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM TASLP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2450" to="2460" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Conformer: Convolution-augmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BERT: Pretraining of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep image features in music information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gwardys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Grzywczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJET</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="321" to="326" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ESResNet: Environmental sound classification based on visual domain models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guzhov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Raue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Rethinking CNN models for audio classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Palanisamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Singhania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11154</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning from betweenclass examples for deep sound recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tokozume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">SpecAugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>UAI</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised filterbank learning using convolutional restricted boltzmann machine for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Sailor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>Interspeech</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Matchboxnet-1d time-channel separable convolutional neural network architecture for speech commands recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08531</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training keyword spotters with limited and synthesized speech data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kilgour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roblek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
