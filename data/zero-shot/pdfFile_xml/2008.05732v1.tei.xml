<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Ensemble of Knowledge Sharing Models for Dynamic Hand Gesture Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Lai</surname></persName>
							<email>kelai@ucalgary.ca</email>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="laboratory">Biometric Technologies Laboratory</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Yanushkevich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of ECE</orgName>
								<orgName type="laboratory">Biometric Technologies Laboratory</orgName>
								<orgName type="institution">University of Calgary</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Ensemble of Knowledge Sharing Models for Dynamic Hand Gesture Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Biometrics</term>
					<term>Gesture Recognition</term>
					<term>Action Recognition</term>
					<term>Recurrent Neural Network</term>
					<term>Machine Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The focus of this paper is dynamic gesture recognition in the context of the interaction between humans and machines. We propose a model consisting of two sub-networks, a transformer and an ordered-neuron long-short-term-memory (ON-LSTM) based recurrent neural network (RNN). Each subnetwork is trained to perform the task of gesture recognition using only skeleton joints. Since each sub-network extracts different types of features due to the difference in architecture, the knowledge can be shared between the sub-networks. Through knowledge distillation, the features and predictions from each sub-network are fused together into a new fusion classifier. In addition, a cyclical learning rate can be used to generate a series of models that are combined in an ensemble, in order to yield a more generalizable prediction. The proposed ensemble of knowledge-sharing models exhibits an overall accuracy of 86.11% using only skeleton information, as tested using the Dynamic Hand Gesture-14/28 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>This paper focuses on gesture analysis in the context of human behavioral biometrics. The paper describes a process of recognizing different gestures using only skeleton joint positions. Behavior biometrics is being heavily utilized in the current developments of "ambient" intelligence and incorporating smart-home sensors for automated health monitoring and risk assessment, particularly in analyzing daily routines in smart homes <ref type="bibr" target="#b0">[1]</ref>.</p><p>The growing concern of elderly patients wanting to live independently is a major motivation for creating an automated system of analyzing behavior biometrics. Due to the recent improvements in biometrics, better systems can be created for more accurate activity detection and prevention, specifically monitoring system that provides support when an emergency occurs. A study by Lam et al. <ref type="bibr" target="#b1">[2]</ref> investigated ambient monitoring and tracking for Alzheimer patients performing daily activities. An embedded sensing system proposed in <ref type="bibr" target="#b2">[3]</ref> was used to monitor how well an elderly patient performs an activity, as well as the potential to use it for determining the severity of the cognitive decline. Using the activityaware smart home behavior data, Alberdi et al. <ref type="bibr" target="#b3">[4]</ref> created regression and classification models capable of predicting mobility, cognition and depression symptoms. <ref type="bibr" target="#b4">[5]</ref> studies the problem of activity prediction and proposes using a combination of imitation learning and regression learners to address the problem.</p><p>In recent years, deep learning techniques have greatly contributed to the increasing accuracy of existing biometric systems <ref type="bibr" target="#b5">[6]</ref>. In the area of human activity recognition (HAR), the development of deep learning methods overcome many existing pattern recognition challenges including: handcrafted features, shallow features, a large number of labeled data <ref type="bibr" target="#b6">[7]</ref>. A 3D CNN proposed by <ref type="bibr" target="#b7">[8]</ref> performs gesture recognition on both color and depth images and has achieved a classification rate of 77.5% classification rate on the VIVA challenge dataset. Chen et al. <ref type="bibr" target="#b8">[9]</ref> suggests using RNN to recognize different gestures by extracting global and finger motion features from a skeleton sequence. Whereas <ref type="bibr" target="#b9">[10]</ref> proposes to recognize dynamic hand gesture using only skeleton-based information with a combination of CNNs and long short term memory (LSTM) recurrent neural networks (RNN). An architecture proposed in <ref type="bibr" target="#b10">[11]</ref> shows the ability to perform a spatial-temporal fusion of video snippets used for action recognition.</p><p>Gestures have been investigated as an alternative to the password for subject authentication. Wu et al. <ref type="bibr" target="#b11">[12]</ref> proposed a two-stream CNN to learn the "style" of subjects performing different gestures which would allow the system to perform subject verification and identification. Another method proposed by Abate et al. <ref type="bibr" target="#b12">[13]</ref> combines two biometrics, ear and arm gestures, collected using a smartphone to perform identity verification. A feasibility study on multi-touch gesture authentication system was reported in <ref type="bibr" target="#b13">[14]</ref>. Study of subject authentication based on analysis of a swipe gesture, Support Vector Machines, Gaussian Mixture Model, and fusion of both systems on four databases showed that horizontal swipes were more discriminatory <ref type="bibr" target="#b14">[15]</ref>.</p><p>The objective of this study is to improve the performance of joint-based gesture recognition. Traditional gesture recognition uses mainly static or video images for analysis. Recent approaches incorporate other types of information such as depth, near-infrared, infrared, and time sequencing <ref type="bibr" target="#b15">[16]</ref>. The proposed approach focuses on using a fusion classifier that is composed of two sub-networks. Both sub-networks are types of deep learning architecture that was previously designed to handle sequence-to-sequence processing, such as language translation and natural language processing. The first subnetwork is based on an encoder-decoder and is called a transformer. The second sub-network is a recurrent neural network (RNN) composed of the ordered neuron long-short-  term-memory (LSTM) cell units. Both sub-network can be independent and fully capable of performing gesture recognition. In this paper, we show that both of these networks can be fused to yield better performance through an approach known as knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ON-LSTM Sub-Network</head><p>The paper is structured as follows: overview of the proposed method is described in Section II, design of experiments and experimental results are provided in Section III, and Section IV summarizes the findings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. APPROACH AND METHODOLOGY</head><p>In this paper, we propose a gesture recognition model composed of an ensemble of four models, where each model consists of four classifiers. The combination of the classifiers is adopted from a process called Feature Fusion Learning proposed in <ref type="bibr" target="#b16">[17]</ref>. Two classifiers are independent sub-networks that are trained to perform gesture recognition, the logits of these classifiers are averaged to create an ensemble classifier, whereas the features of these classifiers are concatenated to create a fusion classifier. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates the proposed model containing the three main classifiers. The remaining classifier, a hidden ensemble classifier, does not directly output any prediction but is used purely to transfer its knowledge to the fusion classifier.</p><p>The two independent classifiers are the two sub-networks, the transformer network and the ordered-neuron LSTM (ON-LSTM) based RNN. The ensemble classifier is a singlelayered network that averages the result of the logit inputs. Those units are named after logistic regression that estimates the parameters of a logistic model, using the unit of measurement for the log-odds scale called a logit, or logistic unit.</p><p>The fusion classifier is a simple three-layer multi-layer perceptron (MLP) that combines the feature inputs from the other networks.</p><p>The first sub-network uses a transformer structure which is a deep learning architecture that is designed to tackle sequence-to-sequence problems. The Universal Transformer <ref type="bibr" target="#b17">[18]</ref> is used in this paper as one of the sub-networks for analyzing the skeleton joints to recognize different gestures. <ref type="table" target="#tab_0">Table I</ref> describes the transformer sub-network used in this paper. A transformer block consists of several components including a multi-head self-attention, residual connection, layer normalization, feed-forward/transition function, residual connection and layer normalization as shown in <ref type="figure" target="#fig_2">Figure  2</ref>. The other sub-network consists of ON-LSTM cells which are proposed in <ref type="bibr" target="#b18">[19]</ref> as an initial step in integrating tree structures into an LSTM cell. The ON-LSTM shows promise in recognizing gestures as skeleton joints are interconnected. As well, it is suggested that normal LSTM may not be able to capture the relationship between different joints. The ON-LSTM cell is a modified version of the LSTM cell which introduces a master forget and master input gate. The purpose of these gates is to control the update operations of the cell states at a high level. This change governed by a new activation function, cumax, allows the cell to apply different updates to separate segments <ref type="bibr" target="#b18">[19]</ref>. <ref type="table" target="#tab_0">Table II</ref> describes the ON-LSTM sub-network used in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mul?-Head Self-A?en?on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Normaliza?on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed Forward</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer Normaliza?on</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Mish Activation</head><p>The first technique applied to improve the performance of the model is the replacement of the rectified linear activation units (ReLU) with Mish. Mish is non-monotonic and is continuously differentiable which may resolve gradients problem associated with rectified linear units, specifically in the case for input values less than equal to 0 <ref type="bibr" target="#b19">[20]</ref>. It was shown to provide better promise than Swish and ReLU <ref type="bibr" target="#b19">[20]</ref>. Swish is used in Bidirectional Encoder Representations from Transformers (BERT) which has reached state-of-the-art performance in natural language processing. Mish is defined as follows:</p><formula xml:id="formula_0">f (x) = x ? tanh(ln(1 + e x ))<label>(1)</label></formula><p>where x is the input value, tanh is the hyperbolic tangent function, and ln is the natural logarithmic function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Knowledge Distillation</head><p>Another technique involves knowledge distillation, specifically the transfer of the dark knowledge from the ensemble classifier to the fusion classifier. Dark knowledge refers to the hidden information learned by the models and can be revealed by calculating the softened probability based on a temperature T , as defined in Equation 2 <ref type="bibr" target="#b20">[21]</ref>. When T = 1, the resulting probability is the same as the result of a softmax function. The dark knowledge from the fusion classifier is then also distilled to the two sub-networks.</p><formula xml:id="formula_1">? i,m = exp (Logit i m /T ) N j exp (Logit j m /T )<label>(2)</label></formula><p>where N is the total number of classes, T is the temperature parameter, Logit i m is the i th class's logit output from m network.</p><p>In this paper, we apply two knowledge distillation proposed in <ref type="bibr" target="#b16">[17]</ref>: ensemble knowledge distillation and fusion knowledge distillation. Ensemble knowledge distillation (EKD) transfers knowledge from the ensemble classifier to the fusion classifier. The fusion knowledge distillation (FKD) transfers the new knowledge of the ensemble classifier to the individual sub-networks. <ref type="figure" target="#fig_3">Figure 3</ref> illustrates the two types of knowledge distillation used to transfer knowledge between the classifiers.</p><p>EKD loss is defined as the Kullback-Leibler divergence between the softened distribution of the ensemble classifier and the softened distribution of the fusion classifier. FKD loss is defined as the Kullback-Leibler divergence between the softened distribution of the fusion classifier and the softened distribution of the individual networks.</p><p>The loss for the individual sub-network is the combination of the cross-entropy (CE) loss and the FKD loss defined as follows:</p><formula xml:id="formula_2">L m = N i ? i,f log( ? i,f ? i,m ) FKD Loss ? N i y i log(? i,m ) CE Loss<label>(3)</label></formula><p>The loss for the fusion classifier is the combination of the original CE loss accompanied with the EKD loss defined in <ref type="bibr">Equation 4</ref> as follows:</p><formula xml:id="formula_3">L f = N i ? i,e log( ? i,e ? i,f ) EKD Loss ? N i y i log(? i,f ) CE Loss (4)</formula><p>where L m is the loss for the m sub-network, L f is the loss for the fusion classifier, y i is the truth label for the i th index in a one-hot-encoded label, and ? i,m , ? i,f , and ? i,e represents the soften probabilities for the m sub-network, fusion classifier, and ensemble classifier, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. AdamWR and an Ensemble of Models</head><p>In this paper, an ensemble of models is used to further boost the performance of the recognition accuracy. Specifically, the outputs of the four models are combined to yield a more confident prediction.</p><p>To achieve the creation of multiple models without retraining a model, a technique that combines a cyclical learning rate and warm restart is used. The cyclical learning rate is a technique that cyclically varies a learning rate between two bounds. This technique shows an improvement over monotonically decreasing a learning rate and relieves the necessary processing of fine-tuning the learning rate <ref type="bibr" target="#b21">[22]</ref>. Cosine annealing, another technique that performs a warm restart of the training process with a learning rate adjusting to a cosine wave <ref type="bibr" target="#b22">[23]</ref>. By using cosine annealing, the model parameters can be saved at the end of each cycle.</p><p>Built upon using cyclical learning rates, it is possible to save the model parameters at the end of each cycle <ref type="bibr" target="#b23">[24]</ref>. At the end of the training, K models are saved after K cycles, and through the ensemble fusion of the M models, the system can obtain better performance. Since each model is optimized for a specific cycle of learning rate and that each cycle is capable of dislodging from a local minimum, the fusion of multiple models contributes to different decisions.</p><p>Lastly, a weight decay is applied to the Adam optimizer as a way to incorporate better regularization in the model <ref type="bibr" target="#b24">[25]</ref>. Two common ways of regularization are used in neural networks, specifically L1 and L2 regularization. The application of these regularization reduces bias and variance, respectively. However, the incorporation of L2 regularization is different for adaptive gradient methods, such as Adam. In <ref type="bibr" target="#b24">[25]</ref>, a technique to decouple weight decay from the gradient is proposed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Data Augmentation</head><p>Due to the nature of deep neural networks, it is often beneficial to have a large training dataset. In this paper, we apply different augmentation techniques on the DHG-14/28 dataset to greatly increase the number of training samples. The creation of synthetic time-series data, specifically for skeleton joints, is different from conventional augmentation of images. Four augmentation technique is described in <ref type="bibr" target="#b25">[26]</ref>, which is used to augment inertial measurement unit data. In this paper, we adopt three of the described technique, namely jittering, scaling and time-warping, and generate a fourth technique by combining the previous three methods. <ref type="figure">Figure 4</ref> shows an example of data augmentation applied on the x-coordinates of the wrist joint for subject 1 while performing the grabbing gesture. <ref type="figure">Fig. 4</ref>. Data augmentation on the x-coordinate value of wrist joint. 1) Jittering: Simulating sensor noise by applying Gaussian noise to the x, y, and z-coordinates of the joint. The noise is a Gaussian distribution with a mean of 0 and a variance calculated based on the coordinate axes.</p><formula xml:id="formula_4">X = X + N (? x = 0, V ar(X))<label>(5)</label></formula><p>2) Scaling: Modify the magnitude of the data by applying a random scalar value between 0.75 and 1.25 to the x, y, and z-coordinates of the joints. X = X ? Random(0.75, 1.25)</p><p>3) Time-warping: Adjusting the overall duration of the time-series data. A time-series with length T is interpolated to a series with length T * V where V is a random value between 0.5 and 2. A V &gt; 1 increases the length of the time-series while a V &lt; 1 shortens the series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head><p>The proposd model was trained for 4 cycles with the decoupled weight decay Adam optimizer with warm restart (AdamWR). For each subsequent cycle, the number of epochs is raised by a factor of 1.5. In total there are 10+15+23+35= 83 epochs. For each cycle, a cosine-based cyclical learning rate with a starting learning rate ? = 0.001, ? 1 = 0.9, ? 2 = 0.999, and weight decay of 0.001 is used. For the transformer blocks, 11 heads were selected for self attention. A temperature T = 3 is used to compute the softened probability for knowledge distillation. At each cycle, a snapshot of the model is saved for an ensemble of models. In addition, a time-step of 64 with a batch size of 512 is used for the training of the model. The input data for the model are the skeleton joint positions encoded as world coordinates (22 joints by x, y, and z-coordinates). The output of the networks is the predictions from the four classifiers, fusion, transformer, and ON-LSTM.</p><p>For evaluating the performance of the model, we choose to use a specific type of K-fold cross-validation, that divides each fold based on subjects. This leave-one subject-out crossvalidation (LOOCV) is further described in <ref type="bibr" target="#b26">[27]</ref> and is one of the main methods for comparing different gesture recognition algorithms. Given the DHG-14/28 dataset contains 20 subjects, the dataset is augmented by a factor of 40 and split into 20 folds, a fold for each subject. When performing LOOCV, the 19 augmented-folds are used for training while the remaining fold is used for validation. A third set, the testing set, consists of the original un-augmented validation fold is used for calculating the final accuracy. Performance of the system is evaluated in terms of the accuracy of action recognition, defined as follows: Acc = T P + T N T P + F P + T N + F N where T P , T N , F P , and F N represent the number of true positives, true negatives, false positives and false negatives, respectively. Accuracy reflects the system's ability to accept genuine actions while rejecting imposter actions. Due to using the LOOCV method, the overall reported accuracy is the result of averaging the accuracies for each model trained on different subjects. For example, given twenty subjects, there are twenty models; each model is trained on nineteen subjects and is tested on the remaining subject, resulting in twenty estimated accuracy values. The overall accuracy is the average of these twenty values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The dynamic hand gesture 14/28 (DHG-14/28) <ref type="bibr" target="#b26">[27]</ref> was chosen as the database, and this is one of the few databases containing data collected using a depth camera (Intel Re-alSense F200) sensor. Both depth and skeleton information for various hand gestures is available. In the DHG-14/28 <ref type="bibr" target="#b26">[27]</ref> dataset, 20 unique individuals are performing 5 iterations of 14 gestures using two types of finger configurations, thus forming 28 sets of gestures, to a total of 2800 sequences. The depth information is saved in the form of images with a resolution of 480x640 in 16-bits. The skeleton information contains 22 joint locations of a hand described in both 2D and 3D coordinates saved in 44x1 and 66x1 vector format, respectively. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates an example of the rotate clockwise gesture where the skeleton joints are overlayed on top of the depth images. For the DHG-14/28 dataset, each gesture is individually classified into two main categories: fine-grained and coarsegrained gestures. <ref type="table" target="#tab_0">Table III</ref> provides a list of all the gestures and the corresponding grain categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental results</head><p>In our experiment, we evaluated the performance of our proposed model using the DHG-14/28 dataset. <ref type="table" target="#tab_0">Table IV</ref> provides the performance of the different classifiers within the proposed model evaluated at different cycles. Due to snap-shotting the model according to the learning rate associated with cosine annealing, four different models can be evaluated. For example, the first cycle is associated with the model snap-shotted after 10 epochs. It can be seen that the performance between the different cycles is relatively similar, but when combined in an ensemble together, it can lead to a performance increase. This indicates that the prediction from each cycle is slightly different and that new information can be retrieved by observing the discrepancy in prediction.</p><p>Table V provides a list of gesture recognition methods applied to the DHG-14/28 dataset. Our proposed ensemble model yields similar but slightly improved accuracy compared to the other methods. The main contribution to the higher accuracy is a significant increase in recognizing "fine"-grained gestures but at a slight cost of decreasing "coarse"-grained gestures. To further evaluate the model, a confusion matrix was generated to capture the distribution of the different gestures being recognized. <ref type="figure" target="#fig_5">Figure 6</ref> illustrates the performance of the proposed ensemble model when recognizing the 14 gestures. Each row represents the sample gesture and each column indicates the model's prediction. For example, the last row of <ref type="figure" target="#fig_5">Figure 6</ref> refers to the Shake gesture and that 95% of these samples are correctly recognized by the model while the remaining 5% of the samples are split amongst R-CW, R-CCW, and S-V gestures. <ref type="figure" target="#fig_5">Figure 6</ref> reveals that one of the main reasons for the increase in performance for "fine"-grained gestures is due to the model's ability to distinguish between the grabbing and pinching gestures. It was mentioned previously in <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b26">[27]</ref> that the grabbing and pinching gesture is very similar and that the main difference lies in the amplitude of the action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>This paper proposes an ensemble of knowledge-sharing models designed to improve the performance of gesture recognition. Each model consists of multiple sub-networks that are fully capable of independently performing gesture recognition. However, since each network is developed using a different architecture, namely transformer and ON-LSTM, the features each network extracts are different. We propose to fuse the results from each of the sub-networks to create a more robust model. By applying knowledge distillation, the proposed model benefits from the knowledge learned by each of the sub-networks. The proposed model yields an accuracy of 86.11% using only skeleton joint positions. Also, the model reveals that using knowledge distillation to fuse features helps in identifying hard-to-recognize gestures. This knowledge sharing property helps optimize the subnetworks which, therefore, learn how to distinguish between troublesome classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>The overall design of the proposed model. The model consists of two independent sub-networks, Transformer and ON-LSTM, whose processed skeleton features are concatenated and classified to create a fusion classifier. The outputs of the fusion classifier are analyzed at four different instances and ensembled together to generate an ensemble prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Structure of a transformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Knowledge Distillation between (1) the fusion classifier and the ensemble classifier (EKD f ) and (2) the fusion classifier and the transformer/ON-LSTM classifier (FKDt and FKDo) . Logite is calculated as the average between Logitt and Logito.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Illustrations of the rotate clockwise gesture from the DHG-14/28 [27]. (a) Frame 21, (b) Frame 30, (c) Frame 36, (d) Frame 57.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>The confusion matrix showing accuracies of gesture recognition on 14 gestures when using the proposed method of an ensemble of four models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TRANSFORMER</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">SUB-NETWORK</cell><cell></cell></row><row><cell></cell><cell cols="2">Output Shape</cell><cell>Parameters</cell></row><row><cell>Input</cell><cell>64 ?</cell><cell>66*3</cell><cell>-</cell></row><row><cell>Transformer Block</cell><cell>64 ?</cell><cell>66</cell><cell>52.9K</cell></row><row><cell>Transformer Block</cell><cell>64 ?</cell><cell>66</cell><cell>52.9K</cell></row><row><cell>Transformer Block</cell><cell>64 ?</cell><cell>66</cell><cell>52.9K</cell></row><row><cell>Flatten</cell><cell>4224</cell><cell></cell><cell>-</cell></row><row><cell>Fully-Connected</cell><cell>512</cell><cell></cell><cell>2.16M</cell></row><row><cell>Batch Normalization</cell><cell>512</cell><cell></cell><cell>2048</cell></row><row><cell>Mish Activation</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Dropout (0.5)</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Fully-Connected</cell><cell>512</cell><cell></cell><cell>263K</cell></row><row><cell>Batch Normalization</cell><cell>512</cell><cell></cell><cell>2048</cell></row><row><cell>Mish Activation</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Dropout (0.5)</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Fully-Connected</cell><cell>14</cell><cell></cell><cell>7182</cell></row><row><cell>Batch Normalization</cell><cell>14</cell><cell></cell><cell>56</cell></row><row><cell>Softmax Activation</cell><cell>14</cell><cell></cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell cols="3">ON-LSTM SUB-NETWORK</cell><cell></cell></row><row><cell></cell><cell cols="2">Output Shape</cell><cell>Parameters</cell></row><row><cell>Input</cell><cell>64 ?</cell><cell>66*3</cell><cell>-</cell></row><row><cell>ON-LSTM</cell><cell>64 ?</cell><cell>660</cell><cell>1.9M</cell></row><row><cell>ON-LSTM</cell><cell>660</cell><cell></cell><cell>3.5M</cell></row><row><cell>Fully-Connected</cell><cell>512</cell><cell></cell><cell>338K</cell></row><row><cell>Batch Normalization</cell><cell>512</cell><cell></cell><cell>2048</cell></row><row><cell>Mish Activation</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Dropout (0.5)</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Fully-Connected</cell><cell>512</cell><cell></cell><cell>263K</cell></row><row><cell>Batch Normalization</cell><cell>512</cell><cell></cell><cell>2048</cell></row><row><cell>Mish Activation</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Dropout (0.5)</cell><cell>512</cell><cell></cell><cell>-</cell></row><row><cell>Fully-Connected</cell><cell>14</cell><cell></cell><cell>7182</cell></row><row><cell>Batch Normalization</cell><cell>14</cell><cell></cell><cell>56</cell></row><row><cell>Softmax Activation</cell><cell>14</cell><cell></cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III LIST</head><label>III</label><figDesc>OF GESTURES IN THE DHG-14/28 DATASET</figDesc><table><row><cell>Gesture</cell><cell>Grain</cell><cell>Tag Name</cell></row><row><cell>Grab</cell><cell>Fine</cell><cell>G</cell></row><row><cell>Tap</cell><cell>Coarse</cell><cell>T</cell></row><row><cell>Expand</cell><cell>Fine</cell><cell>E</cell></row><row><cell>Pinch</cell><cell>Fine</cell><cell>P</cell></row><row><cell>Rotation Clockwise</cell><cell>Fine</cell><cell>R-CW</cell></row><row><cell>Rotation Counter-clockwise</cell><cell>Fine</cell><cell>R-CCW</cell></row><row><cell>Swipe Right</cell><cell>Coarse</cell><cell>S-R</cell></row><row><cell>Swipe Left</cell><cell>Coarse</cell><cell>S-L</cell></row><row><cell>Swipe Up</cell><cell>Coarse</cell><cell>S-U</cell></row><row><cell>Swipe Down</cell><cell>Coarse</cell><cell>S-D</cell></row><row><cell>Swipe X</cell><cell>Coarse</cell><cell>S-X</cell></row><row><cell>Swipe V</cell><cell>Coarse</cell><cell>S-V</cell></row><row><cell>Swipe +</cell><cell>Coarse</cell><cell>S-+</cell></row><row><cell>Shake</cell><cell>Coarse</cell><cell>Sh</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV ACCURACY</head><label>IV</label><figDesc>(%) PERFORMANCE AT EACH CYCLE ON THE DHG-14/28</figDesc><table><row><cell></cell><cell cols="2">DATASET</cell><cell></cell><cell></cell></row><row><cell>Cycle</cell><cell>Network</cell><cell>Fine</cell><cell>Coarse</cell><cell>Both</cell></row><row><cell></cell><cell>Transformer</cell><cell>80.50</cell><cell>84.94</cell><cell>83.36</cell></row><row><cell>1</cell><cell>ON-LSTM</cell><cell>76.70</cell><cell>86.28</cell><cell>82.86</cell></row><row><cell></cell><cell>Fusion</cell><cell>76.70</cell><cell>83.39</cell><cell>81.00</cell></row><row><cell></cell><cell>Transformer</cell><cell>76.70</cell><cell>81.83</cell><cell>80.00</cell></row><row><cell>2</cell><cell>ON-LSTM</cell><cell>71.40</cell><cell>84.17</cell><cell>79.61</cell></row><row><cell></cell><cell>Fusion</cell><cell>78.00</cell><cell>86.33</cell><cell>83.36</cell></row><row><cell></cell><cell>Transformer</cell><cell>79.90</cell><cell>84.94</cell><cell>83.14</cell></row><row><cell>3</cell><cell>ON-LSTM</cell><cell>76.30</cell><cell>86.83</cell><cell>83.07</cell></row><row><cell></cell><cell>Fusion</cell><cell>77.80</cell><cell>87.33</cell><cell>83.93</cell></row><row><cell></cell><cell>Transformer</cell><cell>81.60</cell><cell>86.39</cell><cell>84.68</cell></row><row><cell>4</cell><cell>ON-LSTM</cell><cell>77.40</cell><cell>87.00</cell><cell>83.57</cell></row><row><cell></cell><cell>Fusion</cell><cell>78.00</cell><cell>86.72</cell><cell>83.61</cell></row><row><cell></cell><cell>Transformer</cell><cell>81.70</cell><cell>86.72</cell><cell>84.93</cell></row><row><cell>Ensemble</cell><cell>ON-LSTM</cell><cell>78.00</cell><cell>87.39</cell><cell>84.04</cell></row><row><cell></cell><cell>Fusion</cell><cell>81.20</cell><cell>88.83</cell><cell>86.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc></figDesc><table><row><cell cols="4">COMPARISON OF ACCURACY (%) WITH VARIOUS METHODS ON THE</cell></row><row><cell cols="2">DHG-14/28 DATASET</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Fine</cell><cell>Coarse</cell><cell>Both</cell></row><row><cell>SOCJ+HoHD+HoHR [28]</cell><cell>73.60</cell><cell>88.33</cell><cell>83.07</cell></row><row><cell>NIUKF-LSTM [29]</cell><cell>-</cell><cell>-</cell><cell>84.92</cell></row><row><cell>SL-fusion-Average [30]</cell><cell>76.00</cell><cell>90.72</cell><cell>85.46</cell></row><row><cell>CNN+LSTM [10]</cell><cell>78.00</cell><cell>89.80</cell><cell>85.60</cell></row><row><cell>MFA-Net [31]</cell><cell>75.60</cell><cell>91.39</cell><cell>85.75</cell></row><row><cell>Ensemble of Models (Ours)</cell><cell>81.20</cell><cell>88.83</cell><cell>86.11</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was partially supported by Natural Sciences and Engineering Research Council of Canada through Discovery Grant "Biometric Intelligent Interfaces", and by the Department of National Defenses Innovation for Defense Excellence and Security (IDEaS) program, Canada.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automated cognitive health assessment using smart home monitoring of complex tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Dawadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitter-Edgecombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1302" to="1313" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Smartmind: Activity tracking and monitoring for patients with alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Y</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Advanced Information Networking and Applications</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="453" to="460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Embedded assessment of aging adults: a concept validation with stakeholders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pervasive Computing Technologies for Healthcare</title>
		<imprint>
			<date type="published" when="2010-03" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Smart home-based prediction of multidomain symptoms related to alzheimer&apos;s disease</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Aramendi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weakley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmitter-Edgecombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Goenaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Basarab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Carrasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1720" to="1731" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning activity predictors from sensor data: Algorithms, evaluation, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Minor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Cook</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2744" to="2757" />
			<date type="published" when="2017-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A survey on deep learning based approaches for action and gesture recognition in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Clapes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellantonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Escalante</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ponce-L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bar?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="476" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning for sensor-based activity recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hand gesture recognition with 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Motion feature augmented recurrent neural network for skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="2881" to="2885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional neural networks and long short-term memory for skeleton-based human activity and hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>N??ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cabido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Pantrigo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Montemayor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>V?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="80" to="94" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional twostream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Two-stream cnns for gesture-based verification and identification: Learning user style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="42" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">I-am: implicitly authenticate me person authentication on mobile devices through ear shape and arm gesture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Abate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nappi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricciardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Systems, Man, and Cybernetics: Systems</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multitouch gesture-based authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sae-Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Memon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Isbister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on information forensics and security</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="568" to="582" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking touchscreen biometrics for mobile authentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fierrez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pozo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinez-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Morales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2720" to="2733" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Feature fusion for online mutual knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09058</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Universal transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mish: A self regularized non-monotonic neural activation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08681</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="464" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Snapshot ensembles: Train 1, get m for free</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Int. Conf. on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Times-series data augmentation and deep learning for construction equipment activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Rashid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Engineering Informatics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">100944</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1206" to="1214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Skeleton-based dynamic hand gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">De</forename><surname>Smedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wannous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Vandeborre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hand joints-based gesture recognition for noisy dataset using nested interval unscented kalman filter with lstm network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The visual computer</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6-8</biblScope>
			<biblScope unit="page" from="1053" to="1063" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CNN+RNN Depth and Skeleton based Dyanamic Hand Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Yanushkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mfanet: Motion feature augmented network for dynamic hand gesture recognition from skeletal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">239</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
