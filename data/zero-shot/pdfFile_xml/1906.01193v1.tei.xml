<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Triangulation Learning Network: from Monocular to Stereo 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengyi</forename><surname>Qin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Wang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
							<email>yanlu@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Triangulation Learning Network: from Monocular to Stereo 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we study the problem of 3D object detection from stereo images, in which the key challenge is how to effectively utilize stereo information. Different from previous methods using pixel-level depth maps, we propose employing 3D anchors to explicitly construct object-level correspondences between the regions of interest in stereo images, from which the deep neural network learns to detect and triangulate the targeted object in 3D space. We also introduce a cost-efficient channel reweighting strategy that enhances representational features and weakens noisy signals to facilitate the learning process. All of these are flexibly integrated into a solid baseline detector that uses monocular images. We demonstrate that both the monocular baseline and the stereo triangulation learning network outperform the prior state-of-the-arts in 3D object detection and localization on the challenging KITTI dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection aims at localizing amodal 3D bounding boxes of objects of specific classes in the 3D space <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">29]</ref>. The detection task is relatively easier <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15]</ref> when active 3D scan data are provided. However, the active scan data are of high cost and limited in scalability. We address the problem of 3D detection from passive imagery data, which require only low-cost hardware, adapt to different scales of objects, and offer fruitful semantic features.</p><p>Monocular 3D detection with a single RGB image is highly ill-posed because of the ambiguous mapping from 2D images to 3D geometries, but still attracts research efforts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">30]</ref> because of its simplicity of design. Adding more input views can provide more information for 3D reasoning, which is ubiquitously demonstrated in the traditional multi-view geometry <ref type="bibr" target="#b9">[10]</ref> area by finding dense patch-level correspondences for points and then estimate their 3D locations by triangulation. The geometric methods deal with patch-level points with local features, while no semantic clues at object-level are considered.</p><p>Stereo data with pairs of images are more suitable for 3D detection, since the disparities between left and right images can reveal spacial variance, especially in the depth dimension. While extensive work <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b4">5]</ref> has been done on deep-learning based stereo matching, their main focus is on the pixel level rather than object level. 3DOP <ref type="bibr" target="#b2">[3]</ref> uses stereo images for 3D object detection and achieves state-of-the-art results. Nevertheless, later we show that we can obtain comparable results using only a monocular image, by properly placing 3D anchors and extending the region-proposal network (RPN) <ref type="bibr" target="#b28">[28]</ref> to 3D. Therefore, we are motivated to reconsider how to better exploit the potential of stereo images for accurate 3D object detection.</p><p>In this paper, we propose the stereo Triangulation Learning Network (TLNet) for 3D object detection from stereo images, which is free of computing pixel-level depth maps and can be easily integrated into the baseline monocular detector. The key idea is to use a 3D anchor box to explicitly construct object-level geometric correspondences of its two projections on a pair of stereo images, from which the network learns to triangulate a targeted object near the anchor. In the proposed TLNet, we introduce an efficient feature reweighting strategy that strengthens informative feature channels by measuring left-right coherence. The reweighting scheme filters out the signals from noisy and mismatched channels to facilitate the learning process, enabling our network to focus more on the key parts of an object. Without any parameters, the reweighting strategy imposes little on computational burden.</p><p>To examine our design, we first propose a solid baseline monocular 3D detector, with an overview shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. In combination with TLNet, we demonstrate that significant improvement can be achieved in 3D object detection and localization in various scenarios. Additionally, we provide quantitative analysis of the feature reweighting strategy in TLNet to have a better understanding of its effects. In summary, our contributions are three-fold:</p><p>? A solid baseline 3D detector that takes only a monocular image as input, which has comparable performance with its state-of-the-art stereo counterpart.</p><p>? A triangulation learning network that leverages the geometric correlations of stereo images to localize targeted 3D objects, which outperforms the baseline model by a significant margin on the challenging KITTI <ref type="bibr" target="#b7">[8]</ref> dataset.</p><p>? A feature reweighting strategy that enhances informative channels of view-specific RoI features, which benefits triangulation learning by biasing the network attention towards the key parts of an object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Monocular 3D Detection. Due to the information loss in the depth dimension, 3D object detection is in particular difficult given only a monocular image. Mono3D <ref type="bibr" target="#b1">[2]</ref> integrates semantic segmentation and context priors to generate 3D proposals. Extra computation is needed for semantic and instance segmentation, which slows down its running time. Xu et al. <ref type="bibr" target="#b30">[30]</ref> utilizes a stand-alone disparity estimation model <ref type="bibr" target="#b21">[22]</ref> to generate 3D point clouds from a monocular image, and then perform 3D detection using multi-level concatenated RoI features from RGB and the point cloud maps. However, it has to use extra data, i.e., ground truth disparities, to train the stand-alone model. Other methods <ref type="bibr" target="#b13">[14]</ref> leverage 3D CAD models to synthesize 3D object templates to supervise the training and guide the geometric reasoning in inference. While the previous methods are dependent on additional data for 3D perception, the proposed monocular baseline model can be trained using only ground truth 3D bounding boxes, which saves considerations on data acquisition.</p><p>Multi-View based 3D Detection. MV3D <ref type="bibr" target="#b3">[4]</ref> takes RGB images and multi-view projections of LIDAR 3D points as input, and then fuse the RoI pooled features for 3D bounding box prediction. By projecting the point clouds to bird'seye-view (BEV) maps, it first generates 2D bounding box proposals in BEV, then fixes the height to obtain 3D proposals. RoIAlign <ref type="bibr" target="#b10">[11]</ref> is performed in front-view and BEV point cloud maps and also in image to extract multi-view features, which are used to predict object classes and regress the final 3D bounding box. AVOD <ref type="bibr" target="#b16">[17]</ref> aggregates features from front view and BEV of LIDAR point to jointly generate proposals and detect objects. It is an early-fusion method, where the multi-view features are fused before the region-proposal stage <ref type="bibr" target="#b10">[11]</ref> to increase the recall rate. The most related approach to ours is 3DOP <ref type="bibr" target="#b2">[3]</ref> that uses stereo images for object detection. However, 3DOP <ref type="bibr" target="#b2">[3]</ref> directly relies on the disparity maps calculated from image pairs, resulting in its high computational cost and the imprecise estimation at distant regions. Our network is free of calculating pixel-level disparity maps. Instead, it learns to triangulate the target from left-right RoIs.</p><p>Learning based Stereo. Zbontar and LeCun <ref type="bibr" target="#b31">[31]</ref> propose a stereo matching network to learning a similarity measure on small image patches to estimate the disparity at each location of the input image. Because comparing the image patches for each disparity would increase the computational cost combinationally, the authors propose to propagate the full-resolution images to compute the matching cost for all pixels in only one forward pass. DispNet <ref type="bibr" target="#b23">[24]</ref> concatenates the stereo image pairs as input to learn the disparity and scene flow. Chen et al. <ref type="bibr" target="#b4">[5]</ref> design a convolutional spatial propagation network to estimate the affinity matrix for depth estimation. While the existing studies mainly focus on pixel-level learning, we primarily explore the instancelevel learning for 3D object detection using the intrinsic geometric correspondence of the stereo images.</p><p>Triangulation. Triangulation localizes a point by forming triangles to it from known points, which is of universal applications in 3D geometry estimation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>. In RGB-D based SLAM <ref type="bibr" target="#b5">[6]</ref>, triangulation can be utilized to create a sparse graph modeling the correlations of points and retrieve accurate navigation information in complex environment. It is also useful in camera calibration and motion tracking <ref type="bibr" target="#b11">[12]</ref>. In stereo vision, typical triangulation <ref type="bibr" target="#b31">[31]</ref> requires matching the 2D features in left and right frames at pixel level to estimate spacial dimensions, which can be computationally expensive and time consuming <ref type="bibr" target="#b23">[24]</ref>.</p><p>To avoid such computation and fully exploit stereo information for 3D object detection, we propose using 3D anchors as reference to detect and localize the object via learnable triangulation in a forward pass.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>In this section, we first present the baseline model that predicts oriented 3D bounding boxes from a base image I, and then introduce the triangulation learning network integrated in the baseline for a pair of stereo image (I l , I r ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>An oriented 3D bounding box is defined as</head><formula xml:id="formula_0">B = (C, S, ?), where C = (c x , c y , c z ) is the 3D center, S = (h, w, l)</formula><p>is the size along axis, and ? is the angle between itself and the vertical axis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Baseline Monocular Network</head><p>The baseline network taking a monocular image as the input is composed of a backbone and three subsequent modules, i.e., the front view anchor generation, the 3D box proposal and refinement. The three-stage pipeline progressively reduces the searching space by selecting confident anchors, which highly reduces computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Front View Anchor Generation</head><p>Following the dimension reduction principle, we first reduce the searching space in the 2D front view. The input image I is divided into a G x ? G y grid, where each cell predicts its objectness. The output objectness represents the confidence how likely the cell is surrounded by 2D projections of targeted objects. An example of the confidence map is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. In training, we first calculate 2D projected centers of the ground truth 3D boxes and compute their least Euclidean distance to all cells in the G x ? G y grid. Cells with distances less than 1.8 times their width are considered as foreground. For inference, foreground cells are selected to generate potential anchors.</p><p>Thus, we obtain 3D anchors located at the rays issued from the potential cells in an anchor pool, which contains a set of 3D anchors uniformly sampled at an interval of 0.25m on the ground plane within the view frustum and depth ranging [0, 70] meters. Anchors are represented by its 3D center and the prior size along three axes. There are two anchors with BEV orientation 0 ? and 90 ? for each object class at </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">3D Box Proposal and Refinement</head><p>The multi-stage proposal and refinement mechanism in our baseline network is similar to Faster-RCNN <ref type="bibr" target="#b28">[28]</ref>. In the 3D RPN, the potential anchors from front view generation are projected to the image plane to obtain RoIs. RoIAlign <ref type="bibr" target="#b10">[11]</ref> is adopted to crop and resize the RoI features from the feature maps. Each crop of RoI features is fed to task specific fully connected layers to predict 3D objectness confidence as well as regress the location offsets ?C = (?c x , ?c y , ?c z ) and dimension offsets ?S = (?h, ?w, ?l) to the ground truth. Non-Maximum Supression (NMS) is adopted to keep the top K proposals, where K = 1024 for both training and inference. In the refinement stage, we project those top proposals to image and again use RoIAlign <ref type="bibr" target="#b10">[11]</ref> to crop and resize the region of interest. The RoI features are passed to fully connected layers that classify the object and regress the 3D bounding box offsets (?C, ?S) as well as the orientation vector v ? in local coordinate system as defined in <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Triangulation Learning Network</head><p>The stereo 3D detection is performed by integrating a triangulation learning network into the baseline model. In the following, we first introduce the mechanism of the TL-Net that focuses on object-level triangulation in opposite to computationally expensive pixel-level disparity estimation, and then present the details of its network architecture.  <ref type="figure">Figure 4</ref>. Activations of different channels with different coherence scores. These small feature maps are cropped out using RoIAlign from the last convolutional layer, where the first row is from the left branch and the second row is from the right. Two branches of convolutional layers share the weights. Coherence score si is calculated for channel i. As we can see, channel 17 (g) and 22 (f) have noisy and less concentrated activations, while chanel 24 (a) is clear and informative of the key points of an object, e.g., wheels. Our objective is enhancing channels like (a) and weaken those like (g) and (f), so as to focus the network attention on specific parts of the object, which is empirically beneficial for discerning slight positional difference between where the object is present in left and right RoIs.  <ref type="figure">Figure 5</ref>. TLNet architecture. The inputs are a pair of RoIs obtained by projecting a 3D anchor box to the left and right feature maps with Croi channels. The coherence score si is computed between each left channel F l i and right channel F r i . We reweight the i th channel by multiplying with si. The features are fused and fed to fully-connected layers to predict objectness confidence and 3D bounding box offsets to the reference anchor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right RoI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Anchor Triangulation</head><p>Triangulation is known as localizing 3D points from multiview images in the classical geometry fields, while our objective is to localize a 3D object and estimates its size and orientation from stereo images. To achieve this, we introduce an anchor triangulation scheme, in which the neural network uses 3D anchors as reference to triangulate the targets.</p><p>Considering the RPN stage, we project the pre-defined 3D anchor to stereo images and obtain a pair of left-right RoIs, as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. If the anchor tightly fits the target in 3D, its left and right projections can consistently bound the object in 2D. On the contrary, when the anchor fails in fitting the the object, their geometric differences in 3D are reflected in the visual disparity of the left-right RoI pair. The 3D anchor explicitly constructs correspondences between its projections in multiple views. Since the location and size of the anchor box are already known, modeling the anchor triangulation is conducive to estimating the 3D objectness confidence, i.e., how well the anchor matches a target in 3D, as well as regressing the offsets applied to the box to minimize its variance with the target. Therefore, we propose TLNet towards triangulation learning as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">TLNet Architecture</head><p>The TLNet takes as input a pair of left-right RoI features F l and F r with C roi channels and size H roi ? W roi , which are obtained using RoIAlign <ref type="bibr" target="#b10">[11]</ref> by projecting the same 3D anchor to the left and right frames, as shown in <ref type="figure">Fig. 5</ref>. We utilize the left-right coherence scores to reweight each channel. The reweighted features are fused using element-wise addition and passed to task-specific fully-connected layers to predict the objectness confidence and 3D bounding box offsets, i.e., the 3D geometric variance between the anchor and target.</p><p>Coherence Score. The positional difference between where the target is present in the left and right RoIs reveals the spacial variance between the target box and the anchor box, as illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>. The TLNet is expected to utilize such a difference to predict the relative location of the target to the anchor, estimate whether they are a good match, i.e., objectiveness confidence, and regress the 3D bounding box offsets. To enhance the discriminative power of spacial variance, it is necessary to focus on representational key points of an object. <ref type="figure">Fig. 4</ref> shows that, though without explicit supervision, some channels in the feature maps has learned to extract such key points, e.g., wheels. The coherence score s i for the i th channel is defined as:</p><formula xml:id="formula_1">s i = cos &lt; F l i , F r i &gt;= F l i ? F r i ||F l i || ? ||F r i || (1)</formula><p>where cos is the cosine similarity function for each channel, F l i and F r i are the i th pair of feature maps, i.e., features from the i th channel in left and right RoIs.</p><p>From <ref type="figure">Fig. 4</ref>, we observe that the coherence score s i is lower when the activations are noisy and mismatched, while it is higher if the left-right activations are clear and coherent. In fact, from a mathematical perspective, F l i and F r i can be viewed as a pair of signal vectors. As you flatten along the row dimension, consecutive activations are more likely to align with other consecutive activations in a pair of RoIs, i.e., s i is closer to 1.</p><p>Channel Reweighting. By multiplying the i th channel with s i , we weaken the signals from noisy channels and bias the attention of the following fully-connected layers towards coherent feature representations. The reweighting is done in pairs for both the left and right feature maps, taking the form:</p><formula xml:id="formula_2">F l,re i = s i F l i , F r,re i = s i F r i<label>(2)</label></formula><p>, and it is implemented in the TLNet as illustrated in <ref type="figure">Fig. 5</ref>. We will demonstrate that the reweighting strategy has a positive effect on triangulation learning in the experiments.</p><p>Network Generality. The proposed architecture can be easily integrated into the baseline network by replacing the fully-connected layers after RoIAlign, in both the RPN and the refinement stage. In the refinement stage, classification outputs are object classes instead of objectness confidence as is in RPN. For the CNN backbone, the left and right branches share their parameters. Computing cosine similarity and reweighting do not require any extra parameters, thus imposing little in memory burden. SENet <ref type="bibr" target="#b12">[13]</ref> also involves reweighting feature channels. Their goal is to model the cross-channel relationships of a single input, while ours is to measure the coherence of a pair of stereo inputs and select confident channels for triangulation learning. In addition, their weights are learned by fully-connected layers, whose behavior is less interpretable. Our weights are the pairwise cosine similarities with clear physical significance. F l i and F r i can be viewed as two vectors, and s i describes their included angle, i.e., correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation Details</head><p>Network Setup. We choose VGG-16 <ref type="bibr" target="#b22">[23]</ref> as the CNN backbone, but without its fully-connected layers after pool5. Parameters are shared in the left and right branches.</p><p>All the input images are resized to 384 ? 1248 so that they can be divided by 2 for at least 5 times. For the front-view objectness prediction, we use the left branch only. We apply 1 ? 1 convolution to the output feature maps of pool5 and reduce the channels to 2, indicating the background and objectness confidence. Each pixel in the output feature maps represents a grid cell and yields a prediction.</p><p>For the region proposal and refinement stage, we start from the output of conv4 rather than pool5. To improve the performance on small objects, we leverage Feature Pyramid <ref type="bibr" target="#b20">[21]</ref> to upsample the feature maps to the original resolution. Since the region proposal stage aims at filtering background anchors and keep the confident anchors in an efficient manner, channels of the feature maps are reduced to 4 using 1 ? 1 convolution before RoIAlign <ref type="bibr" target="#b10">[11]</ref> and fully connected layers to save computational cost. For the refinement stage, however, we use full channels containing more information to yield finer predictions.</p><p>Training. All the weights are initialized by Xavier initializer <ref type="bibr" target="#b8">[9]</ref> and no pretrained weights are used. L2 regularization is applied to the model parameters with a decay rate of 5e-3. We first train the front-view objectness map for 20K iterations, then along with the RPN for another 40K iterations. In the next 60K iterations we add the refinement stage. For the above 120K iterations, the network is trained using Adam optimizer <ref type="bibr" target="#b15">[16]</ref> at a learning rate of 1e-4. Finally, we use SGD to further optimize the network for 20K iterations at the same learning rate. The batchsize is always set to 1 for the input. The network is trained using a single GPU of NVidia Tesla P40.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We evaluate the proposed network on the challenging KITTI dataset <ref type="bibr" target="#b7">[8]</ref>, which contains 7481 training images and 7518 testing images with calibrated camera parameters. Detection is evaluated in three regimes: easy, moderate and hard, according to the occlusion and truncation levels. We use the train1/val1 split setup in the previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, where each set contains half of the images. Objects of class Car are chosen for evaluation. Because the number of cars exceeds that of other objects by a significant margin, they are more suitable to assess a data-hungry deep-learning network. We compare our baseline network with state-of-theart monocular 3D detectors, MF3D <ref type="bibr" target="#b30">[30]</ref>, Mono3D <ref type="bibr" target="#b1">[2]</ref> and MonoGRNet <ref type="bibr" target="#b27">[27]</ref>. For stereo input, we compare our network with 3DOP <ref type="bibr" target="#b2">[3]</ref>.</p><p>Metrics. For 3D detection, we follow the official settings of KITTI benchmark to evaluate the 3D Average Precision (AP 3D ) at different 3D IoU thresholds. To evaluate the bird's eye view (BEV) detection performance, we use the   BEV Average Precision (AP BEV ). We also provide results for Average Location Precision (AP LOC ), in which a ground truth object is recalled if there is a predicted 3D location within certain distance threshold. Note that we obtain the detection results of Mono3D and 3DOP from the authors so as to evaluate them with different IoU thresholds. The evaluation of MF3D follows their original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D Object Detection</head><p>Monocular Baseline. 3D detection results are shown in <ref type="table" target="#tab_1">Table 1</ref>. Our baseline network outperforms monocular methods MF3D <ref type="bibr" target="#b30">[30]</ref> and Mono3D <ref type="bibr" target="#b1">[2]</ref> in 3D detection. For moderate scenarios, our AP 3D exceeds MF3D [30] by 4.50% at IoU = 0.5 and 4.65% at IoU = 0.7. For easy scenarios the gap is smaller. Since the raw detection results of MF3D <ref type="bibr" target="#b30">[30]</ref> are not publicly available, detailed quantitative comparison cannot be made. But it is possible that a part of the performance gap comes from the object proposal stage, where MF3D <ref type="bibr" target="#b30">[30]</ref> proposes on the image plane, while we directly propose in 3D. In moderate and hard cases where objects are occluded and truncated, 2D proposals can have difficulties retrieving the 3D information of a target that is partly unseen. In addition, the wide margin with Mono3D [2] reveals a better expressiveness of learnt features than handcrafted features.</p><p>TLNet. By combining with the proposed TLNet, our method outperforms the baseline network and 3DOP <ref type="bibr" target="#b2">[3]</ref> across all 3D IoU thresholds in easy, moderate and hard scenarios. Under IoU thresholds of 0.3 and 0.5, stereo triangulation learning brings ? 10% improvement in AP 3D .</p><p>In comparison with 3DOP <ref type="bibr" target="#b2">[3]</ref>, our method achieves ? 10% better AP 3D across all regimes. Note that 3DOP <ref type="bibr" target="#b2">[3]</ref> needs to estimate the pixel-level disparity maps from stereo images to calculate depths and generate proposals, which is erroneous at distant regions. Instead, the proposed method utilizes 3D anchors to construct geometric correspondence between left-right RoI pairs, then triangulates the targeted objects using coherent features. According to the curves in <ref type="figure" target="#fig_7">Fig. 8 (a)</ref>, our main method has a higher precision than 3DOP <ref type="bibr" target="#b2">[3]</ref> and a higher recall than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">BEV Object Detection</head><p>Monocular Baseline. Results are presented in <ref type="table" target="#tab_2">Table 2</ref>. Compared with 3D detection, we still evaluate the same set of 3D bounding boxes, but the vertical axis is disregarded <ref type="figure">Figure 6</ref>. Qualitative comparison. Orange bounding boxes are detection results, while the green boxes are ground truths. For our main method, we also visualize the projected 3D bounding boxes in image, i.e., the first and forth rows. The lidar point clouds are visualized for reference but not used in both training and evaluation. It is shown that the triangulation learning method can reduce missed detections and improve the performance of depth prediction at distant regions.</p><p>for BEV IoU calculation. The baseline keeps its advantages over MF3D in moderate and hard scenarios. Note that MF3D uses extra data to train pixel-level depth maps. In easy cases, objects are clearly presented, and pixel-level predictions with sufficient local features can obtain more accurate results in statistics. However, pixel-level depth maps with high resolution are unfortunately not always available, indicating their limitations in real applications.</p><p>TLNet. Not surprisingly, by use of TLNet, we outperform the monocular baseline under all IoU thresholds in various scenarios. At IoU threshold 0.3 and 0.5, the triangulation learning yields ? 8% increase in AP BEV . Our method also surpasses 3DOP by a notable margin, especially for strict evaluation criteria, e.g., under IoU threshold of 0.7, which reveals the high precision of our predictions. Such performance improvement mainly comes from two aspects: 1) the solid monocular baseline already achieves comparable results with 3DOP; 2) the stereo triangulation learning scheme further enhances the capability of our baseline model in object localization. <ref type="figure" target="#fig_7">Fig. 8 (b</ref>  <ref type="table">Table 4</ref>. Effect of reweighting on AP 3D . Three fusion methods in <ref type="figure">Fig. 9</ref> are compared, including concatenation, direct addition and the proposed reweighting strategy. der the strict distance threshold of 0.5m, indicating the high precision of our top predictions.</p><p>TLNet. TLNet boosts the baseline performance marginally, especially for distance threshold of 2m and 1m, where there is ? 10% gain in AP LOC . According to the Recall-Precision curves in <ref type="figure" target="#fig_7">Fig. 8 (c)</ref>, TLNet increases both the precision and recall. The maximum precision is close to 1.0 because most of the top predictions are correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>3D bounding boxes predicted by the baseline network and our stereo method are presented in <ref type="figure">Fig. 6</ref>. In general, the predicted orange bounding boxes matches the green ground truths better when TLNet is integrated into the baseline model. As shown in (a) and (c), our method can reduce depth error, especially when the targets are far away from the camera. Object targets missed by the baseline in (b) and (f) are successfully detected. The heavily truncated car in the right-bottom of (d) is also detected, since the object proposals are in 3D, regardless of 2D truncation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>In TLNet, the small feature maps obtained by use of RoIAlign <ref type="bibr" target="#b10">[11]</ref> are not directly fused. In order to focus more attention on the key parts of an object and reduce noisy signals, we first calculate the pairwise cosine similarity cos i as coherence score, then reweight corresponding channels by multiplying with s i . See 3.2.2 and <ref type="figure">Fig. 5</ref> for details. In the followings, we examine the effectiveness of reweighting by replacing it with other two fusion methods, i.e., direct concatenation and element-wise addition, as is shown in <ref type="figure">Fig. 9</ref>.  Comparisons between their AP 3D and AP BEV are presented in <ref type="table">Table 4</ref>. The evaluation corresponds with the empirical analysis in 3.2.2. Since the left and right branches share their weights in the backbone network, the same channels in left and right RoIs are expected to extract the same kind of features. Some of these features are more suitable for efficient triangulation learning, which are strengthened by reweighting. Note that the coherence score s i is not fixed for each channel, but is dynamically determined by the RoI pairs cropped out at specific locations.  <ref type="figure">Figure 9</ref>. Feature fusion methods in TLNet. (c) is the proposed strategy. It first computes the coherence score of each pair of channels, then reweights the channels and adds them together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a novel network for performing accurate 3D object detection using stereo information. We build a solid baseline monocular detector, which is flexibly extended to stereo by combining with the proposed TLNet. The key idea is to use 3D anchors to construct geometric correspondences between its projections in stereo images, from which the network learns to triangulate the targeted object in a forward pass. We also introduce an efficient channel reweighting method to strengthen informative features and weaken the noisy signals. All of these are integrated into our baseline detector and achieve state-of-the-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of the proposed 3D detection pipeline. The baseline monocular network is indicated with blue background, and can be easily extended to stereo inputs by duplicating the baseline and further integrating with the proposed TLNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Front view anchor generation. Potential anchors are of high objectness in the front view. Only the potential anchors are fed into RPN to reduce searching space and save computational cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Anchor triangulation. By projecting the 3D anchor box to stereo images, we obtain a pair of RoIs. The left RoI establishes a geometric correspondence with the right one via the anchor box. The nearby target is present in both RoIs with slightly positional differences. Our TLNet takes the RoI pair as input and utilizes the 3D anchor as reference to localize the targeted object. the same location. For different classes, we calculate the prior size by averaging corresponding samples in the training split. An example of generated anchors are shown inFig. 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results for persons.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>3D localization at distance threshold of 1m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Recall-precision curves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>3D detection performance. Average Precision of 3D bounding boxes on KITTI<ref type="bibr" target="#b7">[8]</ref> validation set. The LiDAR based method VeloFCN<ref type="bibr" target="#b19">[20]</ref> is listed for reference but not compared.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell>Easy</cell><cell>AP 3D (IoU=0.3) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP 3D (IoU=0.5) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP 3D (IoU=0.7) Moderate</cell><cell>Hard</cell></row><row><cell>VeloFCN</cell><cell>LiDAR</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>67.92</cell><cell>57.57</cell><cell>52.56</cell><cell>15.20</cell><cell>13.66</cell><cell>15.98</cell></row><row><cell>Mono3D</cell><cell>Mono</cell><cell>28.29</cell><cell>23.21</cell><cell>19.49</cell><cell>25.19</cell><cell>18.20</cell><cell>15.22</cell><cell>2.53</cell><cell>2.31</cell><cell>2.31</cell></row><row><cell>MF3D</cell><cell>Mono</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>47.88</cell><cell>29.48</cell><cell>26.44</cell><cell>10.53</cell><cell>5.69</cell><cell>5.39</cell></row><row><cell>MonoGRNet</cell><cell>Mono</cell><cell>72.17</cell><cell>59.57</cell><cell>46.08</cell><cell>50.51</cell><cell>36.97</cell><cell>30.82</cell><cell>13.88</cell><cell>10.19</cell><cell>7.62</cell></row><row><cell>3DOP</cell><cell>Stereo</cell><cell>69.79</cell><cell>52.22</cell><cell>49.64</cell><cell>46.04</cell><cell>34.63</cell><cell>30.09</cell><cell>6.55</cell><cell>5.07</cell><cell>4.10</cell></row><row><cell>Ours (baseline)</cell><cell>Mono</cell><cell>72.91</cell><cell>55.72</cell><cell>49.19</cell><cell>48.34</cell><cell>33.98</cell><cell>28.67</cell><cell>13.77</cell><cell>9.72</cell><cell>9.29</cell></row><row><cell>Ours</cell><cell>Stereo</cell><cell>78.26</cell><cell>63.36</cell><cell>57.10</cell><cell>59.51</cell><cell>43.71</cell><cell>37.99</cell><cell>18.15</cell><cell>14.26</cell><cell>13.72</cell></row><row><cell>Method</cell><cell>Data</cell><cell>Easy</cell><cell>AP BEV (IoU=0.3) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP BEV (IoU=0.5) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP BEV (IoU=0.7) Moderate</cell><cell>Hard</cell></row><row><cell>VeloFCN</cell><cell>LiDAR</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>79.68</cell><cell>63.82</cell><cell>62.80</cell><cell>40.14</cell><cell>32.08</cell><cell>30.47</cell></row><row><cell>Mono3D</cell><cell>Mono</cell><cell>32.76</cell><cell>25.15</cell><cell>23.65</cell><cell>30.50</cell><cell>22.39</cell><cell>19.16</cell><cell>5.22</cell><cell>5.19</cell><cell>4.13</cell></row><row><cell>MF3D</cell><cell>Mono</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>55.02</cell><cell>36.73</cell><cell>31.27</cell><cell>22.03</cell><cell>13.63</cell><cell>11.60</cell></row><row><cell>MonoGRNet</cell><cell>Mono</cell><cell>73.10</cell><cell>60.66</cell><cell>46.86</cell><cell>54.21</cell><cell>39.69</cell><cell>33.06</cell><cell>24.97</cell><cell>19.44</cell><cell>16.30</cell></row><row><cell>3DOP</cell><cell>Stereo</cell><cell>71.41</cell><cell>57.78</cell><cell>51.91</cell><cell>55.04</cell><cell>41.25</cell><cell>34.55</cell><cell>12.63</cell><cell>9.49</cell><cell>7.59</cell></row><row><cell>Ours (baseline)</cell><cell>Mono</cell><cell>74.18</cell><cell>57.04</cell><cell>50.17</cell><cell>52.72</cell><cell>37.22</cell><cell>32.16</cell><cell>21.91</cell><cell>15.72</cell><cell>14.32</cell></row><row><cell>Ours</cell><cell>Stereo</cell><cell>81.11</cell><cell>65.25</cell><cell>58.15</cell><cell>62.46</cell><cell>45.99</cell><cell>41.92</cell><cell>29.22</cell><cell>21.88</cell><cell>18.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>BEV detection performance. Average Precision of BEV bounding boxes on KITTI [8] validation set. Different from typical 2D detection evaluation, the bounding boxes are orientated, i.e., not necessarily aligned on each axis.</figDesc><table><row><cell>Method</cell><cell>Data</cell><cell>Easy</cell><cell>AP LOC (&lt;2m) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP LOC (&lt;1m) Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>AP LOC (&lt;0.5m) Moderate</cell><cell>Hard</cell></row><row><cell>Mono3D</cell><cell>Mono</cell><cell>47.21</cell><cell>35.82</cell><cell>35.40</cell><cell>17.74</cell><cell>14.80</cell><cell>14.05</cell><cell>4.78</cell><cell>4.06</cell><cell>4.03</cell></row><row><cell>MonoGRNet</cell><cell>Mono</cell><cell>83.09</cell><cell>64.82</cell><cell>55.79</cell><cell>56.29</cell><cell>42.29</cell><cell>35.01</cell><cell>28.38</cell><cell>19.73</cell><cell>18.06</cell></row><row><cell>3DOP</cell><cell>Stereo</cell><cell>84.53</cell><cell>65.37</cell><cell>64.10</cell><cell>60.84</cell><cell>45.11</cell><cell>40.35</cell><cell>27.53</cell><cell>19.39</cell><cell>17.64</cell></row><row><cell>Ours (baseline)</cell><cell>Mono</cell><cell>75.50</cell><cell>59.33</cell><cell>52.92</cell><cell>58.11</cell><cell>41.05</cell><cell>36.59</cell><cell>30.82</cell><cell>21.25</cell><cell>18.03</cell></row><row><cell>Ours</cell><cell>Stereo</cell><cell>86.78</cell><cell>72.17</cell><cell>66.27</cell><cell>69.15</cell><cell>51.68</cell><cell>47.68</cell><cell>37.36</cell><cell>27.42</cell><cell>24.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>3D localization performance. Average Location Precision of 3D bounding boxes under different distance thresholds on KITTI [8] validation set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast detection of multiple textureless 3-d objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="108" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.03217</idno>
		<title level="m">Rgb-d slam in dynamic environments using points correlations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">Mask r-cnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dt-slam: Deferred triangulation for robust slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 2nd International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="609" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Ssd-6d: Making rgb-based 3d detection and 6d pose estimation great again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV 2017)</title>
		<meeting>the International Conference on Computer Vision (ICCV 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A new 3d object pose detection method using lidar shape set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-U</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">882</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02294</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2d-driven 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lahoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll&amp;apos;ar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03144v2</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition(CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4040" to="4047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal camera motion estimation by condensation and robust statistics distance measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="119" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Petroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Oostrum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Stappers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bailes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhandari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D R</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burgay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Burke-Spolaor</surname></persName>
		</author>
		<editor>A. D. Cameron, D. J. Champion, R. P. Eatough, C. M. L. Flynn, A. Jameson, S. Johnston, E. F</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">A fast radio burst with a low dispersion measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Possenti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Van Straten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tiburzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.10773</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Three-dimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-level fusion based 3d object detection from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2345" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stereo matching by training a convolutional neural network to compare image patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-view self-supervised deep learning for 6d pose estimation in the amazon picking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
