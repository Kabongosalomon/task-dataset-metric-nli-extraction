<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confidence Guided Stereo 3D Object Detection with Split Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyao</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
						</author>
						<title level="a" type="main">Confidence Guided Stereo 3D Object Detection with Split Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate and reliable 3D object detection is vital to safe autonomous driving. Despite recent developments, the performance gap between stereo-based methods and LiDARbased methods is still considerable. Accurate depth estimation is crucial to the performance of stereo-based 3D object detection methods, particularly for those pixels associated with objects in the foreground. Moreover, stereo-based methods suffer from high variance in the depth estimation accuracy, which is often not considered in the object detection pipeline. To tackle these two issues, we propose CG-Stereo, a confidence-guided stereo 3D object detection pipeline that uses separate decoders for foreground and background pixels during depth estimation, and leverages the confidence estimation from the depth estimation network as a soft attention mechanism in the 3D object detector. Our approach outperforms all state-of-the-art stereo-based 3D detectors on the KITTI benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>3D object detection is vital to applications such as autonomous driving. Many LiDAR-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> achieve strong performance due to the accurate depth information that LiDAR provides. Compared with LiDAR, a stereo camera setup is less expensive and provides more dense information. In addition, stereo detection could add redundancy to an autonomous driving system and help reduce safety risks in combination with LiDAR methods. Recent stereo-based methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> have shown promising performance, although the detection performance gap between stereo and LiDAR configurations is still considerable.</p><p>One recent state-of-the-art stereo-based approach is Pseudo-LiDAR <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, which first estimates disparities with a stereo matching network, converts the estimated disparities into a 3D point cloud, and then feeds the estimated point cloud to a LiDAR-based 3D object detector. However, compared with the LiDAR point cloud, the estimated point cloud often has poor depth estimation, particularly as depth increases, where it no longer preserves the shape of the objects. One attribute of this method is that the stereo matching algorithm jointly estimates both foreground and background pixels and does not learn specifically the depth and shape of the foreground objects. <ref type="bibr" target="#b7">[8]</ref> shows that the depth distribution and pattern for foreground pixels and background pixels are different, and treating foreground and background pixels equally leads to sub-optimal results in a monocular depth estimation pipeline. In this paper, we propose to use two separate decoders for foreground and background pixels in a Authors are with the University of Toronto Institute for Aerospace Studies. email: (chengyao.li@mail.utoronto.ca), (kujason.ku@mail.utoronto.ca), (stevenw@utias.utoronto.ca) <ref type="figure">Fig. 1</ref>: A comparison between our proposed depth estimation module with our baseline HD 3 on KITTI dataset. Using LIDAR measurements (shown in white) as a reference, our proposed method (shown in dark cyan) is able to learn the depth and shape of the car more accurately compared with our baseline (shown in yellow). The 3D bounding box is shown in red. stereo matching network to provide better estimates of object shape and depth. The foreground and background masks can be obtained from image segmentation. In cooperation with the point cloud loss from <ref type="bibr" target="#b8">[9]</ref>, we show in <ref type="figure">Fig. 1</ref> that with our approach the depth and shape of the object can be significantly improved. With further experiments, we show that such improvement also leads to better 3D object detection performance.</p><p>In contrast to LiDAR measured point clouds, the accuracy of stereo point clouds greatly varies across a scene. For constant pixel-level disparity error, the depth error increases as depth increases because of the effect of triangulation. In addition, the estimated point clouds also suffer from poor depth estimates at object boundaries, because it can be hard for a stereo matching algorithm to determine whether a pixel belongs to the object or the background <ref type="bibr" target="#b5">[6]</ref>. In the original Pseudo-LiDAR pipeline <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, the estimated point cloud is directly fed into a 3D object detector which does not consider any uncertainty information. To address this issue, we propose to encode the uncertainty output from the depth estimation module as an additional layer in the point cloud serving as a soft attention mechanism. This method allows the network to focus on points with high confidence and mitigates the effect of low confidence points such as points</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 2:</head><p>A comparison between object bounding box detections without confidence (shown in teal) and with confidence (shown in green). Ground truth bounding box is shown in red. In the right image, points with higher confidence are shown in lighter colors. With confidence as an additional layer, the network is able to focus more on the points with high confidence (cycled in yellow) and ignore the points at object boundaries that have low confidence (cycled in white).</p><p>on the object boundary, as shown in <ref type="figure">Fig. 2</ref>.</p><p>In this paper, we propose CG-Stereo, a confidence guided stereo 3D object detection pipeline. To summarize, our main contributions are as follows:</p><p>? We propose the use of separate depth decoders for foreground and background pixels in the stereo matching network, which leads to improved depth estimation accuracy of the foreground pixels and improved object detection performance. <ref type="bibr">?</ref> We propose the use of confidence estimates from the stereo matching algorithm as a soft attention mechanism to guide the object detector network to focus more on the points with higher quality depth information, leading to further improvement in object detection accuracy. <ref type="bibr">?</ref> We demonstrate state-of-the-art performance that exceeds existing stereo-based methods on the challenging KITTI 3D object detection benchmark <ref type="bibr" target="#b9">[10]</ref> for all three object classes. Specifically, our approach surpasses the next best-performing method by 1.4%, 6.7%, and 12.7% AP at 0.7 IOU on cars, pedestrians and cyclists, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Stereo depth estimation. For stereo vision, depth is often estimated by determining the stereo correspondences between the left and right images. Stereo matching is a well-established field of research <ref type="bibr" target="#b11">[11]</ref>, with a long history of classical methods <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>. With the recent development of deep learning, end-to-end learning methods have shown significant improvements in this task. A standard learning approach of stereo matching is to construct a 3D cost volume to minimize the matching cost <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>.</p><p>Chang et al. <ref type="bibr" target="#b15">[15]</ref> propose a pyramid pooling module for incorporating global context followed by a stacked hourglass 3D CNN. Yin et al. <ref type="bibr" target="#b17">[17]</ref> determine the stereo matching by decomposing the full match density into multiple scales hierarchically first and then compose a global match density. This method not only achieves state-of-the-art results on established benchmarks but also predicts a confidence map that indicates the certainty of the estimation for each pixel. Our stereo 3D detection method takes advantage of the confidence estimation and demonstrates they can be used to improve stereo 3D object detection performances.</p><p>LiDAR-based 3D object detection. LiDAR-based object detection methods have shown strong performances and are widely used in autonomous driving since LiDAR provides accurate point clouds in terms of object depth and shape. Recent methods either use voxelization <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, PointNet <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b2">[3]</ref>, or a combination of the two <ref type="bibr" target="#b21">[21]</ref>, <ref type="bibr" target="#b22">[22]</ref>, <ref type="bibr" target="#b23">[23]</ref> to learn features from point cloud data. Taking advantage of the mature pipeline of LiDAR-based detectors, the performance is transferable to stereo-based detectors.</p><p>Stereo-based 3D object detection. In recent years, stereo-based 3D object detection methods have shown promising improvements in performance. Stereo R-CNN <ref type="bibr" target="#b6">[7]</ref> combines 2D proposals from both left and right images along with the sparse keypoints to generate coarse 3D bounding boxes, and then refines the bounding box using the photometric alignment of left and right regions of interest (ROIs). TLNet <ref type="bibr" target="#b24">[24]</ref> projects the predefined anchor box to stereo images to obtain a pair of RoIs, learns to offset these RoIs, and uses triangulation to localize the objects. RT3DStereo <ref type="bibr" target="#b26">[25]</ref> proposes to use semantic information together with disparity information to recover 3D bounding boxes. However, they do not take advantage of the semantic information to obtain better depth estimation. Pseudo-LiDAR <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> proposes to mimic the LiDAR signal by converting the depth map to a point cloud, and then feed this point cloud to a LiDAR-based detector. This intuitive method reduces the performance gap between the stereo-based methods and LiDAR-based methods. However, the point cloud from stereo matching preserves streaking artifacts at the object boundaries, leading to inaccurate bounding box estimates. OC-Stereo <ref type="bibr" target="#b5">[6]</ref> tries to solve this issue by estimating disparity only on the associated 2D bounding box area. However, this approach requires the 2D detection of the objects to be successful in both left and right images, which is difficult for objects that are truncated on image boundaries or are occluded from one view. It also completely ignores the background pixels which provide context to the 3D scene. Our method estimates the point cloud for both foreground and background pixels and keeps the points belonging to the ground plane since they contain useful contextual information in the 3D detection phase. The most recent state-of-the-art method, DSGN <ref type="bibr" target="#b27">[26]</ref>, proposes the use of a differentiable 3D volumetric representation of the environment to solve stereo 3D object detection. Their <ref type="figure">Fig. 3</ref>: Overview of our network. A semantic segmentation network first determines the foreground masks and background masks in the left image. The stereo matching network then estimates the disparities of the foreground and background pixels separately using two decoders and outputs the confidence associated with the estimation for each pixel. The disparity map is converted into a 3D point cloud with a confidence score as an additional layer. Points belonging to the background are filtered out except for the ones belonging to the ground plane. The resulting point cloud is then fed into a point cloud-based 3D object detector.</p><p>method achieves remarkable results on the KITTI car class, but the performances on pedestrians are not as competitive with the state-of-the-art. In comparison, our decomposed architecture allows us to perform well on pedestrians and cyclists even with the limited training data available for these classes in the KITTI dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ARCHITECTURE</head><p>The overall pipeline of our method is shown in <ref type="figure">Fig. 3</ref>. First, a semantic segmentation network determines the foreground pixels and background pixels in the left image. We define foreground pixels as the pixels that belong to the objects of interest and background as all other pixels. Then, the stereo matching network estimates the disparities of the foreground and background pixels separately with two separate decoders. It also generates a confidence map associated with each pixel representing the certainty of the network's estimation. The disparity map is converted into a 3D point cloud with confidence as an additional layer. Points belonging to the background are filtered out except for the ones that lie on the ground plane. We use the same method as 3DOP <ref type="bibr" target="#b28">[27]</ref> for stereo ground plane generation. The remaining point cloud is finally fed into a LiDAR-based 3D object detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Semantic Segmentation</head><p>For the 3D object detection task, it is common to segment the sensor input depending on the objects of interest. Pseudo-LiDAR <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> converts the stereo image pair to a point cloud and then relies on a LiDAR-based 3D object detector to find objects. However, compared with LiDAR point clouds, the estimated point clouds have lower accuracy and thus are harder to segment. In addition, the texture and color information is lost in this process. We argue that it is possible to leverage image segmentation information in stereo object detection for improved detection accuracy, rather than relying on the LiDAR-based 3D detector exclusively. We also show that the foreground and background masks from semantic segmentation improve the depth estimation of the foreground pixels in Section III-B, which contributes to higher 3D object detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Stereo Split Depth Estimation</head><p>Our stereo depth estimation is performed via stereo matching and the proposed formulation is agnostic to any stereo matching algorithm. We build on top of HD 3 due to its state-of-the-art performance and ability to run in real-time. In addition, due to its probabilistic framework for match distribution estimation, an uncertainty associated with the estimate at every pixel can be naturally derived <ref type="bibr" target="#b17">[17]</ref>.</p><p>Stereo Matching Architecture. HD 3 is designed for learning probabilistic pixel correspondences in both optical flow and stereo matching tasks <ref type="bibr" target="#b17">[17]</ref>, and we employ their stereo matching implementation as a baseline. The core idea of HD 3 is to decompose the full discrete match distributions of pixel-wise correspondences into multiple scales hierarchically, estimate the local matching distributions at each scale, and then compose them from all levels. The resulting distributions at each pixel in the reference image are referred to as match densities. At each image scale level l, a cost volume is constructed to find the correlation between the pixels in both images and a density decoder is trained to estimate the decomposed match density p l . For more details, readers may refer to the original HD 3 paper <ref type="bibr" target="#b17">[17]</ref>.</p><p>Our modified version of the HD 3 network is shown in <ref type="figure">Fig. 4</ref>. We only show one level for simplicity. Instead of relying on one density decoder for the entire image, we have two parallel density decoders with the same structure for foreground and background pixels, respectively. At each level l, each decoder takes the feature maps F l , cost volume, and the density embedding from the previous level E l?1 f g <ref type="figure">Fig. 4</ref>: Modified HD 3 network at the l th level. Instead of one density decoder for the entire image, we use separate density decoders for foreground pixels and background pixels, respectively, which allows us to optimize the weights specifically for each task.</p><p>or E l?1 bg as input, and outputs an estimated match density p l f g or p l bg , and the density embedding at the current level E l f g or E l bg . Then, we use the foreground and background masks, denoted as m l f g and m l bg , to mask out the output from the two decoders, and then fuse them. The match density is then converted into an estimated residual disparity E[g l ] at the current level. The model-inherent uncertainty can be estimated by applying a softmax operation and a max-pooling operation to the estimated match density at the highest level.</p><p>Loss Function. We adapt the foreground-background sensitive loss function from a monocular-based method <ref type="bibr" target="#b7">[8]</ref> and add the point cloud loss from <ref type="bibr" target="#b8">[9]</ref>. The total loss is defined as</p><formula xml:id="formula_0">L total = ? f L f g + (1 ? ? f )L bg + ?L pc<label>(1)</label></formula><p>where ? f is the weight coefficient representing the degree of preference for foreground pixels. L f g and L bg are the Kullback-Leibler divergence loss for foreground pixels and background pixels, respectively, ? is a weight coefficient, and L pc is the point cloud loss, which is set to be a smooth L1 loss of the difference between the foreground point cloud p c with its ground truth point cloud p gt in camera frame. We include the loss for background pixels because there is interdependence between foreground and background pixels for inferring the depth <ref type="bibr" target="#b7">[8]</ref>, and background provides context and support for 3D box regression <ref type="bibr" target="#b2">[3]</ref>. We include a point cloud loss because it directly penalizes the estimated 3D point cloud and further improves the 3D detection accuracy as demonstrated in Sec. V-B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Point Cloud Generation</head><p>The disparity map is converted to 3D points using the camera projection model as shown in Eq. 2.</p><formula xml:id="formula_1">x = (u ? c u )z f u , y = (v ? c v )z f v , z = f u b d (2)</formula><p>where x, y, z is the position of the points, (c u , c v ) is the camera center, (f u , f v ) is the focal length, b is the baseline, and d is the estimated disparity for a given pixel. Image segmentation is a relatively mature field and can provide robust results. As a consequence, rather than feeding the entire point cloud to the 3D object detector, we leverage the segmentation masks and feed only the points that are estimated to be from the foreground pixels. We also keep the points that belong to the ground plane as they provide useful contextual information and supporting information for generating proposals in the detection stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Confidence Map</head><p>In comparison with LiDAR point clouds, the estimated point clouds often suffer from high variance in the accuracy of depth. To consider this difference, we take advantage of the confidence estimation from the stereo matching algorithm and encode this information in the point cloud simply as an additional layer as shown in Eq. 3.</p><formula xml:id="formula_2">p c = ? ? ? ? x y z ? ? ? ? ?<label>(3)</label></formula><p>where ? represents the confidence for each point. <ref type="figure">Fig. 5</ref> shows the relationship between the confidence estimation and the estimation error for all the pixels belonging to cars on KITTI validation set. The inverse relationship shown in <ref type="figure">Fig. 5</ref> proves that the confidence estimates indicate the accuracy of the estimation and the quality of the estimated point clouds. Adding the confidence estimation as an additional layer is shown to improve object detection performance in Section V-B. <ref type="figure">Fig. 5</ref>: Disparity Error vs. Confidence Estimation from modified HD 3 for all pixels belonging to cars in the validation set on KITTI object detection benchmark <ref type="bibr" target="#b9">[10]</ref>. Each box represents a 5% range in confidence. The median is shown on top of the box. There is a trend that a higher confidence estimation indicates a higher quality of the estimation output. We do not include plots with confidence lower than 50%, because they contain significantly fewer samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. 3D Box Regression Network.</head><p>In the detection phase, we choose the open-source PointR-CNN as our 3D object detector for its strong performance, and because it works directly on point clouds without voxelization, allowing us to encode the confidence estimation into the points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION DETAILS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic</head><p>Segmentation. We employ VideoProp-LabelRelax <ref type="bibr" target="#b29">[28]</ref> as the semantic segmentation network during inference. Since the KITTI semantic segmentation dataset <ref type="bibr" target="#b9">[10]</ref> has only 200 labelled images, the network and model was instead trained on Mapillary <ref type="bibr" target="#b30">[29]</ref> and Cityscapes <ref type="bibr" target="#b31">[30]</ref> before being finetuned on KITTI. There is a class discrepancy between the object detection benchmark and the semantic segmentation benchmark on KITTI. On the object detection benchmark, the cyclist class is a single stand-alone class, but on the semantic segmentation benchmark, the rider and bike classes are separate. To solve this issue, we dilate the rider masks and check if they overlap with a bike mask. If there is an overlap, we keep the union of the original rider mask and the bike mask as a cyclist mask. All other bike masks are discarded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stereo Depth Estimation.</head><p>For our stereo matching algorithm, we use the model pretrained on the FlyingThings3D Dataset <ref type="bibr" target="#b32">[31]</ref>, and then train the proposed two-decoder network on the training split of the KITTI object detection dataset. The foreground and background decoders have the same pre-trained weights before training on KITTI. To train on KITTI, we use the depth map generated by depth completion <ref type="bibr" target="#b33">[32]</ref> and their corresponding point clouds as ground truth. To obtain the ground truth instance segmentation masks used during training, we follow <ref type="bibr" target="#b8">[9]</ref> and project ground truth points within the 3D labels to the image as the foreground masks. Training is performed for 375 epochs, with a batch size of 32 and a learning rate of 5 ? 10 ?4 . The learning rate decays by 0.5 at the 125 th , 187 th , and 250 th epochs. We apply horizontal flipping as data augmentation. Specifically, we increase the number of training samples by switching the left image and right image and horizontally flipping both of them. For this module, We train one model for cars, and another model for pedestrians and cyclists.</p><p>3D Object Detection. The region proposal network of PointRCNN is trained for 200 epochs with a batch size of 16 and a learning rate of 0.001, and the 3D box refinement network is trained for 50 epochs with a batch size of 8 and a learning rate of 0.001. For augmentations, we follow the original paper of PointRCNN <ref type="bibr" target="#b2">[3]</ref>. Similar to PointRCNN, we subsample 16,384 points for each scene. Specifically, we sample half of the points that have depth larger than 20 m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head><p>We evaluate our proposed method on the widely used KITTI 3D object detection dataset <ref type="bibr" target="#b9">[10]</ref>. Specifically, we first compare the 3D object detection results with the state-of-the-art stereo-based detectors, then validate each contribution through ablation studies, and finally show qualitative results. KITTI contains 7,481 stereo image-pairs for training and 7,518 for testing. The benchmark also has annotations for 3 classes, which are cars, pedestrians and cyclists. Each annotation is categorized as easy, moderate, and hard based on the 2D box height, occlusion, and truncation. We follow the same training and validation split as other methods <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. We also submit our results for all three classes to the online KITTI test server. KITTI recently changed its evaluation metrics on the test server. For the results on test set and in the ablation studies, we use the new KITTI metric which is mean average precision with 40 recall positions. For a fair comparison with other approaches, the results on the validation set are compared using the original KITTI metric with 11 recall positions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. AP Comparison with State-of-the-Art Methods</head><p>We compare our method with state-of-the-art stereo-based methods on the KITTI benchmark in <ref type="table" target="#tab_1">Tables I, II and III.</ref> For the car class, our approach outperforms all state-of-the-art methods on the KITTI validation split in all categories. On the test set, our method ranked the first among all stereobased methods on all three difficulties in both AP 3D and AP BEV . Specifically for moderate difficulty, we show a 1.40% increase in AP 3D , and a 1.39% increase in AP BEV . For pedestrians and cyclists, our proposed method outperforms all other stereo-based methods by significant margins. Most noticeably, on the test server, we have 6.73% and    12.72% AP increase in the 3D moderate category at 0.7 IoU for pedestrians and cyclists, respectively. For classes with limited data, our decomposed pipeline allows us to pretrain the sub-modules using additional datasets, which performs significantly better than other methods that lack this ability. The total inference time of our method is 0.57s on average on a GeForce RTX 2080 Ti GPU, which is faster than the current state-of-the-art method DSGN (0.68s) <ref type="bibr" target="#b27">[26]</ref> and is comparable with other stereo-based methods on the KITTI leaderboard <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation Studies</head><p>We analyze the effect of each added modules in <ref type="table">Table.</ref> IV. The baseline is the original HD 3 network with background points filtered out and PointRCNN as the 3D detector. For depth estimation, we follow <ref type="bibr" target="#b7">[8]</ref> and use mean absolute relative error (absRel) and scale invariant logarithmic error (SILog) as the evaluation metrics. To better observe depth estimation accuracy improvements from the modifications made to HD3 for the more challenging but underrepresented pixels at greater depths, we present depth estimation errors for all foreground pixels with greater than 20 m depth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Split Depth Estimation.</head><p>To investigate the effect of split depth estimation, we train the modified HD 3 network without the point cloud loss and feed the resulting point cloud directly to the 3D detector without a confidence score layer. The separate decoders allow us to improve the depth estimation for foreground pixels by 29.8% (from 0.047 to 0.033 absRel), and this leads to improvements in the AP 3D by 1.81% and AP BEV by 1.31%.</p><p>Effect of Point Cloud loss. We analyze the effect of point cloud loss by feeding the point cloud to the 3D detector without confidence estimation. Point cloud loss further improves the foreground depth estimation from 0.033 to 0.027 absRel. Including a point cloud loss in the pipeline also helps to obtain better 3D object detection accuracy, improving the AP 3D by 2.27% and AP BEV by 0.29%.</p><p>Effect of Confidence Estimation. Finally, adding confidence estimation as an additional layer in the 3D detector boosts the 3D detection performance by another 1.02% and BEV performance by another 1.73%. This shows that the 3D detection network benefits from the confidence estimation generated by the depth estimation module.  Sensitivity to Semantic Segmentation. The performance of our method depends on the quality of the semantic segmentation. On the KITTI benchmark, there are only 200 images with semantic ground truth for training, which limits the performance of the semantic segmentation network. To investigate the performance upper bound of our method, we perform experiments using the labels that are generated from <ref type="bibr" target="#b34">[33]</ref> as the segmentation masks on the KITTI validation split. <ref type="table">Table V</ref> shows that with the labelled masks, there is a 5.36% and a 3.84% improvement in AP 3D and AP BEV . This experiment suggests that our proposed method has the potential to obtain even better performance on other datasets with more accurate semantic segmentation.</p><p>Masks AP 3D AP BEV VideoProp-LabelRelax <ref type="bibr" target="#b29">[28]</ref> 57.57 71.16 Labels <ref type="bibr" target="#b34">[33]</ref> 62.93 75.00 TABLE V: Comparisons of AP 3D and AP BEV for moderate difficulty for cars at 0.7 IoU using estimated segmentation masks <ref type="bibr" target="#b29">[28]</ref> with using labelled masks <ref type="bibr" target="#b34">[33]</ref> on KITTI validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Qualitative Results</head><p>Fig <ref type="figure" target="#fig_0">. 6</ref> shows the estimated point cloud, the ground truth bounding box, and the final detections of our proposed method on the KITTI validation split. The right image suggests that even with the significant improvements compared with the previous state-of-the-art methods, pedestrian and cyclist classes remain challenging for our stereo-based detector because of the limited training samples and the high intra-class variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this paper, we present CG-Stereo, a confidence-guided stereo 3D object detection pipeline with split depth estimation. Taking advantage of the mature development of image segmentation, the stereo matching network can learn the depth for foreground and background pixels separately, and achieve better foreground depth estimation and 3D object detection performance as a result. We also show that encoding the confidence estimation from the stereo matching network into the point cloud as a soft attention mechanism guides the 3D object detector to focus more on the accurate points and further boosts 3D object detection accuracy. Our proposed method outperforms all state-of-theart stereo-based methods on the KITTI 3D object detection benchmark. Future work includes evaluating our proposed method on other autonomous driving datasets <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b36">[35]</ref>, <ref type="bibr" target="#b37">[36]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 6 :</head><label>6</label><figDesc>Qualitative results of our method on several samples in the KITTI validation split. The ground truth labels and detections are shown in red and green, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>/ 29.22 14.26 / 21.88 13.72 / 18.83 59.51 / 62.46 43.71 / 45.99 37.99 / 41.92 Stereo-RCNN [7] 54.11 / 68.50 36.69 / 48.30 31.07 / 41.47 85.84 / 87.13 66.28 / 74.11 57.24 / 58.93 PL:F-PointNet [4] 59.4 / 72.8 39.8 / 51.8 33.5 / 44.0 89.5 / 89.8 75.5 / 77.6 66.3 / 68./ 87.31 57.82 / 68.69 54.63 / 65.80 90.58 / 97.04 87.01 / 88.58 79.76 / 80.34</figDesc><table><row><cell>Method</cell><cell>Easy</cell><cell>0.7 IoU Moderate</cell><cell>Hard</cell><cell>Easy</cell><cell>0.5 IoU Moderate</cell><cell>Hard</cell></row><row><cell>TLNet [24]</cell><cell cols="6">18.15 2</cell></row><row><cell>PL:AVOD [4]</cell><cell cols="3">61.9 / 74.9 45.3 / 56.8 39.0 / 49.0</cell><cell cols="3">88.5 / 89.0 76.4 / 77.5 61.2 / 68.7</cell></row><row><cell>PL++:AVOD [5]</cell><cell cols="3">63.2 / 77.0 46.8 / 63.7 39.8 / 56.0</cell><cell cols="3">89.0 / 89.4 77.8 / 79.0 69.1 / 70.1</cell></row><row><cell>PL++:PIXOR [5]</cell><cell>-/ 79.7</cell><cell>-/ 61.1</cell><cell>-/ 54.5</cell><cell>-/ 89.9</cell><cell>-/ 78.4</cell><cell>-/ 74.7</cell></row><row><cell cols="4">PL++:P-RCNN [5] 67.9 / 82.0 50.1 / 64.0 45.3 / 57.3</cell><cell cols="3">89.7 / 89.8 78.6 / 83.8 75.1 / 77.5</cell></row><row><cell>OC-Stereo [6]</cell><cell cols="6">64.07 / 77.66 48.34 / 65.95 40.39 / 51.20 89.65 / 90.01 80.03 / 80.63 70.34 / 71.06</cell></row><row><cell>DSGN [26]</cell><cell cols="3">73.21 / 83.24 54.27 / 63.91 47.71 / 57.83</cell><cell>-/ -</cell><cell>-/ -</cell><cell>-/ -</cell></row><row><cell>Ours</cell><cell>76.17</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Car Localization and Detection. AP 3D / AP BEV on KITTI validation set. The results are evaluated using the original KITTI metric with 11 recall positions.</figDesc><table><row><cell>Method</cell><cell cols="3">AP3D Easy Moderate Hard</cell><cell cols="3">APBEV Easy Moderate Hard</cell></row><row><cell>RT3DStereo [25]</cell><cell>29.90</cell><cell>23.28</cell><cell>18.96</cell><cell>58.81</cell><cell>46.82</cell><cell>38.38</cell></row><row><cell>Stereo-RCNN [7]</cell><cell>47.58</cell><cell>30.23</cell><cell>23.72</cell><cell>61.92</cell><cell>41.31</cell><cell>33.42</cell></row><row><cell>PL:AVOD [4]</cell><cell>54.53</cell><cell>34.05</cell><cell>28.25</cell><cell>67.30</cell><cell>45.00</cell><cell>38.40</cell></row><row><cell>PL++:P-RCNN [5]</cell><cell>61.11</cell><cell>42.43</cell><cell>36.99</cell><cell>78.31</cell><cell>58.01</cell><cell>51.25</cell></row><row><cell>OC-Stereo [6]</cell><cell>55.15</cell><cell>37.60</cell><cell>30.25</cell><cell>68.89</cell><cell>51.47</cell><cell>42.97</cell></row><row><cell>DSGN [26]</cell><cell>73.50</cell><cell>52.18</cell><cell>45.14</cell><cell>82.90</cell><cell>65.05</cell><cell>56.60</cell></row><row><cell>Ours</cell><cell>74.39</cell><cell>53.58</cell><cell>46.50</cell><cell>85.29</cell><cell>66.44</cell><cell>58.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Car Localization and Detection. AP 3D and AP BEV on KITTI test set. The results are evaluated using the new KITTI metric with 40 recall positions. Several methods are not available on the leaderboard.</figDesc><table><row><cell>Method</cell><cell cols="3">AP3D Easy Moderate Hard</cell><cell cols="3">APBEV Easy Moderate Hard</cell></row><row><cell></cell><cell></cell><cell cols="2">Pedestrian</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RT3DStereo [25]</cell><cell>3.28</cell><cell>2.45</cell><cell>2.35</cell><cell>4.72</cell><cell>3.65</cell><cell>3.00</cell></row><row><cell>OC-Stereo [6]</cell><cell>24.48</cell><cell>17.58</cell><cell>15.60</cell><cell>29.79</cell><cell>20.80</cell><cell>18.62</cell></row><row><cell>DSGN [26]</cell><cell>20.53</cell><cell>15.55</cell><cell>14.15</cell><cell>26.61</cell><cell>20.75</cell><cell>18.86</cell></row><row><cell>Ours</cell><cell>33.22</cell><cell>24.31</cell><cell>20.95</cell><cell>39.24</cell><cell>29.56</cell><cell>25.87</cell></row><row><cell></cell><cell></cell><cell cols="2">Cyclist</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RT3DStereo [25]</cell><cell>5.29</cell><cell>3.37</cell><cell>2.57</cell><cell>7.03</cell><cell>4.10</cell><cell>3.88</cell></row><row><cell>OC-Stereo [6]</cell><cell>29.40</cell><cell>16.63</cell><cell>14.72</cell><cell>32.47</cell><cell>19.23</cell><cell>17.11</cell></row><row><cell>DSGN [26]</cell><cell>27.76</cell><cell>18.17</cell><cell>16.21</cell><cell>31.23</cell><cell>21.04</cell><cell>18.93</cell></row><row><cell>Ours</cell><cell>47.40</cell><cell>30.89</cell><cell>27.73</cell><cell>55.33</cell><cell>36.25</cell><cell>32.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>Pedestrian and Cyclist Localization and Detection. AP 3D and AP BEV on KITTI test set. The results are evaluated using the new KITTI metric with 40 recall positions. Several methods are not available on the leaderboard.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Ablation Studies. Comparison of depth estimation for foreground pixels, and comparisons of AP 3D and AP BEV at 0.7 IoU for moderate difficulty for the car class. Both are evaluated on KITTI validation set. L pc denotes the point cloud loss. absRel denotes the mean absolute relative error and SILog denotes the scale invariant logarithmic error.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object-centric stereo matching for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)</title>
		<meeting>the IEEE International Conference on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Task-aware monocular depth estimation for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07701</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection leveraging accurate proposals and shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="3354" to="3361" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A fast local descriptor for dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stereo by intra-and inter-scanline search using dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="139" to="154" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Computing visual correspondence with occlusions using graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>Eighth IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="508" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pyramid stereo matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5410" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ga-net: Guided aggregation net for end-to-end stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Prisacariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical discrete distribution decomposition for match density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6044" to="6053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Pvrcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.13192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="7607" to="7615" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Realtime 3d object detection for automated driving using stereo vision and semantic information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>K?nigshof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Salscheider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Intelligent Transportation Systems Conference (ITSC)</title>
		<meeting>the IEEE Intelligent Transportation Systems Conference (ITSC)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1405" to="1410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dsgn: Deep stereo geometry network for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03398</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8856" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4990" to="4999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The cityscapes dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scharw?chter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop on the Future of Datasets in Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4040" to="4048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">In defense of classical image processing: Fast depth completion on the cpu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference on Computer and Robot Vision (CRV)</title>
		<meeting>the 15th Conference on Computer and Robot Vision (CRV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beat the mturkers: Automatic image labeling from weak 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3198" to="3205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kretzschmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dotiwalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patnaik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caine</surname></persName>
		</author>
		<title level="m">Scalability in perception for autonomous driving: Waymo open dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1912</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11027</idno>
		<title level="m">nuscenes: A multimodal dataset for autonomous driving</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pitropov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rebello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waslander</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10117</idno>
		<title level="m">Canadian adverse driving conditions dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
