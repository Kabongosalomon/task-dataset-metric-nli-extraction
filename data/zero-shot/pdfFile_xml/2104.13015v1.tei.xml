<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Underwater Image Enhancement via Medium Transmission-Guided Multi-Color Space Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongyi</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Junhui</forename><forename type="middle">Hou</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Runmin</forename><surname>Cong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunle</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Underwater Image Enhancement via Medium Transmission-Guided Multi-Color Space Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-underwater imaging</term>
					<term>image enhancement</term>
					<term>color correction</term>
					<term>scattering removal</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Underwater images suffer from color casts and low contrast due to wavelength-and distance-dependent attenuation and scattering. To solve these two degradation issues, we present an underwater image enhancement network via medium transmission-guided multi-color space embedding, called Ucolor. Concretely, we first propose a multi-color space encoder network, which enriches the diversity of feature representations by incorporating the characteristics of different color spaces into a unified structure. Coupled with an attention mechanism, the most discriminative features extracted from multiple color spaces are adaptively integrated and highlighted. Inspired by underwater imaging physical models, we design a medium transmission (indicating the percentage of the scene radiance reaching the camera)guided decoder network to enhance the response of network towards quality-degraded regions. As a result, our network can effectively improve the visual quality of underwater images by exploiting multiple color spaces embedding and the advantages of both physical model-based and learning-based methods. Extensive experiments demonstrate that our Ucolor achieves superior performance against state-of-the-art methods in terms of both visual quality and quantitative metrics. The code is publicly available at: https://li-chongyi.github.io/Proj Ucolor.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>as micro phytoplankton and non-algal particulate in water, which causes low contrast. An effective solution to recover underlying clean images is of great significance for improving the visual quality of images captured in water and accurately understanding underwater world. The quality degradation degrees of underwater images can be implicitly reflected by the medium transmission that represents the percentage of the scene radiance reaching the camera. Hence, physical model-based underwater image enhancement methods <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b7">[8]</ref> mainly focus on the accurate estimation of medium transmission. With the estimated medium transmission and other key underwater imaging parameters such as the homogeneous background light, a clean image can be obtained by reversing an underwater imaging physical model. Though physical model-based methods can achieve promising performance in some cases, they tend to produce unstable and sensitive results when facing challenging underwater scenarios. This is because 1) estimating the medium transmission is fundamentally ill-posed, 2) estimating multiple underwater imaging parameters is knotty for traditional methods, and 3) the assumed underwater imaging models do not always hold.</p><p>Recently, deep learning technology has shown impressive performance on underwater image enhancement <ref type="bibr" target="#b9">[10]</ref>- <ref type="bibr" target="#b11">[12]</ref>. These deep learning-based methods often apply the networks arXiv:2104.13015v1 [cs.CV] 27 Apr 2021 that were originally designed for other visual tasks to underwater images. Thus, their performance is still far behind when compared with current deep visual models <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>. The main reason is that the design of current deep underwater image enhancement models neglects the domain knowledge of underwater imaging.</p><p>In this work, we propose to solve the issues of color casts and low contrast of underwater images by leveraging rich encoder features and exploiting the advantages of physical model-based and learning-based methods. Unlike previous deep models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> that only employ the features extracted from RGB color space, we examine the feature representations through a multi-color space encoder network, then highlight the most representative features via an attention mechanism. Such a manner effectively improves the generalization capability of deep networks, and also incorporates the characteristics of different color spaces into a unified structure. This is rarely studied in the context of underwater image enhancement. Inspired by the conclusion that the quality degradation of underwater images can be reflected by the medium transmission <ref type="bibr" target="#b18">[19]</ref>, we propose a medium transmission-guided decoder network to enhance the response of our network towards quality-degraded regions. The introduction of medium transmission allows us to incorporate the advantage of physical model-based methods into deep networks, which accelerates network optimization and improves enhancement performance. Since our method is purely data-driven, it can tolerate the errors caused by inaccurate medium transmission estimation.</p><p>In <ref type="figure" target="#fig_0">Fig. 1</ref>, we present a representative example by the proposed Ucolor against two underwater image enhancement methods. As shown, both the classical fusion-based method <ref type="bibr" target="#b8">[9]</ref>  <ref type="figure" target="#fig_0">(Fig. 1(c)</ref>) and the deep learning-based method <ref type="bibr" target="#b9">[10]</ref>  <ref type="figure" target="#fig_0">(Fig.  1(d)</ref>) fail to cope with the challenging underwater image with greenish tone and low contrast well. In contrast, our Ucolor ( <ref type="figure" target="#fig_0">Fig. 1(b)</ref>) achieves the visually pleasing result in terms of color, contrast, and naturalness. The main contributions of this paper are highlighted as follows.</p><p>? We propose a multi-color space encoder network coupled with an attention mechanism for incorporating the characteristics of different color spaces into a unified structure and adaptively selecting the most representative features. <ref type="bibr">?</ref> We propose a medium transmission-guided decoder network to enforce the network to pay more attention to quality-degraded regions. It explores the complementary merits between domain knowledge of underwater imaging and deep neural networks. ? Our Ucolor achieves state-of-the-art performance on several recent benchmarks in terms of both visual quality and quantitative metrics. The rest of this paper is organized as follows. Section II presents the related works of underwater image enhancement. Section III introduces the proposed method. In Section IV, the qualitative and quantitative experiments are conducted. Section V concludes this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In addition to extra information <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> and specialized hardware devices <ref type="bibr" target="#b21">[22]</ref>, underwater image enhancement can be roughly classified into two groups: traditional methods and deep learning-based methods. Traditional Methods. Early attempts aim to adjust the pixel values for visual quality improvement, such as dynamic pixel range stretching <ref type="bibr" target="#b22">[23]</ref>, pixel distribution adjustment <ref type="bibr" target="#b23">[24]</ref>, and image fusion <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. For example, Ancuti et al. <ref type="bibr" target="#b8">[9]</ref> first obtained the color-corrected and contrast-enhanced versions of an underwater image, then computed the corresponding weight maps, finally combined the advantages of different versions. Ancuti et al. <ref type="bibr" target="#b24">[25]</ref> further improved the fusionbased underwater image enhancement strategy and proposed to blend two versions that are derived from a white-balancing algorithm based on a multiscale fusion strategy. Most recently, based on the observation that the information contained in at least one color channel is close to completely lost under adverse conditions such as hazy nighttime, underwater, and non-uniform artificial illumination, Ancuti et al. <ref type="bibr" target="#b26">[27]</ref> proposed a color channel compensation (3C) pre-processing method. As a pre-processing step, the 3C operator can improve traditional restoration methods. Although these physical model-free methods can improve the visual quality to some extent, they omit the underwater imaging mechanism and thus tend to produce either over-/under-enhanced results or introduce artificial colors. For example, the color correction algorithm in <ref type="bibr" target="#b8">[9]</ref> is not always reliable when encountering diverse and challenging underwater scenes. Compared with these methods, our method takes underwater image formation models into account and employs the powerful learning capability of deep networks, making the enhanced images look more natural and visually pleasing.</p><p>The widely used underwater image enhancement methods are physical model-based, which estimate the parameters of underwater imaging models based on prior information. These priors include red channel prior <ref type="bibr" target="#b27">[28]</ref>, underwater dark channel prior <ref type="bibr" target="#b2">[3]</ref>, minimum information prior <ref type="bibr" target="#b3">[4]</ref>, blurriness prior <ref type="bibr" target="#b28">[29]</ref>, general dark channel prior <ref type="bibr" target="#b29">[30]</ref>, etc. For example, Peng and Cosman <ref type="bibr" target="#b28">[29]</ref> proposed an underwater image depth estimation algorithm based on image blurriness and light absorption. With the estimated depth, the clear underwater image can be restored based on an underwater imaging model. Peng et al. <ref type="bibr" target="#b29">[30]</ref> further proposed a generalization of the dark channel prior to deal with diverse images captured under severe weather. A new underwater image formation model was proposed in <ref type="bibr" target="#b18">[19]</ref>. Based on this model, an underwater image color correction method was presented using underwater RGB-D images <ref type="bibr" target="#b20">[21]</ref>.</p><p>These physical model-based methods are either timeconsuming or sensitive to the types of underwater images <ref type="bibr" target="#b30">[31]</ref>. Moreover, the accurate estimation of complex underwater imaging parameters challenges current physical model-based methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b27">[28]</ref>- <ref type="bibr" target="#b29">[30]</ref>. For example, the blurriness prior used in <ref type="bibr" target="#b28">[29]</ref> does not always hold, especially for clear underwater images. In contrast, our method can more accurately restore underwater images by exploiting the advantages of both physical model-based and data driven-based methods. Deep Learning Models. The emergence of deep learning has led to considerable improvements in low-level visual tasks <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b36">[37]</ref>. There are several attempts made to improve the performance of underwater image enhancement through deep Overview of the architecture of Ucolor. Our Ucolor consists of a multi-color space encoder network and a medium transmission-guided decoder network. In our method, we normalize the values of the medium transmission map to [0,1] and feed the reverse medium transmission map (denoted as RMT) to the medium transmission guidance module. 'downsampling' is implemented by max pooling, while 'upsampling' is implemented by bilinear interpolation. 'dense connections' represents the concatenation operation along the channel dimension for each set of features from the corresponding convolutional layer in different color-space encoder paths. 'convolutional layer' has the kernel of size 3?3 and stride 1. In the Ucolor, all convolutional layers adopt kernels of size 3?3 and stride 1. A detailed network structure with the hyper-parameters can be found in the Supplementary Material. learning strategy <ref type="bibr" target="#b37">[38]</ref>. As a pioneering work, Li et al. <ref type="bibr" target="#b10">[11]</ref> employed a Generative Adversarial Network (GAN) and an image formation model to synthesize degraded/clean image pairs for supervised learning. To avoid the requirement of paired training data, a weakly supervised underwater color correction network (UCycleGAN) was proposed in <ref type="bibr" target="#b9">[10]</ref>. Furthermore, Guo et al. <ref type="bibr" target="#b16">[17]</ref> introduced a multi-scale dense GAN for robust underwater image enhancement. Li et al. <ref type="bibr" target="#b11">[12]</ref> proposed to simulate the realistic underwater images according to different water types and an underwater imaging physical model. With ten types of synthesized underwater images, ten underwater image enhancement (UWCNN) models were trained, in which each UWCNN model was used to enhance the corresponding type of underwater images. Recently, Li et al. <ref type="bibr" target="#b17">[18]</ref> collected a real paired underwater image dataset for training deep networks and proposed a gated fusion network to enhance underwater images. This proposed gated deep model requires three preprocessing images including a Gamma correction image, a contrast improved image, and a whitebalancing image as the inputs of the gated network. A wavelet corrected transformation was proposed for underwater image enhancement in <ref type="bibr" target="#b38">[39]</ref>. Yang et al. <ref type="bibr" target="#b39">[40]</ref> proposed a conditional generative adversarial network to improve the perceptual quality of underwater images.</p><p>These underwater image enhancement models usually apply the existing deep network structures for general purposes to underwater images and neglect the unique characteristics of underwater imaging. For example, <ref type="bibr" target="#b9">[10]</ref> directly uses the CycleGAN <ref type="bibr" target="#b33">[34]</ref> network structure, and <ref type="bibr" target="#b17">[18]</ref> adopts a simple multi-scale convolutional network. For unsupervised models <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, they still inherit the disadvantage of GAN-based models, which produces unstable enhancement results. In <ref type="bibr" target="#b11">[12]</ref>, facing an input underwater image, how to select the corresponding UWCNN model is challenging. Consequently, the robustness and generalization capability of current deep learning-based underwater image enhancement models are limited and unsatisfactory.</p><p>In contrast to existing deep learning-based underwater image enhancement methods, our method has the following unique characteristics: 1) the multi-color space encoder network coupled with an attention mechanism that enables the diverse feature representations from multi-color space and adaptively selects the most representative information; 2) the medium transmission-guided decoder network that incorporations the domain knowledge of underwater imaging into deep structures by tailoring the attention mechanism for emphasizing the quality-degraded regions; 3) our method does not require any pre-processing steps and adopts supervised learning, thus producing more stable results; 4) our method adopts endto-end training and is able to handle most underwater scenes in a unified structure; and 5) our method achieves outstanding performance on various underwater image datasets. These innovations provide new ideas for exploring the complementary merits between domain knowledge of underwater imaging and deep learning strategy and the advantages of multi-color space encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>We present the overview architecture of Ucolor in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>In the multi-color space encoder network, an underwater image first goes through color space transformation. Three encoder paths named HSV path, RGB path, and Lab path are formed. In each path, the input is forwarded to three serial redisual-enhancement modules, thus obtaining three levels of feature representations using a 2? downsampling operation (noted <ref type="figure" target="#fig_1">Fig. 2</ref>   the Lab path. We then concatenate the same level features of these three parallel paths to form three sets of multi-color space encoder features. At last, we separately feed these three sets of features to the corresponding channel-attention module that serves as a tool to spotlight the most representative and informative features. In the medium transmission-guided decoder network, the selected encoder features by channelattention modules and the same sizes of reverse medium transmission (RMT) map are forwarded to the medium transmission guidance module for emphasizing quality-degraded regions. Here, we employ the max pooling operation to achieve different sizes of RMT maps. Then, the outputs of the medium transmission guidance modules are fed to the corresponding residual-enhancement module. After three serial residualenhancement modules and two 2? upsampling operations, the decoder features are forwarded to a convolution layer for reconstructing the result.</p><p>In what follows, we detail the key components of our method, including the multi-color space encoder (Sec. III-A), the residual-enhancement module (Sec. III-B), the channelattention module (Sec. III-C), the medium transmission guidance module (Sec. III-D), and the loss function (Sec. III-E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multi-Color Space Encoder</head><p>Compared with terrestrial scene images, the color deviations of underwater images cover more comprehensive ranges, differing from the bluish or greenish tone to a yellowish one. The diversity in color casts severely limits the traditional network architectures <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. Inspired by the traditional enhancement algorithms that operate in various color spaces <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b42">[43]</ref>, we extract features in three color spaces (RGB, HSV, and Lab) where the same image has different visual representations in various color systems as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Concretely, the image is easy to store and display in RGB color space because of its strong color physical meaning. However, the three components (R, G, and B) are highly correlated, which are easy to be affected by the changes of luminance, occlusion, shadow, and other factors. By contrast, HSV color space can intuitively reflect the hue, saturation, brightness, and contrast of the image. Lab color space makes the colors better distributed, which is able to express all the colors that the human eye can perceive.</p><p>These color spaces have obvious differences and advantages. To combine their properties for underwater image enhancement, we incorporate the characteristics of different color spaces into a unified deep structure, where all the image degradation related components (color, hue, saturation, intensity,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>residual-enhancement module convolutional layer</head><p>Leaky ReLU pixel-wise addition and luminance) can be taken into account. Moreover, the color difference of two points with a small distance in one color space may be large in other color spaces. Thus, the multiple color spaces embedding can facilitate the measurement of color deviations of underwater images. Additionally, the multicolor space encoder brings more nonlinear operations during color space transformation. It is known that the nonlinear transformation generally improves the performance of deep models <ref type="bibr" target="#b43">[44]</ref>. In the ablation study, we will analyze the contribution of each color space. <ref type="figure" target="#fig_3">Fig. 4</ref> presents the details of the residual-enhancement module. This residual-enhancement module aims to preserve the data fidelity and address gradient vanishing <ref type="bibr" target="#b44">[45]</ref>. In each residual-enhancement module, the convolutional layers have an identical number of filters. The numbers of filters are progressively increased from 128 to 512 by a factor 2 in the encoder network while they are decreased from 512 to 128 by a factor 2 in the decoder network. All the convolutional layers have the same kernel sizes of 3?3 and stride 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Residual-Enhancement Module</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Channel-Attention Module</head><p>In view of the specific definition of each color space, these features extracted from three color spaces should have different contributions. Therefore, we employ a channel-attention module to explicitly exploit the interdependencies between the channel features extracted from different color spaces. The details of the channel-attention module are depicted in <ref type="figure" target="#fig_5">Fig. 5</ref>.  Assume the input features F = Cat(F 1 , F 2 , ? ? ? , F N ) ? R N ?H?W , where F is a feature map from one path at a specific level (the level is denoted as 1, 2, and 3 in <ref type="figure" target="#fig_1">Fig. 2)</ref>, N is the number of feature maps, Cat represents the feature concatenation; and H and W are the height and width of input image, respectively. We first perform the global average pooling on input features F, leading to a channel descriptor z ? R N ?1 , which is an embedded global distribution of channel-wise feature responses. The k-th entry of z can be expressed as:</p><formula xml:id="formula_0">z k = 1 H ? W H i W j F k (i, j),<label>(1)</label></formula><p>where k ? [1, N ]. To fully capture channel-wise dependencies, a self-gating mechanism <ref type="bibr" target="#b45">[46]</ref> is used to produce a collection of per-channel modulation weights s ? R N ?1 :</p><formula xml:id="formula_1">s = ?(W 2 * (?(W 1 * z))),<label>(2)</label></formula><p>where ?(?) represents the Sigmoid activation function, ?(?) represents the ReLU activation function, * denotes the convolution operation, and W 1 and W 2 are the weights of two fullyconnected layers with the numbers of their output channels equal to N r and N , respectively, where r is set to 16 for reducing the computational costs. At last, these weights are applied to input features F to generate rescaled features U ? R N ?H?W . Moreover, to avoid gradient vanishing problem and keep good properties of original features, we treat the channel-attention weights in an identical mapping fashion:</p><formula xml:id="formula_2">U = F ? F ? s,<label>(3)</label></formula><p>where ? and ? denote the pixel-wise addition and pixel-wise multiplication, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Medium Transmission Guidance Module</head><p>According to the image formation model in bad weather <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, which is widely used in image dehazing and underwater image restoration algorithms <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, the quality-degraded image can be expressed as: where x indicates the pixel index, I is the observed image, J is the clear image, A is the homogeneous background light, and T is the medium transmission that represents the percentage of scene radiance reaching the camera after reflecting in the medium, indicating the degrees of quality degradation in different regions.</p><formula xml:id="formula_3">I c (x) = J c (x)?T (x)?A c (x)?(1?T (x)), c ? {r, g, b}, (4)</formula><p>We incorporate the medium transmission map into the decoder network via the proposed medium transmission guidance module. Specifically, we use the reverse medium transmission (RMT) map (denoted as T ? R H?W ) as the pixel-wise attention map. The RMT map T is obtained by 1-T (T ? R H?W is the medium transmission map in the range of [0,1], and 1 ? R H?W is the matrix with all elements equal to 1), which indicates that the higher quality degradation pixels should be assigned larger attention weights.</p><p>Since the corresponding ground truth medium transmission map of an input underwater image is not available in practice, it is difficult to train a deep neural network for the estimation of medium transmission map. To solve this issue, we employ prior-based estimation algorithms to obtain the medium transmission map. Inspired by the robust general dark channel prior <ref type="bibr" target="#b29">[30]</ref>, we estimate the medium transmission map as:</p><formula xml:id="formula_4">T (x) = max c,y??(x) ( A c ? I c (y) max(A c , 1 ? A c ) ),<label>(5)</label></formula><p>whereT is the estimated medium transmission map, ?(x) represents a local patch of size 15?15 centered at x, and c denotes the color channel. As shown, the medium transmission estimation is related to the homogeneous background light A. In <ref type="bibr" target="#b29">[30]</ref>, the homogeneous background light is estimated based on the depth-dependent color change. Due to the limited space, we refer the readers to <ref type="bibr" target="#b29">[30]</ref> for more details. We will compare and analyze the effects of the medium transmission maps estimated by different algorithms in the ablation study.</p><p>With the RMT map, the schematic illustration of the proposed medium transmission guidance module is shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. As shown, we utilize the RMT map as a feature selector to weight the importance of different spatial positions of the features. The high-quality degradation pixels (the pixels with larger RMT values) are assigned higher weights, which can be expressed as:</p><formula xml:id="formula_5">V = U ? U ? T ,<label>(6)</label></formula><p>where V ? R M ?H?W and U ? R M ?H?W respectively represent the output features after the medium transmission guidance module and the input feature. We treat the RMT weights as an identity connection to avoid gradient vanishing and tolerate the errors caused by inaccurate medium transmission estimation. Besides, our purely data-driven framework also tolerates the inaccuracy of medium transmission maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Function</head><p>Following previous works <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, to achieve a good balance between visual quality and quantitative scores, we use the linear combination of the 2 loss L 2 and the perceptual loss L per , and the final loss L f for training our network is experessed as:</p><formula xml:id="formula_6">L f = L 2 + ?L per ,<label>(7)</label></formula><p>where ? is empirically set to 0.01 for balancing the scales of different losses. Specifically, the 2 loss measures the difference between the reconstructed result? and corresponding ground truth J as:</p><formula xml:id="formula_7">L 2 = H m=1 W n=1 (?(m, n) ? J(m, n)) 2 .<label>(8)</label></formula><p>The perceptual loss is computed based on the VGG-19 network ? <ref type="bibr" target="#b43">[44]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b50">[51]</ref>. Let ? j (?) be the jth convolutional layer. We measure the distance between the feature representations of the reconstructed result J and ground truth image J as:</p><formula xml:id="formula_8">L per = H m=1 W n=1 |? j (?)(m, n) ? ? j (J)(m, n)|.<label>(9)</label></formula><p>We compute the perceptual loss at layer relu5 4 of the VGG-19 network. An ablation study towards the loss function will be presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we first describe the implementation details, then introduce the experiment settings. We compare our method with representative methods and provide a series of ablation studies to verify each component of Ucolor. We show the failure case of our method at the end of this section. Due to the limited space, more experimental results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>To train the Ucolor, we randomly selected 800 pairs of underwater images from UIEB <ref type="bibr" target="#b17">[18]</ref> underwater image enhancement dataset. The UIEB dataset includes 890 real underwater images with corresponding reference images. Each reference image was selected by 50 volunteers from 12 enhanced results. It covers diverse underwater scenes, different characteristics of quality degradation, and a broad range of image content, but the number of underwater images is inadequate to train our network. Thus, we incorporated 1,250 synthetic underwater images selected from a synthesized underwater image dataset <ref type="bibr" target="#b11">[12]</ref>, which includes 10 subsets denoted by ten types of water (I, IA, IB, II, and III for open ocean water and 1, 3, 5, 7, and 9 for coastal water). To augment the training data, we randomly cropped image patches of size 128?128.</p><p>We implemented the Ucolor using the MindSpore Lite tool <ref type="bibr" target="#b51">[52]</ref>. A batch-mode learning method with a batch size of 16 was applied. The filter weights of each layer were initialized by Gaussian distribution, and the bias was initially set as a constant. We used ADAM for network optimization and fixed the learning rate to 1e ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Settings</head><p>Benchmarks. For testing, we used the rest 90 pairs of real data of the UIEB dataset, denoted as Test-R90, while 100 pairs of synthetic data from each subset of <ref type="bibr" target="#b11">[12]</ref> forming a total of 1k pairs, denoted as Test-S1000. We also conducted comprehensive experiments on three more benchmarks, i.e., Test-C60 <ref type="bibr" target="#b17">[18]</ref>, SQUID <ref type="bibr" target="#b52">[53]</ref>, and Color-Check7 <ref type="bibr" target="#b24">[25]</ref>. Test-C60 contains 60 real underwater images without reference images provided in the UIEB dataset <ref type="bibr" target="#b17">[18]</ref>. Different from Test-R90, the images in Test-C60 are more challenging, which fail current methods. The SQUID <ref type="bibr" target="#b52">[53]</ref> dataset contains 57 underwater stereo pairs taken from four different dive sites in Israel. We used the 16 representative examples presented in the project page of SQUID 1 for testing. Specifically, for four dive sites (Katzaa, Michmoret, Nachsholim, Satil), four representative samples were selected from each dive site. Each image has a resolution of 1827?2737. Color-Check7 contains 7 underwater Color Checker images taken with different cameras provided in <ref type="bibr" target="#b24">[25]</ref>, which are employed to evaluate the robustness and accuracy of underwater color correction. The cameras used to take the Color Checker pictures are Canon D10, Fuji Z33, Olympus Tough 6000, Olympus Tough 8000, Pentax W60, Pentax W80, and Panasonic TS1, denoted as Can D10, Fuj Z33, Oly T6000, Oly T8000, Pen W60, Pen W80, and Pan TS1 in this paper. Compared Methods. We compared our Ucolor with ten methods, including one physical model-free method (Ancuti et al. <ref type="bibr" target="#b8">[9]</ref>), three physical model-based methods (Li et al. <ref type="bibr" target="#b3">[4]</ref>, Peng et al. <ref type="bibr" target="#b28">[29]</ref>, GDCP <ref type="bibr" target="#b29">[30]</ref>), four deep learning-based methods (UcycleGAN <ref type="bibr" target="#b9">[10]</ref>, Guo et al. <ref type="bibr" target="#b16">[17]</ref>, Water-Net <ref type="bibr" target="#b17">[18]</ref>, UWCNN <ref type="bibr" target="#b11">[12]</ref>), and two baseline deep models (denoted as Unet-U <ref type="bibr" target="#b40">[41]</ref> and Unet-RMT <ref type="bibr" target="#b40">[41]</ref>) that are trained using the same training data and loss functions as our Ucolor. Different from Ucolor, Unet-U and Unet-RMT employ the structure of Unet <ref type="bibr" target="#b40">[41]</ref>. In addition, the inputs of Unet-U and Unet-RMT are an underwater image and the concatenation of an underwater image and its RTM map that is estimated using the same algorithm as our Ucolor, respectively. The comparisons with the two baseline deep models aim at demonstrating the advantages of our network architecture and supplementing the compared deep learning-based methods.</p><p>Since the source code of Ancuti et al. <ref type="bibr" target="#b8">[9]</ref> is not publicly available, we used the code 2 implemented by other researchers to realize code of <ref type="bibr" target="#b8">[9]</ref>. For Li et al. <ref type="bibr" target="#b3">[4]</ref>, Peng et al. <ref type="bibr" target="#b28">[29]</ref>, GDCP <ref type="bibr" target="#b29">[30]</ref>, UcycleGAN <ref type="bibr" target="#b9">[10]</ref>, and Water-Net <ref type="bibr" target="#b17">[18]</ref>, we used the released codes to produce their results. The results of Guo et al. <ref type="bibr" target="#b16">[17]</ref> were provided by the authors. Note that UcycleGAN <ref type="bibr" target="#b9">[10]</ref> is an unsupervised methods, i.e., training with unpaired data, and thus there is no need to retrain it with our training data. Same as our Ucolor, Water-Net <ref type="bibr" target="#b17">[18]</ref> randomly selected the same number of training data from the UIEB dataset for training. For UWCNN <ref type="bibr" target="#b11">[12]</ref>, we used the original UWCNN models, in which each UWCNN model was trained using the underwater images synthesized by one type of water. We discarded the UWCNN typeIA and UWCNN typeIB models because their results are similar to those of UWCNN typeI. Besides, we also retrained the UWCNN model (denoted as UWCNN retrain) using the same training data as our Ucolor. Evaluation Metrics. For Test-R90 and Test-S1000, we conducted full-reference evaluations using PSNR and MSE metrics. Following <ref type="bibr" target="#b17">[18]</ref>, we treated the reference images of Test-R90 as the ground-truth images to compute the PSNR and MSE scores. A higher PSNR or a lower MSE score denotes that the result is closer to the reference image in terms of image content.</p><p>For Test-C60 and SQUID that do not have corresponding ground truth images, we employ the no-reference evaluation metrics UCIQE <ref type="bibr" target="#b53">[54]</ref> and UIQM <ref type="bibr" target="#b54">[55]</ref> to measure the performance of different methods. A higher UCIQE or UIQM score suggests a better human visual perception. Please note that the scores of UCIQE and UIQM cannot accurately reflect the performance of underwater image enhancement methods in some cases. We refer the readers to <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b52">[53]</ref> for the discussions and visual examples. In our study, we only provide the scores of UCIQE and UIQM as the reference for the following research. In addition, we also provide the scores of NIQE <ref type="bibr" target="#b55">[56]</ref> of different methods as the reference though it was not originally devised for underwater images. A lower NIQE score suggests better image quality. Although SQUID provides a script to evaluate color reproduction and transmission map estimation for the underwater image, this script requires the estimated transmission map or the results having a resolution of 1827?2737. However, the compared deep learning-based methods do not need to estimate the transmission maps. Moreover, our current GPU cannot process the input image with such a high resolution. Besides, the sizes of the color checker in the images of SQUID are too small to be cropped for full-reference evaluations. Instead, we resized the images in the SQUID testing dataset to a size 2 https://github.com/bilityniu/underimage-fusion-enhancement of 512?512 and processed them by different methods. We conducted a user study to measure the perceptual quality of results on Test-C60 and SQUID. Specifically, we invited 20 human subjects to score the perceptual quality of the enhanced images independently. The scores of perceptual quality range from 1 to 5 (worst to best quality). These subjects were trained by observing the results from 1) whether the results introduce color deviations; 2) whether the results contain artifacts; 3) whether the results look natural; and 4) whether the results have good contrast and visibility.</p><p>For Color-Check7, we measured the dissimilarity of color between the ground-truth Macbeth Color Checker and the corresponding enhanced results. To be specific, we extracted the color of 24 color patches from the ground-truth Macbeth Color Checker. Then, we respectively cropped the 24 color patches for each enhanced result and computed the average color values of each color patch. At last, we followed the previous method <ref type="bibr" target="#b24">[25]</ref> to employ CIEDE2000 <ref type="bibr" target="#b56">[57]</ref> to measure the relative perceptual differences between the corresponding color patches of ground-truth Macbeth Color Checker and those of enhanced results. The smaller the CIEDE2000 value, the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visual Comparisons</head><p>In this section, we conduct visual comparisons on diverse testing datasets. We refer readers to the Google Drive link 3 for all results of different methods (about 7.4 GB).</p><p>We first show the comparisons on a synthetic image in <ref type="figure">Fig. 7</ref>. The competing methods either fail to dehaze the input image or they introduce undesirable color artifacts. All the methods under comparison fail to recover the complete scene structure. Our result ( <ref type="figure">Fig. 7(l)</ref>) is closest to the ground-truth images and obtains the best PSNR/MSE scores. Furthermore, the RMT map shown in <ref type="figure">Fig. 7(b)</ref> indicates that the high degradation regions have large pixel values. With the RMT map, our method highlights these regions and enhances them well.</p><p>We then show the results of different methods on a real underwater image with obvious greenish color deviation in <ref type="figure" target="#fig_7">Fig. 8</ref>. In <ref type="figure" target="#fig_7">Fig. 8(a)</ref>, the greenish color deviation significantly hides the structural details of the underwater scene. In terms of color, GDCP <ref type="bibr" target="#b29">[30]</ref>, UcycleGAN <ref type="bibr" target="#b9">[10]</ref>, UWCNN typeII <ref type="bibr" target="#b11">[12]</ref> and UWCNN retrain <ref type="bibr" target="#b11">[12]</ref> introduce extra color artifacts. All the compared methods under-enhance the image or introduce the over-saturation. In comparison, our Ucolor effectively removes the greenish tone and improves the contrast without obvious over-enhancement and over-saturation.Although UWCNN retrain <ref type="bibr" target="#b11">[12]</ref>, Unet-U <ref type="bibr" target="#b40">[41]</ref>, and Unet-RMT <ref type="bibr" target="#b40">[41]</ref> were trained with the same data as our Ucolor, their performance is not as good as our Ucolor, which demonstrates the advantage of our specially designed network structure for underwater image enhancement.</p><p>We also show comparisons on challenging underwater images sampled from Test-C60 in <ref type="figure" target="#fig_9">Fig. 9</ref>. These underwater images suffer from high backscattering and color deviations (a) input (b) RMT map (c) GT (d) GDCP <ref type="bibr" target="#b29">[30]</ref> (e) Guo et al. <ref type="bibr" target="#b16">[17]</ref> (f) UcycleGAN <ref type="bibr" target="#b9">[10]</ref> (g) UWCNN typeII <ref type="bibr" target="#b11">[12]</ref> (h) UWCNN retrain <ref type="bibr" target="#b11">[12]</ref> (i) Water-Net <ref type="bibr" target="#b17">[18]</ref> (j) Unet-U <ref type="bibr" target="#b40">[41]</ref> (k) Unet-RMT <ref type="bibr" target="#b40">[41]</ref> (l) Ucolor <ref type="figure">Fig. 7</ref>. Visual comparisons on a synthetic underwater image sampled from Test-S1000. The numbers on the top-left corner of each image refer to its PSNR (dB)/MES (?10 3 ).</p><p>(a) input (b) RMT map (c) reference (d) GDCP <ref type="bibr" target="#b29">[30]</ref> (e) Guo et al. <ref type="bibr" target="#b16">[17]</ref> (f) UcycleGAN <ref type="bibr" target="#b9">[10]</ref> (g) UWCNN typeII <ref type="bibr" target="#b11">[12]</ref> (h) UWCNN retrain <ref type="bibr" target="#b11">[12]</ref> (i) Water-Net <ref type="bibr" target="#b17">[18]</ref> (j) Unet-U <ref type="bibr" target="#b40">[41]</ref> (k) Unet-RMT <ref type="bibr" target="#b40">[41]</ref> (l) Ucolor  (c) GDCP <ref type="bibr" target="#b29">[30]</ref> (d) Guo et al. <ref type="bibr" target="#b16">[17]</ref> (e) WaterNet <ref type="bibr" target="#b17">[18]</ref> (f) UWCNN typeII <ref type="bibr" target="#b11">[12]</ref> (g) UWCNN retrain <ref type="bibr" target="#b11">[12]</ref> (h) Ucolor  (c) Peng et al. <ref type="bibr" target="#b28">[29]</ref> (d) GDCP <ref type="bibr" target="#b29">[30]</ref> (e) Water-Net <ref type="bibr" target="#b17">[18]</ref> (f) UWCNN typeII <ref type="bibr" target="#b11">[12]</ref> (g) UWCNN retrain <ref type="bibr" target="#b11">[12]</ref> (h) Ucolor <ref type="figure" target="#fig_0">Fig. 10</ref>. Visual comparisons on challenging underwater images sampled from SQUID. From left to right, the images were taken from four different dive sites Katzaa, Michmoret, Nachsholim, and Satil. The number on the top-left corner of each image refers to its perceptual score (the larger, the better).  as shown in <ref type="figure" target="#fig_9">Fig. 9(a)</ref>. For these images, all competing methods cannot achieve satisfactory results. Some of them even introduce artifacts, such as GDCP <ref type="bibr" target="#b29">[30]</ref>, Guo et al. <ref type="bibr" target="#b16">[17]</ref>, and UWCNN typeII <ref type="bibr" target="#b11">[12]</ref>. Additionally, some methods introduce artificial colors. For example, the red object in the fourth image becomes purple and the sand in the first two images becomes reddish. In contrast, our Ucolor not only recovers the relatively realistic color but also enhances details, which is credited to the effective designs of multiple color spaces embedding and the introduction of medium transmission-guided decoder structure. The perceptual scores of our results also suggest the visually pleasing quality of our results.</p><p>We present the results of different methods on challenging underwater images sampled from SQUID in <ref type="figure" target="#fig_0">Fig. 10</ref>. As presented, the input underwater images challenge all underwater image enhancement methods. Ancuti et al. <ref type="bibr" target="#b8">[9]</ref> achieves better contrast than the other methods but produces color deviations, e.g., the reddish tone in the third image and the greenish tone in the fourth image. Our Ucolor dehazes the input image, thus improving the contrast of input images. Moreover, our method does not produce obvious artificial colors on the third image. According to the quantitative scores, we can find that the artificial colors significantly affect the perceptual scores given by subjects.</p><p>To analyze the robustness and accuracy of color correction, we conduct the comparisons on the underwater Color Checker image in <ref type="figure" target="#fig_0">Fig. 11</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 11(a)</ref>, the professional underwater camera (Pentax W60) also inevitably introduces All the visual comparisons demonstrate that our Ucolor not only renders visually pleasing results but also generalizes well to different underwater scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative Comparisons</head><p>We first perform quantitative comparisons on Test-S1000 and Test-R90. The average scores of PSNR and MSE of different methods are reported in <ref type="table" target="#tab_2">Table I</ref>. As presented in <ref type="table" target="#tab_2">Table I</ref>, our Ucolor outperforms all competing methods on Test-S1000 and Test-R90. Compared with the second-best performer, our Ucolor achieves the percentage gain of 20%/59% and 4.1%/1.3% in terms of PSNR/MSE on Test-S1000 and Test-R90, respectively. There are two interesting findings from the quantitative comparisons. 1) Although the medium transmission map used in our Ucolor is the same as the traditional GDCP <ref type="bibr" target="#b29">[30]</ref>, the performance is significantly different. Such a result suggests that the performance of underwater image enhancement can be improved by the effective combination of domain knowledge with deep neural networks. 2) The performance of Unet-U <ref type="bibr" target="#b40">[41]</ref> is better than Unet-RMT <ref type="bibr" target="#b40">[41]</ref>, which suggests that simple concatenation of input image and its reverse medium transmission map cannot improve the underwater image enhancement performance of deep model, and even decreases the performance. 3) The generalization capability of UWCNN models <ref type="bibr" target="#b11">[12]</ref> is limited because they require the images to be taken in the accurate type of water as inputs. Due to the limited space, we only present the results of the original UWCNN model that performs the best in <ref type="table" target="#tab_2">Table I</ref> in the following experiments, i.e., UWCNN typeII <ref type="bibr" target="#b11">[12]</ref>.</p><p>Next, we conduct a user study on Test-C60 and SQUID. The average perceptual scores of the results by different methods are reported in <ref type="table" target="#tab_2">Table II</ref>, where it can be observed that these two challenging testing datasets fail most underwater image enhancement methods in terms of perceptual quality. Some methods such as Li et al. <ref type="bibr" target="#b3">[4]</ref> and UcycleGAN <ref type="bibr" target="#b9">[10]</ref> even achieve lower perceptual scores than inputs. For the Test-60 testing dataset, the deep learning-based methods achieve relatively higher perceptual scores. Among them, our Ucolor is superior to the other competing methods. For the SQUID testing dataset, the traditional fusion-based method <ref type="bibr" target="#b8">[9]</ref> obtains the highest perceptual score while our Ucolor ranks the second best. Other deep learning-based methods achieve similar perceptual scores. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, all deep learningbased methods cannot handle the color deviations of images in SQUID well, while the haze can be removed. In contrast, the fusion-based method <ref type="bibr" target="#b8">[9]</ref> achieves better contrast, thus obtaining a higher perceptual score. Observing the scores of non-reference image quality assessment metrics, we can see that <ref type="bibr">Li et al. [4]</ref> obtains the best performance in terms of UIQM and UCIQE scores. For the NIQE scores, our Ucolor achieves the lowest NIQE score on the SQUID testing set while Ancuti et al. <ref type="bibr" target="#b8">[9]</ref> obtain the best performance on the Test-C60 testing set.</p><p>To demonstrate the robustness to different cameras and the accuracy of color restoration, we report the average CIEDE2000 scores on Color-Checker7 in <ref type="table" target="#tab_2">Table III</ref>. For the cameras of Pentax W60, Pentax W80, Cannon D10, Fuji Z33, and Olympus T6000, our Ucolor obtains the lowest color dissimilarity. Moreover, our Ucolor achieves the best average score across seven cameras. Such results demonstrate the superiority of our method for underwater color correction. It is interesting that some methods achieve worse performance in terms of the average CIEDE2000 score than the original input, which suggests that some competing methods cannot recover the real color and even break the inherent color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We perform extensive ablation studies to analyze the core components of our method, including the multi-color space encoder (MCSE)), the medium transmission guidance module (MTGM), and the channel-attention module (CAM). Additionally, we analyze the combination of the 1 loss and the perceptual loss. More specifically,</p><p>? w/o HSV, w/o Lab, and w/o HSV+Lab stand for Ucolor without the HSV, Lab, and both HSV and Lab color spaces encoder paths, respectively. ? w/ 3-RGB means that all inputs of three encoder paths are RGB images. ? w/o MTGM refers to the Ucolor without the medium transmission guidance module. ? w/ RDCP and w/ RUDCP are the models by replacing the medium transmission map estimated via <ref type="bibr" target="#b29">[30]</ref> with the algorithms in <ref type="bibr" target="#b57">[58]</ref> and <ref type="bibr" target="#b2">[3]</ref>, respectively. ? w/o CAM stands for the Ucolor without the channelattention module. ? w/o perc loss means that the Ucolor is trained only with the constraint of 1 loss. The quantitative PSNR (dB) and MSE (?10 3 ) values on Test-S1000 and Test-R90 are presented in <ref type="table" target="#tab_2">Table IV</ref>. The visual comparisons of the effects of reverse medium transmission maps, the contributions of each color space encoder path, the effectiveness of channle-attention module, and the effect of perceptual loss are shown in <ref type="figure" target="#fig_0">Figs. 12, 13, 14</ref>, and 15, respectively. The conclusions drawn from the ablation studies are listed as follows.</p><p>1) As presented in <ref type="table" target="#tab_2">Table IV</ref>, our full model achieves the best quantitative performance across two testing datasets when compared with the ablated models, which implies the effectiveness of the combinations of MCSE, MTGM, and CAM modules.</p><p>2) Our RMT map can relatively accurately assign the highquality degradation regions a higher weight than the RDCP map and the RUDCP map, thus achieving better visual quality, especially for high scattering regions as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>. Additionally, the quantitative performance of the Ucolor is better than the ablated models w/ RDCP, w/ RUDCP, and w/o MTGM, which suggests the importance of an accurate medium transmission map.</p><p>3) The ablated models w/o HSV+Lab and w/ 3-RGB produce comparable performance as shown in <ref type="table" target="#tab_2">Table IV</ref> and <ref type="figure" target="#fig_0">Fig. 13</ref>. The results indicate that aimlessly adding more parameters or the same color encoder will not bring extra representational power to better enhance underwater images. In contrast, a well-design multi-color space embedding helps to learn more powerful representations to improve enhancement performance. In addition, removing any one of the three encoder paths (w/o HSV and w/o Lab) will decrease the performance as shown in <ref type="table" target="#tab_2">Table IV.</ref> 4) The ablated model w/o CAM produces an under-saturated result as shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> This may be induced by removing the CAM module that integrates and highlights the features extracted from multiple color spaces.</p><p>5) The quantitative results in <ref type="table" target="#tab_2">Table IV</ref> show that only using the 1 loss can slightly improve the quantitative performance on Test-S1000 in terms of PNSR and MSE values. However, from the visual results in <ref type="figure" target="#fig_0">Fig. 15</ref>, it can be observed that the enhanced image by Ucolor trained with the full loss function (i.e., the combination between the 1 loss and the perceptual loss) is better than that by Ucolor trained without the perceptual loss. Thus, it is necessary to add the perceptual loss for improving the visual quality of final results. Note that only using the perceptual loss for training does not make sense, and thus we did not conduct such an ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Failure Case</head><p>When facing underwater images with limited lighting, our Ucolor, as well as other state-of-the-arts might not work well. <ref type="figure" target="#fig_0">Fig. 16</ref> presents an example where both our Ucolor and latest deep learning-based Water-Net <ref type="bibr" target="#b17">[18]</ref> fail to produce a visually compelling result when processing an underwater image with limited lighting. The potential reason lies in few such images in the training data sets. Thus, it is difficult for supervised learning-based networks such as Water-Net and Ucolor to handle such underwater images. The stronger ability and more diverse training data that handle such kinds of underwater images will be our future goal. Ablation study of the effects of the reverse medium transmission map. RDCP map represents the reverse DCP map, where the DCP map was estimated by <ref type="bibr" target="#b57">[58]</ref>. RUDCP map represents the reverse UDCP map, where the UDCP map was estimated by <ref type="bibr" target="#b2">[3]</ref>. Compared with the RDCP and RUDCP maps, the RMT map can more accurately indicate the degradation of underwater image, thus leading to better enhancement performance of Ucolor.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>We have presented a deep underwater image enhancement model. The proposed model learns the feature representations from diverse color spaces and highlights the most discriminative features by the channel-attention module. Besides, the domain knowledge is incorporated into the network by employing the reverse medium transmission map as the attention weights. Extensive experiments on diverse benchmarks have demonstrated the superiority of our solution and the effectiveness of multi-color space embedding and the reverse medium transmission guided decoder network structure. The effectiveness of the key components of our method has been verified in the ablation studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Visual comparisons on a real underwater image. Our Ucolor removes both the greenish color deviation and the effect of scattering. In contrast, the compared methods either remain the color deviation or introduce extra color artifacts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Overview of the architecture of Ucolor. Our Ucolor consists of a multi-color space encoder network and a medium transmission-guided decoder network. In our method, we normalize the values of the medium transmission map to [0,1] and feed the reverse medium transmission map (denoted as RMT) to the medium transmission guidance module. 'downsampling' is implemented by max pooling, while 'upsampling' is implemented by bilinear interpolation. 'dense connections' represents the concatenation operation along the channel dimension for each set of features from the corresponding convolutional layer in different color-space encoder paths. 'convolutional layer' has the kernel of size 3?3 and stride 1. In the Ucolor, all convolutional layers adopt kernels of size 3?3 and stride 1. A detailed network structure with the hyper-parameters can be found in the Supplementary Material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>A visual example of the channels extracted from different color spaces. For visualization, we normalize the values to the range of [0,1]. The channels in (b)-(g) are represented by heatmaps, where the color ranging from blue to red represents the value from small to large.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The schematic illustration of the residual-enhancement module. Each residual-enhancement module is composed of two residual blocks, where each block is built by three stacked convolutions followed by the Leaky ReLU activation function, except for the last one. After each residual block, a pixelwise addition is used as an identity connection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The schematic illustration of the channel-attention module. The channel-attention module performs feature recalibration using global information. After going through global average pooling and fully-connected layers, the informative features are emphasized and the less usefull features are suppressed in the input features F , thus obtaining rescaled features U .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The schematic illustration of the medium transmission guidance module. The RMT map T as a feature selector is used to weight the importance of different spatial positions of the input features U , thus obtaining highlighted output features V.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 .</head><label>8</label><figDesc>Visual comparisons on a typical real underwater image with obvious greenish color deviation and low-contrast sampled from Test-R90. The numbers on the top-left corner of each image refer to its PSNR (dB)/MES (?10 3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Peng et al.<ref type="bibr" target="#b28">[29]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 .</head><label>9</label><figDesc>Visual comparisons on challenging underwater images sampled from Test-C60. The number on the top-left corner of each image refers to its perceptual score (the larger, the better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Ancuti et al.<ref type="bibr" target="#b8">[9]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 .</head><label>11</label><figDesc>Visual comparisons on a Color Checker image taken by a Pentax W60 camera sampled from Color-Checker7. The values of CIEDE2000 metric for the regions of Color Checker are reported on the top-left corner of the images (the smaller, the better).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Fig. 12. Ablation study of the effects of the reverse medium transmission map. RDCP map represents the reverse DCP map, where the DCP map was estimated by [58]. RUDCP map represents the reverse UDCP map, where the UDCP map was estimated by [3]. Compared with the RDCP and RUDCP maps, the RMT map can more accurately indicate the degradation of underwater image, thus leading to better enhancement performance of Ucolor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 13 .</head><label>13</label><figDesc>Ablation study of the contributions of each color space encoder path. Ucolor (three color spaces encoder) can achieve vivid color, high contrast, and clear details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 14 .Fig. 15 .Fig. 16 .</head><label>141516</label><figDesc>Ablation study of the effectiveness of the channel-attention module for highlighting the multi-color space features. More realistic result is achieved by Ucolor (with channle-attention module) than the ablated model w/o CAM.(a) input (b) Ucolor (c) w/o perc loss Ablation study towards the perceptual loss. By adding a perceptual loss to the 1 loss, the visual quality of final result is improved. Failure case. The input underwater image has limited lighting.Although our Ucolor cannot effectively enhance this image, it does not introduce color casts like Water-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>as 1, 2, and 3). Simultaneously, we enhance the RGB path by densely connecting the features of the RGB path with the corresponding features of the HSV path and</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I THE</head><label>I</label><figDesc>EVALUATIONS OF DIFFERENT METHODS ON TEST-S1000 AND TEST-R90 IN TERMS OF AVERAGE PSNR (DB) AND MSE (?10 3 ) VALUES. THE BEST RESULT IS IN RED UNDER EACH CASE.</figDesc><table><row><cell></cell><cell cols="2">Test-S1000</cell><cell cols="2">Test-R90</cell></row><row><cell>Methods</cell><cell>PSNR?</cell><cell>MSE?</cell><cell>PSNR?</cell><cell>MSE?</cell></row><row><cell>input</cell><cell>12.96</cell><cell>4.60</cell><cell>16.11</cell><cell>2.03</cell></row><row><cell>Ancuti et al. [9]</cell><cell>13.27</cell><cell>5.15</cell><cell>19.19</cell><cell>0.78</cell></row><row><cell>Li et al. [4]</cell><cell>14.29</cell><cell>3.64</cell><cell>16.73</cell><cell>1.38</cell></row><row><cell>Peng et al. [29]</cell><cell>13.04</cell><cell>4.53</cell><cell>15.77</cell><cell>1.72</cell></row><row><cell>GDCP [30]</cell><cell>11.67</cell><cell>5.98</cell><cell>13.85</cell><cell>3.40</cell></row><row><cell>Guo et al. [17]</cell><cell>15.78</cell><cell>2.57</cell><cell>18.05</cell><cell>1.18</cell></row><row><cell>UcycleGAN [10]</cell><cell>14.73</cell><cell>3.13</cell><cell>16.61</cell><cell>1.65</cell></row><row><cell>Water-Net [18]</cell><cell>15.47</cell><cell>3.26</cell><cell>19.81</cell><cell>1.02</cell></row><row><cell>UWCNN type1 [12]</cell><cell>16.27</cell><cell>2.68</cell><cell>13.62</cell><cell>3.52</cell></row><row><cell>UWCNN type3 [12]</cell><cell>15.70</cell><cell>2.87</cell><cell>12.84</cell><cell>4.23</cell></row><row><cell>UWCNN type5 [12]</cell><cell>14.78</cell><cell>2.94</cell><cell>13.26</cell><cell>3.65</cell></row><row><cell>UWCNN type7 [12]</cell><cell>12.38</cell><cell>4.35</cell><cell>13.02</cell><cell>3.67</cell></row><row><cell>UWCNN type9 [12]</cell><cell>12.83</cell><cell>3.85</cell><cell>12.79</cell><cell>3.89</cell></row><row><cell>UWCNN typeI [12]</cell><cell>10.44</cell><cell>6.42</cell><cell>10.57</cell><cell>6.24</cell></row><row><cell>UWCNN typeII [12]</cell><cell>17.51</cell><cell>2.59</cell><cell>14.75</cell><cell>2.57</cell></row><row><cell>UWCNN typeIII [12]</cell><cell>17.41</cell><cell>2.39</cell><cell>13.26</cell><cell>3.40</cell></row><row><cell>UWCNN retrain [12]</cell><cell>15.87</cell><cell>2.74</cell><cell>16.69</cell><cell>1.71</cell></row><row><cell>Unet-U [41]</cell><cell>19.14</cell><cell>1.22</cell><cell>18.14</cell><cell>1.32</cell></row><row><cell>Unet-RMT [41]</cell><cell>17.93</cell><cell>1.43</cell><cell>16.89</cell><cell>1.71</cell></row><row><cell>Ucolor</cell><cell>23.05</cell><cell>0.50</cell><cell>20.63</cell><cell>0.77</cell></row></table><note>various color casts. Both traditional and learning-based meth- ods change the colors of input from an overall perspective. As indicated by the CIEDE2000 values on the results, our Ucolor achieves the best performance (8.21 under CIEDE2000 metric) in terms of the accuracy of color correction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II THETABLE III THE</head><label>IIIII</label><figDesc>AVERAGE PERCEPTUAL SCORES (PS), UIQM [55] SCORES, UCIQE [54] SCORSE, AND NIQE [56] SCORES OF DIFFERENT METHODS ON TEST-C60 AND SQUID. THE BEST RESULT IS IN RED UNDER EACH CASE. "-" REPRESENTS THE RESULTS ARE NOT AVAILABLE. COLOR DISSIMILARITY COMPARISONS OF DIFFERENT METHODS ON COLOR-CHECK7 IN TERMS OF THE CIEDE2000. THE BEST RESULT IS IN RED UNDER EACH CASE.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Test-C60</cell><cell></cell><cell></cell><cell></cell><cell cols="2">SQUID</cell></row><row><cell>Methods</cell><cell></cell><cell>PS?</cell><cell>UIQM?</cell><cell>UCIQE?</cell><cell>NIQE?</cell><cell>PS?</cell><cell cols="2">UIQM?</cell><cell>UCIQE?</cell><cell>NIQE?</cell></row><row><cell>input</cell><cell></cell><cell>1.34</cell><cell>0.84</cell><cell>0.48</cell><cell>7.14</cell><cell>1.21</cell><cell>0.82</cell><cell></cell><cell>0.42</cell><cell>4.93</cell></row><row><cell cols="2">Ancuti et al. [9]</cell><cell>2.11</cell><cell>1.22</cell><cell>0.62</cell><cell>4.94</cell><cell>2.93</cell><cell>1.30</cell><cell></cell><cell>0.62</cell><cell>5.01</cell></row><row><cell>Li et al. [4]</cell><cell></cell><cell>1.22</cell><cell>1.27</cell><cell>0.65</cell><cell>5.32</cell><cell>1.00</cell><cell>1.34</cell><cell></cell><cell>0.66</cell><cell>4.81</cell></row><row><cell cols="2">Peng et al. [29]</cell><cell>2.07</cell><cell>1.13</cell><cell>0.58</cell><cell>6.01</cell><cell>2.34</cell><cell>0.99</cell><cell></cell><cell>0.50</cell><cell>4.39</cell></row><row><cell>GDCP [30]</cell><cell></cell><cell>1.98</cell><cell>1.07</cell><cell>0.56</cell><cell>5.92</cell><cell>2.47</cell><cell>1.11</cell><cell></cell><cell>0.52</cell><cell>4.48</cell></row><row><cell>Guo et al. [17]</cell><cell></cell><cell>2.63</cell><cell>1.11</cell><cell>0.60</cell><cell>5.71</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">UcycleGAN [10]</cell><cell>1.01</cell><cell>0.91</cell><cell>0.58</cell><cell>7.67</cell><cell>1.16</cell><cell>1.11</cell><cell></cell><cell>0.56</cell><cell>5.93</cell></row><row><cell>Water-Net [18]</cell><cell></cell><cell>3.52</cell><cell>0.97</cell><cell>0.56</cell><cell>6.04</cell><cell>2.78</cell><cell>1.03</cell><cell></cell><cell>0.54</cell><cell>4.72</cell></row><row><cell cols="2">UWCNN typeII [12]</cell><cell>2.19</cell><cell>0.77</cell><cell>0.47</cell><cell>6.76</cell><cell>2.72</cell><cell>0.69</cell><cell></cell><cell>0.44</cell><cell>4.60</cell></row><row><cell cols="2">UWCNN retrain [12]</cell><cell>2.91</cell><cell>0.84</cell><cell>0.49</cell><cell>6.66</cell><cell>2.67</cell><cell>0.77</cell><cell></cell><cell>0.46</cell><cell>4.38</cell></row><row><cell>Unet-U [41]</cell><cell></cell><cell>3.37</cell><cell>0.94</cell><cell>0.50</cell><cell>6.12</cell><cell>2.61</cell><cell>0.82</cell><cell></cell><cell>0.50</cell><cell>4.38</cell></row><row><cell cols="2">Unet-RMT [41]</cell><cell>3.04</cell><cell>1.03</cell><cell>0.52</cell><cell>6.12</cell><cell>2.53</cell><cell>0.82</cell><cell></cell><cell>0.49</cell><cell>5.16</cell></row><row><cell>Ucolor</cell><cell></cell><cell>3.74</cell><cell>0.88</cell><cell>0.53</cell><cell>6.21</cell><cell>2.82</cell><cell>0.82</cell><cell></cell><cell>0.51</cell><cell>4.29</cell></row><row><cell>Methods</cell><cell cols="2">Pen W60</cell><cell>Pen W80</cell><cell>Can D10</cell><cell>Fuj Z33</cell><cell cols="2">Oly T6000</cell><cell cols="2">Oly T8000</cell><cell>Pan TS1</cell><cell>Avg</cell></row><row><cell>input</cell><cell>13.82</cell><cell></cell><cell>17.26</cell><cell>16.13</cell><cell>16.37</cell><cell cols="2">14.89</cell><cell></cell><cell>23.14</cell><cell>19.06</cell><cell>17.24</cell></row><row><cell>Ancuti et al. [9]</cell><cell>12.48</cell><cell></cell><cell>13.30</cell><cell>14.28</cell><cell>11.43</cell><cell cols="2">11.57</cell><cell></cell><cell>12.58</cell><cell>10.63</cell><cell>12.32</cell></row><row><cell>Li et al. [4]</cell><cell>15.41</cell><cell></cell><cell>17.56</cell><cell>18.52</cell><cell>25.01</cell><cell cols="2">16.01</cell><cell></cell><cell>17.12</cell><cell>12.03</cell><cell>17.38</cell></row><row><cell>Peng et al. [29]</cell><cell>13.16</cell><cell></cell><cell>16.01</cell><cell>14.78</cell><cell>14.09</cell><cell cols="2">12.24</cell><cell></cell><cell>14.79</cell><cell>19.59</cell><cell>14.95</cell></row><row><cell>GDCP [30]</cell><cell>15.49</cell><cell></cell><cell>24.32</cell><cell>16.89</cell><cell>13.73</cell><cell cols="2">12.76</cell><cell></cell><cell>16.82</cell><cell>12.93</cell><cell>16.13</cell></row><row><cell>Guo et al. [17]</cell><cell>12.29</cell><cell></cell><cell>15.50</cell><cell>14.58</cell><cell>16.65</cell><cell cols="2">39.71</cell><cell></cell><cell>15.14</cell><cell>12.40</cell><cell>18.04</cell></row><row><cell>UcycleGAN [10]</cell><cell>21.19</cell><cell></cell><cell>21.23</cell><cell>22.96</cell><cell>26.28</cell><cell cols="2">20.88</cell><cell></cell><cell>23.42</cell><cell>19.02</cell><cell>22.14</cell></row><row><cell>Water-Net [18]</cell><cell>12.51</cell><cell></cell><cell>19.57</cell><cell>15.44</cell><cell>12.91</cell><cell cols="2">17.55</cell><cell></cell><cell>21.73</cell><cell>18.84</cell><cell>16.94</cell></row><row><cell>UWCNN typeII [12]</cell><cell>16.73</cell><cell></cell><cell>20.55</cell><cell>17.73</cell><cell>17.20</cell><cell cols="2">16.31</cell><cell></cell><cell>17.94</cell><cell>20.97</cell><cell>18.20</cell></row><row><cell>UWCNN retrain [12]</cell><cell>13.64</cell><cell></cell><cell>20.33</cell><cell>14.91</cell><cell>13.38</cell><cell cols="2">14.72</cell><cell></cell><cell>18.11</cell><cell>20.19</cell><cell>16.47</cell></row><row><cell>Unet-U [41]</cell><cell>11.22</cell><cell></cell><cell>15.17</cell><cell>13.32</cell><cell>11.91</cell><cell cols="2">10.87</cell><cell></cell><cell>15.12</cell><cell>17.31</cell><cell>13.56</cell></row><row><cell>Unet-RMT [41]</cell><cell>12.37</cell><cell></cell><cell>19.01</cell><cell>15.57</cell><cell>14.80</cell><cell cols="2">13.26</cell><cell></cell><cell>16.47</cell><cell>19.55</cell><cell>15.86</cell></row><row><cell>Ucolor</cell><cell>8.21</cell><cell></cell><cell>10.59</cell><cell>12.27</cell><cell>8.11</cell><cell>7.22</cell><cell></cell><cell></cell><cell>14.42</cell><cell>14.54</cell><cell>10.77</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV QUANTITATIVE</head><label>IV</label><figDesc>RESULTS OF THE ABLATION STUDY IN TERMS OF AVERAGE PSNR (DB) AND MSE (?10 3 ) VALUES.</figDesc><table><row><cell>Modules</cell><cell>Baselines</cell><cell cols="2">Test-S1000 PSNR? MSE ?</cell><cell cols="2">Test-R90 PSNR? MSE ?</cell></row><row><cell></cell><cell>full model</cell><cell>23.05</cell><cell>0.50</cell><cell>20.63</cell><cell>0.77</cell></row><row><cell></cell><cell>w/o HSV</cell><cell>16.52</cell><cell>1.83</cell><cell>16.08</cell><cell>1.97</cell></row><row><cell></cell><cell>w/o Lab</cell><cell>18.33</cell><cell>1.37</cell><cell>17.54</cell><cell>1.50</cell></row><row><cell>MCSE</cell><cell>w/o HSV+Lab</cell><cell>16.62</cell><cell>1.88</cell><cell>15.91</cell><cell>2.10</cell></row><row><cell></cell><cell>w/ 3-RGB</cell><cell>16.59</cell><cell>2.06</cell><cell>15.84</cell><cell>2.11</cell></row><row><cell></cell><cell>w/o MTGM</cell><cell>17.02</cell><cell>1.91</cell><cell>17.37</cell><cell>1.59</cell></row><row><cell>MTGM</cell><cell>w/ RDCP</cell><cell>18.74</cell><cell>0.87</cell><cell>18.09</cell><cell>1.01</cell></row><row><cell></cell><cell>w/ RUDCP</cell><cell>18.94</cell><cell>0.83</cell><cell>17.56</cell><cell>1.14</cell></row><row><cell>CAM</cell><cell>w/o CAM</cell><cell>16.36</cell><cell>1.88</cell><cell>16.02</cell><cell>2.02</cell></row><row><cell>loss function</cell><cell>w/o perc loss</cell><cell>23.11</cell><cell>0.49</cell><cell>18.29</cell><cell>0.98</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://csms.haifa.ac.il/profiles/tTreibitz/datasets/ambient forwardlooking/ index.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://drive.google.com/file/d/1zrynw05ZgkVMAybGo9Nhqg2mmIYIMVOT/ view?usp=sharing</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank Codruta O. Ancuti, Cosmin Ancuti, Christophe De Vleeschouwer, and Philippe Bekaert for providing the underwater Color Checker images <ref type="bibr" target="#b24">[25]</ref>. We thank Yecai Guo, Hanyu Li, and Peixiain Zhuang for providing their results <ref type="bibr" target="#b16">[17]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is the space of attenuation coefficients in underwater computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shlesinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Loya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Iluz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4931" to="4940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by wavelength compensation and dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Underwater depth estimation and image restoration based on single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Drews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Botelho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comput. Graph. Appl</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Underwater image enhancement by dehazing with minimum information loss and histogram distribution prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Underwater image restoration based on minimum information loss principle and optical properties of underwater imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1993" to="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Diving into haze-lines: Color restoration of underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Mach. Vis. Conf. (BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A hybrid method for underwater image correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="67" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian retinex underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhancing underwater images and videos by fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting><address><addrLine>1, 2, 6, 7, 10</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Emerging from water: Underwater image color correction based on weakly supervised color transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">WaterGAN: Unsupervised generative network to enable real-time color correction of monocular underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Skinner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eustice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robot. Autom. Lett</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Underwater scene prior inspired deep underwater image and video enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Zeroreference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1780" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to enhance low-light image via zero-reference deep curve estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Nested network with two-stream pyramid for salient object detection in optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="9156" to="9166" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ASIF-Net: Attention steered interweave fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cybernetics</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="100" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Underwater image enhancement using a multiscale dense generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Ocean. Engin</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An underwater image enhancement benchmark dataset and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A revised underwater image formation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6723" to="6732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Colourconsistent structure-from-motion models using underwater imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bryson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">robot.: Sci. Syst. VIII</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sea-thru: A method for removing water from underwater images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Akkaynak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1682" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Clear underwater vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Karpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhancing the low quality images using unsupervised colour correction method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Odetayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talib</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Syst., Man, Cybern. (ICSMC)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Underwater image quality enhancement through integrated color moodel with rayleigh distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Isa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="219" to="230" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Color balance and fusion for underwater image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Underwater image enhancement using adaptive retinal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5580" to="5595" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Color channel compoensation (3c): A fundamental pre-processing step for image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Vleeschouwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2653" to="2665" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic red-channel underwater image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galdran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Picon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alvarez-Gila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commu. and Image Repre</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="145" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Underwater image restoration based on image blurriness and light absorption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Cosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generalization of the dark channel prior for single image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cosman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-world underwater enhancement: Challenges benchmarks and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">iEEE Trans. Circuits Syst. Video Technol</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4861" to="4875" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deblurring images via dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2315" to="2328" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical features driven residual learning for depth map super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2545" to="2557" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Low-light image enhancement via a deep hybrid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4364" to="4375" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zeroreference deep curve estimation for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Comput. Vis. Pattern Rec. (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1780" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Diving deeper into underwater image enhancement: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">115978</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Exempar based underwater image enhancement augmented by wavelet corrected transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jamadandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Mudenagudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. Workshop (CVPRW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Underwater image enhancement based on conditional generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="115" to="723" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">U-net: Convolutional network for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Med. Image Comput. Comput. Ass. Inter. (MICCAI)</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hue-preserving color image enhancement without gamut problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1591" to="1598" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn. (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vision and the atmosphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="233" to="254" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis. (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Twitter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mindspore</surname></persName>
		</author>
		<ptr target="https://www.mindspore.cn/,2020.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Underwater single image color restoration using haze-lines and a new quantitative dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Treibitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An underwater color image quality evaluation metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sowmya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Human-visual-system-inspired underwater image quality measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Panetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agaian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Ocean. Engin</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Making a &quot;completely blind&quot; image quality analyzer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The ciede2000 color-difference formula: Implementation notes, supplementary test data, and mathematical observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dalal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Color Res. Appl</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="21" to="30" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">he was a joint-training Ph.D. Student with Australian National University, Australia. He was a postdoctoral fellow with the Department of Computer Science</title>
	</analytic>
	<monogr>
		<title level="m">His current research focuses on image processing</title>
		<meeting><address><addrLine>Tianjin, China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Chongyi Li received the Ph.D. degree from the School of Electrical and Information Engineering, Tianjin University ; City University of Hong Kong, Hong Kong ; Nanyang Technological University (NTU)</orgName>
		</respStmt>
	</monogr>
	<note>He is currently a research fellow with the School of Computer Science and Engineering. and deep learning, particularly in the domains of image restoration and enhancement</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Australia in Cyber-Physical Systems, adjunct Lecturer at Australian National University (ANU) and visiting fellow at University of Technology Sydney (UTS)</title>
	</analytic>
	<monogr>
		<title level="m">Saeed Anwar is a research Scientist at Data61, Commonwealth Scientific and Industrial Research Organization (CSIRO)</title>
		<imprint>
			<publisher>ANU</publisher>
		</imprint>
		<respStmt>
			<orgName>CSIRO and College of Electrical &amp; Computer Science (CECS), Australian National University</orgName>
		</respStmt>
	</monogr>
	<note>He completed his PhD at the Computer Vision Research Group (CVRG) at Data61</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">His research interests fall into the general areas of visual computing, such as image/video/3D geometry data representation, processing and analysis, semi/un-supervised data modeling, and data compression and adaptive transmission Dr. Hou was the recipient of several prestigious awards, including the Chinese Government Award for Outstanding Students Study Abroad from China Scholarship Council in 2015, and the Early Career Award (3/381) from the Hong Kong Research Grants Council</title>
	</analytic>
	<monogr>
		<title level="m">2012, and the Ph.D. degree in electrical and electronic engineering from the School of Electrical and Electronic Engineering</title>
		<meeting><address><addrLine>Guangzhou, China; Xi&apos;an, China; Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>China University of Technology ; Nanyang Technological University ; City University of Hong Kong</orgName>
		</respStmt>
	</monogr>
	<note>He is a member of Multimedia Systems &amp; Applications Technical Committee (MSA-TC), IEEE CAS. He is currently serving as an Associate Editor for IEEE Transactions on Circuits and Systems for Video Technology, The Visual Computer, and Signal Processing: Image Communication, and the Guest Editor for the IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. He also served as an Area Chair of ACM MM 2019 and 2020, IEEE ICME 2020, and WACV 2021. He is a senior member of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
