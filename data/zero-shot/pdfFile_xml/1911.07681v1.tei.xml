<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GLMNet: Graph Learning-Matching Networks for Feature Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jiang</surname></persName>
							<email>jiangbo@ahu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Technology</orgName>
								<orgName type="institution">Anhui University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GLMNet: Graph Learning-Matching Networks for Feature Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, graph convolutional networks (GCNs) have shown great potential for the task of graph matching. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. One important aspect of graph matching is the construction of two matching graphs. However, the matching graphs we feed to existing graph convolutional matching networks are generally fixed and independent of graph matching, which thus are not guaranteed to be optimal for the graph matching task. Also, existing GCN matching method employs several general smoothing-based graph convolutional layers to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learn a pair of optimal graphs that best serve graph matching task.</p><p>(2) It further employs a Laplacian sharpening convolutional module to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments on two benchmarks demonstrate the effectiveness of GLMNet and advantages of its main modules.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Many problems of interest in computer vision and pattern recognition area can be formulated as a problem of finding consistent correspondences between two sets of features which is known as feature matching problem. Feature set that incorporates the pairwise constraint can be represented via an attribute graph whose nodes represent the unary descriptors of feature points and edges encode the pairwise relationships among different feature points. Based on this graph representation, feature matching can then be reformulated as graph node matching problem.</p><p>Graph matching generally first operates with both node and edge affinities that encode similarities between node and edge descriptors in two graphs. Then, it is can be formulated mathematically as an Integral Quadratic Programming (IQP) problem with permutation constraint on related solution to encode the one-to-one matching constraints <ref type="bibr" target="#b25">[26]</ref>. It is known to be NP-hard. Thus, many methods normally solve it approximately by relaxing the discrete permutation constraint and finding local optimal solutions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref>. In addition, to obtain better node/edge affinities, learning methods have been investigated to determine the more optimal parameters in node/edge affinity computation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>. Recently, deep learning methods have also been developed for matching problem <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b23">24]</ref>. The main benefit of deep learning matching methods is that they can conduct visual feature representation, node/edge affinity learning and matching optimization together in an end-to-end manner. Zanfir et al., <ref type="bibr" target="#b26">[27]</ref> propose an end-to-end graph matching model which makes it possible to learn all parameters of the graph matching process. Wang et al., <ref type="bibr" target="#b23">[24]</ref> recently propose to explore graph convolutional networks (GCNs) for graph matching which conducts graph node embedding and matching simultaneously in a unified network.</p><p>Inspired by recent deep graph matching methods, in this paper, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. Overall, the main contributions of this paper are three aspects.</p><p>First, one important aspect of (feature) graph matching is the construction of two matching graphs. Existing deep graph matching models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref> generally use fixed structure graphs, such as k-NN, Delaunary graph, etc., which thus are not guaranteed to best serve the matching task. To address this issue, we propose to adaptively learn a pair of optimal graphs for the matching task and integrate graph learning and graph matching simultaneously in a unified end-to-end network architecture.</p><p>Second, existing GCN based graph matching model <ref type="bibr" target="#b23">[24]</ref> adopts the general smoothing based graph convolution operation <ref type="bibr" target="#b13">[14]</ref> for graph node embedding which may encourage the feature embedding of each node becoming more similar to those of its neighboring nodes <ref type="bibr" target="#b15">[16]</ref>. This is desirable for graph node labeling or classification tasks <ref type="bibr" target="#b13">[14]</ref>, but undesirable for the matching task because extensive smoothing convolution may dilute the discriminatory information. To alleviate this affect, we propose to incorporate a Laplacian sharpening based graph convolution operation <ref type="bibr" target="#b18">[19]</ref> for graph node embedding and matching task. Laplacian sharpening process can be regarded as the counterpart of Laplacian smoothing which encourages the embedding of each node farther away from its neighbors.</p><p>Third, existing deep graph matching methods generally utilize a doubly stochastic normalization for the final matching prediction <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref>. This generally ignores the discrete one-to-one matching constraints in matching optimization/prediction. To overcome this issue, we develop a novel constraint regularized loss to further incorporate the one-toone matching constraints in matching prediction.</p><p>Experimental results including ablation studies demonstrate the effectiveness of our GLMNet and advantages of devised components including graph learning-matching architecture, Laplacian sharpening convolution for discriminative embedding, and constraint regularized loss to encode one-to-one matching constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Graph convolutional networks</head><p>Recently, Graph Convolutional Networks (GCNs) have been widely studied to deal with graph node embedding and learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19]</ref>. The main advantage of GCNs is that they provide an end-to-end learning which thus can be incorporated into some other specific deep learning architectures. One can refer work <ref type="bibr" target="#b24">[25]</ref> for more comprehensive review. Here, we briefly review some works that are related with our model in this paper. By exploring the first-order approximation of spectral filters, Kipf et al., <ref type="bibr" target="#b13">[14]</ref> propose a simple Graph Convolutional Network (GCN) for graph node representation and semi-supervised learning. Li et al., <ref type="bibr" target="#b15">[16]</ref> interpret GCNs <ref type="bibr" target="#b13">[14]</ref> from graph Laplacian smoothing and show that feature representations of graph nodes will become more similar as network depth increases. Recently, Park et al., <ref type="bibr" target="#b18">[19]</ref> introduce a novel Laplacian sharpening convolution operation and propose a symmetric graph convolutional autoencoder model for unsupervised graph representation learning. To further incorporate graph learning into GCNs, Veli?kovi? et al., <ref type="bibr" target="#b22">[23]</ref> propose Graph Attention Networks (GATs) for graph based semi-supervised learning. Li et al., <ref type="bibr" target="#b22">[23]</ref> present an adaptive graph CNNs, in which the graph is learned adaptively via a metric learning method. Jiang et al., <ref type="bibr" target="#b12">[13]</ref> propose Graph Learning-convolutional Network (GLCN) for graph node semi-supervised classification by integrating both graph learning and convolutional representation together in a unified network architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep graph matching</head><p>Graph matching is a fundamental problem in computer vision and pattern recognition area and has been widely studied. Recently, deep learning models have been developed for graph matching problem. Nowak et al., <ref type="bibr" target="#b17">[18]</ref> propose to explore graph neural networks (GNNs) for solving the general Quadratic Assignment Problem (QAP) which can be used for graph matching problem. Li et al., <ref type="bibr" target="#b16">[17]</ref> propose Graph Matching Networks (GMNs) for learning the similarity of graph structured objects. This work focuses on learning the similarity between two graphs. Differently, here we focus on graph node one-to-one matching problem. Zanfir et al., <ref type="bibr" target="#b26">[27]</ref> propose an end-to-end graph matching model which integrates node feature extraction, node/edge affinities learning and matching optimization together in a unified network. Wang et al., <ref type="bibr" target="#b23">[24]</ref> recently propose to explore graph convolutional networks (GCNs) for graph matching task which conducts graph node embedding and matching prediction simultaneously in a unified network. The core of this method <ref type="bibr" target="#b23">[24]</ref> is to learn an optimal embedding for graph matching task based on which the final graph matching prediction can be approximately transferred as a linear assignment problem and thus can be solved via a Sinkhorn operation <ref type="bibr" target="#b0">[1]</ref>.</p><p>Following to this research direction, in this paper, we propose a new Graph Learning-Matching Network (GLMNet) by further exploiting graph convolutional networks for graph matching task. In contrast to previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref>, the main contributions of GLMNet are follows. First, it incorporates graph learning into graph matching network. To our best knowledge, this is the first time to incorporate graph learning into graph matching to build an end-to-end learning network. Second, GLMNet employs a more reasonable sharpening-based graph convolutional embedding for graph node embedding and matching task. Third, in GLMNet, a new constraint regularized loss function is designed to encode the one-to-one matching constraints in graph matching optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>Problem Formulation. Let M and D denote two feature sets of two images I, I , respectively. The aim of feature matching is to determine the consistent correspondences between features of two images with one-to-one matching constraints (i.e., each feature in M can match at most one feature in D and vice versa). To do so, for each feature in M and D, we first extract a unary feature descriptor for it. Let X = (x 1 , x 2 ? ? ? x m ) and Y = (y 1 , y 2 ? ? ? y n ) denote the collection of unary feature descriptors for these two sets, where m, n denote the sizes of two feature sets, respectively. Also, for each pair of feature points in M and D, one can extract the binary relationship between them. Therefore, one can build two graphs G(X, A x ) and G (Y, G y ) for feature sets M and D whose nodes denote the feature points attributed by feature descriptors X, Y and edges A x , A y encode the binary relationships between feature points. Based on these representations, the task of feature matching can then be formulated as graph matching that aims to determine the consistent correspondences between two graph node (feature) sets by considering both 1) how well the nodes' descriptors are matched and 2) how well the edges' attributes (relationships) are preserved <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>One kind of popular approaches for graph matching problem is to utilize graph embedding based approaches that aim to first embed the nodes of two graphs into a common feature space and then utilize a metric learning technique to find the point correspondences in the feature space <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22]</ref>. Comparing with original feature space X, Y , the node representations in the embedding space further incorporate the information of graph structure and thus can become more discriminatively for the matching problem <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>. Wang et al., <ref type="bibr" target="#b23">[24]</ref> recently propose to integrate graph embedding and matching optimization together via a supervised GCN architecture which can obviously boost their respectively performance.</p><p>Overview of Approach. Inspired by recent work <ref type="bibr" target="#b23">[24]</ref>, our aim in this paper is to conduct the above node feature extraction, graph construction, graph node embedding and matching optimization together in an end-to-end network framework. We call our approach as Graph Learning-Matching Convolutional Network (GLMNet). <ref type="figure" target="#fig_0">Figure 1</ref> shows the overview of GLMNet which contains the following four modules.</p><p>? Deep feature extraction: We utilize a CNN to extract the feature descriptors of all feature points for two matching images.</p><p>? Graph learning module: We develop a graph learning module to adaptively learn a pair of optimal graphs for graph matching problem.</p><p>? Graph convolutional embedding module: We employ a novel GCN architecture to learn discriminative node embeddings for the node affinity learning and matching prediction.</p><p>? Affinity learning and matching prediction: Based on the proposed graph convolutional embeddings, we finally conduct an affinity learning module for matching prediction.</p><p>In the following sections, we present the details of these modules respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Deeply learned node feature extraction</head><p>In this paper, we focus on image feature (key-points, regions, etc) matching task. We adopt a CNN for their feature extraction, which are constructed by interpolating on CNNs feature map <ref type="bibr" target="#b23">[24]</ref>. Specifically, we adopt a five-layer VGG-16 network <ref type="bibr" target="#b19">[20]</ref> to extract a 1024 dimension feature descriptor for each keypoint/region. The parameters of the used VGG network are pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref>. In the following, we denote X = (x 1 , x 2 ? ? ? x m ) ? R p?m and Y = (y 1 , y 2 ? ? ? y n ) ? R p?n as the collections of feature descriptors for two feature sets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Graph learning architecture</head><p>One important aspect of graph matching is the featurebased graph construction G(X, A x ) and G (Y, A y ). Constructing a good graph to represent feature relationships is generally important for graph convolutional embedding and matching tasks. Traditional human established graphs generally use fixed parameters to determine the graph structure and thus are not guaranteed to best serve the matching task. To overcome this issue, we propose to learn a pair of optimal graphs A x , A y adaptively and further provide a unified graph learning-matching network architecture for the matching task.</p><p>Let X = (x 1 , x 2 ? ? ? x m ) ? R n?p be n data features. Our graph learning aims to seek a function A x (i, j) = ?(x i , x j ; ? x ) with parameter ? x to represent the pairwise relationship between data x i and x j . Here, we implement ?(x i , x j ; ? x ) via a single-layer neural network, which is parameterized by a weight vector ? x . Inspired by previous works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b12">13]</ref>, we propose to define ?(x i , x j ; ?) as</p><formula xml:id="formula_0">A x (i, j) = ?(x i , x j ; ? x ) = exp(?(? T x [x i ||x j ])) n j=1 exp(?(? T x [x i ||x j ])<label>(1)</label></formula><p>where || denotes the concatenation operation and ?(?) denotes an activation function, such as ReLU(?) = max(0, ?).</p><p>In some cases, when an initial graph A x is available, we can thus incorporate it into the above graph learning as</p><formula xml:id="formula_1">A x (i, j) = ?(x i , x j , A x ; ? x ) =? x (i, j) exp(?(? T x [x i ||x j ])) n j=1 A x (i, j) exp(?(? T x [x i ||x j ]))<label>(2)</label></formula><p>In summary, we thus learn two optimal graphs for two feature sets respectively as,</p><formula xml:id="formula_2">A x (i, j) = ?(x i , x j , A x ; ? x ) (3) A y (i, j) = ?(y i , y j , A y ; ? y )<label>(4)</label></formula><p>where A x and A y denote the initial graphs for two feature sets respectively, and ? x , ? y denote trainable parameters. In our experiments, we set ? x = ? y to encourage consistent graph learning across two feature sets <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Graph convolutional module</head><p>As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our graph convolutional module involves two intra-graph convolutional submodules, i.e., smoothing convolution layer and sharpening convolution layer, and one cross-graph convolutional submodule.</p><p>Intra-graph Convolutional module. The aim of intragraph convolutional embedding is to generate discriminative embeddings of graph nodes for matching problem by taking in considering both unary feature representations of nodes and binary relationships of edges. Note that existing GCNs mostly adopt (Laplacian) smoothing based graph convolution operations in node representation which encourage the latent representation of each node similar to those of its neighboring nodes as depth increases <ref type="bibr" target="#b15">[16]</ref>. This is desirable for graph node labeling or classification tasks, but undesirable for the matching task. Because extensive smoothing convolution may dilute the discriminatory information. To alleviate this affect, inspired by recent work <ref type="bibr" target="#b18">[19]</ref>, we propose to further incorporate Laplacian sharpening convolution and employ both smoothing and sharpening convolution operations for graph node embeddings. Laplacian smoothing convolution. Given input node embeddings {X (k) , Y (k) } of two graphs in the k-th hidden layer, we can obtain the optimal graphs {A (k)</p><p>x , A (k) y } by using Eqs. <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>. Then, Laplacian smoothing convolution aims to conduct the layer-wise propagation as</p><formula xml:id="formula_3">X (k+1) = ? (1 ? ?)X (k) ? (k) n + ?? (k) x X (k) ? (k) e<label>(5)</label></formula><formula xml:id="formula_4">Y (k+1) = ? (1 ? ?)Y (k) ? (k) n + ?? (k) y Y (k) ? (k) e<label>(6)</label></formula><p>where k = 0, 1 ? ? ? K ? 1 and ?(?) denotes an activation function, such as ReLU(?) = max(0, ?), and parameter ? ? (0, 1) balances two terms. In our experiments, we set ? = 0.5.? e } are shared across two graphs, which can encourage to learn consistent node embeddings across two graphs, as suggested in other works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b10">11]</ref>. Laplacian sharpening convolution. To further enhance the discriminative ability of graph node embeddings, we also employ a Laplacian sharpening convolutional module in our GLMNet. Laplacian sharpening can be regarded as the counterpart of Laplacian smoothing which encourages the embedding of each node farther away from its neighbors <ref type="bibr" target="#b18">[19]</ref>. Formally, given input node embeddings {X (k) , Y (k) } of two matching graphs in the k-th hidden layer, we can obtain the optimal graphs {A (k)</p><p>x , A (k) y } by using Eqs. <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>. Then, we propose to conduct Laplacian sharpening convolution as,</p><formula xml:id="formula_5">X (k+1) = ? (1 +?)X (k) ? (k) n ??? (k) x X (k) ? (k) e<label>(7)</label></formula><formula xml:id="formula_6">Y (k+1) = ? (1 +?)Y (k) ? (k) n ??? (k) y Y (k) ? (k) e<label>(8)</label></formula><p>where parameter? &gt; 0 balances two terms and is set to 0.75 in our experiments.? Cross-graph Convolutional Module. Similar to work <ref type="bibr" target="#b23">[24]</ref>, we further leverage a cross graph convolutional learning module to mine the correlations between the embeddings of two graphs. Given embeddings X (k) ? R m?d k and Y (k) ? R n?d k of two graphs, we first compute the co-affinity matrix C (k) xy between them as,</p><formula xml:id="formula_7">C (k) xy (i, j) = exp (X (k) T W Y (k) ) ij ? ? R m?n<label>(9)</label></formula><p>where W ? R d k ?d k is a trainable weight matrix. We can also compute C yx }, we then conduct cross-graph convolutional learning as</p><formula xml:id="formula_8">X (k+1) = C (k) xy Y (k) ||X (k) ? (k) xy (10) Y (k+1) = C (k) yx X (k) ||Y (k) ? (k) yx<label>(11)</label></formula><p>where || denotes the concatenation operation to incorporate the original feature information in cross graph convolution.</p><formula xml:id="formula_9">Parameters ? xy = {? (k) xy , ? (k)</formula><p>yx } denote the layer-specific trainable weight matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Matching prediction and loss function</head><p>Affinity Metric Learning. Using the above graph convolutional embeddings, the final matching prediction can be formulated as node-to-node affinity metric learning in the embedding space. Let X ? R m?d , Y ? R n?d denote the output node embeddings of two graphs, respectively. Then, the node-to-node affinity (similarity) matrix C can be learned as</p><formula xml:id="formula_10">C(i, j) = exp ( XM Y T ) ij ? ? R m?n<label>(12)</label></formula><p>where C(i, j) denotes the similarity between node i in the first graph G and node j in the second graph G . M ? R d?d denotes the learnable weight matrix of this affinity function.</p><p>For one-to-one matching problem, the ideal matching prediction C should satisfy the permutation constraint, i.e., there exists only one non-zero element in each row/column of matrix C. One possible way is to use a post-discretization operation (e.g.,Hungarian) on the learned C. However, the discretization operation is not differentiable, making the training of network more difficultly. Thus, one can use the continuous Sinkhorn operation <ref type="bibr" target="#b20">[21]</ref> to make the final matching prediction C satisfy the doubly-stochastic constraint, i.e.,</p><formula xml:id="formula_11">C = Sinkhorn( C)<label>(13)</label></formula><p>This Sinkhorn process has been shown effectively for permutation prediction and approximation <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b0">1]</ref>. Additionally, we will introduce a constraint regularized loss to further encourage the permutational matching prediction, as discussed below. Constraint Regularized Loss. To further incorporate the one-to-one matching constraints, we develop a constraint regularized loss function to encourage the predicted matching solution satisfying the permutation constraint. To do so, we first define an indicative matrix U ? R mn?mn which denotes the conflict relationships among different assignments/matches, i.e.,</p><formula xml:id="formula_12">U ij,kl = 1 if i = k, j = l or i = k, j = l, 0 otherwise.<label>(14)</label></formula><p>where i, k = 1, 2, ? ? ? m and j, l = 1, 2, ? ? ? n. For the oneto-one matching problem, the ideal permutational matching solution C should satisfy</p><formula xml:id="formula_13">i,j k,l U ij,kl C ij C kl = 0<label>(15)</label></formula><p>Note that, the above final output matching prediction C is doubly stochastic and thus nonnegative. The motivates us to develop the following constraint regularized loss to encourage the learned matching prediction satisfying the one-to-one matching constraints as much as possible,</p><formula xml:id="formula_14">L con = i,j k,l U ij,kl C ij C kl<label>(16)</label></formula><p>Cross Entropy Loss. For the matching prediction, we use cross entropy loss function. Let P ? R m?n , P ij ? {0, 1} denotes the ground truth permutation matrix solution. Then, we adopt the cross entropy loss to train our model. The cross entropy loss is defined as <ref type="bibr" target="#b23">[24]</ref> </p><formula xml:id="formula_15">L sol = ? i,j P ij log(C ij ) + (1 ? P ij ) log(1 ? C ij ) (17)</formula><p>Thus, the final overall loss function to train our GLMNet network in an end-to-end manner is formulated as</p><formula xml:id="formula_16">L = L sol + ?L con<label>(18)</label></formula><p>where ? &gt; 0 balances two terms and is set to 0.1 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate the effectiveness of the proposed GLMnet on two benchmark datasets (PASCAL VOC <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b1">2]</ref>, WILLOW-ObjectClass <ref type="bibr" target="#b5">[6]</ref>) and compare it with other competing methods including HARG-SSVM <ref type="bibr" target="#b5">[6]</ref>, GMN <ref type="bibr" target="#b26">[27]</ref>, GMN-PL <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b23">24]</ref>, PIA-GM-OL <ref type="bibr" target="#b23">[24]</ref>, PIA-GM <ref type="bibr" target="#b23">[24]</ref> and PCA-GM <ref type="bibr" target="#b23">[24]</ref>.   <ref type="bibr" target="#b19">[20]</ref> to extract image features, which is pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref>. More specifically, we extract features from relu4 2 and relu5 1 for fair comparison with previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>. We concatenate these two kind of features together to form a 1024 dimension feature representation. Our embedding network consists of three convolutional layers including one graph learningsmoothing convolutional layer, one cross-graph convolutional layer and one graph learning-sharpening convolutional layer. The final embedding vectors of all nodes in two graphs are 2048 dimension. The balancing parameters ?,? in GLM-Net are fixed and set to 0.5, 0.75 respectively on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on PASCAL VOC dataset</head><p>Our first evaluation is performed on PASCAL VOC dataset <ref type="bibr" target="#b9">[10]</ref> with Berkeley annotations of keypoints <ref type="bibr" target="#b1">[2]</ref>. This annotated dataset consists of 20 different categories of images which are varying from its scale, pose and illumination. Each image is annotated with 6?23 inlier keypoints. Following the setting of previous works <ref type="bibr" target="#b23">[24]</ref>, we use 7020 annotated images which involve all 20 categories for training and 1682 images for testing. All images are cropped around its bounding box and resized to 256 ? 256 before feeding to our network for training and testing. <ref type="table" target="#tab_0">Table 1</ref> summarizes the comparison results on this dataset. The results of other comparison methods have been reported in work <ref type="bibr" target="#b23">[24]</ref> and here we use them directly. From <ref type="table" target="#tab_0">Table 1</ref>, one can note that, the proposed GLMNet obviously outperforms other recent methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>. Overall, GLMNet gains 3.7% improvement over PCA-GM <ref type="bibr" target="#b23">[24]</ref> which is also adopting GCN for graph matching and thus most related with our GLMNet. This clearly demonstrates the effectiveness and advantage of the proposed GLMNet on solving graph based feature matching problem. Also, GLMNet generally performs better than the other comparison methods on most image categories, indicating the robustness of GLMNet. <ref type="figure" target="#fig_4">Figure 2</ref> shows some matching examples on this dataset. One can note that, GLM-Net can obtain the correct matches for image pairs that are with large appearance and pose changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation on WILLOW-ObjectClass dataset</head><p>This dataset contains object class images which are selected from Caltech-256 (Face, Duck, and Wine bottle) and PASCAL VOC2007 (Motorbike and Car) <ref type="bibr" target="#b9">[10]</ref> datasets. The images of these classes are selected such that each class contains at least 40 images. Similar to PASCAL VOC dataset <ref type="bibr" target="#b9">[10]</ref>, each image is annotated with 10 keypoints. We also resize each image into 256 ? 256 before feeding to our matching network. Following the experiment setting <ref type="bibr" target="#b23">[24]</ref>, we first filter out the overlapping images in PascalVOC.</p><p>Then, we initialize model weights by training the network on Pascal VOC Keypoint dataset <ref type="bibr" target="#b1">[2]</ref>. They are later fine-tuned on the Willow dataset which denote as GMLNet-Willow. Table 2 summarizes the comparison results on this dataset. For comparison methods HARG-SSVM <ref type="bibr" target="#b5">[6]</ref>, GMN-Willow <ref type="bibr" target="#b26">[27]</ref> and PCA-GM-Willow <ref type="bibr" target="#b23">[24]</ref>, we list the results on this dataset that have been reported in previous work <ref type="bibr" target="#b23">[24]</ref>. From <ref type="table" target="#tab_1">Table  2</ref>, one can note that, the proposed GLMNet performs better than the other comparison related methods on this dataset. Overall, GMLNet-Willow gains 2.4% and 11.5% improvements over PCA-GM-Willow <ref type="bibr" target="#b23">[24]</ref> and GMN-Willow <ref type="bibr" target="#b26">[27]</ref>, respectively. This further demonstrates the effectiveness of the proposed GLMNet for graph matching problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>To justify the effectiveness of three main components (graph learning, Laplacian sharpening convolution and constraint regularization loss L con ) in the proposed GLMNet model, we conduct ablation experiments on PASCAL VOC dataset <ref type="bibr" target="#b9">[10]</ref>. <ref type="table" target="#tab_2">Table 3</ref> summarizes the ablation study results, where tick in the table denotes the module is used in our model. For comparison, we also report the result of PCA-GM <ref type="bibr" target="#b23">[24]</ref> as the baseline. Here, we can note that (1) graph learning module can significantly improve the matching results which clearly indicates the advantage of the proposed graph learning architecture to adaptively learn optimal graphs for graph matching problem. (2) Incorporating Laplacian sharpening convolution is beneficial for learning more discriminative node embeddings which thus obviously improves the final matching accuracy. (3) The proposed constraint regularization loss is useful to guide the more accurate graph matching prediction by further incorporating the desired one-to-one matching constraints in matching optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Works</head><p>This paper proposes a novel Graph Learning-Matching Network (GLMNet) model for graph matching. GLMNet integrates graph learning and graph matching architectures together in a unified end-to-end network, which can learn a pair of optimal matching graphs that best serve the task of graph matching. GLMNet employs a Laplacian sharpening convolution to generate more discriminative node embeddings for matching task. The proposed constraint regularized loss is further designed for GLMNet training to encode oneto-one matching constraints. Experimental results on two benchmarks demonstrate that GLMNet obviously outperforms recent fixed-graph deep graph matching methods. Note that, GLMNet is not limited to deal with the task of two graph matching. In the future, we will adapt GLMNet to address the more general multiple graph matching task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Architecture of the proposed GLMNet which mainly contains node feature extraction, graph learning, graph convolutional embedding and node affinity metric learning. The CNN model, graph learning and graph convolutional embedding and affinity metric are all learnable in an end-to-end manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>e</head><label></label><figDesc>} denote layer-specific trainable weight matrices. Here, we use two trainable weight matrices {?(k) n , ? (k)e } to learn node unary representation and propagation representation respectively, as suggested in work<ref type="bibr" target="#b23">[24]</ref>. The network parameters ? (k) = {?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>y</head><label></label><figDesc>respectively. Similar to Eqs.<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref>, here we also use trainable parameter matrices? (k) = {? (k) n , ? (k)e } to learn node unary representation and propagation representation respectively. These parameter matrices are shared across two graphs to encourage to learn consistent node representations across two matching graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Some matching examples of GLMNet on PASCAL VOC test-set. Colors identify the predicted matching between key-points. Note that, GLMNet can obtain the correct matches for image pairs that are with large appearance and pose changes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison results of different methods on Pascal VOC dataset. Results indicated with * are taken from<ref type="bibr" target="#b23">[24]</ref>. The best results are marked as bold.</figDesc><table><row><cell>Method</cell><cell cols="2">areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean</cell></row><row><cell>GMN*</cell><cell>31.9 47.2 51.9 40.8 68.7 72.2 53.6 52.8 34.6 48.6 72.3 47.7 54.8 51.0</cell><cell>38.6 75.1 49.5 45.0 83.0 86.3 55.3</cell></row><row><cell>GMN-PL*</cell><cell>31.1 46.2 58.2 45.9 70.6 76.4 61.2 61.7 35.5 53.7 58.9 57.5 56.9 49.3</cell><cell>34.1 77.5 57.1 53.6 83.2 88.6 57.9</cell></row><row><cell cols="2">PIA-GM-OL* 39.7 57.7 58.6 47.2 74.0 74.5 62.1 66.6 33.6 61.7 65.4 58.0 67.1 58.9</cell><cell>41.9 77.7 64.7 50.5 81.8 89.9 61.6</cell></row><row><cell>PIA-GM*</cell><cell>41.5 55.8 60.9 51.9 75.0 75.8 59.6 65.2 33.3 65.9 62.8 62.7 67.7 62.1</cell><cell>42.9 80.2 64.3 59.5 82.7 90.1 63.0</cell></row><row><cell>PCA-GM*</cell><cell>40.9 55.0 65.8 47.9 76.9 77.9 63.5 67.4 33.7 65.5 63.6 61.3 68.9 62.8</cell><cell>44.9 77.5 67.4 57.5 86.7 90.9 63.8</cell></row><row><cell>GLMNet</cell><cell>52.0 67.3 63.2 57.4 80.3 74.6 70.0 72.6 38.9 66.3 77.3 65.7 67.9 64.2</cell><cell>44.8 86.3 69.0 61.9 79.3 91.3 67.5</cell></row><row><cell cols="2">4.1. Network architecture and parameter setting</cell><cell></cell></row><row><cell cols="2">For feature extraction module, we adopt a deep convo-</cell><cell></cell></row><row><cell cols="2">lutional neural network (VGG-16)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison results of different methods on WILLOW-ObjectClass dataset. Results indicated with * are taken from<ref type="bibr" target="#b23">[24]</ref>. The best results are marked as bold.</figDesc><table><row><cell>Method</cell><cell cols="3">face mbike car duck bottle mean</cell></row><row><cell>HARG-SSVM* [6]</cell><cell>91.2</cell><cell>44.4 58.4 55.2 66.6</cell><cell>63.2</cell></row><row><cell>GMN-Willow* [27]</cell><cell>99.3</cell><cell>71.4 74.3 82.8 76.7</cell><cell>80.9</cell></row><row><cell cols="3">PCA-GM-Willow* [24] 100.0 76.7 84.0 93.5 96.9</cell><cell>90.2</cell></row><row><cell>GLMNet-Willow</cell><cell cols="2">100.0 89.7 93.6 85.4 93.4</cell><cell>92.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Result of ablation studies.</figDesc><table><row><cell>PCA-GM Graph learning</cell><cell>Constraint loss L con</cell><cell>Laplacian sharpening</cell><cell>Accuracy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>67.5</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell>66.9</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell>66.6</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>63.8</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Given any matrix A, its row-normalized Laplacian is defined asD ?1 A, where D is the diagonal matrix with D ii = j A ij</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Ranking via sinkhorn propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.1925</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spectral networks and locally connected networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An eigenspace projection clustering method for inexact graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kosinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="515" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1048" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning graphs to match</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reweighted random walks for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="492" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Optimal correspondences from pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Enqvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Josephson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1295" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A unified multiple graph learning and convolutional network model for co-saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1375" to="1382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonnegative orthogonal graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4089" to="4095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semisupervised learning with graph learning-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11313" to="11320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="45" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph matching networks for learning the similarity of graph structured objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dullien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3835" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revised note on learning quadratic assignment with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Science Workshop</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2018" />
			<publisher>DSW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Symmetric graph convolutional autoencoder for unsupervised graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6519" to="6528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A relationship between arbitrary positive matrices and doubly stochastic matrices. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sinkhorn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="876" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph matching based on spectral embedding with missing value</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3768" to="3779" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning combinatorial embedding networks for deep graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A short survey of recent advances in graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2016 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning of graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2684" to="2693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factorized graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
