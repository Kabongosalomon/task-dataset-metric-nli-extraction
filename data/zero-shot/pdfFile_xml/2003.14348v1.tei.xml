<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-03-31">31 Mar 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ching</surname></persName>
							<email>tom.chen1@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingchen</forename><surname>1?</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Distributed Data Lab, Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ava</forename><surname>Khonsari</surname></persName>
							<email>ava.khonsari@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Distributed Data Lab, Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Lashkari</surname></persName>
							<email>amir.lashkari1@huawei.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Distributed Data Lab, Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mina</forename><surname>Rafi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazari</forename><surname>1?</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Distributed Data Lab, Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaspreet</forename><surname>Singh</surname></persName>
							<email>jaspreet.singh.sambee@huawei.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sambee</forename><surname>1?</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Distributed Data Lab, Huawei Technologies</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><forename type="middle">A</forename><surname>Nascimento</surname></persName>
							<email>2mario.nascimento@ualberta.ca</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Computing Science</orgName>
								<orgName type="institution">University of Alberta</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-03-31">31 Mar 2020</date>
						</imprint>
					</monogr>
					<note>UniformAugment: A Search-free Probabilistic Data Augmentation Approach</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data augmentation</term>
					<term>image processing</term>
					<term>image classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Augmenting training datasets has been shown to improve the learning effectiveness for several computer vision tasks. A good augmentation produces an augmented dataset that adds variability while retaining the statistical properties of the original dataset. Some techniques, such as AutoAugment and Fast AutoAugment, have introduced a search phase to find a set of suitable augmentation policies for a given model and dataset. This comes at the cost of great computational overhead, adding up to several thousand GPU hours. More recently RandAugment was proposed to substantially speedup the search phase by approximating the search space by a couple of hyperparameters, but still incurring nonnegligible cost for tuning those. In this paper we show that, under the assumption that the augmentation space is approximately distribution invariant, a uniform sampling over the continuous space of augmentation transformations is sufficient to train highly effective models. Based on that result we propose UniformAugment, an automated data augmentation approach that completely avoids a search phase. In addition to discussing the theoretical underpinning supporting our approach, we also use the standard datasets, as well as established models for image classification, to show that UniformAugment's effectiveness is comparable to the aforementioned methods, while still being highly efficient by virtue of not requiring any search.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data augmentation is an effective technique for training machine learning models, especially for image classification. Data augmentation virtually increases the dataset size in order to help regularizing the model, and increases the diversity of the data to help the model learn invariance. While this ideally leads to improvement in accuracy and generalization, increasing the diversity of the data ? Contributed equally. requires some care. While, during training, we want the augmented data point to be different from the original one, we still want it to be a correct representative of the original label. For example, in image classification, if an image is cropped at an area that does not contain the main object, the augmented image is no longer representative of the original class. Therefore, to balance the diversity vs. correctness trade off, conventional data augmentations, such as cropping, blurring, etc, require domain expertise and manual work in order to deliver an effective augmentation strategy.</p><p>Recent research has focused on automatically finding optimal data augmentations while avoiding manual design. Automated augmentation methods first assume augmentation space generated by a set of standard augmentation operations and their parameters, and then use a search algorithm to find a set of augmentation operations and their parameters within this space that reduce the loss of the model. However, searching through the augmentation space imposes substantial computational overhead. Column two of <ref type="table">Table 1</ref> shows the size of the augmentation space for the current automatic augmentation methods: AutoAugment (AA) <ref type="bibr" target="#b2">[3]</ref>, Fast AutoAugment (FAA) <ref type="bibr" target="#b11">[12]</ref>, Population Based Augmentation (PBA) <ref type="bibr" target="#b7">[8]</ref>, and RandAugment (RA) <ref type="bibr" target="#b3">[4]</ref>. Moreover, in terms of GPU hours, AA and FAA require 15000 and 450 GPU hours respectively on ImageNet <ref type="bibr" target="#b4">[5]</ref> dataset. Due to the high cost of searching algorithms, these methods do not scale to train large models and/or large datasets. <ref type="table">Table 1</ref>. Accuracy results for AA, FAA, PBA, RA and UA on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b9">[10]</ref> and ImageNet classification tasks. The models for the datasets are Shake-Shake 26 2x96d (SS) <ref type="bibr" target="#b5">[6]</ref>, Wide-ResNet-28-2 (WRN), and ResNet50 (RN50) <ref type="bibr" target="#b6">[7]</ref>, respectively. Search space is the number of possible augmentations in the augmentation space. (N/A means no published result is available). In this work, we propose UniformAugment (UA), a search-free data augmentation method that is efficient and scalable. We hypothesize that if the augmentation space is approximately invariant, that is, the majority of the augmented data remains representative of the original target label and within the distribution of the dataset, then searching for optimal augmentations within this space is not necessary. In that case, uniformly sampling the augmentation operations from that augmentation space may be just as effective, thus forgoing the expen-sive cost of the search algorithm. Our extensive experiments have shown that by using the same augmentation operations as presented in AA, but by sampling their parameters from a continuous space instead of a discrete space as done in AA's original proposal, we could achieve comparable results. The last line in <ref type="table">Table 1</ref> shows a summary of our results, compared to methods mentioned above, using different datasets and models.</p><p>In addition to extensive experiments, we also analyzed our method and other automated augmentation methods using the theoretical framework proposed by Chen et al. <ref type="bibr" target="#b1">[2]</ref>. Using that framework, we show why uniform sampling is actually expected to achieve good results given an approximately invariant augmentation space. Besides being efficient and effective, UA can be easily implemented and applied to different models and tasks. One can view the design of UA as an instance of Occams Razor principle, i.e., one where the simplest solution is preferred.</p><p>The rest of the paper is organized as follows. In Section 2, we present related works on automated data augmentations. Section 3 presents our main contribution, the UniformAugment algorithm, after discussing the theoretical analysis supporting it. Next, in Section 4, we compare our results with a baseline conventional data augmentation methods as well as AA, PBA, FAA, and RA, using standard datasets and models. Finally, Section 5 concludes the paper and offers directions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>There has been extensive research on data augmentation for computer vision tasks. As mentioned earlier, traditional data augmentation methods require manual design and are usually dataset/domain dependent. For example random flip, and color distortions are commonly used for natural image datasets such as CIFAR-10 <ref type="bibr" target="#b9">[10]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref>, while elastic distortions and scalings are more common on MNIST <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> dataset. Even though there is no universal agreement on which type of augmentations are always beneficial (or prejudicial), there is wide acknowledgement that designing an effective augmentation strategy for a model requires domain knowledge and manual work.</p><p>Recently there has been increased emphasis on searching the optimal set of augmentation policies/strategies which includes a combination of augmentation functions applied to each sample during training. Recent advancements in such search-based methods, inspired by Neural Architecture Search (NAS) <ref type="bibr" target="#b19">[20]</ref>, gave rise to algorithms like AutoAugment (AA) <ref type="bibr" target="#b2">[3]</ref> which uses Reinforcement Learning (RL) to search for an optimal augmentation policy for a given task. They alternately train a child model and a Recurrent Neural Network (RNN) controller to learn the best policies on a proxy task and then use those policies for training the final model. Unfortunately this searching process requires extensive computing power, to the order of many thousands of GPU hours. Fast AutoAugment (FAA) <ref type="bibr" target="#b11">[12]</ref> utilizes Bayesian Optimization to find a data augmentation policy trained for density matching to improve the generalization ability of the model but this again requires an expensive separate search phase on a proxy task, in order of hundreds of GPU hours.</p><p>With the augmentation search space defined by AA being widely used as a de facto standard one, emphasis has been put on optimizing the complexity of finding augmentation policies. Population Based Augmentation (PBA) <ref type="bibr" target="#b7">[8]</ref> aims to find an augmentation schedule by having multiple workers follow an "exploitand-explore" procedure. This approach discards the worst performing model and focuses on the better models while simultaneously exploring the search space of augmentation functions. Even though PBA is more time efficient in comparison to AA, it still uses proxy tasks and at the same time needs very large amounts of memory for the workers working concurrently.</p><p>Contrasting to all the approaches above, UA does not require any search nor any hyperparameter tuning. Therefore, it is much more efficient than all those approaches and the results shown in Section 4, demonstrate that it is a just as effective augmentation method.</p><p>In order to further reduce the computational complexity involved in the aforementioned methods, RandAugment (RA) <ref type="bibr" target="#b3">[4]</ref> reduces the search space used in AA to 2 hyperparameters: number of operators and a single global distortion which can control the magnitude of all augmentation operations at once. A set of augmentation operations are selected uniformly and a grid-search is performed to find the optimal value of those hyperparameters. A noteworthy conclusion in <ref type="bibr" target="#b3">[4]</ref> is that learning proxy tasks can provide sub-optimal results for a given task at hand. RA reduces the search space significantly but still needs extra computation to do grid search in order to find suitable hyperparameters. RA samples uniformly from a discrete search space, as introduced in AA, but we demonstrate in later sections that sampling uniformly in a continuous approximate invariant space can eliminate the need to search for any hyperparameters.</p><p>There has also been several theoretical works on data augmentation. Rajput et al. <ref type="bibr" target="#b12">[13]</ref> analyzed how addictive spherical data augmentations affects the margin of classifiers. LeJeune et al. <ref type="bibr" target="#b10">[11]</ref> proposed a Hessian-based rugosity measure as regularization penalty for training, and they showed the connection between data augmentation and regularization with this rugosity measure. Chapelle et al. <ref type="bibr" target="#b0">[1]</ref> proposed vicinity risk minimization principle that samples virtual training point out of the vicinity of Gaussian distribution centered at the actual training point. Zhang et al. <ref type="bibr" target="#b17">[18]</ref> uses this vicinity risk minimization principle to show how Mixup can help improve generalization. Finally, Chen et al. <ref type="bibr" target="#b1">[2]</ref> use group invariance to show that data augmentation reduces the variance of loss and the gradient vector, which in turn reduces the empirical risk. In fact, the theoretical analysis supporting our approach (presented next) is largely based in that result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UniformAugment</head><p>In context of supervised learning, let X be the data space, Y the label space, and P the joint distribution of data and label where (x, y) ? P. We aim to find a mapping function f * between X and Y by estimating the parameters of function f ? (X) ? Y which minimizes a pre-defined loss function l(f ? (x), y) over the data distribution P. This is also known as minimizing the expected risk of function f ? , and it can be expressed as:</p><formula xml:id="formula_0">f * = arg min ? l(f ? (x), y)dP(x, y)<label>(1)</label></formula><p>Unfortunately, in most cases it is impossible to gain access to the true data distribution P. In practice, we estimate f * byf which aims to minimize the Empirical Risk (ER) over a given data set</p><formula xml:id="formula_1">{(x i , y i )} n i=1</formula><p>which is assumed to mimic the data distribution P. Hence, the minimization problem becomes:</p><formula xml:id="formula_2">f * ?f = arg min ? 1 n n i=1 l(f ? (x i ), y i )<label>(2)</label></formula><p>While training, an augmentation transform t ? T is applied to data point (x i , y i ), where T is a set of approximate invariant augmentation transforms. From the Group Theory perspective, T is a group of approximate invariant augmentation transforms acting on data set {(x i , y i )} n i=1 in a way that the augmented and original data have the approximate equality in distribution. With this in mind, an augmented data point t(x i ), for all t ? T , form an orbit of element x i .</p><p>Assuming augmentation transform t ? T is sampled from the probability distribution T, then the ER minimization equation can be re-written as <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_3">f = arg min ? 1 n n i=1 l(f ? (t(x i )), y i )dT(t)<label>(3)</label></formula><p>The integral in the equation above can be interpreted as the average loss of augmented data over group T . The averaging of the loss leads to reduction in variance of loss and gradient vector, which in turn tightens the ER bound between the empirical risk and the expected risk <ref type="bibr" target="#b1">[2]</ref>. If the gradient of loss varies significantly along the augmentation orbits of x i , then the training procedure effectively denoises the gradient, which leads to a better training procedure. Consider T n = T ? T ? ... ? T are the augmentations acting on the data set {(x i , y i )} n i=1 elementwise, as per <ref type="bibr" target="#b1">[2]</ref> the order of averaging and minimization can be reversed and the ER minimization estimator becomes:</p><formula xml:id="formula_4">f = E t1,...,tn?T arg min ? 1 n n i=1 l(f ? (t i (x i )), y i )<label>(4)</label></formula><p>In practice, this objective is minimized iteratively over a number of epochs with Stochastic Gradient Descent, so it is mandatory to find and apply the augmentation transforms with high variations epoch-wise. But, it is computationally expensive to re-do the search and calculate the above expectation every epoch, since it requires to calculate the gradient of loss for a high number of augmentation transform orbits. In order to avoid such a search, the augmentation transforms are uniformly sampled from the continuous approximate invariant augmentation space for all data points throughout the training procedure to cover the augmentation space uniformly. That is the main thrust supporting our proposed approach, UniformAugment (UA). In other words, UA assumes that the augmentation distribution T is a Uniform distribution over its parameters. With this alteration, the final ER minimization estimator is given by:</p><formula xml:id="formula_5">f = E t1,...,tn?U arg min ? 1 n n i=1 l(f ? (t i (x i )), y i )<label>(5)</label></formula><p>AA searches for augmentation policies that minimize the ER estimator on a proxy model with a subset of full data set and assumes that the found policies are transferable for training new model architectures with full data set. It has been shown that searching the proxy model to obtain the augmentation policies for a larger task is questionable <ref type="bibr" target="#b3">[4]</ref>; and although transferring these policies increased the model accuracy, AA did not include a comparison with a search-free approach to justify the search performance. FAA applies Bayesian Optimization search to find policies that minimize the loss on subsets of the data set in different folds. In other words, these augmentation policies aim to transform the unseen data points in a way similar to data set model is trained on which is not helpful for model generalization. Moreover, these policies minimize the loss of a trained model, so there is no guarantee that they minimize the loss over the training procedure. Unlike other approaches which search for an optimal set of policies for a specific data set, UA gives the model a chance to learn from the whole approximate invariant augmentation space by (1) uniformly sampling the augmentation transforms and (2) iteratively updating the model parameters with gradient descent. Finally, we argue that UA is also a suitable comparison to measure the effectiveness of any augmentation search algorithm.</p><p>Given the above, UA's implementation is very simple as illustrated in Algorithm 1. For each data point (x, y) in the data set, multiple augmentation operations can be applied. Hyperparameter NumOps is the total number of augmentation transforms applied to the input data point in a sequential manner. Each augmentation transform is selected from augmentation transform set T . Since it is assumed that the distribution of augmentation transform set T is Uniform, we uniformly sample an operation from the augmentation transform set. Each augmentation transform t has two parameters, one is the probability of applying the transform p, and the other is the augmentation magnitude ? ? [0, 1] which determines the level of distortion. In UA, the distributions of p and ? are Uniform(0, 1) as well. At the end, each selected transform t is applied to the input data x with probability p and magnitude ?. A continuous augmentation parameter space covers a wider range of values and adds more variety to the data when compared to a discrete parameter space. The alternative, discretizing the augmentation parameter space, reduces the augmentation variance by limiting the space into a limited number of options for parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: UniformAugment algorithm</head><formula xml:id="formula_6">Input : (x, y) ? {(xi, yi)} n i=1 Set : (x,?) ? (x, y) for j ? 1 to N umOps do tij ? t ? T ? U {T } ; pij ? p ? U (0,1) ; ?ij ? ? ? U (0,1) ; (x,?) ? tij (x,?, pij, ?ij ) ; end Return: (x,?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the following we present the results of our experiments confirming that under the assumption that if the augmentation space is approximately invariant UA is capable of yielding training performance comparable to the approaches reviewed in Section 2.</p><p>We evaluate UA's performance on an image classification task using CI-FAR10, CIFAR100 <ref type="bibr" target="#b9">[10]</ref> and ImageNet <ref type="bibr" target="#b4">[5]</ref> datasets. We compared UA results with those published by other recent methods, namely AA <ref type="bibr" target="#b2">[3]</ref>, FAA <ref type="bibr" target="#b11">[12]</ref>, PBA <ref type="bibr" target="#b7">[8]</ref> and RA <ref type="bibr" target="#b3">[4]</ref> 3 , as well as Baselines which only include the default augmentations for of the following models: WideResNet <ref type="bibr" target="#b16">[17]</ref>, Shake-Shake <ref type="bibr" target="#b5">[6]</ref>, ResNet-50 <ref type="bibr" target="#b6">[7]</ref> and ResNet-200 <ref type="bibr" target="#b15">[16]</ref>. For a fair comparison we used the 15 augmentation functions listed in <ref type="table" target="#tab_4">Table 5</ref> which were used by AA and other recent works. The ranges for those operations are listed in the "Default" column of <ref type="table" target="#tab_4">Table 5</ref> as well. Following the related work, each augmentation transform consists of two consecutive augmentation operations, i.e., the hyperparameter NumOps in Algorithm 1 is set to 2. Finally, the training hyperparameters used for the classification models are the same as in <ref type="bibr" target="#b11">[12]</ref> and we report the results averaged over 5 runs.</p><p>Recall that, contrary to all of the competitors, UA requires no search phase, therefore in terms of efficiency, i.e., computing time, a comparison is not meaningful as UA is vastly superior to those methods.</p><p>CIFAR10. <ref type="table" target="#tab_1">Table 2</ref> shows the results of our experiments using UA on Wide-ResNet-40-2 (WRN-40-2), Wide-ResNet-28-10 (WRN-28-10) <ref type="bibr" target="#b16">[17]</ref> and Shake-Shake (SS) <ref type="bibr" target="#b5">[6]</ref> models. UA's accuracy is between 1-1.8% better than the Baseline and ? 0.5% higher than CutOut augmentation. More importantly, the results are very much comparable to those by AA, PBA and FAA, namely around 0.1%, with zero time spent on search phase, which is a constant w.r.t. UA in all experiments that follow. CIFAR100. We also tested UA on CIFAR100 for Wide-ResNet-40-2 (WRN-40-2), Wide-ResNet-28-10 (WRN-28-10) and Shake-Shake (SS). <ref type="table" target="#tab_2">Table 3</ref> shows 5-6%, improvement for Wide-ResNet-40-2, and 1-2% improvement for Wide-ResNet-28-10 and Shake-Shake models comparing to the Baseline and CutOut. The error rates for UA are, as it was the case for CIFAR10, very much on par with AA, PBA, RA and FAA. ImageNet. Finally, we tested UA on ImageNet for ResNet-50 <ref type="bibr" target="#b6">[7]</ref> and ResNet-200 <ref type="bibr" target="#b15">[16]</ref> models. <ref type="table" target="#tab_3">Table 4</ref> shows classification error rates of UA comparing to Baseline, AA, FAA and RA methods. The error rates are 1.3% and 1.9% lower than the Baseline for ResNet-50 and ResNet-200 respectively. As expected, once more, UA results are not only comparable, but are slightly better than AA's. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Investigating the "approximately invariant augmentation space" hypothesis</head><p>UA is based on the assumption that an approximately invariant space for augmentations is given. Under that assumption uniformly sampling parameters is sufficient, i.e., there is no need to perform any search for policies or hyperparameters. Indeed, the results presented in the previous section confirm that UA performs on par with existing search-based methods. This strongly suggests that, for the examined datasets, our assumption holds. We further support the validity of our assumption by running experiments to explore the impact of the ranges of the augmentation operations and NumOps hyperparameter on UA's performance. A good augmentation space is one which is aggressive while still keeping the space approximately data invariant. To the best of our knowledge, no previous work detailed how they determined the default transforms and their ranges and, with the exception of RA, the same is true for the NumOps parameter. Furthermore, while similar experiments would be extremely expensive for the search-based approaches, they are very much practical for UA, given that it is search-free.</p><p>Investigating the range of magnitudes for augmentation transforms.</p><p>In the experiments above we used the same augmentation operations, listed in <ref type="table" target="#tab_4">Table 5</ref>, that were used in other works, e.g., AA, FAA and PBA. The column named "Default" in that table shows the values those papers and ours used.</p><p>The question we aim to answer now is whether those magnitude ranges yield an approximate invariant augmentation space. For that we used two different sets of ranges, denoted as "Narrow" and "Wide" in <ref type="table" target="#tab_4">Table 5</ref>; as the name implies they shorten and enlarge, respectively, the range of possible values one can draw from for each augmentation operation. The hypothesis is if the range is too narrow, there will be less diversity in the augmented data, whereas if the ranges are too wide the augmented data would be out-of-distribution. In both cases this would lead to sub-optimal learning. In order to verify the validity of the hypothesis, we trained WideResNet 28x10 on CIFAR-10 dataset with UA using all three magnitude ranges, and compared their performance on the validation set. The results in <ref type="table">Table 6</ref> suggest our hypothesis is correct, and that the ranges of values used in the experiments are appropriate.</p><p>The effect of NumOps. Similar to the experiment above we now seek to confirm that NumOps = 2 yields an approximately invariant augmentation space. Again, similarly to the case above, the intuition is that if NumOps = 1, less diversity is added to the learning but if NumOps ? 1 the augmented image will become out-of-distribution. <ref type="figure" target="#fig_0">Figure 1</ref> shows the error rate on validation set for different values of NumOps of UA on CIFAR10. It confirms the intuition that while a single augmentation does not add much diversity to the data, more than 4 augmentations quickly degenerates the image generating out of distribution samples, and reducing the effectiveness. Moreover, it further suggests that choosing NumOps as 2 is indeed a very reasonable choice for the datasets investigated, which is in line with RA's proposal <ref type="bibr" target="#b3">[4]</ref>. The two sets of experiments above clearly support our hypothesis that the sets of hyperparameters we use form an approximately invariant augmentation space. But it also leads to a more fundamental question: Can one discover (or design) approximately invariant augmentation space efficiently for a given domain? While we defer that for future work, an affirmative answer would be very useful as it would imply that a highly efficient and effective search-free approach, such as UA, could be applied to such domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>Most of the recently proposed automatic augmentation methods are computationally expensive as they rely on the use of a proxy model and search for the optimum augmentation operations and their parameters (for that given proxy model) for the augmentation operations. In this paper we demonstrated that if the augmentation space is approximately invariant, which relatively restricts generating out of distribution samples, the need to search within this space for optimum parameters is eliminated. This observation lead to our main contribution, UniformAugment (UA). That is, if the assumption of an approximately invariant augmentation space holds, it suffices to uniformly sample augmentation operations from such a space. This avoids any search while leading the trained model to deliver performance comparable to the existing search-based methods. Even though we cannot guarantee that the default predefined ranges for transform operations used in this and many other papers do yield a approximately invariant augmentation space, the very positive results we obtained seem to suggest as much, for the datasets and models we used. Indeed, we argue that an important direction for further research is to develop a methodology to search for an approximately invariant augmentation space for a given task and domain. This is more important than searching for the optimum parameters within a given augmentation space. If such approximately invariant augmentation space is obtained then no further search is needed, and an approach such as UA, being resilient to different datasets and/or updates to those, is bound to perform well for that task/domain.</p><p>Another interesting direction for further work is to compare UA's effectiveness against approaches using adversarial methods for policy generation <ref type="bibr" target="#b18">[19]</ref>. While that approach does not have a proxy-model search per se, it requires a more expensive training process. It would be interesting to also deploy UA in a similar setting using batch augmentation <ref type="bibr" target="#b8">[9]</ref>, to investigate its effectiveness with larger augmented batches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Error rates on CIFAR10 validation set for WideResNet 28x10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Average error rate of different augmentation methods on CIFAR10</figDesc><table><row><cell>Model</cell><cell cols="7">Baseline Cutout AA PBA FAA RA UA</cell></row><row><cell>WRN-40-2</cell><cell>5.6</cell><cell>4.1</cell><cell cols="5">3.7 N/A 3.6 N/A 3.75</cell></row><row><cell>WRN-28-10</cell><cell>3.9</cell><cell>3.1</cell><cell cols="2">2.6 2.6</cell><cell>2.7</cell><cell cols="2">2.7 2.67</cell></row><row><cell>SS (26 2x32d)</cell><cell>3.6</cell><cell>3</cell><cell cols="2">2.5 2.5</cell><cell cols="3">2.5 N/A 2.49</cell></row><row><cell>SS (26 2x96d)</cell><cell>2.9</cell><cell>2.6</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>1.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Average error rate of different augmentation methods on CIFAR100</figDesc><table><row><cell>Model</cell><cell cols="3">Baseline Cutout AA PBA FAA RA</cell><cell>UA</cell></row><row><cell>WRN-40-2</cell><cell>26</cell><cell>25.2</cell><cell cols="2">20.7 N/A 20.6 N/A 20.99</cell></row><row><cell>WRN28-10</cell><cell>18.8</cell><cell>18.4</cell><cell cols="2">17.1 16.7 17.3 16.7 17.18</cell></row><row><cell>SS(26 2x96d)</cell><cell>17.1</cell><cell>16</cell><cell cols="2">14.3 15.3 14.6 N/A 15.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Average error rate of different augmentation methods on ImageNet</figDesc><table><row><cell>Model</cell><cell cols="2">Baseline AA FAA RA</cell><cell>UA</cell></row><row><cell>ResNet50</cell><cell>23.7</cell><cell cols="2">22.4 22.4 22.4 22.37</cell></row><row><cell>ResNet200</cell><cell>21.5</cell><cell cols="2">20.0 19.4 N/A 19.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>List of augmentation image transformations and their ranges. (N/A denotes a binary operation with no magnitude.)</figDesc><table><row><cell>Transform</cell><cell cols="2">Narrow</cell><cell>Default</cell><cell>Wide</cell></row><row><cell>ShearX(Y)</cell><cell cols="2">[-0.15,0.15]</cell><cell cols="2">[-0.3,0.3] [-0.9,0.9]</cell></row><row><cell cols="5">TranslateX(Y) [-0.225,0.225] [-0.45,0.45] [-1,+1]</cell></row><row><cell>Rotate</cell><cell cols="2">[-15,15]</cell><cell>[-30,30]</cell><cell>[-90,90]</cell></row><row><cell>AutoContrast</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Invert</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Equalize</cell><cell>N/A</cell><cell></cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>Solarize</cell><cell cols="2">[0,256]</cell><cell>[0,256]</cell><cell>[0,256]</cell></row><row><cell>Posterize</cell><cell>[6,8]</cell><cell></cell><cell>[4,8]</cell><cell>[2,8]</cell></row><row><cell>Contrast</cell><cell cols="2">[0.5,1.5]</cell><cell>[0.1,1.9]</cell><cell>[0.01,2]</cell></row><row><cell>Color</cell><cell cols="2">[0.5,1.5]</cell><cell>[0.1,1.9]</cell><cell>[0.01,2]</cell></row><row><cell>Brightness</cell><cell cols="2">[0.5,1.5]</cell><cell>[0.1,1.9]</cell><cell>[0.01,2]</cell></row><row><cell>Sharpness</cell><cell cols="2">[0.5,1.5]</cell><cell>[0.1,1.9]</cell><cell>[0.01,2]</cell></row><row><cell>Cutout</cell><cell cols="2">[0,0.1]</cell><cell>[0,0.2]</cell><cell>[0,0.6]</cell></row><row><cell cols="5">Table 6. UA's accuracy on CIFAR10 and WideResNet 28x10, using different ranges</cell></row><row><cell>for the augmentation operations</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Narrow Default Wide</cell></row><row><cell></cell><cell>2.77</cell><cell>2.67</cell><cell>2.99</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The missing experiments in those works are denoted by "N/A" in all tables that follow.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgements</head><p>The first five authors contributed equally to this work. M. Nascimento's contributed while on sabbatical leave at the Distributed Data Lab, Huawei Technologies, Canada. We gratefully acknowledge fruitful discussions with Robin Grosman's as well as her support during the development of this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vicinal risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13 (NIPS)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="416" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dobriban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10905</idno>
		<title level="m">A group-theoretic framework for data augmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">RandAugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719[cs.CV</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485[cs.LG</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Population based augmentation: Efficient learning of augmentation policy schedules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05393[cs.CV</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ben-Nun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Giladi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09335[cs.LG</idno>
		<title level="m">Augment your batch: better training with larger batches</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lejeune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Javadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11639[cs.LG</idno>
		<title level="m">Implicit rugosity regularization via data augmentation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast autoaugment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS) 32</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6662" to="6672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajput</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03177[cs.LG</idno>
		<title level="m">Does data augmentation lead to positive margin</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nishimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yokoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03229[cs.CV</idno>
		<title level="m">Apac: Augmented pattern classification with neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh Intl. Conf. on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="958" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning strict identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4432" to="4440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>87.1-87.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the British Machine Vision Conference (BMVC)</title>
		<meeting>of the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412[cs.LG</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11188[cs.CV</idno>
		<title level="m">Adversarial autoaugment</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578[cs.LG</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
