<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised and self-adaptative techniques for cross-domain person re-identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bertocco</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Fernanda</forename><surname>Andal?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Anderson</forename><surname>Rocha</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised and self-adaptative techniques for cross-domain person re-identification</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY</title>
						<imprint>
							<biblScope unit="volume">16</biblScope>
							<biblScope unit="page">2021</biblScope>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-Identification</term>
					<term>Unsupervised Learn- ing</term>
					<term>Deep Learning</term>
					<term>Curriculum Learning</term>
					<term>Network Ensemble</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Person Re-Identification (ReID) across nonoverlapping cameras is a challenging task, and most works in prior art rely on supervised feature learning from a labeled dataset to match the same person in different views. However, it demands the time-consuming task of labeling the acquired data, prohibiting its fast deployment in forensic scenarios. Unsupervised Domain Adaptation (UDA) emerges as a promising alternative, as it performs feature adaptation from a model trained on a source to a target domain without identity-label annotation. However, most UDA-based methods rely upon a complex loss function with several hyper-parameters, hindering the generalization to different scenarios. Moreover, as UDA depends on the translation between domains, it is crucial to select the most reliable data from the unseen domain, avoiding error propagation caused by noisy examples on the target data -an often overlooked problem. In this sense, we propose a novel UDA-based ReID method that optimizes a simple loss function with only one hyper-parameter and takes advantage of triplets of samples created by a new offline strategy based on the diversity of cameras within a cluster. This new strategy adapts and regularizes the model, avoiding overfitting the target domain. We also introduce a new self-ensembling approach, which aggregates weights from different iterations to create a final model, combining knowledge from distinct moments of the adaptation. For evaluation, we consider three well-known deep learning architectures and combine them for the final decision. The proposed method does not use person re-ranking nor any identity label on the target domain and outperforms state-of-the-art techniques, with a much simpler setup, on the Market to Duke, the challenging Market1501 to MSMT17, and Duke to MSMT17 adaptation scenarios.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>comprises the primary techniques to find possible people, or groups of people, involved in an event and to, ultimately, propose candidate suspects for further investigation <ref type="bibr" target="#b0">[1]</ref>.</p><p>Person ReID aims to match the same person in different non-overlapping views in a camera system. Thanks to the considerable discrimination power given by deep learning, recent works <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref> consider supervised feature learning on a labeled dataset, which yields high values of mean Average Precision (mAP) and top Ranking accuracy.</p><p>However, the labeling of massive datasets demanded by deep learning is time-consuming and error-prone, especially when targeting forensic applications. In this context, Unsupervised Domain Adaptation (UDA) aims to adapt a model trained on a source dataset to a target domain without the need for identity information of the target samples. Most ReID methods that follow this approach are based on label proposing, in which feature vectors of target images are extracted and clustered. Upon unsupervised training, these clusters receive pseudo-labels for the adaptation to the target domain.</p><p>Several works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> apply the pseudolabeling principle by developing different ways to propose and refine clusters on the target domain. The aim is to alleviate noisy labels, which can harm feature learning. Our method follows this trend, but we consider a more general clustering algorithm differently from previous work, which can relax the criteria to select data points, allowing clusters with arbitrary densities in feature space. By not forcing all clusters to have the same complexity, we can utilize the density information to better group relevant data points.</p><p>As we are dealing with data from an unknown target domain, clusters can have different degrees of reliability, i.e., contain different quantities of noisy labels. We need to select the most reliable clusters to optimize the model at each iteration of the clustering process. The generated model must also be camera-invariant to generate the same feature representation for an identity, regardless of the camera point of view. Based on these observations, we hypothesize that clusters with more cameras might be more reliable to optimize the model. Suppose that a cluster contains images of the same identity seen from two or more cameras. In this case, the model was able to embed these images close to each other in the feature space, overcoming differences in illumination, pose, and occlusion, which are inherently present in different camera vantage points.</p><p>We argue that the greater the number of different cameras in a cluster, the more reliable this cluster is to optimize the model. Following this idea, we propose a new way to create triplets arXiv:2103.11520v3 [cs.CV] 7 Feb 2022 of samples in an offline manner. We select one sample as an anchor for each camera represented in a cluster and two others as positive and negative examples. As a positive example, we choose a sample from one of the other represented cameras. In contrast, the negative example is a sample from a different cluster but with the same camera as the anchor. Consequently, the greater the number of cameras in a cluster, the more diverse the triplets to train the model. With this approach, we give more importance to the more reliable clusters, regularize the model, and alleviate the dependency on hyper-parameters by using a single-term and single-hyper-parameter triplet loss function. This technique brings robustness and generability to the final model, easing its adaptation to different scenarios.</p><p>Another important observation is that, at different points of the adaptation from a source to a target domain, the model holds different levels of knowledge as different portions of the target data are considered each time. Thus, we argue that the model has complementary knowledge in different iterations during training. Based on this, we propose a self-ensembling strategy to summarize the knowledge from various iterations into a unique final model.</p><p>Finally, based on recent advances in ensemble-based methods for ReID <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, we propose to combine the knowledge acquired by different architectures. Unlike prior work, we avoid complex training stages by simply assembling the results from different architectures only during evaluation time.</p><p>To summarize, the contributions of our work are:</p><p>? A new approach to creating diverse triplets based on the variety of cameras represented in a cluster. This approach helps the model to be camera-invariant and more robust in generating the same person's features from different perspectives. It also allows us to leverage a single-term and single-hyper-parameter triplet loss function to be optimized.</p><p>? A novel self-ensembling fusion method, which enables the final model to summarize the complementary knowledge acquired during training. This method relies upon the knowledge hold by the model in different checkpoints of the adaptation process.</p><p>? A novel ensemble technique to take advantage of the complementarity of different backbones trained independently. Instead of applying the typical knowledge distilling <ref type="bibr" target="#b13">[14]</ref> or co-teaching <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref> methods, which add complexity to the training process, we propose using an ensemble-based prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Several works address Unsupervised Domain Adaptation for Person Re-Identification. They can be roughly divided into three categories: generative, attribute alignment, and label proposing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generative Methods</head><p>ReID generative methods aim to synthesize data by translating images from a source to a target domain. Once data from the source dataset is labeled, the translated images on the target context receive the same labels as the corresponding original images. The main idea is to transfer low-and midlevel characteristics from the target domain, such as background, illumination, resolution, and even clothing, to the images in the source domain. These methods create a synthetic dataset of labeled images with the same conditions as the target domain. And to adapt the model, they apply supervised training. Some works in this category are SPGAN <ref type="bibr" target="#b16">[17]</ref>, PTGAN <ref type="bibr" target="#b17">[18]</ref>, AT-Net <ref type="bibr" target="#b18">[19]</ref>, CR-GAN <ref type="bibr" target="#b19">[20]</ref>, PDA-Net <ref type="bibr" target="#b20">[21]</ref>, and HHL <ref type="bibr" target="#b21">[22]</ref>. Besides transferring the characteristics from source to target domain for image-level generation, DG-Net++ <ref type="bibr" target="#b22">[23]</ref> also applies label proposing through clustering. The final loss is the aggregation of the GAN-based loss function to generate images, along with the classification loss defined for the proposed labels. By doing this, they perform the disentangling and adaptation of the features on the target domain.</p><p>CCSE <ref type="bibr" target="#b23">[24]</ref> performs camera mining and, using a GANbased model, generates synthetic data for an identity considering the point of view of each other camera, increasing the number of images available for training. They leverage new clustering criteria to avoid creating massive clusters comprising most of the dataset and potentially having two or more true identities assigned to the same pseudo-label. Finally, they train directly from ImageNet, without considering any specific source domain. In comparison, our solution does not require synthetic images since we explore the cross-camera information inside each cluster using only real images. This leads our method to outperform CCSE considering the same training conditions (unsupervised scenario).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attribute Alignment Methods</head><p>These methods seek to align common attributes in both domains to ease transferring knowledge from source to target. Such features can be clothing items (backpacks, hats, shoes) and other soft-biometric attributes that might be common to both domains. These works align mid-level features and enable the learning of higher semantic features on the target domain. Works such as TJ-AIDL <ref type="bibr" target="#b24">[25]</ref> consider a fixed set of attributes. However, source and target domains can have substantial context differences, leading to potentially different attributes. For example, the source domain could be recorded in an airport and the target domain in a shopping center. To obtain a better generalization, in <ref type="bibr" target="#b25">[26]</ref>, the authors propose the Multi-task Mid-level Feature Alignment (MMFA) technique to enable the method to learn attributes from both domains and align them for a better generalization on the target domain. Other methods, such as UCDA <ref type="bibr" target="#b26">[27]</ref> and CASCL <ref type="bibr" target="#b27">[28]</ref>, aim to align attributes by considering images from different cameras on the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Label Proposing Methods</head><p>Methods in this category predict possible labels for the unlabeled target domain by leveraging clustering methods (Kmeans <ref type="bibr" target="#b28">[29]</ref>, DBSCAN <ref type="bibr" target="#b29">[30]</ref>, among others). Once the target data is pseudo-labeled, the next step is to train models to learn discriminative features on the new domain. PUL <ref type="bibr" target="#b6">[7]</ref> applies the Curriculum Learning technique to adapt a model learned on a source domain to a target domain. However, as K-means is used to cluster the features, it is not possible to account for camera variability. As K-means generates only convex clusters, it cannot find more complex cluster structures, hindering the performance. UDAP <ref type="bibr" target="#b7">[8]</ref> and ISSDA-ReID <ref type="bibr" target="#b30">[31]</ref> utilize DBSCAN as the clustering algorithm along with labeling refinement. SSG <ref type="bibr" target="#b8">[9]</ref> also applies DBSCAN to cluster features of the whole, upper, and low-body parts of identities of interest. The final loss is the sum of individual triplet losses in each feature space (body part). Similar to our work, they use a source domain to pre-train the model and the target domain for adaptation. However, they do not perform cross-camera mining, cluster filtering, nor ensembling. These elements of our solution allow it to outperform SSG in all adaptation scenarios.</p><p>ECN <ref type="bibr" target="#b31">[32]</ref>, ECN-GPP <ref type="bibr" target="#b32">[33]</ref>, MMCL <ref type="bibr" target="#b33">[34]</ref>, and Dual-Refinement <ref type="bibr" target="#b34">[35]</ref> use a memory bank to store features, which is updated along the training to avoid the direct use of features generated by the model in further iterations. The authors aim to avoid propagating noisy labels to future training steps, contributing to keeping and increasing the discrimination of features during training.</p><p>PAST <ref type="bibr" target="#b9">[10]</ref> applies HDBSCAN <ref type="bibr" target="#b35">[36]</ref> as the clustering method, which is similar to OPTICS [37] -the algorithm of choice in our work. However, the memory complexity of OPTICS is O(n), while for HDBSCAN is O(n 2 ), making our model more memory efficient in the clustering stage.</p><p>MMT <ref type="bibr" target="#b11">[12]</ref>, MEB-Net <ref type="bibr" target="#b12">[13]</ref>, ACT <ref type="bibr" target="#b37">[38]</ref>, SSKD <ref type="bibr" target="#b38">[39]</ref>, and ABMT <ref type="bibr" target="#b15">[16]</ref> are ensemble-based methods. They consider two or more networks and leverage mutual teaching by sharing one network's outputs with the others, making the whole system more discriminative on the target domain. However, training models in a mutual-teaching regime brings complexity in memory and to the general training process. Besides that, noisy labels can be propagated to other ensemble models, hindering the training process. Nonetheless, ensemble-based learning provides the best performance among state-of-art methods. We propose using ensembles only during inference to simultaneously eliminate the complexity added to the training, still taking advantage of knowledge complementary between the models.</p><p>Our work is also based on Curriculum Learning with Diversity <ref type="bibr" target="#b39">[40]</ref>, a schema whereby the model starts learning with easier examples, i.e., samples that are correctly classified with a high score early in training. However, in a multiclass problem, one of the classes might have more examples correctly classified early on, making it easier than the other classes. Therefore, in Curriculum Learning with Diversity, the method selects the most confident samples (easier samples) from the easier classes, including some examples from the harder ones. In this way, it enables the model to learn in an easy-to-hard manner, avoiding local minima and allowing better generalization.</p><p>Even though recent work achieves competitive performances, there are some limitations that we aim to address in our work. First, generative methods bring complexity by considering GANs to translate images from a domain to the other. Second, attribute Alignment methods only tackle the alignment of low and mid-level features. Third, methods in both categories need images from source and target domains during adaptation. Finally, the last Label Proposing methods consider mutual-learning or co-teaching, which brings complexity to the training stage.</p><p>Similarly, we assume to have only camera-related information, i.e., we know from which camera (viewpoint) an image was taken. In all steps, we use pseudo-identity information exclusively given by the clustering algorithm without relying on any ground-truth information. We differ from the prior art by using a new diversity learning scheme and generating triplets based on each cluster's diversity of points of view. As we train the whole model, the method also learns highlevel features on the target domain. We simplify the training process by considering one backbone at a time, without mutual information exchange during adaptation. Finally, we apply model ensembling for inference after the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Our approach to Person ReID comprises two phases: training and inference. <ref type="figure">Figure 1</ref> depicts the training process, while <ref type="table" target="#tab_0">Table I</ref> shows the variables used in this work.  During training, we independently optimize n b different backbones to adapt the model to the target domain. This phase is divided into five main stages that are performed iteratively: feature extraction from all data; clustering; cluster selection; cross-camera triplet creation and fine-tuning; feature extraction from pseudo-labeled data.</p><p>After training, we perform the proposed self-ensembling phase to summarize the training parameters in a single final model based on the weighted average of model parameters from each different checkpoint. We perform this step for each backbone independently and, in the end, we have n b selfensembled models.</p><p>During inference, for a pair query/gallery image, we calculate the distance between them considering feature vectors extracted by each of the n b models. Hence, for each query/gallery pair, we have n b distances, one for each of the trained models. We then apply our last ensemble technique: the n b distances are averaged to obtain a final distance. Finally, based on this final distance, we take the label of the closest gallery image as the query label. <ref type="figure">Fig. 1</ref>. Overview of the training phase. We assume to have camera-related information, i.e., we know the camera used to acquire each image; and we do not rely on any ground-truth label information about the identities on the target domain. The pipeline has two flows: the blue flow is executed every K 1 times, and the orange flow is executed K 2 times. Both flows share steps in green. In Stage 1, we initially extract feature vectors for each training image in the target domain using model M , and cluster them using the OPTICS algorithm in Stage 2 to propose pseudo-labels. Afterward, we perform cluster selection in Stage 3, removing outliers and clusters with only one camera. Then, triplets are created based on each cluster's diversity in Stage 4a and used to train the model in Stage 4b. These steps are denoted by the blue flow in which the Clustering and Cluster Selection are performed. Instead of going back to Stage 1, the method follows the orange flow. In Stage 5, we extract feature vectors of the samples selected in Stage 3, and the process continues to Stage 4a and 4b again. The blue flow marks an iteration, while the orange flow is called an epoch. Therefore, in each iteration, we have K 2 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Stages 1 and 2: Feature Extraction from all data and Clustering</head><p>Let D s = {(x s i , y s i )} Ns i=1 be a labeled dataset representing the source domain, formed by N s images x s i and their respective identity labels y s i ; and let D t = {(x t i )} Nt i=1 be an unlabeled target dataset representing the target domain, formed by N t images x t i . Before applying the proposed pipeline, we firstly train a model M in a supervised way, with source dataset D s and its labels. After training, assuming source dataset D s is not available anymore, we perform transfer learning, updating M to the target domain, only considering samples from unlabeled target dataset D t .</p><p>With model M trained on D s , we first extract all feature vectors from images in D t and create a new set of feature vectors {M (x t i )} Nt i=1 . We remove possible duplicates by checking if there is a replacement from one of them, which might be caused by duplicate images on target data. The remaining feature vectors are L2-normalized to embed them into a unit hypersphere. The normalized feature vectors are clustered using the OPTICS algorithm to obtain pseudo labels.</p><p>The OPTICS algorithm <ref type="bibr" target="#b36">[37]</ref> leverages the principle of dense neighborhood, similarly to DBSCAN <ref type="bibr" target="#b29">[30]</ref>. DBSCAN defines the neighborhood of a sample as being formed by its closest feature vectors, with distances lower than a predefined threshold. Clusters are created based on these neighborhoods, and samples not assigned to any cluster are considered outliers. If the threshold changes, other clusters are discovered, current clusters can be split or combined to create new ones. In other words, if we change the threshold, other clusters might appear, creating a different label proposing for the samples. However, clusters that emerge from real labels often have different distributions and densities, indicating that a generally fixed threshold might not be sufficient to detect them. In this sense, OPTICS relaxes DBSCAN by ordering feature vectors in a manifold based on the distances between them, which allows the construction of a reachability plot. Probable clusters with different densities are revealed as valleys in this plot and can be detected by their steepness. With this formulation, we are more likely to propose labels closer to real label distribution on the target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Stage 3: Cluster Selection</head><p>After the first and second stages, feature vectors are either assigned to a cluster or considered outliers. As people can be captured by one or more cameras in a ReID system, the produced clusters are naturally formed by samples acquired by different devices. We hypothesize that clusters with samples obtained by two or more cameras are more reliable than clusters with only one camera.</p><p>If an identity is well described by model M , its feature vectors should be closer in the feature space regardless of the camera. Therefore, clusters with only one camera might be created due to bias to a particular device or viewpoint, and different identities captured by the same camera can be assigned to the same cluster. Besides, if a feature vector is predicted as an outlier by the clustering algorithm, it means that it does not have a good description of its image identity to be assigned to a cluster.</p><p>Based on these observations and for optimization purposes, we filter the feature vectors by discarding outliers and clusters with a single camera type. With camera-related information, it is possible to count the number of images from each camera in a cluster. If all samples in a cluster come from the same camera, it is removed from the feature space. By doing this, we keep in the feature space only clusters with images from at least two cameras. <ref type="figure">Figure 1</ref> depicts this process, from Stage 2 to Stage 3, in which the outlier samples (green points) and clusters with only one camera (magenta points) are removed from the feature space.</p><p>The remaining clusters (the ones with two or more cameras) are considered reliable to fine-tune model M . Furthermore, different clusters have different degrees of reliability based on the number of represented cameras. Suppose images captured by several cameras form a cluster. In that case, it means model M can embed samples of the corresponding identity captured by all of these cameras in the feature space, eliminating pointof-view bias. In contrast, the fewer images from different points of view, the more complex the identity definition. In this sense, we propose a new approach of creating cross-camera triplets of samples to optimize the model by emphasizing cluster diversity and forcing samples of the same identity to be closer in the feature space regardless of their acquisition camera. We initially select, as the anchor, one random sample in cluster c captured by camera cam j . For each camera cam k = cam j in cluster c, we sort all feature vectors from camera cam k based on their distance to the anchor. The positive sample is then selected as being the median feature vector. The median is considered instead of the farthest sample (the hardest example) to avoid selecting a noisy example. We do not choose an easy example (the closest one) to avoid slowing down the model convergence or even getting stuck on a local minimum. To select the negative sample, we first sort all feature vectors from camera cam j belonging to other clusters = c based on their distance to the anchor. As the negative sample, we pick the closest feature vector that has not been assigned yet to a triplet. In this way, we avoid selecting the same negative sample, which brings diversity to the triplets and alleviates the harmful impact if one of the negative samples shares the anchor's same real identity. <ref type="figure" target="#fig_0">Fig. 2</ref>. Cross-Camera Triplet Creation. For each selected cluster, we have at least two cameras. Suppose the represented cluster c has images from three cameras (represented with red, blue, and yellow contours). For each camera, we select m anchors. For each anchor, we create triplets with a positive sample from other cameras in the same cluster and a negative sample with the same camera in other clusters. For instance, for camera red, we select an anchor and we sort, based on the distance, all feature vectors from cameras yellow and blue. Then we select the median feature vector from each one (represented by the arrows coming to the anchor). To select the negative sample, we sort all feature vectors from the same camera but from a cluster = c, and we choose the closest and not previously selected sample. For the triplet with a yellow median sample as positive, we select as negative the closest sample to the red anchor from another cluster (represented by the yellow arrow leaving the anchor). For the triplet with a blue median sample as positive, we select the second closest feature vector to the red anchor from another cluster (since the first closest has already been picked). This explanation assumes m = 1 and is repeated for cameras yellow and blue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Stage 4: Cross-Camera Triplet Creation and Finetuning</head><p>For a cluster c i with n i cameras, we generate a total of n i ? 1 triplets with the same anchor. If we select m anchors for one camera in c i , a total of m(n i ? 1) triplets are created. Considering that this process is repeated for each camera in c i , we have a total of n i m(n i ? 1) triplets for cluster c i . Note that the triplets are created in an offline manner. The offline creation enables us to choose triplets considering a global view of the target data instead of creating them in a batch, which would bring a limited view of the target feature space.</p><p>The number m of anchors of a camera is the same for all clusters. Consequently, the number of triplets generated for a cluster c i is O(n 2 i ). The greater the diversity of cameras in a cluster, the greater its representativeness on the triplets. By emphasizing the clusters with more camera diversity during training, the model learns from easy-to-hard identities and is more robust to different viewpoints. In our experiments we set m = 2 for all adaptation scenarios.</p><p>Due to this new approach of creating cross-camera triplets, we can optimize the model by using the triplet loss <ref type="bibr" target="#b40">[41]</ref> without the need for weight decay or any other regularization term and hyper-parameters. This also suggests that crosscamera triplets help to regularize the model during training.</p><p>After creating the triplets in an offline manner, we optimize the model using the standard triplet loss function:</p><formula xml:id="formula_0">L = 1 |B| (xa,xp,xn)?B [d(x a , x p ) ? d(x a , x n ) + ?] + ,<label>(1)</label></formula><p>where B is a batch of triplets, x a is the anchor, x p is the positive sample and x n is the negative one. ? is the margin that is set to 0.3 and [.] + is the max(0, .) function. This is illustrated in <ref type="figure">Figure 1</ref>, Stage 4b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Stage 5: Feature Extraction from Pseudo-Labeled Samples</head><p>This stage is part of the orange flow performed after Finetuning (Stage 4b). The main idea is to keep the pseudo-labeled clusters from Stage 3, recreating a new set of triplets based on the new distances between samples after the model update in Stage 4b, bringing more diversity to the training phase. To do so, we extract feature vectors only for samples of the pseudo-labeled clusters selected in Stage 3. The orange flow is performed K 2 times, and a complete cycle defines an epoch. The blue flow is performed every K 1 times, and a complete cycle defines an iteration. Therefore, in each iteration, we have K 2 epochs. This concludes the training phase.</p><p>Unlike the five best state-of-the-art methods proposed in the prior art (DG-Net++, MEB-Net, Dual-Refinement, SSKD, and ABMT), our solution is trained with a single-term loss, which contains only one hyper-parameter. Even the weight decay has been removed, as the proposed method can already calibrate the gradient to avoid overfitting, as we show in Section IV. Moreover, prior work performs clustering on the training phase through k-reciprocal Encoding <ref type="bibr" target="#b41">[42]</ref>, which is a more robust distance metric than Euclidean distance. However, it has a higher computational footprint, as it is necessary to check the neighborhood of each sample whenever distances are calculated. For training simplicity, we opt for standard Euclidean distance to cluster the feature vectors. However, as k-reciprocal encoding gives the model higher discrimination, we adopt it during inference time. Therefore, different from previous works, we calculate k-reciprocal encoding only once during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Self-ensembling</head><p>Our last contribution relies upon the curriculum learning theory. Different iterations of the training phase consider different amounts of reliable data from the target domain, as shown in Section IV. This property leads us to hypothesize that knowledge obtained at different iterations is complementary. Therefore, we propose to summarize knowledge from different moments of the optimization in a unique final model. However, as the model discrimination ability increases as more iterations are performed (the model is able to learn from more data), we propose combining the model weights of different iterations by weighting their importance with the amount of reliable data used in the corresponding iteration. We perform this weighted average of the model parameters as:</p><formula xml:id="formula_1">? f inal = p?P p i .? i p?P p i ,<label>(2)</label></formula><p>where ? i represents the model parameters after the i-th iteration and p i is the weight assigned to ? i . Weight p i is obtained based on the reliability of the target domain; if more data from the target domain is considered in an iteration, it means that the model is more confident, and then it can have more discrimination power on the target domain. Hence, p i is equal to the percentage of reliable target data in the i-th iteration. Consequently, a model that takes more data from the target to train will have a higher weight p i . Self-Ensembling is illustrated in <ref type="figure">Figure 3</ref>. Note that we directly deal with the model's learned parameters and create a new one by averaging the weights. We end up with a single model containing a combination of knowledge from different adaptation moments, which significantly boosts performance, as shown in Section V. <ref type="figure">Fig. 3</ref>. Self-Ensembling scheme after training. Different amounts of the target data (with no label information whatsoever) are used to fine-tune the model during the adaptation process. Different models created along the adaptation can be complementary. We create a new final model by weight averaging the models' parameters from different iterations. Weight p i is based on the amount of reliable data from the target domain on the i-th iteration. We end up with a single model encoding knowledge from different moments of the adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Ensemble-based prediction</head><p>After training and performing the self-ensemble fusion, we have a single model adapted from the source to the target domain. However, due to the high performance of ensemblebased methods in recent ReID literature <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, as a last measure, we leverage a combination of n b different architectures to make a final prediction considering even more learned knowledge, which improves performance on the target dataset. We apply the ensemble technique only for inference, different from <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> that leverage a mutual-teaching regime on training time. In turn, we avoid bringing complexity to the training but still take advantage of the complementarity from different architectures during inference.</p><p>To perform the ensemble-based prediction, we first calculate the feature distance of the query to each image on gallery for each of the n b final models. Let f k (x) = M k (x) be the L2normalized feature vector of image x obtained with model M k and d(f k (q), f k (g i )) be the distance between the feature vectors of the query q and of the i-th image gallery g i extracted using the k-th model on ensemble. The final distance between query q and gallery image g i is given by:</p><formula xml:id="formula_2">d f inal (q, g i ) = 1 K K k=1 d(f k (q), f k (g i )),<label>(3)</label></formula><p>where K is the number of models in the ensemble. In this way, we can incorporate knowledge from different models encoded as the distance between two feature vectors. After obtaining the distance between query q and all images in the gallery, we take the label of the closest gallery image as the query label. We consider an equal contribution from each backbone. Without labels on the target domain, it is impossible to evaluate the impact of the individual models and give them proportional weights on the combination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND RESULTS</head><p>This section presents the datasets we adopt in this work and compares the proposed method with the prior art with a comprehensive set of experiments considering different, and challenging, source/target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>To validate our pipeline, we used three large-scale benchmark datasets present on Re-ID literature: As done in previous work in the literature, we remove from the gallery images with the same identity and camera of the query to assess the model performance in a crosscamera matching. Feature vectors are L2-normalized before calculating distances. For evaluation, we calculate the Cumulative Matching Curve (CMC), from which we report Rank-1 (R1), Rank-5 (R5), and Rank-10 (R10), and mean Average Precision (mAP).</p><formula xml:id="formula_3">? Market1501 [</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation details</head><p>In terms of deep-learning architectures, we adopt ResNet50 <ref type="bibr" target="#b44">[45]</ref>, OSNet <ref type="bibr" target="#b3">[4]</ref>, and DenseNet121 <ref type="bibr" target="#b45">[46]</ref>, i.e., n b = 3, all of them pre-trained on ImageNet <ref type="bibr" target="#b46">[47]</ref>. To test them on an adaptation scenario, we choose one of the datasets as the source and another as the target domain. We train the backbone over the source domain and the adaptation pipeline over the target domain. We consider Market1501 and DukeMTMC-ReID as source domains, leaving MSMT17 only as the target dataset (the hardest one in the prior art). This way, we have four possible adaptation scenarios: Market ? Duke, Duke ? Market, Market ? MSMT17, and Duke ? MSMT17. We keep those scenarios (without MSMT17 as a source) to have a fair comparison with state-of-the-art methods. Besides, the most challenging scenario is MSMT17 as the target dataset: we train backbones on simpler datasets (Market and Duke) and adapt their knowledge to a harder dataset, with almost the double number of cameras and with many more identities recorded in different moments of the day and the year. This enables us to test the generalization of our method in adaptation scenarios where source and target domain have substantial differences in the number of identities, camera recording conditions, and environment.</p><p>We used the code available at <ref type="bibr" target="#b47">[48]</ref> to train OSNet and at <ref type="bibr" target="#b12">[13]</ref> to train ResNet50 and DenseNet121 over the source domains. Our source code is based on PyTorch <ref type="bibr" target="#b48">[49]</ref> and it is freely available at https://github.com/Gabrielcb/Unsupervised selfAdaptative ReID.</p><p>After training, we remove the last classification layer from all backbones and use the last layer's output as our feature embedding. We trained our pipeline using the three backbones independently in all scenarios of adaptation. Considering the flows depicted in <ref type="figure">Figure 1</ref>, we perform K 1 = 50 cycles of the blue flow (50 iterations), and, in each one, we perform K 2 = 5 cycles of the orange flow (5 epochs). We consider Adam <ref type="bibr" target="#b49">[50]</ref> as the network optimizer and set the learning rate to 0.0001 in the first 30 iterations. After the 30 th iteration, we divided it by ten and kept it unchanged until reaching the maximum number of iterations. As we show in our experiments, we can set the weight decay to zero since our proposed Cross-Camera Triplet Creation can regularize the model without extra hyperparameters. The triplet batch size is set to 30; batches with 30 triplets are used to update the model in each epoch. The margin in Equation 1 is set to 0.3, and the number of anchors is set to m = 2. We resize the images to 256 ? 128 ? 3 and apply Random Flipping and Random Erasing as data augmentation strategies during training. <ref type="table" target="#tab_0">Tables II and III</ref> show results comparing the proposed method to the state of the art. The proposed method outperforms the other methods regarding mAP and Rank-1 in Market ? Duke by improving those values in 1.8 and 1.7 percentage points (p.p.), respectively, and without re-ranking. In the Duke ? Market scenario, we obtain a solid competitive performance by having values 0.1 p.p. lower only in Rank-1, also without re-ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison with the Prior Art</head><p>In turn, ABMT applies k-reciprocal encoding during training, which is more robust than Euclidean distance. However, it is more expensive to calculate as it is necessary to search for k-reciprocal neighbors of each feature vector in each iteration of the algorithm before clustering. In our case, we only apply the standard Euclidean distance during training, reducing the training time and complexity on adaptation, but still obtaining performance gains. Moreover, we have a singleterm and single-hyper-parameter loss function, while ABMT depends on a loss with three terms and more hyper-parameters. They apply a teacher-student strategy to their training while we perform ensembling only for inference. Therefore, with a more direct pipeline and ensemble prediction, the proposed method has a Rank-1 only 0.1 p.p. lower in the Duke ? Market, while outperforming all methods in all other adaptation scenarios.</p><p>However, to benefit from the k-reciprocal encoding, we also apply it during inference to keep a simpler training process. In this case, the proposed method outperforms the methods in the prior art regarding mAP and Rank-1 in all adaptation scenarios.</p><p>Compared to SSKD in Duke ? Market scenario, we are below it by 0.3 and 0.4 p.p. in Rank-5 and Rank-10, respectively. Considering the closest actual gallery match image to the query (R1), our ensemble retrieves more correct matches, as <ref type="table" target="#tab_0">Table II</ref> shows, with our method outperforming SSKD by 1.2 p.p. in Rank-1 without re-ranking. Even with fewer hyperparameters than SSKD and a more straightforward training process (no co-teaching, simpler loss function, and late ensembling), our method shows competitive results considering the training complexity trade-off.</p><p>Interestingly, the proposed method performs better under more difficult adaptation scenarios. We measure the difficulty of a scenario based on the number of different cameras it comprises. Market, Duke, and MSMT17 have 6, 8, and 15 cameras, respectively. Hence the most challenging adaptation scenario is from Market to MSMT17. We adapt a model from a simpler scenario (6 cameras, all videos recorded in the same day period and the same season of the year) to a more complex target domain (15 cameras -12 outdoors and 3 indoorsrecorded at 3 different day periods -morning, afternoon and noon -in 4 different days -each day on a different season of the year). Market ? MSMT17 is the most challenging adaptation and close to real-world conditions where we might have people recorded along the day and in different locations (indoors and outdoors). In this case, as shown in <ref type="table" target="#tab_0">Table III</ref>, we obtained the highest performance even without re-ranking techniques. The proposed method outperforms the state of the art by 1.5 and 2.1 p.p. in mAP and Rank-1, respectively, on Duke ? MSMT17, and by 2.2 and 4.2 p.p. on the most challenge scenario, Market ? MSMT17.</p><p>There are several reasons why our method performs well. We explicitly design a model to deal with the diversity of cameras and viewpoints by creating a set of triplets based on the different cameras in a cluster. We also keep a more straightforward training, with only one hyper-parameter in our loss function (triplet loss margin). Most works in the ReID literature optimize a loss function with many terms and hyperparameters. They usually consider the Duke ? Market or the Market ? Duke scenarios (or both of them) to perform gridsearching over hyper-parameter values. Once they find the best values, they keep them unchanged for all adaptation setups.</p><p>In ABMT <ref type="bibr" target="#b15">[16]</ref>, the authors do not provide a clear explanation on how they define the hyper-parameter values for their loss function. However, they perform an ablation study over Duke ? Market and Market ? Duke scenarios, so their results might be biased to those specific setups, which gives them one of the best performances. However, when they keep the same values for different and more challenging scenarios, such as Market ? MSMT17 or Duke ? MSMT17, they obtain worse results than ours by a large margin. This shows that our method provides a better generalization capability brought by a simpler loss function and a more diverse training. It prevents us from choosing specific hyper-parameter values and be biased to a specific adaptation setup. Consequently, we achieve the best performances, especially in the most challenging scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Discussion</head><p>As we aim to re-identify people in a camera system in an unsupervised way, we must be robust to hyper-parameters that require adjustments based on grid-searching using true label information, keeping the training process (and adaptation to a target domain) as simple as possible. If a pipeline is complex and too sensitive to hyper-parameters, it might be challenging to train and deploy it on a real investigation scenario, where we do not have prior knowledge about the people of interest. This complexity leads to sub-optimal performance. This has already been pointed out in <ref type="bibr" target="#b54">[55]</ref>. The authors claim that most works rely on many hyper-parameters during the adaptation stage, which can help or hinder the performance, depending on the value assigned to them and which adaptation scenario is considered.</p><p>SSKD <ref type="bibr" target="#b38">[39]</ref> is an ensemble-based method leveraging three deep models in a co-teaching training regime with a four-term loss function with three hyper-parameters. One of the terms of their final loss function is a multi-similarity loss <ref type="bibr" target="#b55">[56]</ref>, with three extra hyper-parameters to train the model.</p><p>MEB-Net has complex training by relying on a co-training technique with three deep neural networks in which each one learns with the others. Each of these three networks has its separate loss function with six terms, and their overall loss function is a weighted average of the individual loss functions from each model on the ensemble.</p><p>ABMT also leverages a teacher-student model where the teacher and student networks share the same architecture, increasing time and memory complexity during training. Moreover, they utilize a three-term loss function to optimize both models with three hyper-parameters controlling the contribution of each term to the final loss. They update the teacher weights based on the exponential moving average (EMA) of the student weights, in order to avoid error label amplification on training. This also adds another parameter to control the inertia in the teacher weights' EMA. The authors do not perform an ablation study regarding the hyper-parameter value variation to assess their impact on final performance.</p><p>Based on these observations, our proposed model better captures the diversity of real cases, by considering a loss function with a single term and that is less sensitive to hyperparameters (only margin ? needs to be selected). In such setups, it is difficult to select hyper-parameter values correctly, as we might not know any information about the identities on the target domain. The self-ensembling also summarizes the whole training into a single model by using each checkpoint's confidence values over the target data, without using any hyper-parameter or human-defined value. Even adopting a more straightforward formulation, we still obtain state-of-theart performance on the Market ? Duke scenario and competitive performance on the Duke ? Market scenario. Each architecture in our work is trained in parallel without any coteaching strategy. After self-ensembling, the joint contribution from different backbones is applied only on evaluation time, avoiding label propagation of noisy examples (e.g., potential outliers) but still taking advantage of the complementarity between them.</p><p>Our assumptions are the same as recent prior art <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b23">[24]</ref>. We assume to know from which camera an image of a person was recorded but not the identity. We rely on camera information to filter out clusters elements captured by only one camera and create the cross-camera triplets.</p><p>We also assume that at least two cameras have captured most identities and all of them have non-overlapping vantage points. All prior art holds this assumption as defined by the datasets and train/test split division.</p><p>Finally, we assume that training on a source domain related to Person Re-Identification gives the model a basic knowledge to adapt to the target domain. This knowledge enables the model to propose better initial clusters on early iterations, grouping feature vectors from the same identity recorded from different cameras. The pipeline starts the adaptation with more reliable pseudo-labels in the clustering step and progressively creates more clusters representing more identities on the target domain. All works hold this assumption in <ref type="table" target="#tab_0">Table II</ref> that do not have the (*) after their name.</p><p>Section V shows that our pipeline still performs well even without pre-training in a source dataset. In other words, we take the backbone trained over ImageNet and directly apply it without any previous ReID-related knowledge. Even in this setup, we can achieve competitive performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Analysis</head><p>We now provide qualitative analysis by highlighting regions of the top-10 gallery images returned for a given query image. The redder the color of a region, the more important it is to the ranking. As explained in Section IV-A, the correct matches always come from cameras different from the query's camera. The green contour denotes a true positive, the red contour a false positive, and the blue color the query image. We present successful cases (when the first gallery image is a true positive) and failure cases (when the first gallery image is a false positive) for each camera on Market1501 and DukeMTMC-ReID datasets. We show two successful cases and two failure cases (one for each dataset) in <ref type="figure">Figures 4 and 5</ref> considering ResNet50 as the backbone. For visualizations for all cameras of both datasets, please refer to the Supplementary Material. MSMT17 was not considered as the dataset agreement does not allow the reproduction of the images in any format.</p><p>Figures 4a and 5a depict two successful cases on Market ? Duke and Duke ? Market scenarios, respectively. In both cases, we see that our model finds fine-grained details on the image leading to a correct match. As an example, <ref type="figure">Figure 4a</ref> shows the model focusing on the red jacket, even in a different pose and under occlusion (7 th and 10 th image from left to right). <ref type="figure">Figure 5a</ref> shows that the model can overcome pose changes of the query on a cross-view setup. The query only shows the person's back, but the closest image is a true match  showing the person from the front. The same happens on the second closest image, where the identity has its back recorded by another camera; and on the fourth and fifth closest images, only the right side is captured. The third closest image not only records a different position of the query, but also has a different resolution. This shows that the model effectively overcomes identity pose changes and resolution on cross-view cameras.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figures 4b and 5b</head><p>depict failure cases to show the limitations of the method. The errors happen when there is no person on the image -see 4b, which has been fully occluded by the car. In this case, the method does not have any specific region to focus on, and then the gallery images are almost fully activated. Another failure case happens when the identity is on a motorcycle <ref type="figure">(Figure 5b</ref>) along with another identity which led to mismatching cases where there is no identity (distractor images on the gallery) or images with parts of a bike. In the Supplementary Material, we provide more successful and failure cases in other cameras for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results on an Unsupervised Scenario</head><p>This section explores the possibilities of our method when not performing any pre-training on a source domain. Here the method starts with backbones trained over ImageNet directly. This is a harder case as we eliminate the possibility of having prior knowledge of the person re-identification problem. This requires the backbones to adapt themselves to the target, not relying on any identity-related annotation coming from the source domain. <ref type="table" target="#tab_0">Table II</ref> shows the results denoted by "Ours(w/o Re-Ranking)*". In this case, we keep ? = 0.05 when Duke is the target, as in previous results, and ? = 0.03 when Market is the target. The value ? = 0.05 was too strict, leading to clusters with images from only one camera for the Market dataset. Section V presents a deeper analysis of different choices of ? on the clustering process.</p><p>However, when we consider Duke as the target domain, the model without source pre-training is the third best. We lose 3.8 and 2.6 p.p. to the equivalent pre-trained model in mAP and Rank-1, respectively, and we lose 2.0 and 0.9 p.p. compared to ABMT, outperforming all other methods. This shows that, although our model is not completely robust to the backbone initialization, it is still capable of mining discriminative fea-tures, even without pre-training, proving comparative or better results when compared to the state of the art.</p><p>The proposed method outperforms all others in the same conditions (no pre-training, denoted with a star in <ref type="table" target="#tab_0">Table II)</ref>. The difference to the best one (CycAs) is 2.9 and 4.7 p.p. on mAP and Rank-1 when Market is the target, and in 8.7 and 4.5 p.p. on mAP and Rank-1 when Duke is the target.</p><p>We conclude that the previous training on a ReID sourcerelated dataset is important for better performances on the task. However, when no ReID source domain is available, our methods can still provide competitive results, mainly in the more challenging scenario (Duke as target).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION STUDY</head><p>This section shows the contribution of each part of the pipeline to the final result. In each experiment, we change one of the parts and keep the others unchanged. If not explicitly mentioned, we consider ResNet50 as the backbone, OPTICS with hyper-parameter ? = 0.05, and self-ensembling applied after training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Impact of the Clustering Hyper-parameter</head><p>Although we have only one hyper-parameter in the loss function, we still need to set hyper-parameter ? of the OPTICS clustering algorithm, a threshold in the range [0, 1]. The closer ? is to 1, the stronger is the criteria to define a cluster; that is, we might have many samples not assigned to any cluster, which leads to several detected outliers (if ? = 1, all feature vectors are detected as outliers). In contrast, the closer ? is to 0, the more relaxed the criteria is, and more samples are assigned to clusters (if ? = 0, all feature vectors are grouped into a single cluster). In <ref type="figure" target="#fig_3">Figure 6</ref>, we show the impact of the threshold ? for the Market ? Duke and Duke ? Market scenarios. The best value for ? changes according to the adaptation scenario. This is expected when dealing with different unseen target domains. In both cases, Rank-1, Rank-5, and Rank-10 curves are more stable than the mAP curve, showing that the parameter does not impact the retrieval of true positive images. The best Rank-1 values are obtained for ? between 0.04 and 0.08 considering both scenarios and, in the more challenging one (Market ? Duke), it achieves the second-best value when ? = 0.05, for both mAP and Rank-1. Although the best performance is achieved when ? = 0.07 (best mAP and Rank-1), it relies on an unstable point in the setup of Duke ? Market, and it is only marginally better than ? = 0.05 for Market ? Duke. Rank-5 and Rank-10 tend to be more stable in both cases. Thus we adopt ? = 0.05 in all scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Impact of Curriculum Learning</head><p>In our pipeline, Stage 3 is responsible for cluster selection. After running the clustering algorithm, a feature vector can be an outlier, assigned to a cluster with only one camera or assigned to a cluster with two or more cameras. We argue that feature space cleaning is essential for better adaptation, and that feature vectors in a cluster with at least two cameras are more reliable than ones assigned as outliers or to cluster with a single camera. Then, we consider the curriculum learning principle to select the most confident samples and learn in an easy-to-hard manner. To achieve this, we remove the outliers and the clusters with only one camera. To check the impact of this removal, we performed four experiments in which we alternate between keeping the outliers and the clusters with only one camera. The results are summarized in <ref type="table" target="#tab_0">Table IV</ref>.</p><p>We observe a performance gain on most metrics, especially on mAP and Rank-1, when we apply our cluster selection strategy. If we keep the outliers in the feature space (first and third rows in <ref type="table" target="#tab_0">Table IV)</ref>, we face the most significant performance drop in both adaptation scenarios. It shows the importance of removing outliers after the clustering stage; otherwise, they can be considered in the creation of triplets, increasing the number of false negatives (for instance, selecting negative samples of the same real class) and, consequently, hindering the performance. We see a lower performance drop by keeping clusters with only one camera but without outliers (second row), indicating that those clusters do not hinder the performance much, but might contain noisy samples for model updating. It is more evident when we verify that the most gains were over mAP and lower gains over Rank-1 in the last row. This demonstrates that if we keep one-camera clusters, the model can still retrieve most of the gallery's correct images but with lower confidence. Hence, the cluster selection criteria effectively improves our model generalization and we apply it in all adaptation scenarios.</p><p>With this strategy, we observe that the percentage of feature vectors from the target domain kept in the feature space increases during the adaptation, as shown in <ref type="figure">Figures 7c and 8c</ref>. In fact, reliability, mAP and Rank-1 increase during training <ref type="figure">(Figures 7 and 8)</ref>, which means that the model becomes more robust in the target domain as more iterations are performed. This demonstrates the curriculum learning importance, where easier examples at the beginning of the training (images whose feature vectors are assigned to clusters with at least two cameras in early iterations) are used to give an initial knowledge about the unseen target domain and allow the model to increase its performance gradually.</p><p>As a direct consequence, the number of clusters with only one camera removed from the feature space decreases, as shown in <ref type="figure">Figure 9</ref>. This means that the model learns to group cross-view images in the same cluster.</p><p>For the Market ? Duke scenario, the initial percentage of removed clusters is higher than on Duke ? Market.  This is expected as the former is a more complex case, so initial clusters tend to have several images grouped due to the camera bias, which leads to a higher number of clusters comprising images recorded from only one camera. For the same reason, the final percentage for Market ? Duke is higher than Duke ? Market. In this last case, all backbones tend to stabilize between 20% and 30% of clusters removed in the last iterations.</p><p>What if all identities are captured by only one camera? In this extreme case, we hypothesize that the model can still adapt to the target domain. However, the performance will be limited, as different identities could be grouped in the same cluster, increasing the false positive rate. This happens because one of our assumptions is that each identity should be captured by at least two cameras. In fact, this is inherited directly from the Person Re-Identification problem. Moreover, our method utilizes this assumption to create the triplets, enabling a better adaptation to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impact of self-ensembling</head><p>To check the contribution of our proposed self-ensembling method explained in Section III-E, we take the best checkpoint of our model during adaptation in both scenarios, considering all backbones, and compare it with the self-ensembled model. Note that we select the best model only for reference. In practice, we do not know the best checkpoint during training since we do not have any identity-label information. Our goal here is merely to show that our self-ensembling method leads to a final model that outperforms any checkpoint individually. Even if we do not have any label information to choose the best one during training, the self-ensembling can summarize the whole training process in a final model, which is better than all checkpoints. <ref type="table" target="#tab_5">Table V</ref> shows these results.</p><p>Our proposed self-ensembling method can improve discriminative power over the target domain by summarizing the whole training during adaptation. The method outperforms the best models in mAP by 2.0, 4.5 and 4.3 p.p., on Duke ? Market, for ResNet50, OSNet and DenseNet121, respectively. Similarly, for Market ? Duke we achieve an improvement of 1.6, 2.2 and 3.3 p.p. in mAP for ResNet50, OSNet and DenseNet121, respectively. We can also observe gains for all backbones in both scenarios considering Rank-1. Therefore, our proposed self-ensembling strategy increases the number of correct examples retrieved from the gallery and their confidence. It shows that different checkpoints trained with different percentages of the data from the target domain have complementary information. Besides, as the self-ensembling is performed at the parameter level, without human supervision and considering each checkpoint's confidence, it reduces the memory footprint by eliminating all unnecessary checkpoints and keeping only the self-ensembled final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Impact of Ensemble-based prediction</head><p>To increase discrimination ability, we combine distances computed by all considered architectures (Equation 3) for the final inference. Results are shown in <ref type="table" target="#tab_0">Table VI</ref>.</p><p>The ensembled model outperforms the individual models by 3.3, 5.2 and 0.9 p.p. regarding Rank-1, on Duke ? Market, for ResNet50, OSNet and DenseNet, respectively. The same can be observed for Market ? Duke, in which Rank-1 is improved by 3.3, 2.9 and 1.6 p.p. for ResNet50, OSNet and DenseNet121, respectively. Results for all the other metrics also increase for both adaptation scenarios. Therefore, we can effectively combine knowledge encoded in models with different architectures. By performing it only for inference, we keep a simpler training process and still can take advantage of the ensembled knowledge from different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Processing Footprint</head><p>To measure the processing footprint of our pipeline (training and inference), we consider two representative adaptation scenarios: Market ? Duke and Market ? MSMT17. As explained, the first setup represents a mildly difficult case and the second is the most challenging one. <ref type="table" target="#tab_0">Table VII</ref> shows the time measurements.</p><p>The overall time to execute the pipeline and the whole training on Market ? Duke scenario is smaller than Market ? MSMT17's, as expected, given that the latter is a more complex setup. As the number of training images is higher, the number of proposed clusters is also higher on MSMT17. This leads to an increase in clustering, filtering, and overall training times.</p><p>OSNet is the backbone that takes less time on both adaptation setups, because of its feature embedding size. For ResNet50 and DenseNet121, the embeddings have 2,048 dimensions while OSNet has 512. This allows a faster clustering, as <ref type="table" target="#tab_0">Table VII</ref> shows. Considering the same adaptation scenario, the clustering step is the most affected by the backbone and its respective embedding size. This is why ResNet50 and DenseNet121 present more similar training times and OSNet is the fastest one.</p><p>The inference time is calculated assuming that all gallery feature vectors have been extracted and stored. It is the average time to predict the label of one query based on the ranking of the gallery images, following the protocol presented in Section IV-B. The difference between both adaptation scenarios is due to the gallery size. As explained in Section IV-A, MSMT17 has a gallery size more than 4? bigger than Duke's.</p><p>For all experiments, we used two GTX 1080 Ti GPUs. One of them is used exclusively for clustering with an implementation based on <ref type="bibr" target="#b57">[58]</ref>, and the other for pipeline training, for each backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS AND FUTURE WORK</head><p>In this work, we tackle the problem of cross-domain Person Re-Identification (ReID) with non-overlapping cameras, especially targeting forensic scenarios with fast deployment requirements. We propose an Unsupervised Domain Adaptation (UDA) pipeline, with three novel techniques: (1) crosscamera triplet creation aiming at increasing diversity during training; (2) self-ensembling, to summarize complementary information acquired at different iterations during training; and (3) an ensemble-based prediction technique to take advantage of the complementarity between different trained backbones.</p><p>Our cross-camera triplet creation technique increases the model's invariance to different points-of-view and types of cameras in the target domain, and increases the regularization of the model, allowing the use of a single-term single-hyperparameter triplet loss function. Moreover, we showed the importance of having this more straightforward loss function. It is less biased towards specific scenarios and helps us achieve state-of-art results in the most complex adaptation setups, surpassing prior art by a large margin in most cases.</p><p>The self-ensembling technique helps us increase the final performance by aggregating information from different checkpoints throughout the training process, without human or label supervision. This is inspired by the reliability measurement, which shows that our models learn from more reliable data as more iterations are performed. Furthermore, this process is done in an easy-to-hard manner to increase model confidence gradually.</p><p>Finally, our last ensemble technique takes advantage of the complementarity between different backbones, enabling us to achieve state-of-the-art results without adding complexity to the training, differently from the mutual-learning strategies used in current methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b15">[16]</ref>. It is important to note that both ensembling strategies are done after training to generate a final model and a final prediction.</p><p>Because the training process is more straightforward than other state-of-the-art methods and does not need information on the target domain's identities, our work is easily extendable to other adaptation scenarios and deployed in actual investigations and other forensic contexts.</p><p>A key aspect of our method also shared with other recent methods in the literature <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b32">[33]</ref>, is that it requires information about the camera used to acquire each sample. That is, we suppose we know, a priori, the device that captured each image. This information does not need to be the specific type of camera but, at least, information about different camera models. Without this information, our model could face suboptimal performance, as it would not be able to take advantage of the diversity introduced by the cross-camera triplets. To address this drawback, we aim to extend this work by incorporating techniques for automatic camera attribution <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, allowing the identification of the camera used to acquire an image or identifying whether the same camera acquired a pair of images.</p><p>Regarding the clustering process, our method requires that all selected samples are considered during this phase, which demands pairwise distance calculation between all feature vectors. Therefore, this approach may introduce higher processing times to the pipeline. In this sense, we also aim to extend our method to scale to very large datasets by introducing online deep clustering and self-supervised techniques directly in the pipeline.</p><p>Another possible extension of our pipeline can be its application to general object re-identification, such as vehicle ReID, to mine critical objects of interest in an investigation. For example, with Person ReID, this could enable a joint analysis by matching mined identities and objects to propose relations between them during an event's analysis finally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Unsupervised and self-adaptative techniques for cross-domain person re-identification Gabriel Bertocco, Fernanda Andal?, Member, IEEE, and Anderson Rocha, Senior Member, IEEE</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. SUPLEMENTARY MATERIAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Full Comparison with Prior Art</head><p>In the main article, we compared our results with recent state-of-the-art methods. In this supplementary material, we present the full set of results, comparing our method with methods proposed before 2018, published in top-tier journals and conferences <ref type="table" target="#tab_0">(Tables I and II</ref>). Following the same conclusions discussed in the main article, our method outperforms all works from 2018 and previous years.</p><p>LOMO <ref type="bibr">[?]</ref>, BOW [?], and UMDL [?] are hand-craftedbased methods. They directly compute feature vectors over pixel values without using a neural network. UMDL also learns a shared dictionary to mine meaningful attributes from the target dataset, however, in a much simpler setup than any deep-learning method. They then calculate the distance between query and gallery images. This makes them scalable and fast deployable. However, since hand-crafted features usually do not describe high-level features from images, the methods fail when used to match the same person from different camera views. The substantial differences caused by changes of illumination, resolution, and pose of the identities bring a high non-linearity to the feature space that is not captured by hand-crafted-based methods. We surpass UMDL by 65. <ref type="bibr" target="#b2">3</ref>  SpCL [?] is similar to ours in the sense that it increases the cluster reliability during the clustering stage as the training progresses. However, it does not apply any strategy considering diversity as we do by creating diverse triplets considering all cameras comprised in a cluster. Besides, they leverage both source and target domain images on adaptation stages and enable their model to use the source labeled identity to bring some regularization to the adaptation process. Differently, our method does not use the source domain images after finetuning and leverages the adaptation process relying only on target images. As shown in the main article, we outperform them by 1.2 and 4.2 p.p. in the most challenging Market ? MSMT17 in mAP and Rank-1, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Full Qualitative Analysis</head><p>In this section we extend the qualitative analysis presented in the main article. We show the successful and failure cases for a query from each camera for Market1501 and DukeMTMC-ReID datasets.</p><p>In <ref type="figure">Figures 1 and 3</ref>, we observe some successful cases with the activation maps for the top-10 closest gallery images to the query. We adapted the implementation from [?] to visualize the activation maps. In both scenarios, we see that our model is able to find fine-grained details on the images, enabling it to correct match the query to the gallery images. For example, in <ref type="figure">Figure 1d</ref>, the model tends to focus on shoes and parts of the face, while in <ref type="figure">Figure 1a</ref> the focus is on regions depicting hair and pants. We conclude that our method is able to distinguish the semantic parts of the body and soft-biometric attributes which are vital to Person Re-Identification. It is also important arXiv:2103.11520v3 [cs.CV] 7 Feb 2022 to note that the query and the correct matches are always from different cameras, which also confirms that our model is able to overcome different camera conditions. Analyzing the errors in <ref type="figure">Figures 1h and 3f</ref>, we see they are mainly caused by similar clothes, but the method is still able to recover at least the closest gallery image (Rank-1 image). <ref type="figure" target="#fig_0">Figures 2 and 4</ref> depict the failure cases for one of the backbones, showing the limitations of the method. The failures are mainly related to similar clothes or soft-biometric attributes.</p><p>Another source of errors is occlusion. <ref type="figure" target="#fig_0">Figure 2a</ref> is an example where the person has been fully occluded by a car. As there is no person, the method does not have any specific region to focus on and then the gallery images are almost fully activated. In <ref type="figure">Figure 4d</ref>, the target identity is on a motorcycle together with another person, which led them to be in the same Despite the high probability of people wearing similar clothing items, it is marginal the chance of them being dressed exactly the same way, however this is the situation shown in <ref type="figure">Figure 4b</ref>. bounding box. In this case, the method erroneously retrieves images with no identity on it (distractor images on gallery) or images with parts of a bike. <ref type="figure">Figure 4a</ref> shows an interesting failure case, where the model focus uniquely on the drawing on the person's shirt in the query image. The method returns gallery images of other identities with similar shirt drawings. Despite the failure, it is interesting to note that our method was able to focus on fine-grained details to find matches, and not activate the whole image or large parts of it.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>K 2 i</head><label>2</label><figDesc>Number of iterations of the orange flow in Figure 1 c i i-th cluster in the feature space n i Number of cameras in cluster c i cam j j-th camera in a cluster x s i i-th image in the source domain x t i i-th image in the target domain y s Label of the i-th image in the source domain Ns Number of images in the source domain Nt Number of images in the target domain m Number of anchors per camera in a cluster ? Margin parameter of the Triplet Loss B Batch of triplets in an iteration</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows the triplet creation process. A triplet is formed by an anchor, a positive, and a negative sample. During optimization, the distance from the anchor to the positive sample should be minimized, while the distance to the negative sample should be maximized. Ideally, positive and negative samples should be hard-to-classify examples for the current model M as easy examples do not bring diversity to the learning process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>The most activated regions in the gallery image given a query on DukeMTMCReID (Market ? Duke scenario) for ResNet50. (a) Successful match; (b) Failure case. Highlighting image regions most activated on gallery image given query on Market1501 after run Duke ? Market scenario on ResNet50. (a) Successful match; (b) Failure case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Impact of clustering hyper-parameter ?. Results on (a) Market ? Duke, and (b) Duke ? Market.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .Fig. 8 .Fig. 9 .</head><label>789</label><figDesc>Progress on Rank-1, mean Average Precision and Reliability on target dataset, in the Market1501 to DukeMTMC-ReID scenario. Progress on Rank-1, mean Average Precision and Reliability on target dataset on DukeMTMC-ReID to Market1501 scenario. Percentage of cluster removed along the training iterations on (a) Market ? Duke and (b) Duke ? Market scenarios considering the three backbones trained independently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 Fig. 1 . 8 Fig. 2 . 6 Fig. 3 . 6 Fig. 4 .</head><label>81826364</label><figDesc>Successful cases considering one query from each camera on Duke. This results are obtained with the ResNet50 backbone after the Market ? Duke adaptation. Failure cases considering one query from each camera on Duke. This results are obtained with the ResNet50 backbone after the Market ? Duke adaptation. Successful cases considering one query from each camera on Market. This results are obtained with the ResNet50 backbone after the Duke ? Market adaptation. Failure cases considering one query from each camera on Market. This results are obtained with the ResNet50 backbone after the Duke ? Market adaptation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell>VARIABLES' MEANING IN THIS WORK</cell></row><row><cell>Variable n b M</cell><cell>Meaning Number of different backbones in the Ensemble Model backbone</cell></row><row><cell>K</cell><cell></cell></row></table><note>1 Number of iterations of the blue flow in Figure 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>43]: It has 12,936 images of 751 identities in the training set and 19,732 images in the testing set. The testing set is still divided into 3,368 images for the query set and 15,913 images for the gallery set. Following previous work, we removed the "junk" images from the gallery set, so 451 images are discarded. This dataset has a total of six non-overlapping cameras. Each identity is captured by at least two cameras. It has 16,522 images of 702 identities in the training set and 19,889 images in the testing set. The testing set is also divided into 2,228 query images and 17,661 gallery images of other 702 identities. The dataset has a total of eight cameras. Each identity is captured by at least two cameras. It is the most challenging ReID dataset present in the prior art. It comprises 32,621 images of 1,401 identities in the training set and 93,820 images of 3,060 identities in the testing set. The testing set is divided into 11,659 images for a query set and 82,161 images for a gallery set. It comprises 15 cameras recorded in three day periods (morning, afternoon, and noon) on four different days. Besides, of the 15 cameras, 12 are outdoor cameras, and three are indoor cameras. Each identity is captured by at least two cameras.</figDesc><table /><note>? DukeMTMC-ReID [44]:? MSMT17 [18]:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON MARKET1501 TO DUKEMTMC-REID AND DUKEMTMCRE-ID TO MARKET1501 ADAPTATION SCENARIOS. WE REPORT MAP, RANK-1, RANK-5, AND RANK-10, COMPARING TO SEVERAL STATE-OF-ART METHODS. THE BEST RESULT IS SHOWN IN BOLD, THE SECOND IN UNDERLINE AND THE THIRD IN italic. WORKS WITH (*) DO NOT PRE-TRAIN THE MODEL IN ANY SOURCE DATASET BEFORE ADAPTATION.</figDesc><table><row><cell>Method SSL [51]* CCSE [24]* UDAP [8] MMCL [34] ACT [38] ECN-GPP [33] HCT [52]* SNR [53] AD-Cluster [11] MMT [12] CycAs [54]* DG-Net++ [23] MEB-Net [13] Dual-Refinement [35] SSKD [39] ABMT [16] Ours (w/o Re-Ranking)* Ours (w/o Re-Ranking) Ours (w/ Re-Ranking)</cell><cell>reference CVPR 2020 TIP 2020 PR 2020 CVPR 2020 AAAI 2020 TPAMI 2020 CVPR 2020 CVPR 2020 CVPR 2020 ICLR 2020 ECCV 2020 ECCV 2020 ECCV 2020 arXiv 2020 arXiv 2020 WACV 2020 This Work This Work This Work</cell><cell>mAP 37.8 38.0 53.7 60.4 60.6 63.8 56.4 61.7 68.3 71.2 64.8 61.7 76.0 78.0 78.7 80.4 67.7 78.4 88.0</cell><cell>Duke ? Market R5 71.7 R1 83.8 73.7 84.0 75.8 89.5 84.4 92.8 80.5 -84.1 92.8 80.0 91.6 82.8 -86.7 94.4 87.7 94.9 84.8 -82.1 90.2 89.9 96.0 90.9 96.4 91.7 97.2 93.0 -89.5 94.8 92.9 96.9 93.8 96.4</cell><cell>R10 87.4 87.9 93.2 95.0 -95.4 95.2 -96.5 96.9 -92.7 97.5 97.7 98.2 -96.5 97.8 97.4</cell><cell>mAP 28.6 30.6 49.0 51.4 54.5 54.4 50.7 58.1 54.1 65.1 60.1 63.8 66.1 67.7 67.2 70.8 68.8 72.6 82.7</cell><cell>Market ? Duke R5 52.5 R1 63.5 56.1 66.7 68.4 80.1 72.4 82.9 72.4 -74.0 83.7 69.6 83.4 76.3 -72.6 82.5 78.0 88.8 77.9 -78.9 87.8 79.6 88.3 82.1 90.1 80.2 90.6 83.3 -82.4 90.6 85.0 92.1 87.2 92.5</cell><cell>R10 68.9 71.5 83.5 85.0 -87.4 87.4 -85.5 92.5 -90.4 92.2 92.5 93.3 -92.5 93.9 93.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III RESULTS</head><label>III</label><figDesc>ON MARKET1501 TO MSMT17 AND DUKEMTMCRE-ID TO MSMT17 ADAPTATION SCENARIOS. WE REPORT MAP, RANK-1, RANK-5, AND RANK-10, COMPARING TO SEVERAL STATE-OF-ART METHODS. THE BEST RESULT IS SHOWN IN BOLD, THE SECOND IN UNDERLINE AND THE THIRD IN italic. WORKS WITH (*) DO NOT PRE-TRAIN THE MODEL IN ANY SOURCE DATASET BEFORE ADAPTATION.</figDesc><table><row><cell>Method ECN [32] CCSE [24]* SSG [9] ECN-GPP [33] MMCL [34] MMT [12] CycAs [54]* DG-Net++ [23] Dual-Refinement [35] SSKD [39] ABMT [16] SpCL [57] Ours (w/o Re-Ranking) Ours (w/ Re-Ranking)</cell><cell>reference CVPR 2019 TIP 2020 ICCV 2019 TPAMI 2020 CVPR 2020 ICLR 2020 ECCV 2020 ECCV 2020 arXiv 2020 arXiv 2020 WACV 2020 NeurIPS 2020 This Work This Work</cell><cell>mAP 10.2 9.9 13.3 16.0 16.2 23.3 26.7 22.1 26.9 26.0 33.0 -34.5 46.6</cell><cell>Duke ? MSMT17 R1 R5 30.2 41.5 31.4 41.4 32.2 -42.5 55.9 43.6 54.3 50.1 63.9 50.1 -48.8 60.9 55.0 68.4 53.8 66.6 61.8 ---63.9 75.3 69.6 77.1</cell><cell>R10 46.8 45.7 51.2 61.5 58.9 69.8 -65.9 73.2 72.0 --79.6 80.4</cell><cell>mAP 8.5 9.9 13.2 15.2 15.1 22.9 26.7 22.1 25.1 23.8 27.8 31.0 33.2 45.2</cell><cell>Market ? MSMT17 R1 R5 25.3 36.3 31.4 41.4 31.6 -40.4 53.1 40.8 51.8 49.2 63.1 50.1 -48.4 60.9 53.3 66.1 49.6 63.1 55.5 -58.1 69.6 62.3 74.1 68.1 76.0</cell><cell>R10 42.1 45.7 49.6 58.7 56.7 68.8 -66.1 71.5 68.8 -74.1 78.5 79.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV IMPACT</head><label>IV</label><figDesc>OF CURRICULUM LEARNING, WHEN CONSIDERING DIFFERENT CLUSTER SELECTION CRITERIA. WE TESTED OUR METHOD WITH AND WITHOUT OUTLIERS AND WITH AND WITHOUT CLUSTERS WITH ONLY ONE CAMERA IN THE FEATURE SPACE. ALL EXPERIMENTS CONSIDER RESNET50 AS THE BACKBONE WITH SELF-ENSEMBLING APPLIED AFTER TRAINING.</figDesc><table><row><cell>w/o outliers -</cell><cell>w/o cluster with one camera -</cell><cell>mAP 50.9</cell><cell cols="2">Duke ? Market R1 R5 79.2 89.5</cell><cell>R10 92.8</cell><cell>mAP 32.7</cell><cell cols="2">Market ? Duke R1 R5 56.7 68.5</cell><cell>R10 72.9</cell></row><row><cell></cell><cell>-</cell><cell>72.4</cell><cell>89.5</cell><cell>95.2</cell><cell>96.7</cell><cell>66.8</cell><cell>81.1</cell><cell>90.2</cell><cell>92.4</cell></row><row><cell>-</cell><cell></cell><cell>49.1</cell><cell>79.8</cell><cell>89.5</cell><cell>92.6</cell><cell>32.7</cell><cell>57.2</cell><cell>68.4</cell><cell>72.3</cell></row><row><cell></cell><cell></cell><cell>74.1</cell><cell>89.6</cell><cell>95.3</cell><cell>97.1</cell><cell>67.8</cell><cell>81.7</cell><cell>90.0</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V IMPACT</head><label>V</label><figDesc>OF SELF-ENSEMBLING. WE CONSIDER A WEIGHTED AVERAGE OF THE PARAMETERS OF THE BACKBONE IN DIFFERENT MOMENTS OF THE ADAPTATION. "BEST" REFERS TO RESULTS OBTAINED WITH THE CHECKPOINT WITH HIGHEST RANK-1 DURING ADAPTATION. "FUSION" IS THE FINAL MODEL CREATED THROUGH THE PROPOSED SELF-ENSEMBLING METHOD. THE BEST RESULTS ARE IN BOLD. BASED PREDICTION. PERFORMANCE WITH AND WITHOUT MODEL ENSEMBLE DURING INFERENCE. BEST VALUES ARE IN BOLD.TABLE VII TIME EVALUATION. WE CALCULATE EACH TIME IN HH:MM:SS FOR TRAINING AND IN MILLISECONDS (MS) FOR INFERENCE. On training, we analyze the time taken to cluster and filter (Stages 2 and 3), one round of fine-tuning (Stage 4b), one epoch (time taken to perform K 2 iterations of orange flow), and the whole pipeline training. On inference, we calculate the time to predict the identity of a query image given the gallery feature vectors.</figDesc><table><row><cell></cell><cell cols="2">ResNet (Best) ResNet (Fusion) OSNet (Best) OSNet (Fusion) DenseNet (Best) DenseNet (Fusion)</cell><cell>mAP 72.1 74.1 60.7 65.2 72.6 76.9</cell><cell cols="2">Duke ? Market R1 R5 89.0 95.5 89.6 95.3 85.8 93.5 87.7 94.8 90.1 95.6 92.0 96.5</cell><cell>R10 97.1 97.1 95.9 96.6 97.1 97.7</cell><cell>mAP 66.2 67.8 65.1 67.3 66.0 69.3</cell><cell>Market ? Duke R1 R5 81.5 89.5 81.7 90.0 81.7 90.3 82.1 90.5 81.7 90.1 83.4 91.3</cell><cell>R10 92.2 92.6 92.1 92.4 92.4 93.0</cell></row><row><cell cols="7">TABLE VI IMPACT OF ENSEMPLE-Duke ? Market mAP R1 R5 R10 ResNet (Fusion) 74.1 89.6 95.3 97.1 OSNet (Fusion) 65.2 87.7 94.8 96.6 DenseNet (Fusion) 76.9 92.0 96.5 97.7 Ensembled model 78.4 92.9 96.9 97.8</cell><cell>mAP 67.8 67.3 69.3 72.6</cell><cell>Market ? Duke R1 R5 81.7 90.0 82.1 90.5 83.4 91.3 85.0 92.1</cell><cell>R10 92.6 92.4 93.0 93.9</cell></row><row><cell>ResNet OSNet DenseNet Ensemble</cell><cell>Clustering + filtering 00:03:55 00:01:53 00:04:06 -</cell><cell cols="3">Market ? Duke Finetuning Epoch 00:08:55 00:13:34 00:08:56 00:11:14 00:08:33 00:13:36 --</cell><cell>Whole Training 11:31:19 09:33:04 11:33:14 -</cell><cell cols="2">Inference Clustering + Filtering 5ms 00:16:45 4ms 00:07:41 4ms 00:16:46 6ms -</cell><cell>Market ? MSMT17 Finetuning Epoch 00:09:36 00:28:08 00:12:20 00:20:59 00:11:27 00:31:13 --</cell><cell>Whole training 23:00:55 17:49:40 26:32:08 -</cell><cell>Inference 13ms 11ms 13ms 22ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>and 66.5 percentage points (p.p) on mAP and Rank-1 when considering Market ? Duke scenario and by 66.0 and 58.4 p.p. considering Duke ? Market. This shows the power of deep neural networks, which effectively describe identities in a non-overlapping camera system under different point-ofviews. All other works published in and after 2018 have been described in the main article. MMFA [?] and TJ-AIDL [?] are methods based on low-and mid-level attribute alignment by leveraging deep convolutional neural networks. Since they do not encourage the networks to be robust to different point-ofviews, their performance is lower than more recent proposed pseudo-labeling methods (PCB-PAST [?], SSG [?], UDAP [?], AD-Cluster [?], among others) and ensemble-based methods (ACT [?], MMT [?], MEB-Net [?], SSKD [?], ABMT [?]). The same can be observed for PTGAN [?], SPGAN and SPGAN+LMP [?], which are GAN-based methods that aim to transfer images from source to target domain, replicating the same camera conditions of the target domain in the labeled source images. However, transferring only cameralevel features, such as color, contrast and resolution, are not enough. People on the source domain might be in different poses and contexts from the ones on the target domain, and then those methods cannot fully describe images on the target domain considering these constraints. In more recent works, researchers have proposed further processing, such as pseudolabeling (DG-Net++ [?]), pose alignment (PDA-Net [?]), and context-alignment (CR-GAN [?]). Our method can surpass all these GAN-based methods by a large margin. Compared to the most powerful of them, DG-Net++, we outperform it by 16.7 and 10.8 p.p on mAP and Rank-1 in the Duke ? Market scenario, and in Market ? Duke by 8.8 and 6.1 p.p. MSMT17 is the most recent and challenging Person ReID benchmark, and this is why there are fewer works considering it for evaluation. PTGAN was the first to considered it, both being proposed in [?], in 2018. We outperform them by a substantial margin of 31.2 and 52.1 p.p. in mAP and Rank-1 in Duke ? MSMT17 and by 30.3 and 52.1 p.p in the challenging Market ? MSMT17 scenario.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE I RESULTS</head><label>I</label><figDesc>ON MARKET1501 TO DUKEMTMC-REID AND DUKEMTMCRE-ID TO MARKET1501 ADAPTATION SCENARIOS. WE REPORT MAP, RANK-1, RANK-5, AND RANK-10, COMPARING DIFFERENT METHODS. THE BEST RESULT IS SHOWN IN BOLD, THE SECOND IN UNDERLINE AND THE THIRD IN italic. WORKS WITH (*) DO NOT PRE-TRAIN THE MODEL IN ANY SOURCE DATASET BEFORE ADAPTATION.</figDesc><table><row><cell>Method LOMO [?] BOW [?] UMDL [?] PTGAN [?] PUL [?] MMFA [?] SPGAN [?] TJ-AIDL [?] SPGAN+LMP [?] HHL [?] ATNet [?] CamStyle [?] MAR [?] PAUL [?] ECN [?] ISSDA-ReID [?] PDA-Net [?] CR-GAN [?] PCB-PAST [?] UCDA [?] SSG [?] CASCL [?] SSL [?]* CCSE [?]* UDAP [?] MMCL [?] ACT [?] ECN-GPP [?] HCT [?]* SNR [?] AD-Cluster [?] MMT [?] CycAs [?]* DG-Net++ [?] MEB-Net [?] Dual-Refinement [?] SSKD [?] ABMT [?] Ours (w/o Re-Ranking)* Ours (w/o Re-Ranking) Ours (w/ Re-Ranking)</cell><cell>reference CVPR 2015 ICCV 2015 CVPR 2016 CVPR 2018 TOMM 2018 ArXiv CVPR 2018 CVPR 2018 CVPR 2018 ECCV 2018 CVPR 2019 TIP 2019 CVPR 2019 CVPR 2019 CVPR 2019 CVPR 2019 ICCV 2019 ICCV 2019 ICCV 2019 ICCV 2019 ICCV 2019 ICCV 2019 CVPR 2020 TIP 2020 PR 2020 CVPR 2020 AAAI 2020 TPAMI 2020 CVPR 2020 CVPR 2020 CVPR 2020 ICLR 2020 ECCV 2020 ECCV 2020 ECCV 2020 arXiv 2020 arXiv 2020 WACV 2020 This Work This Work This Work</cell><cell>mAP 8.0 14.8 12.4 -20.5 27.4 22.8 26.5 26.7 31.4 25.6 27.4 40.0 40.1 43.0 63.1 47.6 54.0 54.6 30.9 58.3 35.5 37.8 38.0 53.7 60.4 60.6 63.8 56.4 61.7 68.3 71.2 64.8 61.7 76.0 78.0 78.7 80.4 67.7 89.5 78.4 88.0</cell><cell>Duke ? Market R5 27.2 R1 41.6 35.8 52.4 34.5 52.6 38.6 -45.5 60.7 56.7 75.0 51.5 70.1 58.2 74.8 57.7 75.8 62.2 78.8 55.7 73.2 58.8 78.2 67.7 81.9 68.5 82.4 75.1 87.6 81.3 92.4 75.2 86.3 77.7 89.7 78.4 -60.4 -80.0 90.0 65.4 80.6 71.7 83.8 73.7 84.0 75.8 89.5 84.4 92.8 80.5 -84.1 92.8 80.0 91.6 82.8 -86.7 94.4 87.7 94.9 84.8 -82.1 90.2 89.9 96.0 90.9 96.4 91.7 97.2 93.0 -94.8 96.5 92.9 96.9 93.8 96.4</cell><cell>R10 49.1 60.3 59.6 66.1 66.7 81.8 76.8 81.1 82.4 84.0 79.4 84.3 -87.4 91.6 95.2 90.2 92.7 --92.4 86.2 87.4 87.9 93.2 95.0 -95.4 95.2 -96.5 96.9 -92.7 97.5 97.7 98.2 -68.8 97.8 97.4</cell><cell>mAP 4.8 8.3 7.3 -16.4 24.7 22.3 23.0 26.2 27.2 24.9 25.1 48.0 53.2 40.4 54.1 45.1 48.6 54.3 31.0 53.4 37.8 28.6 30.6 49.0 51.4 54.5 54.4 50.7 58.1 54.1 65.1 60.1 63.8 66.1 67.7 67.2 70.8 82.4 72.6 82.7</cell><cell>Market ? Duke R5 12.3 R1 21.3 17.1 28.8 18.5 31.4 27.4 -30.0 43.4 45.3 59.8 41.1 56.6 44.3 59.6 46.4 62.3 46.9 61.0 45.1 59.5 48.4 62.5 67.1 79.8 72.0 82.7 63.3 75.8 72.8 82.9 63.2 77.0 68.9 80.2 72.4 -47.7 -73.0 80.6 59.3 73.2 52.5 63.5 56.1 66.7 68.4 80.1 72.4 82.9 72.4 -74.0 83.7 69.6 83.4 76.3 -72.6 82.5 78.0 88.8 77.9 -78.9 87.8 79.6 88.3 82.1 90.1 80.2 90.6 83.3 -90.6 92.5 85.0 92.1 87.2 92.5</cell><cell>R10 26.6 34.9 37.6 50.7 48.5 66.3 63.0 65.0 68.0 66.7 64.2 68.9 -86.0 80.4 85.9 82.5 84.7 --83.2 77.8 68.9 71.5 83.5 85.0 -87.4 87.4 -85.5 92.5 -90.4 92.2 92.5 93.3 -93.9 93.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE II RESULTS</head><label>II</label><figDesc>ON MARKET1501 TO MSMT17 AND DUKEMTMCRE-ID TO MSMT17 ADAPTATION SCENARIOS. WE REPORT MAP, RANK-1, RANK-5, AND RANK-10, COMPARING DIFFERENT METHODS. THE BEST RESULT IS SHOWN IN BOLD, THE SECOND IN UNDERLINE AND THE THIRD IN italic. WORKS WITH (*) DO NOT PRE-TRAIN THE MODEL IN ANY SOURCE DATASET BEFORE ADAPTATION.</figDesc><table><row><cell>Method PTGAN [?] ECN [?] CCSE [?]* SSG [?] ECN-GPP [?] MMCL [?] MMT [?] CycAs [?]* DG-Net++ [?] Dual-Refinement [?] SSKD [?] ABMT [?] SpCL [?] Ours (w/o Re-Ranking) Ours (w/ Re-Ranking)</cell><cell>reference CVPR 2018 CVPR 2019 TIP 2020 ICCV 2019 TPAMI 2020 CVPR 2020 ICLR 2020 ECCV 2020 ECCV 2020 arXiv 2020 arXiv 2020 WACV 2020 NeurIPS 2020 This Work This Work</cell><cell>mAP 3.3 10.2 9.9 13.3 16.0 16.2 23.3 26.7 22.1 26.9 26.0 33.0 -34.5 46.6</cell><cell>Duke ? MSMT17 R1 R5 11.8 -30.2 41.5 31.4 41.4 32.2 -42.5 55.9 43.6 54.3 50.1 63.9 50.1 -48.8 60.9 55.0 68.4 53.8 66.6 61.8 ---63.9 75.3 69.6 77.1</cell><cell>R10 27.4 46.8 45.7 51.2 61.5 58.9 69.8 -65.9 73.2 72.0 --79.6 80.4</cell><cell>mAP 2.9 8.5 9.9 13.2 15.2 15.1 22.9 26.7 22.1 25.1 23.8 27.8 31.0 33.2 45.2</cell><cell>Market ? MSMT17 R1 R5 10.2 -25.3 36.3 31.4 41.4 31.6 -40.4 53.1 40.8 51.8 49.2 63.1 50.1 -48.4 60.9 53.3 66.1 49.6 63.1 55.5 -58.1 69.6 62.3 74.1 68.1 76.0</cell><cell>R10 24.4 42.1 45.7 49.6 58.7 56.7 68.8 -66.1 71.5 68.8 -74.1 78.5 79.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>We thank the financial support of the S?o Paulo Research Foundation (FAPESP) through the grants D?j?Vu #2017/12646-3 and #2019/15825-1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Forensic event analysis: From seemingly unrelated data to understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Padilha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Andal?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Security and Privacy</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="23" to="32" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-scale deep learning architectures for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5399" to="5408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3702" to="3712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Salience-guided cascaded suppression network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3300" to="3310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unity style transfer for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6887" to="6896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Communications, and Applications</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">107173</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Selfsimilarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6112" to="6121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8222" to="8231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adcluster: Augmented discriminative clustering for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9021" to="9030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.01526</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multiple expert brainstorming for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01546</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Enhancing diversity in teacherstudent networks via asymmetric branches for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imageimage domain adaptation with preserved self-similarity and domaindissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person transfer GAN to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adaptive transfer network for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7202" to="7211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Instance-guided context rendering for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="232" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7919" to="7929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Joint disentangling and adaptation for cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via cross-camera similarity exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5481" to="5490" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transferable joint attributeidentity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01440</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A novel unsupervised camera-aware domain adaptation framework for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8080" to="8089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by camera-aware similarity consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6922" to="6931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="137" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="226" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification with iterative self-supervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1536" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to adapt invariance in memory for person reidentification</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">990</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Dual-refinement: Joint label and feature refinement for unsupervised domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13689</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Density-based clustering based on hierarchical density estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Campello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moulavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="160" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">OPTICS: ordering points to identify the clustering structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ankerst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="49" to="60" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Asymmetric co-teaching for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">SSKD: Selfsupervised knowledge distillation for cross domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05972</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-paced learning with diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2078" to="2086" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Re-ranking person reidentification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1318" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Torchreid: A library for deep learning person reidentification in Pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10093</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via softened similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3390" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Style normalization and restitution for generalizable person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3143" to="3152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">CycAs: Self-supervised cycle association for learning re-identifiable descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07577</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for person re-identification through source-guided pseudo-labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dubourvieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ainouz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09445</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multisimilarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.02713</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Hierarchical density-based clustering based on gpu accelerated data indexing strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mour?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sachetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Parthasarathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="951" to="961" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Open set source camera attribution and device linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D O</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="92" to="101" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A survey on digital camera identification methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernacki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Forensic Science International: Digital Investigation</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">300983</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Computer Science with a focus on digital forensics and machine learning at the Artificial Intelligence Lab. (Recod.ai) at the Institute of Computing, University of Campinas, Brazil, where he received a B.Sc. in Computing Engineering in 2019. His research interests include machine learning, computer vision, and digital forensics</title>
		<ptr target="Contacthimatgabriel.bertocco@ic.unicamp.br" />
	</analytic>
	<monogr>
		<title level="m">Gabriel Bertocco is currently pursuing his Ph.D. in</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Andal? received a Ph.D. in Computer Science from the same university in 2012, during which she was a visiting researcher at Brown University. She worked as a researcher at Samsung and as a postdoctoral researcher in collaboration with Motorola</title>
	</analytic>
	<monogr>
		<title level="m">Fernanda Andal? is a researcher associated to the Artificial Intelligence Lab. (Recod.ai) at the Institute of Computing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Campinas, Brazil</orgName>
		</respStmt>
	</monogr>
	<note>Since then, she works at The LEGO Group, Denmark, devising machine learning solutions for their digital products. She is an IEEE member and was the 2016-2017 Chair of the IEEE Women in Engineering (WIE) South Brazil Section. Her research interests include machine learning and computer vision</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">He is the Chair of the Artificial Intelligence Lab. (Recod.ai) at the Institute of Computing, University of Campinas. He was the Chair of the IEEE Information Forensics and Security Technical Committee for 2019-2020 term. Finally, Prof. is an IEEE Senior Member, a Microsoft, Google and Tan Chi Tuan Faculty Fellow and is listed among the Top-1% of most influential scientists worldwide</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Anderson Rocha has been an associate professor at the Institute of Computing, the University of Campinas, Brazil ; Stanford Univ/Plos Biology</orgName>
		</respStmt>
	</monogr>
	<note>Rocha received his Ph.D. in Computer Science from the University of Campinas. His research interests include Artificial Intelligence, Reasoning for complex data, and Digital Forensics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
