<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Donley</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Tourbabin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Suk</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Broyles</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Applied Research</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Applied Research</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><forename type="middle">Krishna</forename><surname>Ithapu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravish</forename><surname>Mehra</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Facebook Reality Labs Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy Communication in Noisy Environments</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-augmented reality (AR)</term>
					<term>cocktail party</term>
					<term>beam- forming</term>
					<term>speech enhancement</term>
					<term>egocentric</term>
					<term>audio</term>
					<term>video</term>
					<term>dataset</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Augmented Reality (AR) as a platform has the potential to facilitate the reduction of the cocktail party effect. Future AR headsets could potentially leverage information from an array of sensors spanning many different modalities. Training and testing signal processing and machine learning algorithms on tasks such as beam-forming and speech enhancement require high quality representative data. To the best of the author's knowledge, as of publication there are no available datasets that contain synchronized egocentric multi-channel audio and video with dynamic movement and conversations in a noisy environment. In this work, we describe, evaluate and release a dataset that contains over 5 hours of multi-modal data useful for training and testing algorithms for the application of improving conversations for an AR glasses wearer. We provide speech intelligibility, quality and signal-to-noise ratio improvement results for a baseline method and show improvements across all tested metrics. The dataset we are releasing contains AR glasses egocentric multi-channel microphone array audio, wide field-ofview RGB video, speech source pose, headset microphone audio, annotated voice activity, speech transcriptions, head bounding boxes, target of speech and source identification labels. We have created and are releasing this dataset to facilitate research in multi-modal AR solutions to the cocktail party problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE ability to communicate in noisy environments is a challenge for many people and has been a popular research topic for many decades. The difficulty that arises is often referred to as the cocktail party effect <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. As our world moves towards a new era of computing in augmented reality (AR) and virtual reality (VR) there are many benefits to be had from an all day wearable computing device, including the potential to eliminate the cocktail party effect once and for all. The biggest obstacle to developing systems to overcome this effect is a lack of realistic data that reflects the specific scenario and provides multi-modal information. Machine learning and signal processing algorithms can benefit from leveraging high quality multi-sensor data, however, until now, there has not been a single dataset that provides all relevant sensor data from an AR headset in an environment that induces the cocktail party effect.</p><p>A critically differentiating aspect of an AR headset-based dataset is its egocentric nature and the associated dynamic Please send correspondence to: EasyComDataset@fb.com movement. There are few egocentric datasets publicly available that also provide data useful for solving the cocktail party effect. The Epic Kitchens dataset <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, originally released in 2018 and an extended version released in 2020, contains egocentric video and single channel audio recordings of activities that took place in kitchens. The environments that were recorded do not contain noise or conversations that are useful for solving the cocktail party problem. The COSINE speech dataset <ref type="bibr" target="#b4">[5]</ref>, released in 2009, contains egocentric audio recordings with seven microphones and in naturally noisy environments. However, the dataset does not contain other sensor modalities, such as video, nor does it contain annotated labels or data from a head mounted device. The EgoCom dataset <ref type="bibr" target="#b5">[6]</ref>, released in 2020, contains egocentric video and two channel audio recordings of conversations from pairs of glasses. However, the dataset does not contain acoustic noise necessary for the problem, there is no clock synchronization between devices and the multi-channel microphone data is lossy compressed. The Amazon Dinner Party Corpus (DiPCo) <ref type="bibr" target="#b6">[7]</ref>, released in 2019, contains 39 channels of audio as well as annotations to specifically help address the cocktail party problem in the form of a dinner party. The CHiME-5 <ref type="bibr" target="#b7">[8]</ref> and CHiME-6 <ref type="bibr" target="#b8">[9]</ref> challenge datasets, released in 2018 and re-run in 2019, respectively, contain 32 audio channels of 4 participants having a dinner party in private homes. The DiPCo, CHiME-5 and CHiME-6 datasets, however, do not contain any video or rigid wearable microphone array recordings. The lack of egocentric perspective limits the use of the datasets and does not facilitate solving the cocktail party problem for users of wearable devices.</p><p>In order to reduce the cocktail party effect, it is desirable to address the problem from many different domains and types of systems, such as voice/speech activity detectors, speaker diarization algorithms, direction of arrival estimators, multi-channel beamforming algorithms, single-channel audio and audio-visual speech enhancement algorithms, automatic speech recognition algorithms, and more. To train and test algorithms and machine learning models for these systems, it is beneficial to use high-quality realistic multi-sensor data that has been recorded in environments that closely match that of where the cocktail party effect is present. Given the limitation of the datasets mentioned earlier, we have collected and are releasing a dataset that contains egocentric multichannel microphone audio, video, positional data, voice activity,  <ref type="bibr" target="#b4">[5]</ref> 2009 7 CHiME-5&amp;6 <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> 2018/19 32 DiPCo <ref type="bibr" target="#b6">[7]</ref> 2019 39 Epic Kitchens <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>  To benchmark different systems using the dataset for the application of mitigating the cocktail party effect, we propose the following task; enhance a target speech source by improving the signal to noise ratio (SNR), speech quality and speech intelligibility whilst not distorting the original speech. Algorithms running on this dataset would also need to cope with the dynamic movement of the sensors that are worn by the participants and should run in real-time, or at least be causal.</p><p>As a baseline starting point, we show that by building a multi-channel acoustic beamformer that is optimized to isotropic/diffuse noise we can obtain good improvements across all three benchmarking measures with minimal distortions in the target speech. The beamformer dynamically steers to the desired speech source of interest, is used on all different wearers of the AR glasses in the dataset and can be run in real-time.</p><p>In this paper, we first describe an example task for which we use the dataset in section II, which we follow with a general description of the dataset in section III and where it can be obtained in section IV. The baseline method used to for the task is explained in section V. Finally, we present and discuss the results for the baseline method in section VI and conclude the paper in VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. CONVERSATIONAL FOCUS TASK</head><p>The main goal to be accomplished by using this dataset is that of conversational focus. The aim is for an algorithm to operate on unseen data and enhance the speech of the person who is intended to be heard by the AR glasses wearer. The enhancement of speech is defined such that there is no noise or interference and that the target speech is undistorted from that of the close microphone signals, or at least, perceptually undistorted. Competing speech should be suppressed to either an inaudible level or a level that is not distracting to the intended conversation. It is also an aim to simultaneously remove all noise. Temporal and spectral distortions or modulations should not be apparent in the enhanced speech and any switching between target speech sources should be unnoticeable.</p><p>In order for any solution to this task be practical, it is imperative that the system run causally, in real-time and with minimal latency. When considering the physical playback mechanism, for an AR application, the levels and delay between the sound originating from the scene and from the playback device should be perceptually justified.</p><p>This task is defined and driven by the motivation to improve conversations in noisy environments, such as restaurants, bars, social gatherings, etc. The cocktail party problem that motivates this task is a problem that occurs for both normal hearing and hard of hearing. It is, therefore, also the goal to derive a solution that is equally performant for people at all levels of hearing ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE EASYCOM DATASET</head><p>In this section, we describe the dataset's general setting, contents and features. The dataset was collected in a room approximately 6m ? 7m ? 3m (width?depth?height) with a reverberation time to 60 dB (RT60) of 645 ms. A table was located in the center of the room and between 3 to 5 participants sat around the table for a session duration of approximately 30 min that was hosted by a single individual, referred to as the 'server'. During each session 10 loudspeakers placed around the room at varying heights played uncorrelated restaurantlike noise where each channel signal was assigned a different loudspeaker each session. The noise was played back at a sound pressure level (SPL) of approximately 71 dB as measured at the positions of the seated participants. There are 12 sessions included in the dataset totaling approximately 5 h and 18 min of conversational recordings. Additionally, there are 3 extra sessions included, which contain minor errors, such as data frame drops.</p><p>One participant per session was provided with an AR glasses headset that recorded multi-channel audio and video, and was tracked with tracking markers. The multi-channel microphone array consisted of two binaural microphones (one in each ear) and an additional 4 microphones distributed around the glasses, All other participants in the session, except the 'server', were each provided with faux glasses that contained tracking markers and a headset microphone. Every participant, except the 'server', had their head pose optically tracked with three dimensional positions and rotations recorded at a rate of 20 frames per second. The headset microphones were recorded at a sampling rate of 48 kHz with a bit depth of 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Methods and Procedure</head><p>The high level procedure undergone for recording the dataset was to initially set up the room, initialize software at the beginning of each session, prepare the hardware for participants, record participant information, start the recording process, run through several interaction tasks and stop/finish the recordings. The initial room set up consisted of adjusting acoustic panels on walls to approximate the reverberation level of a typical occupied restaurant <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> and the installation of an OptiTrack 1 optical tracking system. A table with surrounding chairs and a separate area with a central recording computer were also set up. <ref type="bibr" target="#b0">1</ref> https://optitrack.com/ Prior to recording each session the recording computer software was initialized and prepared for recording. The participants were seated based on the total number of participants for the given session (e.g. see <ref type="figure" target="#fig_2">Fig. 3</ref>). The AR glasses hardware containing the active sensors was fitted to one of the participants. All other participants were then fitted with a headset Rode microphone and faux AR glasses mounted only with OptiTrack infra-red reflectors (optical tracking markers). The participant wearing the AR glasses was asked to face each other participant while an image of the participants faces were captured. All participants were provided with faux names and occupations, and the interactions procedure was described to them. The sound level of the restaurant-like noise was gradually increased to the specified SPL to approximate that of a typical occupied restaurant <ref type="bibr" target="#b10">[11]</ref> and then the data recording was started immediately.</p><p>The participants began the interactions tasks when noise started, coinciding with the beginning of the recording. The interactions tasks were completely improvised. The first task the participants undertook was an introduction to each other using their provided faux names and occupations. These conversations were allowed to continue anywhere up to approximately 3.5 minutes or until the 'server' determined that there was a natural end in the conversation, whichever occurred first. The 'server' then proceeded to take everyone's order from a faux menu, provided to them at that time, by asking participants what they would like to eat and drink from the menu. During pilot phases of the data collection, it was not uncommon for participants to order in a short time, so to facilitate longer interactions  the 'server' would judge and follow up with further questions about the menu order. The aim was for the menu ordering to last approximately 3 minutes. After ordering, participants menus were collected and they were all provided with a single puzzle. The participants were encouraged to think out loud to facilitate conversation or to ask for a new puzzle if they were stuck. The 'server' would provide hints on the puzzles if participants appeared stuck. After solving a puzzle the 'server' would provide a new puzzle and this was repeated until either 10 minutes had passed solving puzzles or the participants solved them all, whichever came first. Following the puzzle solving task, all puzzles were collected and participants were asked to play a game of "I spy", chosen to facilitate simultaneous discussion and head movement. If any of the participants did not know the game, one of those who did was encouraged to explain it to the rest and if not a single participant knew the game, the 'server' would explain the game. The game of "I spy" was allowed to continue for approximately 5 minutes, after which the 'server' would interrupt for the next task. The final task of the recording procedure was to ask each participant to read a set of 20 random command-like sentences from a list (chosen for automatic speech recognition -like tasks). Once the sentences had finished being read by each participant the recording was stopped/finished, the devices were removed from participants and they were allowed to leave.</p><p>After recording, the dataset underwent several postprocessing stages which are outlined in Appendix A. Calibration data for the inter-and intra-modality relationships is included in the dataset and is described in more detail in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Annotated Labels</head><p>We used human annotators to label Voice Activity (VA), transcribe speech, and label Target of Speech (ToS) for the dataset and used face and head detection models to generate face and head bounding boxes for each participant in the scene. To help aid annotations, we embedded photos of each participant with a corresponding identification (ID) number (1-7) into the videos. In order to reduce the background restaurantlike noise, and provide annotators with a clearer audio signal for labeling, we mixed the original audio captured by the AR glasses' microphones with the headset microphone audio worn by each participant.</p><p>1) Voice Activity: To perform annotations, we used Facebook's internal annotation platform, and utilized a video time segmentation tool for all human annotations. The video time segmentation tool used included an audio wave form display and allowed annotators to focus to millisecond precision in order to label the times of VA. Customizable labels and text input were utilized for speech transcription and ToS labeling. We defined VA as any utterance by the participant. This included any sound generated by the participant, including speech, laughing, coughing, and hesitations. VA annotations consisted of time segments of activity and a label to designate the corresponding participant. Annotators were instructed to watch the video and label VA for each participant sequentially by ID number, limiting their focus to labeling one participant at a time. Rejection options were included, which annotators used to designate videos in which personally identifiable information, inappropriate language or profanity was spoken.</p><p>2) Speech Transcriptions and Target of Speech: For speech transcription and ToS annotations we used the VA annotation results as predefined responses, which allowed annotators to append additional labels onto the VA annotations. Annotators were instructed to watch the video, enter the transcribed speech into a text box for each time segment of activity and designate a label corresponding to the ToS. In addition to the participant ID numbers (labeled as numbers 1-7) we included 'Group-Targeted Speech' (labeled as 0) for when the participant was addressing everyone in the scene and 'Non-Targeted Speech' (labeled as 8) when the participant did not address anyone in the group (e.g. when reading sentences aloud). In instances where more than one participant was a target of the speech, but not the whole group, we allowed annotators to designate secondary targets using extra labels. We provided annotators with speech transcription guidelines that would result in transcriptions similar to closedcaptions. The guidelines included capitalization of proper nouns, capitalization of the first word of every sentence and used end-of-sentence punctuation. All words and numbers were spelled out and we used coding conventions for laughter, coughing, throat clear, unintelligible speech, hesitations, and interruptions, labeled as 'L', 'C', 'T', 'U', 'H' and '-', respectively. For mispronunciations, we enclosed the utterance with asterisks (e.g. '*cabernet sauvignon*'). Upon completion of the speech transcriptions and ToS labeling annotations we completed a quality check where annotators reviewed each other's transcriptions and ToS labels to ensure accuracy and consistency.</p><p>3) Face Bounding Boxes: We provide per-frame face ID annotation for all videos in the dataset. We use RetinaFace <ref type="bibr" target="#b11">[12]</ref> to detect all faces from the videos and try to match the detected faces to the ID of the glasses worn by the participants. During the experiment, OptiTrack was used to capture the glasses' locations in the 3D space at all time. In order to project the glasses' 3D coordinates to the 2D image plane, we calibrate the camera using OpenCV 2 to estimate its intrinsics and distortion coefficients. In each frame, we compute the pairwise Euclidean distance between the detected faces and all glasses, and then use constrained Hungarian algorithm to obtain the best match between them. Although this process yields accurate results in most cases, manual correction is still needed because: 1) the face detector could produce false positives; 2) there is no OptiTrack data for the person acting as the 'server'; and 3) the distances computed on the image plane could be misleading when one face is occluding another. However, due to the large size of the dataset, it is impractical to perform manual correction on a frame-by-frame basis. As a remedy, we use an intersectionover-union (IoU) -based tracker <ref type="bibr" target="#b12">[13]</ref> to group the detected faces into tracklets and perform the annotation on the tracklet level. For each tracklet, we initialize its ID to be the majority voting result of the IDs automatically assigned at each frame. We then select one representative image from each tracklet based on head pose and facial landmark detection results given by FAN <ref type="bibr" target="#b13">[14]</ref> and send these images for manual annotation. Comparing to frame-level annotation, this setup reduces the total workload by a factor of 15. Two raters were recruited to perform this manual annotation task. In cases when their results differed, a third rater was enlisted to make the final decision.</p><p>4) Head Bounding Boxes: We track people's heads simultaneously in the dataset. Our method detects the potential targets in images and then we associate the detections through time to generate target trajectories. However, the head detectors are not perfect. Our tracker thus needs to deal with the missing detections and the false alarms. For each video, head tracking gives a set of person trajectories, which which we automatically assign IDs using the corresponding face IDs.</p><p>The proposed method maintains the ID record of each potential trajectory and tries to extend it to match the current head detections. We detect head candidates using Yolo-v3 object detection deep network <ref type="bibr" target="#b14">[15]</ref> trained on the Open Images dataset <ref type="bibr" target="#b15">[16]</ref>. Matching the trajectories and the observations can be formulated as the following optimization problem:</p><formula xml:id="formula_0">min (i,j) (c i,j ? t)x i,j s.t. i x i,j ? 1, j x i,j ? 1 x i,j is binary ?i, j.</formula><p>Here, x i,j is a binary variable that indicates whether trajectory i matches head candidate j; if there is a match x i,j = 1, otherwise x i,j = 0. c i,j is the cost of matching trajectory i with head candidate j. Then matching cost c i,j is defined as</p><formula xml:id="formula_1">c i,j = p j ? H(p i ) 2 + ?d(b j , b i ),</formula><p>where p is a 4-element vector whose elements are the x and y coordinates of the top-left corner and bottom-right corner of the head bounding boxes. For a trajectory, its p is determined by its last position. To compensate for the large motion of the camera in egocentric videos, we estimate the 2?2 homography H between the current video frame and the previous video frame using optical flow <ref type="bibr" target="#b16">[17]</ref>. b is a deep feature that can distinguish between head appearances of different people. The CNN is a modified resnet-18 with input size of 128 ? 128 and output size of 128. It is trained on the Voxceleb2 <ref type="bibr" target="#b17">[18]</ref> dataset using triplet contrastive loss with a margin of 1. The distance d(?, ?) is L 2 distance.</p><p>To avoid trivial all zero solutions, we introduce a threshold, t. There is a potential match if the matching cost is less than the threshold t. All the positive coefficients x have to be zero. Otherwise, they can be set to zero to lower the objective. The matching for all the negative coefficient x must be maximum. The above optimization problem can be solved efficiently using the primal-dual method <ref type="bibr" target="#b18">[19]</ref>.</p><p>We extend the trajectory, i, to include the head detection, j, if the corresponding x i,j = 1. The trajectory i's age is increased by 1 and its life is restored to the maximum value, e.g. 20. If x i,j = 0 ?j, and trajectory i's life is greater than 0, then it extends to a predicted position based on M ?1 (p i ), where M is the homography estimated based on optical flow and M ?1 is its inverse. Trajectory i's age is increased by 1 and its life is decreased by 1. We remove the trajectories who's life is less than zero.</p><p>To deal with false alarm head detections, we only output the trajectories whose lengths are longer than a number, e,g. 5 frames. This can effectively remove false alarms because they are usually not stable and tend to generate short trajectories.</p><p>To assign an ID to each trajectory, we match each head bounding box in a trajectory to a face detection that has the maximum IoU with the head detection, and record the corresponding face ID. A trajectory's ID is then determined by the majority vote of the matched face IDs; each head detection in the trajectory is labeled with the trajectory ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET AVAILABILITY</head><p>The dataset can be downloaded from the following address: The dataset is approximately 79 GB in size and contains separate directories for each type of data. The video in the dataset at the provided link is the compressed video (please contact us for details on uncompressed video). The files are split into one minute durations for all data types. There are 323 one minute segments totaling 5 h and 18 min of data. All data and labels totals 3245 files and approximately 1 trillion 3 https://creativecommons.org/licenses/by-nc/4.0/ data points. The sampling rate for each data type, and further details about the exact structure of the dataset, are included in the root directory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. BASELINE METHOD</head><p>The baseline method to achieve improved conversational focus of the AR glasses wearer aims to utilize noisy signals captured by microphones of the acoustic array to generate single-channel enhanced speech of the desired participant.</p><p>Given that the enhanced speech is required to be distortionless, we analyze a beamforming algorithm commonly referred to as the maximum directivity index (DI) beamformer as the baseline method. The maximum DI beamformer maximizes the directvity index (and consequently the directivity factor), which is an indicator of how well diffuse noise is suppressed. The beamformer does this while keeping the signal arriving in the target direction undistorted. The maximum DI beamformer derivation can be formulated as an optimization problem with the following</p><formula xml:id="formula_2">min h(?) h H (?)R(?)h(?), s.t. h H (?)d(?) = g (1)</formula><p>where ? denotes angular frequency and (?) H is a Hermitian transpose. Considering an N-microphone acoustic array, h(?) denotes a beamformer coefficient vector of size N ? 1 and d(?) is a vector of size N ? 1 that represents the acoustic transfer functions from the desired participant to the microphones on the acoustic array. R(?) is the N ? N multichannel covariance matrix of a spherically isotropic noise field with unit power spectral density. The solution of the optimization problem above is a coefficient vector of the maximum DI beamformer h maxDI (?), which can be obtained with</p><formula xml:id="formula_3">h maxDI (?) = g * R ?1 (?)d(?) d H (?)R ?1 (?)d(?)<label>(2)</label></formula><p>where g is the constraint imposed on the beamformer output to satisfy the distortionless condition of the maximum DI beamformer and (?) * indicates conjugation. If g is set to be a transfer function that represents an acoustic path from the desired talker to one of the microphones on the AR glasses, then the beamformer will retrieve the speech captured by that specific microphone without distortion. In order to compute h maxDI (?), two main components, R(?) and d(?), need to be consistently available while processing the microphone signals. Theoretically R(?) can be derived by spatial integration over a sphere, such as</p><formula xml:id="formula_4">R(?) = 1 4? ? ? d(?, ?, ?)d H (?, ?, ?)sin(?)d?d? (3)</formula><p>where ? and ? are azimuthal and inclination angles, respectively. Using the set of array transfer functions (ATFs) that are readily available from the dataset (see Appendix B), R(?) can be approximated with</p><formula xml:id="formula_5">R(?) = 1 N AT F n m d AT F (? n , ? m , ?)d H AT F (? n , ? m , ?)<label>(4)</label></formula><p>where N AT F is the total number of ATFs in the set. ? n and ? m are the azimuthal and the inclination angles associated with a set of discrete points sampling the sphere for which an ATF vector d AT F (? n , ? m , ?) is measured. As both the desired participant and the AR glasses wearer are moving constantly relative to one another, d(?) is assumed to change over time. Hence the estimate of d(?) needs to be updated over time as opposed to R(?) whose value approximates a weakly time-varying value. We use the output of the OptiTrack tracking system as the estimated positions of the desired participant and the AR glasses wearer. We then calculate the relative angle of the desired participant from the AR glasses wearer. Once the relative angle is estimated, d AT F (?) that is closest to the relative angle is chosen from the set of ATFs and used as the estimate of d(?).</p><p>The processing of the baseline method is conducted on a frame by frame basis. At every frame, the beamformer coefficient vector, h maxDI (?), is updated and frames of multichannel audio samples are filtered by h maxDI (?) in a weighted overlap-add (WOLA) procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. BASELINE RESULTS AND DISCUSSION</head><p>To analyze the baseline method performance, we consider signal-to-noise ratios, speech intelligibility and speech quality. We use 11 intrusive instrumental objective metrics as measures of the performance. Three metrics are related to speech quality, four metrics are related to speech intelligibility and four are related to SNR.</p><p>We use SNR as one of the metrics <ref type="bibr" target="#b19">[20]</ref>, which is well established in the literature. We define the SNR as the ratio of the desired target source signal to all other sounds, where we consider all other sounds as undesired noise. Another metric used is segmental SNR (SegSNR), which is similar to SNR but the mean SNR is only computed over segments where the target source signal is active <ref type="bibr" target="#b19">[20]</ref>. The Signal to Distortion Ratio (SDR) metric is also used and follows a similar definition to SNR but is computed using the implementation described in <ref type="bibr" target="#b20">[21]</ref>. The Scale-Invariant SDR (SI-SDR) is the last of the SNR related metrics used <ref type="bibr" target="#b21">[22]</ref>. The SI-SDR is a variant of SDR that was designed to address some assumptions made in the SDR implementation from <ref type="bibr" target="#b20">[21]</ref>.</p><p>Speech intelligibility was investigated using the Short-Time Objective Intelligibility (STOI) metric, which has high correlation with time-frequency weighted noisy speech and is computed on short-time segments. A more recent version of STOI, known as Extended STOI (ESTOI) was also used in the evaluation. ESTOI has been reported to, additionally, accurately predict intelligibility when highly modulated noise is present. The Hearing-Aid Speech Perception Index (HASPI) version 2 was another intelligibility metric used <ref type="bibr" target="#b25">[25]</ref>. HASPI is a metric that estimates intelligibility using a model of the auditory periphery and is valid for normal-hearing and hearing-impaired listeners. We compute HASQI assuming all participants in the dataset have normal hearing and are speaking at levels of approximately 71 dB SPL. The last of the intelligibility metrics used is Speech Intelligibility In Bits (SIIB) <ref type="bibr" target="#b26">[26]</ref>, which estimates the amount of information shared between a talker and a listener in bits. SIIB has been show to have higher correlation to intelligibility than STOI, ESTOI and HASPI version 1 <ref type="bibr" target="#b30">[30]</ref> across many different datasets <ref type="bibr" target="#b31">[31]</ref>.</p><p>For speech quality estimation we use the Perceptual Evaluation of Speech Quality (PESQ) metric <ref type="bibr" target="#b27">[27]</ref>, which is widely available and commonly used for speech quality evaluations, although it was originally designed for telephony applications. The Hearing-Aid Speech Quality Index (HASQI) version 2 was also used to evaluate speech quality and, like HASPI, is based on a model of the auditory periphery, taking into account the effects of hearing loss. We compute HASQI with the same assumptions we make with HASPI. Finally, the last metric used in the evaluation is the Virtual Speech Quality Objective Listener (ViSQOL) version 3 metric <ref type="bibr" target="#b29">[29]</ref>, which has been shown to have higher correlation to speech quality than PESQ on several datasets <ref type="bibr" target="#b32">[32]</ref>.</p><p>The metric scores are obtained on the EasyCom dataset for all possible target cases (each participant is marked as the desired participant at least once) and the average scores are shown in <ref type="table" target="#tab_0">Table III</ref>. 'Reference Mic' and 'Baseline Method' denote the case where the unprocessed reference microphone signal is used as the degraded signal for each of the metrics and the case where the baseline method output is used as the degraded signal input for each of the metrics, respectively. Two subsets of signals are evaluated as test cases for all metrics, namely 'Noise' and 'Noise + Interferer'. For the 'Noise' test case, the desired participant's portions of the signals (as determined by the VA labels of the dataset) are used as inputs for the metrics only when there is no competing talker present, and noise is always present. For the 'Noise + Interferer' test case, portions of the signal when the desired participant is active are used regardless of whether there is a competing talker present or not, and noise is always present. We ignore cases where the participant wearing the AR glasses is actively talking. In all evaluations, a minimally-noisy clean speech reference signal is used, which is the close microphone signals that are positioned next to the participant's mouths. These signals are not identical to the true clean speech component in the noisy speech signals captured by the AR glasses microphones and so the reported metric values may not be meaningful on their own. <ref type="table" target="#tab_0">Table III</ref> shows the results where we can see that in all test cases the baseline method increases all metrics over the 'Reference Mic', except one. The only metric that does not result in an improvement over the 'Reference Mic' is PESQ and occurs for the 'Noise + Interferer' test case, where the PESQ value for the 'Reference Mic' and 'Baseline Method' are 1.172 and 1.168, respectively. This is likely due to the presence of interfering speech in the reference signal that is provided to the metric for the specific test case. In <ref type="table" target="#tab_0">Table III</ref> we see a 1.2 dB to 3.7 dB of average improvement in the SNRbased metrics. A 0.05 to 0.11 improvement is made on average in most intelligibility-based metrics, except SIIB, which uses units of bits and shows 32 bits to 36 bits of improvement. We disregarded the units while making relative comparisons. Of the improvements that are made in the quality-based metrics, we see a range of average improvement of 0.03 to 0.10.</p><p>The challenging conditions of the dataset make it difficult to improve over the reference microphone signal, however, there Test Case SNR <ref type="bibr" target="#b19">[20]</ref> SegSNR <ref type="bibr" target="#b19">[20]</ref> SDR <ref type="bibr" target="#b20">[21]</ref> SI-SDR <ref type="bibr" target="#b21">[22]</ref> STOI <ref type="bibr" target="#b22">[23]</ref> ESTOI <ref type="bibr" target="#b23">[24]</ref> HASPI <ref type="bibr" target="#b25">[25]</ref> SIIB <ref type="bibr" target="#b26">[26]</ref> PESQ <ref type="bibr" target="#b27">[27]</ref> HASQI <ref type="bibr" target="#b28">[28]</ref> ViSQOL <ref type="bibr" target="#b29">[29]</ref> Reference are many other types of signals that could be leveraged to outdo the baseline method's results as reported here. We call on the research community to improve on these results using the dataset for the tasks outlined in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSIONS</head><p>In this work we have discussed and shown that existing datasets do not provide sufficient data for solving the cocktail party problem from a multi-modal and egocentric point of view, which could be common place in head mounted AR devices. Existing egocentric datasets are missing either the visual modality, pose information and/or noisy environments.</p><p>We have closed the gap in the literature and the gap in available datasets by releasing a dataset with more than five hours of synchronized multi-modal egocentric noisy recordings of natural conversations. To facilitate accelerated research in the area we have open-sourced the high quality dataset with various annotated labels in the different modalities.</p><p>We have proposed a benchmarking task for the dataset and have analyzed a baseline method to address the task. The baseline method leverages acoustic and positional information to enhance targeted speech in real-time. Other included modalities could also be leveraged when addressing the benchmarking task. Our baseline method results in substantial and consistent improvements in speech quality, intelligibility and signal-tonoise ratio -based metrics whilst not distorting the target speech. Our method can also instantaneously switch between target speech sources.</p><p>We challenge the research community to outperform our baseline using the benchmarking task on the provided dataset to help solve the cocktail party problem for AR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A POST-PROCESSING</head><p>After the recording of the raw data, several post-processing steps took place to improve the quality of the dataset. The postprocess reduced the size of the dataset, aligned clean speech reference signals and facilitated more accurate annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Video Compression</head><p>The storage size of the raw uncompressed videos, at several terabytes, is substantially larger than that of the visually lossless (lossy) compressed version, at approximately 42 gigabytes. The video compression was performed using FFMPEG. The videos were encoded from uncompressed MPEG in an AVI container to an MPEG4 file format. The MPEG4 video was compressed using the x264 library with settings using the 'veryslow' preset and a Constant Rate Factor (CRF) of 17.</p><p>The adaptive B-frame decision method was optimal with 8 Bframes between I and P. The direction motion vector prediction was automatic and integer pixel motion estimation method was an uneven multi-hexagon search. The maximum motion vector search range was 24. All partitions of the P, B and I -macroblocks were considered. The number of reference frames was 16. The sub-pixel motion estimation and mode decision was with quantization parameter and trellis RD quantization, enabled on all mode decisions. There were 60 frames for the frame-type lookahead.</p><p>Two channels of the recorded audio were embedded in the compressed video. The binaural microphone audio from channel five (left) and channel six (right) of the microphone array was embedded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Audio Alignment</head><p>The AR glasses microphone array and headset microphones were sampled with two different hardware clocks. Each frame of 2400 samples of audio was timestamped using a central clock. The headset microphone recordings were first coarsely aligned with the time stamps. The alignment was then further refined using the absolute peak of a generalized cross-correlation with phase transform (GCC-PHAT) <ref type="bibr" target="#b33">[33]</ref> using the headset microphone signals and a reference microphone on the AR glasses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B CALIBRATION DATA</head><p>To facilitate effective use of the data across different modalities, calibration data is provided. The calibration data allows for efficient use of the acoustic array, linking the acoustic and visual modalities, reducing optical distortions and refining tracking estimates. None of the provided calibration data that is described in the following is a depiction of any future product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Acoustic Array Transfer Functions</head><p>In order to effectively compute spatial enhancement filters or beamforming filters, the transfer function from each microphone to a point in far-field space is required. This set of transfer functions is commonly called the steering vector, array manifold or array transfer functions (ATFs), the latter is the term used in this work. The ATFs included in this dataset were measured on a head and torso simulator in an anechoic chamber for a discrete set of positions on a sphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Spatial Acoustic-Optical Alignment</head><p>The acoustic array has a spatial response and, hence, the ability to localize sound sources in space. The camera spatially samples photons from the surrounding space and can be correlated with the acoustic space if both spaces are aligned. In order to align these two modalities, the AR glasses were placed on a head and torso acoustic simulator in a specialized acoustic setup. The discrete acoustic source locations in azimuth and elevation were marked in the camera field of view and labeled as the corresponding image pixel coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Optical Camera Intrinsics</head><p>The camera lens path introduces typical lens distortions, which change the way the flat image sensor maps to world coordinates. To compensate for this lens warping, the camera intrinsics and lens de-warp filter parameters were measured. Images of a standard checkerboard were taken with the camera and then used to produce the de-warping filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tracking Marker Locations</head><p>The OptiTrack tracking system used multiple passive infrared reflective markers to track the participants in the scene. Multiple tracking markers were used per pair of glasses, five for the AR glasses recording the scene and four for the other glasses. The resulting tracked position included in the dataset is the center of mass of the tracked marker locations. In order to re-align the tracked positions to a specific marker, the relative positions of the markers and the center of mass are provided in the dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example image frames from the dataset videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The approximate microphone positions on the AR glasses are shown with corresponding channel number. Channels 5 and 6 are binaural microphones. This figure and the microphone positions are not a depiction of any future product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The physical layout of the recording setup is shown along with the participant ID numbers. The sets of IDs/locations for each given number of participants per session is also shown. The concentric rings indicate approximate loudspeaker locations around the room. The distance from participant ID 2 to ID's 3, 4, 5, 6 and 7 was approximately 1.0 m, 1.2 m, 1.1 m, 1.2 m and 1.0 m, respectively. Dimensions are not to scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Annotations and labels are shown for an example frame in the dataset. The tracked poses have been projected into the image space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>https://github.com/facebookresearch/EasyComDataset Please send correspondence to EasyComDataset@fb.com. The entire dataset is available for free under the Creative Commons Attribution Non-Commercial 4.0 International public license agreement (CC-BY-NC-4.0) 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TYPES</head><label>I</label><figDesc>OF DATA INCLUDED IN EXISTING DATASETS AND EASYCOM.</figDesc><table><row><cell>Name</cell><cell>Year</cell><cell># Mics (sync)</cell><cell>Video</cell><cell>Egocentric Mics Camera</cell><cell>Pose Transcribed</cell><cell>Noisy Speech</cell></row><row><cell>COSINE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II TYPES</head><label>II</label><figDesc>OF PROBLEMS THAT CAN BE INVESTIGATED USING EXISTING DATASETS AND EASYCOM.</figDesc><table><row><cell>Use Case</cell><cell>COSINE [5]</cell><cell>Epic Kitchens [3], [4]</cell><cell>CHiME-5&amp;6 [8], [9]</cell><cell>DiPCo [7]</cell><cell>EgoCom [6]</cell><cell>EasyCom</cell></row><row><cell>Voice/Speech Activity Detection</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Speaker Diarization</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Automatic Speech Recognition (ASR)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Noise Robust ASR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single-Ch Speech Enhancement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single-Ch Speech Separation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Single-Ch Noise Reduction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-Ch Speech Enhancement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-Ch Speech Separation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-Ch Noise Reduction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acoustic Beamforming</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio-Visual Speech Enhancement</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio-Visual Speech Separation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio-Visual Noise Reduction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Silent Video Speech Reconstruction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Moving-Array Acoustic Beamforming</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Direction of Arrival Estimation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sound Source Localization and Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Acoustic SLAM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Visual Face Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Visual Body Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Audio-Visual Sound Source Tracking</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Conversational Dynamics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">see Fig. 2 for approximate positions. The microphones recorded</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">audio at a sampling rate of 48 kHz with a bit depth of 32. The</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">video was recorded with a wide-angle camera at a resolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">of 1920 ? 1080 and frame rate of 20 frames per second. The</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">camera lens provided an approximate horizontal field of view</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(FOV) of 120</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>? , vertical FOV of 66 ? and diagonal FOV of 139 ? .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III SCORES</head><label>III</label><figDesc>FOR THE REFERENCE MICROPHONE SIGNAL AND THE BASELINE METHOD OUTPUT.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://opencv.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We would like to thank the research assistant for their excellent work helping collect this dataset (whose name is anonymous for privacy reasons). We would also like to thank everyone who gave valuable feedback on the dataset. Lastly, we extend our thanks to all participants involved in the data collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Some experiments on the recognition of speech, with one and with two ears</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">C</forename><surname>Cherry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Acoust. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="975" to="979" />
			<date type="published" when="1953-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The cocktail party phenomenon: A review of research on speech intelligibility in multiple-talker conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Bronkhorst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acustica</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="117" to="128" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rescaling egocentric vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<idno>arXiv: 2006.13 256 [cs.CV</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">COSINE -a corpus of multi-party conversational speech in noisy environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stupakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hanusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Acoust., Speech and Signal Process</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="4153" to="4156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EgoCom: A multi-person multi-modal egocentric communications dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Northcutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. and Mach. Intell</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">DiPCo -dinner party corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Segbroeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13447</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>eess.AS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">The fifth &apos;CHiME&apos; speech separation and recognition challenge: Dataset, task and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Trmal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10609</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.09249</idno>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>cs.SD</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Acoustical capacity as a means of noise control in eating establishments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Rindel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Baltic-Nordic Acoust. Meeting</title>
		<meeting>Baltic-Nordic Acoust. Meeting</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2429</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Restaurant acoustics-verbal communication in eating establishments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acoust. Pract</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Comput. Vision and Pattern Recognit</title>
		<meeting>IEEE/CVF Conf. Comput. Vision and Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">High-speed tracking-bydetection without using image information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bochinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Eiselein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sikora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. Adv. Video and Signal Based Surveillance (AVSS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vision</title>
		<meeting>IEEE Int. Conf. Comput. Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">YOLOv3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The open images dataset v4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint pattern recognition symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep speaker recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Steiglitz</surname></persName>
		</author>
		<title level="m">Combinatorial Optimization: Algorithms and Complexity. Courier Corporation</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Voicebox: Speech processing toolbox for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brookes</surname></persName>
		</author>
		<ptr target="availablefromwww.ee.ic.ac.uk/hp/staff/dmb/voice-box/voicebox.html" />
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">47</biblScope>
		</imprint>
	</monogr>
	<note>Software</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fevotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SDRhalf-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="626" to="630" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Audio, Speech, and Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An algorithm for predicting the intelligibility of speech masked by modulated noise maskers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Audio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Speech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2009" to="2022" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The hearing-aid speech perception index (HASPI) version 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Arehart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="page" from="167" to="6393" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An instrumental intelligibility metric based on information theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Kuyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Lett</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="115" to="119" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ)</title>
		<idno>ITU-T Rec. P.862</idno>
	</analytic>
	<monogr>
		<title level="j">Int. Telecommun. Union (ITU</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The hearing-aid speech quality index (HASQI) version 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Arehart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Audio Eng. Soc</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="99" to="117" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ViSQOL v3: An open source production ready objective speech and audio metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Quality Multimedia Experience (QoMEX)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The hearing-aid speech perception index (HASPI)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Kates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Arehart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Commun</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="167" to="6393" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An evaluation of intrusive instrumental intelligibility metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Van Kuyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2153" to="2166" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ViSQOL: An objective speech quality model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Skoglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Audio, Speech, Music Process</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The generalized correlation method for estimation of time delay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Knapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, and Signal Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="320" to="327" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
