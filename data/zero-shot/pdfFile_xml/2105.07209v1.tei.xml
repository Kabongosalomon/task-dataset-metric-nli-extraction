<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Aerial-PASS: Panoramic Annular Scene Segmentation in Drone Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaikai</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangdong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Bai</surname></persName>
						</author>
						<title level="a" type="main">Aerial-PASS: Panoramic Annular Scene Segmentation in Drone Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aerial pixel-wise scene perception of the surrounding environment is an important task for UAVs (Unmanned Aerial Vehicles). Previous research works mainly adopt conventional pinhole cameras or fisheye cameras as the imaging device. However, these imaging systems cannot achieve large Field of View (FoV), small size, and lightweight at the same time. To this end, we design a UAV system with a Panoramic Annular Lens (PAL), which has the characteristics of small size, low weight, and a 360 ? annular FoV. A lightweight panoramic annular semantic segmentation neural network model is designed to achieve high-accuracy and real-time scene parsing. In addition, we present the first drone-perspective panoramic scene segmentation dataset Aerial-PASS, with annotated labels of track, field, and others. A comprehensive variety of experiments shows that the designed system performs satisfactorily in aerial panoramic scene parsing. In particular, our proposed model strikes an excellent trade-off between segmentation performance and inference speed suitable, validated on both public street-scene and our established aerial-scene datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the last years, UAV (Unmanned Aerial Vehicle) systems have become relevant for applications in military recognition, civil engineering, environmental surveillance, rice paddy remote sensing, and spraying, etc. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Compared with classic aerial photography and ground photography, the UAV system is more flexible, small in size, low-cost, and suitable for a wider range of application scenarios. Environment perception algorithms based on UAV system needs to be light and efficient enough for the application in mobile computing devices like portable embedded GPU processors.</p><p>However, most optical lens of the UAV systems have a small field of view, and often rely on a complex servo structure to control the pitch of the lens, and post-stitch the collected images to obtain a 360 ? panoramic image <ref type="bibr" target="#b1">[2]</ref>. The expansion of the field of view is essential for real-time monitoring with UAVs. In traditional methods, the control system of UAV is usually very complicated. Since the drone takes images during flight, the post-stitching algorithm is highly computation-demanding, and the images have parallax and exposure differences, which renders the reliability of image stitching rather low. To address this problem, we have designed a lightweight Panoramic Annular Lens (PAL) <ref type="figure">Fig. 1</ref>. Overview of our Aerial-PASS system. Panoramic images captured from the UAV with a PAL camera are unfolded, and with the designed semantic segmentation model, field and track classes are predicted at the pixel level. especially suitable for UAV systems. The system does not require a complicated servo system to control the attitude of the lens. The optical axis of the lens is placed vertically on the ground, and the cylindrical field of view can be horizontally upward by 10 ? and downward by 60 ? <ref type="figure">(Fig. 1</ref>).</p><p>To support fast on-board remote sensing, we further propose a lightweight real-time semantic segmentation network for panoramic image segmentation. The network has an efficient up-sampling module and a multi-scale pooling module for learning objects of different scales in the panoramic images. To facilitate credible evaluation, we collect with our PAL-UAV system and present the first drone-perspective panoramic scene segmentation benchmark Aerial-PASS with annotated labels of critical field sensing categories. We find a superior balance between accuracy and inference speed for the network towards efficient aerial image segmentation and it also achieves the state-of-the-art real-time segmentation performance on the popular Cityscapes dataset <ref type="bibr" target="#b2">[3]</ref>.</p><p>The contributions of this paper are summarized as follows:</p><p>? The designed PAL lens has the advantages of 4K high resolution, large field of view, miniaturization design, and real-time imaging capabilities, etc. It can be applied to UAV survey and identification, autonomous driving, security monitoring, and other fields. ? An efficient real-time semantic segmentation network is proposed for panoramic images and it achieves a state-of-the-art accuracy-speed trade-off on Cityscapes.</p><p>A set of comparison experiments is conducted on the panoramic images collected by our PAL-UAV system. ? An annotated Aerial Panoramic dataset is presented for the first time, which is conducive to the rapid and robust segmentation of target objects in a large field of view from UAV perspectives. Particularly, this work focuses on pixel-wise segmentation of track and field objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS A. Panoramic Annular Imaging</head><p>The optical lens in the drone provides an indispensable visual aid for target recognition and detection. A single large field of view lens can bring a wider field of view to the drone. The current ultra wide-angle optical systems include fisheye lenses <ref type="bibr" target="#b3">[4]</ref>, catadioptric optical systems <ref type="bibr" target="#b4">[5]</ref>, and Panoramic Annular Lens (PAL) imaging systems <ref type="bibr" target="#b5">[6]</ref>. The structure of the PAL is simpler, smaller in size, easy to carry, and better in imaging quality. These advantages make the PAL become the research focus of large field of view optical systems. The PAL imaging system was proposed by Greguss in 1986 <ref type="bibr" target="#b6">[7]</ref>. In 1994, Powell designed the infrared band large field of view PAL imaging system, and showed that the spherical surface type can be used to obtain better imaging quality <ref type="bibr" target="#b7">[8]</ref>. A cluster of scientific research teams has also made great research progress, designing PAL optical systems with longer focal length <ref type="bibr" target="#b8">[9]</ref>, larger field of view <ref type="bibr" target="#b9">[10]</ref>, higher resolution <ref type="bibr" target="#b10">[11]</ref>, and better imaging quality <ref type="bibr" target="#b11">[12]</ref>. The PAL designed in this work is small in size, easy to carry, and it has a field of view of (30 ? ? 100 ? ) ? 360 ? and a 4K high resolution, which can realize real-time large field of view high-resolution single sensor imaging. Thereby, it is perfectly suitable for UAV survey, unmanned driving, security monitoring, and other application fields <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Panoramic Scene Segmentation</head><p>Beginning with the success of Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b14">[15]</ref>, semantic segmentation can be achieved in an end-to-end fashion. Subsequent composite architectures like PSPNet <ref type="bibr" target="#b15">[16]</ref> and DeepLab <ref type="bibr" target="#b16">[17]</ref> have attained remarkable parsing performance. Yet, due to the use of large backbones like ResNet-101 <ref type="bibr" target="#b17">[18]</ref>, top-accuracy networks come with prohibitively high computational complexity, which are not suitable for mobile applications such as drivingand aerial-scene segmentation in autonomous vehicles or drone videos. Plenty of light-weight networks emerge such as SwiftNet <ref type="bibr" target="#b18">[19]</ref>, AttaNet <ref type="bibr" target="#b19">[20]</ref>, and DDRNet <ref type="bibr" target="#b20">[21]</ref>, both seeking a fast and precise segmentation. In our previous works, we have leveraged efficient networks for road scene parsing applications like nighttime scene segmentation <ref type="bibr" target="#b21">[22]</ref> and unexpected obstacle detection <ref type="bibr" target="#b22">[23]</ref>.</p><p>Driving and aerial scene segmentation clearly benefit from expanded FoV, and standard semantic segmentation on pinhole images has been extended to operate on fisheye images <ref type="bibr" target="#b23">[24]</ref>, omnidirectional images <ref type="bibr" target="#b24">[25]</ref>, and panoramic annular images <ref type="bibr" target="#b25">[26]</ref>. The latest progress include omni-supervised learning <ref type="bibr" target="#b26">[27]</ref> and omni-range context modeling <ref type="bibr" target="#b27">[28]</ref> to promote efficient 360 ? driving scene understanding. In particular, Yang et al. <ref type="bibr" target="#b25">[26]</ref> proposed a generic framework by using a panoramic annular lens system, which intertwines a network adaptation method by re-using models learned on standard semantic segmentation datasets. Compared to driving scene segmentation, aerial image segmentation is still predominantly based on pinhole images <ref type="bibr" target="#b28">[29]</ref>- <ref type="bibr" target="#b31">[32]</ref>. In this paper, we lift 360 ? panoramic segmentation to drone videos and propose an Aerial-PASS system to explore the superiority of the ultra-wide FoV for security applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED SYSTEM</head><p>The proposed system consists of the hardware part and the algorithm part. In the hardware part, we have equipped the UAV with our designed PAL camera and use it to collect panoramic image data. To efficiently parse panoramic images, we have designed a lightweight semantic segmentation model with a novel multi-scale receptive field pyramid pooling module for learning a robust feature representation required for 360 ? image segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. UAV with PAL Camera</head><p>The PAL system designed and used in this work has the characteristics of large field of view and lightweight structure, which can realize real-time, single-sensor imaging on drones. The PAL system follows the imaging principle of Flat Cylindrical Perspective (FCP), which can reflect light by the panoramic block from the lateral field of view around the optical axis 360 ? , and then enter the subsequent lens group and image on the two-dimensional image surface. The field of view of the PAL system is generally greater than 180 ? , which is no longer applicable to the classic principle of object-image similarity. At this point, we introduce negative distortion in the optical design to control the height of the image surface, and the commonly used characterization method is F-Theta distortion.</p><p>The PAL system designed in this work is composed of 10 standard spherical lenses, and the glass materials are all from the CDGM company. The PAL block is composed of two glass materials glued together. By coating the transmission film and reflection film on its surface, the light path can be folded. Its structure diagram is shown in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>We have chosen the Inspire 2 series drone module developed by DJI to be equipped with a PAL, which is placed vertically downward to cover a wider field of view on the ground and to avoid the stray light problem caused by direct sunlight. The drone system is flying around 100m above the track and field to collect image data from multiple directions.  The schematic diagram of the experiment is shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. The lateral field of view of the PAL system involved in imaging is 10 ? horizontally upward and 60 ? horizontally downward, and the overall field of view is 360 ? ? 70 ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Aerial Scene Segmentation Model</head><p>For our segmentation method, there are three requirements. The model must be light enough to meet the realtime inference speed demand for future migration to portable computing devices. In addition, The model needs to parse the image with a high accuracy. Further, the model should have multi-scale receptive fields for panoramic images with a ultra-wide angle. Inspired by SwfitNet <ref type="bibr" target="#b18">[19]</ref> and RFNet <ref type="bibr" target="#b22">[23]</ref>, we have designed a lightweight novel U-Net like model with multi-scale receptive fields.</p><p>1) Model Architecture: The proposed network architecture is shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. We adopt ResNet-18 <ref type="bibr" target="#b17">[18]</ref> as our backbone, which is a mainstream light-weight feature extraction network. With ImageNet <ref type="bibr" target="#b32">[33]</ref> pre-trained weights, we can benefit from regularization induced by transfer learning and small operation footprint for real-time prediction. The feature maps after each layer of ResNet is fused with the feature maps in the upsampling modules through skip connection with 1 ? 1 convolution modules. To increase the receptive field, the feature maps are transmitted to Efficient Deep Aggregation Pyramid Pooling module. In the decoder, feature maps are added with lateral features from an earlier layer of the encoder element-wisely, and after convolution, the blended feature maps are upsampled by bilinear interpolation. Through skip connections, high-resolution feature maps full of detail information are blended with low-resolution feature maps with rich semantic information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Efficient Deep Aggregation Pyramid Pooling:</head><p>For aerial panoramic images, many objects are rather small and only take up little ratio of pixels. Receptive field is extremely important in the task for a fine-grain segmentation. Inspired by the context extraction module in DDRNet <ref type="bibr" target="#b20">[21]</ref>, we develop an Efficient Deep Aggregation Pyramid Pooling (EDAPP) module. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the architecture of EDAPP. First, we perform average pooling with kernel size of 5, 9, 17, and global pooling respectively. Single 3 ? 3 or 1 ? 1 convolutions in Spatial Pyramid Pooling (SPP) <ref type="bibr" target="#b33">[34]</ref> is not enough, so after 1 ? 1 convolution upsampling to the same resolution, to efficiently blend the multi-scale contextual information better, we propose to leverage a combination of 3 ? 1 convolution and 1 ? 3 convolution. Another stream consists of only a 1 ? 1 convolution. Asserting an input x, the process can be summarized in the following: <ref type="figure">(Pglobal(x)</ref>)) + y k?1 ), k = n.</p><formula xml:id="formula_0">y k = C 1?1 (x), k = 1; C 1?3 (C 3?1 (UP(C 1?1 (P 2 k +1,2 k?1 (x))) + y k?1 ), 1 &lt; k &lt; n; C 3?3 (UP(C 1?1</formula><p>(1)</p><p>Here, C denotes convolution, UP denotes bilinear upsample, P and P global denote pooling and global pooling respectively. i and j of the P i, j denote the kernel size and stride of the pooling layer. A pair of 3 ? 1 convolution and 1 ? 3 convolution have the same receptive field as a single 3 ? 3 kernel, enhanced directional feature extraction, but less computational complexity and faster inference speed. This architecture helps extracting and integrating deep information with different scales by different pooling kernel sizes. All the features maps are concatenated and blended through a 1 ? 1 convolution. Finally, a 1 ? 1 convolution compress all the feature maps and after that we also add a skip connection with a 1 ? 1 convolution for easier optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Aerial-PASS Dataset</head><p>We collected data in 4 different places using the UAV with a PAL camera, and all the data were under sufficient illumination conditions. In the height of about 100 meters, we collected videos with the length of about 3 hours. Limited to the size of the CMOS in the camera, the image produced can't show all the imaging plane of the lens. In the following deployment phase, the PAL system was calibrated using the interface provided by the omnidirectional camera toolbox <ref type="bibr" target="#b34">[35]</ref>. Before training, the PAL image was unfolded to a familiar rectangle image. The unfolded process is depicted in the following equations:</p><formula xml:id="formula_1">i = r ? r 1 r 2 ? r 1 ? height (2) j = ? 2? ? width<label>(3)</label></formula><p>Here, i and j denote the index of x and y axis of the unfold image, respectively. r 1 and r 2 are the internal and external radii of the PAL image. Width and height are the image size of the unfolded image. In our experiment, we unfolded the PAL image to a 2048?512 image. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the unfolding process.</p><p>We annotated all 462 images out of the 3-hour-long video. We created pixel-wise fine labels on the most critical classes relevant to the application of track detection, and we randomly split out 42 images for the test set. As far as we know, This is the first aerial panoramic image dataset with semantic segmentation annotations in the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>All the experiments were implemented with PyTorch 1.3 on a single 2080Ti GPU with CUDA10.0, cuDNN 7.6.0. We chose Adam <ref type="bibr" target="#b35">[36]</ref> for optimization with a learning rate of 5 ? 10 ?4 , where cosine annealing learning rate scheduling policy <ref type="bibr" target="#b36">[37]</ref> is adopted to adjust the learning rate with a minimum value of 5 ? 10 ?4 in the last epoch and weight decay was set to 1 ? 10 ?4 . The ResNet-18 <ref type="bibr" target="#b17">[18]</ref> backbone was initialized with pre-trained weights from ImageNet <ref type="bibr" target="#b32">[33]</ref> and the rest part of the model was initialized with the Kaiming initialization method <ref type="bibr" target="#b37">[38]</ref>. We updated the pretrained parameters with 4 times smaller learning rate and weight decay rate. The data augment operations consist of scaling with random factors between 0.5 and 2, random horizontal flipping, and random cropping with an output resolution of 512 ? 512. Models were trained for 100 epochs with a batch-size of 6. We used the standard "Intersection over Union (IoU)" metric for the evaluation:</p><formula xml:id="formula_2">IoU = T P T P + FP + FN<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Analysis</head><p>Based on the Aerial-PASS dataset, we have created a benchmark to compare our proposed network with other two competitive real-time semantic segmentation networks: the single-scale SwiftNet <ref type="bibr" target="#b18">[19]</ref> (similar backbone with our network) and ERF-PSPNet <ref type="bibr" target="#b38">[39]</ref> (a lightweight network designed for panoramic segmentation <ref type="bibr" target="#b25">[26]</ref>). All the networks were trained on the training set of Aerial-PASS with the same training strategy and tested on the testing set of the dataset. <ref type="table" target="#tab_0">Table I</ref> shows the numerical performance comparisons of the three efficient networks. Our proposed model outperforms  both networks designed for panoramic segmentation (ERF-PSPNet) and semantic segmentation (SwifNet) by clear gaps. <ref type="figure" target="#fig_5">Fig. 7</ref> shows visualizations of inference labels of our proposed method and other two models, in which green denotes the track and red denotes the field. All the input images are the unfolded PAL images. The labels show the qualitative result of the proposed method. As we can find that our method performs well in both large-scale objects like field and small-scales objects like the boundary of track and other objects thanks to our EDAPP module designed for multi-scale feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with the State of the Art</head><p>To further compare with other state-of-the-art network, we also trained our network on Cityscapes <ref type="bibr" target="#b2">[3]</ref>, which is a largescale RGB dataset that focuses on semantic understanding of urban street scenes. It contains 2975/500/1525 images in the training/validation/testing subsets, both with finely annotated labels on 19 classes. The images cover 50 different cities with a full resolution of 2048 ? 1024. We trained our network on the training set of the Cityscapes dataset and test our network on the validation set. <ref type="table" target="#tab_0">Table II</ref> shows the IoU result of our method and other mainstream real-time semantic segmentation models. Our method has achieved an excellent balance between accuracy and inference speed. <ref type="figure" target="#fig_6">Fig. 8</ref> shows some representative inference results of our proposed method. Overall, the qualitative results verify the generalization capacity of our proposed network for both challenging large-FoV aerial image segmentation and highresolution driving scene segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>MIoU Speed (FPS) FCN8s <ref type="bibr" target="#b14">[15]</ref> 65.3% 2.0 * DeepLabV2-CRF <ref type="bibr" target="#b16">[17]</ref> 70.4% n/a ENet <ref type="bibr" target="#b39">[40]</ref> 58.3% 76.9 * ERF-PSPNet <ref type="bibr" target="#b38">[39]</ref> 64.1% 20.4 SwiftNet <ref type="bibr" target="#b18">[19]</ref> 72% 41.0 Ours 72.8% 39.4 * Speed on half resolution images.</p><p>V. CONCLUSION In this study, we propose a lightweight UAV system Aerial-PASS with a designed Panoramic Annular Lens (PAL) camera and a real-time semantic segmentation network for aerial panoramic image collection and segmentation. The minimization-dedicated PAL camera equipped in the UAV can be used for collecting annular panoramic images without requiring a complicated servo system to control the attitude of the lens. To classify the track and field in the images at the pixel wise, we collect and annotate 462 images and propose an efficient semantic segmentation network. The proposed network has multi-scale reception fields and an efficient backbone, which outperforms other competitive networks on our Aerial-PASS dataset and also has reached the state-ofthe-art performance on the Cityscapes dataset with 39.4 Hz in full resolution on a single 2080Ti GPU processor. In the future, we aim to transplant the algorithm to the portable embedded GPU on the UAV for more field tests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Shaded model of the designed PAL imaging system. It consists of 6 groups of 10 lenses. Different colors represent light in different field of view.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The schematic diagram of the drone system. The green area represents the imaging area and the FoV is (30 ? ? 100 ? ) ? 360 ? . The yellow area represents the blind area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The architecture of the proposed segmentation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>The architecture of the EDAPP module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>The unfolding process of the PAL image. Limited to the size of CMOS, top and bottom sides of the imaging plane are blocked in the image, resulting in two scalloped shadows in the unfolded image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Qualitative semantic segmentation results. From top to bottom row: RGB input image, ERF-PSPNet, SwiftNet, our method, and ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Qualitative results of our method on the Cityscapes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I PER</head><label>I</label><figDesc>-CLASS AND MEAN IOU OF OUR METHOD AND OTHER TWO NETWORKS ON THE TESTING SET OF AERIAL-PASS DATASET.</figDesc><table><row><cell>Network</cell><cell>Track</cell><cell>Field</cell><cell>Others</cell><cell>Mean</cell></row><row><cell>ERF-PSPNet</cell><cell>64.16%</cell><cell>97.67%</cell><cell>92.02%</cell><cell>84.62%</cell></row><row><cell cols="2">SwiftNet (single scale) 63.15%</cell><cell>98.76%</cell><cell>91.63%</cell><cell>84.52%</cell></row><row><cell>Ours</cell><cell>67.67%</cell><cell>99.06%</cell><cell>92.16%</cell><cell>86.30%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF SEMANTIC SEGMENTATION METHODS ON THE VALIDATION SET OF CITYSCAPES.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research was granted from ZJU-Sunny Photonics Innovation Center (No. 2020-03). This research was also funded in part through the AccessibleMaps project by the Federal Ministry of Labor and Social Affairs (BMAS) under the Grant No. 01KM151112. This research was also supported in part by Hangzhou HuanJun Technology Company Ltd. and Hangzhou SurImage Technology Company Ltd.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of unmanned aerial vehicle (UAV) usage for imagery collection in disaster research and management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Friedland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Remote Sensing for Disaster Response</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unmanned aerial vehicle (UAV) for monitoring soil erosion in morocco</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>D&amp;apos;oleire-Oltmanns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marzolff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Ries</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Creating image-based VR using a selfcalibrating fisheye lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Turkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A theory of catadioptric image formation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Characterization of the panoramic annular lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Experimental Mechanics</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Panoramic imaging block for three-dimensional space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greguss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">US Patent</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">763</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Panoramic lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Design of a panoramic annular lens with a long focal length</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-G</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparison of two panoramic front unit arrangements in design of a super wide angle panoramic annular lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Design of high resolution panoramic annular lens system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optical Sensing and Imaging Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Design and implementation of a high-performance panoramic annular lens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Optics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CFVL: A coarse-tofine vehicle localizer with omnidirectional perception across severe appearance variations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE IV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Panoramic annular SLAM with loop closure and global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.13400</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In defense of pretrained ImageNet architectures for real-time semantic segmentation of road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Or?ic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bevandic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>?egvic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">AttaNet: Attention-augmented network for fast and accurate scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep dual-resolution networks for real-time and accurate semantic segmentation of road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06085</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">See clearer at night: Towards robust nighttime semantic segmentation through day-night image conversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time fusion network for RGB-D semantic segmentation incorporating unexpected obstacle detection for road-driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE RA</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Universal semantic segmentation for fisheye urban driving images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<editor>IEEE SMC</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The omniscape dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Sekkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dupuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vasseur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICRA</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PASS: Panoramic annular semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-ITS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Omnisupervised omnidirectional semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-ITS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Capturing omni-range context for omnidirectional segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rei?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Agriculture-vision: A large aerial image database for agricultural pattern analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A relation-augmented fully convolutional network for semantic segmentation in aerial scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Aerial LaneNet: Lane-marking semantic segmentation in aerial imagery using waveletenhanced cost-sensitive symmetric fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Azimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>K?rner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Reinartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">PointFlow: Flowing semantics through points for aerial image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A toolbox for easily calibrating omnidirectional cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE IROS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unifying terrain awareness through real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE IV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
