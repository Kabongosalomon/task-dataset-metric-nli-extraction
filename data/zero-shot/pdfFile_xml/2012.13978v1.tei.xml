<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Wen</surname></persName>
							<email>zhi.wen@mail.mcgill.ca</email>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><forename type="middle">Han</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siva</forename><surname>Reddy</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">McGill University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook CIFAR AI Chair</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Mila -Quebec Artificial Intelligence Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MeDAL: Medical Abbreviation Disambiguation Dataset for Natural Language Understanding Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the biggest challenges that prohibit the use of many current NLP methods in clinical settings is the availability of public datasets. In this work, we present MeDAL, a large medical text dataset curated for abbreviation disambiguation, designed for natural language understanding pre-training in the medical domain. We pre-trained several models of common architectures on this dataset and empirically showed that such pre-training leads to improved performance and convergence speed when fine-tuning on downstream medical tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent work in mining medical texts focus on building deep learning models for different medical tasks, such as mortality prediction <ref type="bibr" target="#b5">(Grnarova et al., 2016)</ref> and diagnosis prediction <ref type="bibr">(Li et al., 2020)</ref>. However, because of the private nature of medical records, there are few large-scale, publicly available medical text datasets that are suitable for pretraining models, and real-world, private datasets are often small-scale and imbalanced. As a result, one of the biggest challenge in building deep learningbased NLP systems for biomedical corpora is the availability of public datasets <ref type="bibr" target="#b18">(Wang et al., 2018)</ref>.</p><p>To tackle this problem, we present Medical Dataset for Abbreviation Disambiguation for Natural Language Understanding (MeDAL) 1 , a large dataset of medical texts curated for the task of medical abbreviation disambiguation, which can be used for pre-training natural language understanding models. <ref type="figure" target="#fig_0">Figure 1</ref> shows an example of sample in the dataset, where the true meaning of the abbreviation 'DHF' is inferred from its context, and <ref type="figure" target="#fig_1">Figure 2</ref> shows the pretraining framework. Although this dataset can be used for building abbreviationexpansion systems, its main purpose is to enable 1 https://github.com/BruceWen120/medal ... for obtaining bovine liver DHF reductase in high yield and ... ... for obtaining bovine liver dihydrofolate reductase in high yield and ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original text:</head><p>Sample in MeDAL: Disambiguate:</p><p>... for obtaining bovine liver dihydrofolate reductase in high yield and ... dengue hemorrhagic fever dihydroxyfumarate diastolic heart failure effective pre-training and improve performance on downstream tasks during fine-tuning. The motivation behind using abbreviation disambiguation as the pre-training task is two-fold. First, abbreviations are widely used in medical records by healthcare professionals and can often be ambiguous <ref type="bibr" target="#b19">(Xu et al., 2007;</ref><ref type="bibr">Islamaj Dogan et al., 2009). 2</ref> The ubiquitousness of abbreviations poses a restriction on building deep learning models for medical tasks, such as mortality prediction <ref type="bibr" target="#b5">(Grnarova et al., 2016)</ref> and diagnosis prediction <ref type="bibr">(Li et al., 2020)</ref>.</p><p>Second, we believe that understanding natural language in a knowledge-rich domain such as medicine requires understanding of domain knowledge at some level, similar to how humans can understand medical text only after receiving medical training. The abbreviation disambiguation task enables models to use domain knowledge to understand the global and local context, as well as the possible meanings of the abbreviation in the medical domain.</p><p>Medical abbreviation disambiguation has long been studied <ref type="bibr" target="#b16">(Skreta et al., 2019;</ref><ref type="bibr">Li et al., 2019;</ref><ref type="bibr" target="#b4">Finley et al., 2016;</ref><ref type="bibr" target="#b9">Joopudi et al., 2018;</ref><ref type="bibr" target="#b7">Jin et al., 2019)</ref> and our work builds upon many of them. In particular, our data generation process is inspired by the reverse substitution tech-  nique <ref type="bibr" target="#b16">(Skreta et al., 2019;</ref><ref type="bibr" target="#b4">Finley et al., 2016)</ref>. Our work differs from them in mainly two aspects. First, instead of trying to improve performance on abbreviation disambiguation itself, we propose to use it as a pre-training task for transfer learning on other clinical tasks. Second, existing datasets for medical abbreviation disambiguation, for instance CASI <ref type="bibr" target="#b15">(Moon et al., 2014)</ref>, are small compared to datasets used for general language model pre-training, and as noted by <ref type="bibr">Li et al. (2019)</ref> some are erroneous. Thus, we chose to construct a new dataset large enough for effective pre-training.</p><p>Our main contributions are: a) we present a large dataset for pre-training on the task of medical abbreviation disambiguation. b) we provide empirical evidence of the benefit of abbreviation pre-training for a wide range of deep learning architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Abbreviation Disambiguation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dataset Summary</head><p>The MeDAL dataset consists of 14,393,619 articles and on average 3 abbreviations per article. The statistics of MeDAL are summarized in <ref type="table">Table 1</ref>.</p><p>The distribution of number of words and the distribution of number of abbreviations are shown in <ref type="figure" target="#fig_2">Figure 3a</ref> and <ref type="figure" target="#fig_2">Figure 3b</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Dataset Creation</head><p>The MeDAL dataset is created from PubMed abstracts which are released in the 2019 annual baseline. <ref type="bibr">3</ref> PubMed is a search engine that indexes scientific publications in biomedical domain. The PubMed corpus contains 18,374,626 valid abstracts with 80 words in each abstract on average.</p><p>We use reverse substitution <ref type="bibr" target="#b16">(Skreta et al., 2019)</ref> to generate samples without human labeling. We identify full terms in text that have known abbreviations and replace them with their abbreviations. For reverse substitution, mappings of abbreviations to expansions established by <ref type="bibr" target="#b21">Zhou et al. (2006)</ref> are used. Mappings where the abbreviation maps to only one expansion or the expansion maps to multiple abbreviations are discarded, resulting in 24,005 valid pairs of mappings. Among the valid mappings are 5,886 abbreviations, which means each abbreviation maps to about 4 expansions on average.</p><p>To avoid completely removing all expansions and making them unseen to models, the expansions are substituted with a pre-defined probability. For our study, expansions are substituted with a probability of 0.3, although our processing scripts allow for other values for future use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Pretraining</head><p>The task of abbreviation disambiguation is treated as a classification problem, where the classes are all possible expansions.</p><p>Considering the huge size of the dataset and the associated computational cost, a subset of 5 million data points are sampled from the complete corpus, which are split into 3 million training samples, 1 million validation samples and 1 million test samples. This subset is used throughout this study.</p><p>When creating this subset, because the distribution of true expansions is highly imbalanced, a sampling strategy is adopted which essentially removes classes in increasing order of frequency in an iterative manner. The sampling strategy works in the following way: from each class label, N C = min(F C , T ) samples that have this label are randomly selected, where F C is the frequency of that class in the unsampled dataset, and T is a threshold that is computed using Algorithm 1 such that each class can have at most T samples, and C N C is equal to the total number of samples N . The strategy iteratively removes classes, and at every iteration decreases N (which corresponds to the number of remaining samples) and L (which corresponds to the number of labels remaining). Then, the rate r is calculated based on how many classes L can fit in the remaining N if each remaining L has exactly r samples. In this way, it is ensured that the moment the current class frequency f C being iterated is greater than the desired rate r, the sampling stops.</p><p>Algorithm 1 Compute threshold T Require: array of class frequency f , N &gt; 0 Sort f in increasing order</p><formula xml:id="formula_0">L ? length(f ) N ? N for each f C ? f do N ? N ? f C L ? L ? 1 r = round(N /L) if f C ? r then return r + 1 end if end for 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Tasks</head><p>Mortality Prediction As a downstream task to evaluate models' performance in clinical settings, mortality prediction aims at predicting the mortality of a patient at the end of a hospital admission, using ICU patient notes. The mortality prediction dataset is generated from MIMIC-III (Johnson  <ref type="bibr">, 2016)</ref>. Medical notes in this MIMIC-III comprise of free-form text documents written by nurses, doctors, and many types of specialists, and are written throughout the patient's stay. Only notes written by physicians and nurses at least twenty-four hours before the end of the discharge time are used, for the goal is to accurately predict whether a patient is at risk of dying by the end of the admission. In order to balance positive and negative samples (roughly 10% of patients expire at the end of an admission) while keeping as much text diversity as possible, we sample at most four notes from each surviving patient.</p><p>The dataset generated has a total of 137,607 negative samples and 138,864 positively-labelled notes. Then, using stratified random splitting, we selected 75%/10%/15% of the patients to be included in the training/validation/test splits. As an example of the ubiquitousness of abbreviations, 'MR' appears 1,612 times in 1,366 samples in the test set alone.</p><p>Diagnosis Prediction Similar to mortality prediction, diagnosis prediction aims to predict the diagnoses associated with a hospital admission from medical notes written during the admission. The same MIMIC-III medical notes and the same splits from mortality prediction are used, with seven training samples that have no diagnosis recorded removed. In MIMIC-III, diagnoses are recorded with International Classification of Diseases (ICD) codes, which are standardized codes designed for billing purposes. We discard minor distinctions of ICD codes under the same category by taking the first three digits (for codes that start with 'E' or 'V' the first four digits) of ICD codes. 4 After grouping, there are 1,204 unique diagnosis codes.</p><p>Top-k recall is used for evaluation of models based on the similarities to real-life medical decision making <ref type="bibr" target="#b2">(Choi et al., 2015)</ref>, which is defined as the number of diagnosis codes in that admission that are present in the top k predictions of the  model, divided by the number of diagnosis codes in that admission in total. Note that since most admissions have multiple diagnoses, a small k would result in a top-k recall less than 100% even if all of the top k predictions are correct. 5 On our dataset, the highest possible top-5, top-10 and top-30 recalls are 50.17%, 79.48% and 99.88% on validation set, and 49.75%, 79.23% and 99.79% on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Models</head><p>The models are first pre-trained on the MeDAL dataset, then pre-trained weights are used to initialize models for training on the downstream tasks. We compared this training strategy with training respective models from scratch to validate the benefit of pre-training.</p><p>LSTM BiLSTM is used as a baseline model. Specifically, the BiLSTM consists of three layers with hidden size of 512. Pre-trained Fasttext model is used for word embeddings <ref type="bibr" target="#b1">(Bojanowski et al., 2017)</ref>. LSTM + Self Attention To allow for leveraging information extracted by LSTM in a flexible manner, soft attention layers are added on top of LSTM. The attention layer is largely based on the soft attention by <ref type="bibr" target="#b0">Bahdanau et al. (2014)</ref>. Its detailed formulation is included in Appendix A.</p><p>Transformers We used the pre-trained ELECTRA-small discriminator <ref type="bibr" target="#b3">(Clark et al., 2020)</ref> as an example of Transformer-based <ref type="bibr" target="#b17">(Vaswani et al., 2017)</ref> model and, since it was not pre-trained on medical text, we compared its performance with or without pre-training on abbreviation disambiguation. Task-specific Output Layer Depending on the task, the output layer can take various forms. For abbreviation disambiguation, the output layer is a fully-connected layer, whose input is the hidden vector at the location of the abbreviation from the previous layers and output space is all possible expansions. For mortality or diagnosis prediction which are not associated with any specific token, hidden vectors from the previous layers need to be first aggregated into one vector. This can be achieved by either a pooling layer or an additional attention layer with a learnable query vector. Then the output layer is a fully connected layer that takes the aggregated vector as input. The attention output layer is illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>. In preliminary experiments we found attention output layer generally improves models' performance compared to max-pooling output layer, and therefore it is used throughout the rest of the study unless otherwise noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Models' performance on the pre-training task, abbreviation disambiguation, is shown in <ref type="figure">Figure 5</ref>. As the goal is not to optimize performance on this  <ref type="table">Table 2</ref>: Results on mortality prediction. Bold font indicates the training strategy (pre-trained or from scratch) that has higher accuracy.</p><p>task, <ref type="figure">Figure 5</ref> serves to confirm the models are properly pre-trained. After pre-training, models are fine-tuned on the two downstream tasks to evaluate the benefit of pretraining. On the mortality prediction task, all three models that are pre-trained perform better than their from-scratch counterparts, shown in <ref type="table">Table 2</ref>.</p><p>The benefit of pre-training is more significant on diagnosis prediction, shown in <ref type="figure">Figure 6</ref>. Both LSTM and LSTM + self attention perform considerably better if they pre-trained. In fact, the two models' performance increase by more than 70% relatively. While for ELECTRA the gain is not as significant, pre-training leads to faster convergence during fine-tuning.</p><p>On the two downstream tasks, experiment results show that pre-training improves ELECTRA's performance even when the model is already fully pre-trained on non-medical texts and is among the state-of-the-art, and bring the other models' performance close to ELECTRA's. This shows that pre-training on the MeDAL dataset can generally improves models capabilities of understanding language in medical domain. The complete results can be found in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Discussion</head><p>In this work, we present MeDAL, a large dataset on abbreviation disambiguation, designed for pretraining natural language understanding models in the medical domain. We pre-trained a variety of models using common architectures and empirically showed that such pre-training leads to improvement in performance as well as faster convergence when fine-tuning on two downstream clinical tasks.</p><p>Following <ref type="bibr" target="#b17">Vaswani et al. (2017)</ref>, the attention layer can be expressed in terms of key, query and value vectors, denoted as k i , q i and v i respectively, where the subscript i denotes the location in the sequence. Specifically, the attention layer in our models is defined as Equation 1. </p><formula xml:id="formula_1">w ij = exp ? ij n exp ? in (1) ? ij in</formula><formula xml:id="formula_2">? ij = tanh(q i ? W a ? k j T + b)<label>(2)</label></formula><p>Here w ij is the weight assigned to location j for location i. Then the output of the attention layer at location i is computed by taking the weighted sum of value vectors at all locations, i.e. o i = n w in ? v n , where o i denotes the output of attention layer at location i. Unless otherwise noted, throughout this paper k i , q i and v i are all equal to the hidden vector at position i from the previous layer h i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Details</head><p>Except for ELECTRA, the rest of the models are trained with Adam optimizer (Kingma and Ba, 2014) with learning rate of 0.001. Text is tokenized using pre-trained Fasttext embeddings <ref type="bibr" target="#b1">(Bojanowski et al., 2017)</ref>. All LSTM modules are bi-directional and have 3 layers, with hidden size of 512. Batch size is set to 64. We experimented with various choices of batch sizes, including 32, 64, 96 and 128, and noted only minimal differences. ELEC-TRA is trained with Adam optimizer with learning rate of 0.00002 and with batch size of 16.     <ref type="figure">Figure 8</ref>: Top-30 recall on diagnosis prediction validation set. 'SA' stands for self attention layer. 'max' represents max-pooling output layer. '(s)' and '(p)' indicates whether the model is trained from scratch or pre-trained, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Experiments Results</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A sample in the MeDAL dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Diagram of using MeDAL for pre-training NLU models in medical domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Distributions of number of words and number of abbreviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Attention output layer for mortality and diagnosis prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Validation accuracy on abbreviation disambiguation. 'SA' stands for self attention layer. Top-5 recall on diagnosis prediction validation set. 'SA' stands for self attention layer. 'max' represents max-pooling output layer. '(s)' and '(p)' indicates whether the model is trained from scratch or pretrained, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 to</head><label>7</label><figDesc>Figure 8show the top-10, and top-30 recalls on diagnosis prediction, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Top-10 recall on diagnosis prediction validation set. 'SA' stands for self attention layer. 'max' represents max-pooling output layer. '(s)' and '(p)' indicates whether the model is trained from scratch or pre-trained, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Equation 1 is computed with Equation 2, where W a and b are learnable parameters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>shows the complete performance of models on diagnosis prediction.</figDesc><table><row><cell>Model</cell><cell cols="2">Top-5 recall</cell><cell cols="2">Validation performance Top-10 recall</cell><cell cols="2">Top-30 recall</cell></row><row><cell></cell><cell cols="6">Pre-trained From scratch Pre-trained From scratch Pre-trained From scratch</cell></row><row><cell>LSTM</cell><cell>26.20%</cell><cell>15.49%</cell><cell>40.00%</cell><cell>26.33%</cell><cell>63.57%</cell><cell>45.78%</cell></row><row><cell>LSTM+SA</cell><cell>28.08%</cell><cell>15.43%</cell><cell>41.75%</cell><cell>26.33%</cell><cell>65.15%</cell><cell>46.33%</cell></row><row><cell>Electra</cell><cell>28.63%</cell><cell>28.08%</cell><cell>42.35%</cell><cell>41.74%</cell><cell>65.64%</cell><cell>65.37%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Test performance</cell><cell></cell><cell></cell></row><row><cell>LSTM</cell><cell>26.94%</cell><cell>15.67%</cell><cell>40.59%</cell><cell>25.97%</cell><cell>65.49%</cell><cell>45.15%</cell></row><row><cell>LSTM+SA</cell><cell>27.47%</cell><cell>15.93%</cell><cell>41.24%</cell><cell>25.97%</cell><cell>65.86%</cell><cell>45.67%</cell></row><row><cell>Electra</cell><cell>27.88%</cell><cell>27.90%</cell><cell>41.76%</cell><cell>41.82%</cell><cell>66.23%</cell><cell>66.49%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Performance on diagnosis prediction ab a Note that, as discussed in Section 3, on our dataset the highest possible top-5, top-10 and top-30 recalls are 50.17%, 79.48% and 99.88% on validation set, and 49.75%, 79.23% and 99.79% on test set. Bold font indicates the training strategy (pre-trained or from scratch) that has higher accuracy.</figDesc><table /><note>b</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For example, 'MR' is a commonly used abbreviation which has a number of possible meanings, including 'morphinone reductase', 'magnetoresistance' and 'menstrual regulation', depending on the context.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.nlm.nih.gov/databases/download/ pubmed medline.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For example, codes 4800 to 4809 represent viral pneumonia of different causes, and they are grouped into one ICD code 480.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For instance, if an admission has 10 diagnoses codes, the highest possible top-5 recall for it would be 5/10 = 50% which is when all of the top 5 predictions are correct.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Enriching Word Vectors with Subword Information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl{_}a{_}00051</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Taha</forename><surname>Bahadori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Doctor AI: Predicting Clinical Events via Recurrent Neural Networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ELECTRA: Pretraining Text Encoders as Discriminators Rather Than Generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Comprehensive Clinical Abbreviation Disambiguation Using Machine-Labeled Training Data. AMIA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">P</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Serguei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reed</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>Mcewan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium proceedings. AMIA Symposium</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="560" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Neural Document Embeddings for Intensive Care Patient Mortality Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulina</forename><surname>Grnarova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><forename type="middle">L</forename><surname>Hyland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding PubMed(R) user search behavior through log analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islamaj Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neveol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1093/database/bap018</idno>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="18" to="018" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep Contextualized Biomedical Abbreviation Expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghua</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="88" to="96" />
		</imprint>
	</monogr>
	<note>BioNLP 2019</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengling</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">Anthony</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A convolutional route to abbreviation disambiguation in clinical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkata</forename><surname>Joopudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Dandala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murthy</forename><surname>Devarakonda</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2018.07.025</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="71" to="78" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Harlan Krumholz, and Dragomir Radev. 2019. A Neural Topic-Attention Model for Medical Term Abbreviation Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irene</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Muhammed Yavuz Nuzumlal?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwani</forename><surname>Caraballo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratheeksha</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><forename type="middle">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuening</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amir Ardalan Kalantari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Dehaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><forename type="middle">M</forename><surname>Ordog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Euijung</forename><surname>Biernacka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janet</forename><forename type="middle">E</forename><surname>Ryu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Jose Davila-Velderrain, and Manolis Kellis. 2020. Inferring multimodal latent topics from electronic health records</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aihua</forename><surname>Frye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariane</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Marelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ahuja</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41467-020-16378-3</idno>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">2536</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploiting Task-Oriented Resources to Learn Word Embeddings for Clinical Abbreviation Expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kusum</forename><forename type="middle">S</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deborah</forename><forename type="middle">L</forename><surname>Mcguinness</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W15-3810</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrim</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serguei</forename><surname>Pakhomov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><forename type="middle">B</forename><surname>James O Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Melton</surname></persName>
		</author>
		<idno type="DOI">10.1136/amiajnl-2012-001506</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="299" to="307" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Training without training data: Improving the generalizability of automated medical abbreviation disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Skreta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryan</forename><surname>Arbabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Brudno</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Attention Is All You Need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majid</forename><surname>Rastegar-Mojarad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrim</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feichen</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Afzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqun</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jbi.2017.11.011</idno>
		<title level="m">Clinical information extraction applications: A literature review</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Saeed Mehrabi, Sunghwan Sohn, and Hongfang Liu</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A study of abbreviations in clinical notes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">D</forename><surname>Stetson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Amia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">Annual Symposium proceedings / AMIA Symposium. AMIA Symposium</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="821" to="825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ADAM: another database of abbreviations in MED-LINE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Torvik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Smalheiser</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btl480</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="2813" to="2818" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
