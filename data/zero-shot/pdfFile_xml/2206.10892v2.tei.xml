<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">I 2 R-Net: Intra-and Inter-Human Relation Network for Multi-Person Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjin</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglin</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
							<email>pengfei@stu.xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meihong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zeng</surname></persName>
							<email>zengming@xmu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">I 2 R-Net: Intra-and Inter-Human Relation Network for Multi-Person Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present the Intra-and Inter-Human Relation Networks (I 2 R-Net) for Multi-Person Pose Estimation. It involves two basic modules. First, the Intra-Human Relation Module operates on a single person and aims to capture Intra-Human dependencies. Second, the Inter-Human Relation Module considers the relation between multiple instances and focuses on capturing Inter-Human interactions. The Inter-Human Relation Module can be designed very lightweight by reducing the resolution of feature map, yet learn useful relation information to significantly boost the performance of the Intra-Human Relation Module. Even without bells and whistles, our method can compete or outperform current competition winners. We conduct extensive experiments on COCO, CrowdPose, and OCHuman datasets. The results demonstrate that the proposed model surpasses all the state-of-the-art methods. Concretely, the proposed method achieves 77.4% AP on CrowPose dataset and 67.8% AP on OCHuman dataset respectively, outperforming existing methods by a large margin. Additionally, the ablation study and visualization analysis also prove the effectiveness of our model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>2D Multi-person Pose Estimation (MPPE) aims to detect and localize the human keypoints for all persons appearing in a given image. Since human pose provides abundant structural and motion information, MPPE has attracted attentions in wide applications, such as human activity understanding, human-object interaction, avatar animation, etc.</p><p>Current MPPE methods can be divided into two fashions, i.e., top-down and bottom-up. Top-down methods <ref type="bibr" target="#b2">[He et al., 2017;</ref><ref type="bibr" target="#b1">Xiao et al., 2018;</ref><ref type="bibr" target="#b1">Fang et al., 2017;</ref><ref type="bibr">Sun et al., 2019;</ref> detect bounding box for each human, then estimate the pose of each human separately. Bottom-up methods <ref type="bibr" target="#b0">[Cao et al., 2019;</ref><ref type="bibr" target="#b4">Newell et al., 2016a;</ref><ref type="bibr">Cheng et al., * Equal Contribution.</ref> ? Corresponding Author. 2020; <ref type="bibr" target="#b4">Papandreou et al., 2018;</ref><ref type="bibr" target="#b2">Jin et al., 2020]</ref> localize all joints in the image, and then group them into individuals. Despite having achieved great performance in MPPE, the above methods produce inferior results in crowded scenes, due to overlapping, self-occlusion, and a large variety of appearances and poses. Top-down methods treat each person separately, neglecting the informative cues of mutual interaction among persons. On the other hand, bottom-up methods jointly detect joints of all persons in an image, potentially correlating joints across persons, but the relationships between persons are vague and agnostic. Recently, several pioneers address the above challenges via decoupling poses <ref type="bibr" target="#b4">Qiu et al., 2020]</ref> or fusing multiple possible poses <ref type="bibr">[Wang et al., 2021a</ref>] at a single person. However, these methods still do not fully take the multiple-human correlations into account, leaving these important interactions unexplored. This work investigates how to leverage the correlations of intra-human or cross-human, e.g., persons with similar poses or persons who closely interact with each other, to improve the accuracy of multi-person pose estimation. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>(a), two people are hugging each other. Their poses show strong intra-human information (e.g., left-right pairs and two keypoints of the same limb) and obvious inter-human relations. In <ref type="figure" target="#fig_0">Figure 1(b)</ref>, these persons are dancing together, indicating that their poses are similar, also providing interhuman cues for more accurate pose estimation, especially in the occluded regions or low-resolution regions.</p><p>Based on above observations, we propose a novel twostage Intra-and Inter-Human Relation Networks (I 2 R-Net) for MPPE. The first stage targets at learning the dependencies among body parts of a single person. It operates at high resolution and generates the locations of small parts, such as the eyes, wrists, and ankles. The second stage operates multiperson collaboratively at low resolution. It aims to capture interactions between instances, like touching hands, joining arms, and crossing legs. It's worth noting that each instance is represented by a low-resolution feature map rather than merely a vector. By doing this, we can obtain variant responses from regions, since the cross-instance interaction depends on spatial semantic information. Meanwhile, the lowresolution feature map significantly reduces the computation cost of the second stage but is sufficient to capture useful correlation information. We call the first Intra-Human Relation Module, and the second Inter-Human Relation Module. The two stages are sequentially stacked with skip-connection.</p><p>This two-stage framework is structure flexible and function specialized for MPPE. The Intra-Human Relation Module can be an arbitrary single-person pose estimation method, aiming to explore part-level patterns for keypoint detection. Moreover, the Inter-Human Relation Module can be implemented by any non-local model, to pay more attention to correlations in semantic features. The two modules are able to work collaboratively to infer multi-person poses, because the first module offers high-quality pose information to the next module, while the second module aids the first to eliminate ambiguities in occluded or less discriminative regions.</p><p>We evaluate our method on common benchmarks for crowded scenes including CrowdPose and OCHuman. Experiments have proved that our method surpasses all stateof-the-art approaches by large margins. We achieve 77.4% AP on CrowdPose, outperforming HRFormer-B by 5.0%. We achieve 67.8% AP on OCHuman dataset, outperforming TransPose-H by 5.5%. In addition, we bring improvements on COCO dataset, which means our method can generally work in non-crowded scenes.</p><p>In summary, the contributions of our work are as follows:</p><p>? We propose a two-stage MPPE framework that not only relates each body part in a single person, but also builds connections among multi-person in the images. The proposed method bridges the gap between pose estimation and human-human interactions.</p><p>? The framework is designed to be flexible. The module coping with intra-human relations can be arbitrary pose estimation method. While the module that models interhuman relations is very lightweight.</p><p>? Extensive experiments show that, without bells and whistles, our method significantly surpasses state-ofthe-art methods on challenging datasets like CrowdPose, OCHuman, and COCO. Bottom-up Framework. Bottom-up methods first predict all identity-free body keypoints from an uncropped image, then group these body keypoints into different individuals. Most of them adopt heatmap for keypoint detection, and focus on how to identify and group keypoints to the corresponding person. OpenPose <ref type="bibr" target="#b0">[Cao et al., 2019]</ref> proposed the part affinity field to reveal the relationship between two keypoints in the same limb. During the grouping stage, after calculating the line integral score between two candidate keypoints, the pair with the highest score is associated. This approach is denoted as PAF family. Another popular fashion is Associative Embedding, called AE <ref type="bibr" target="#b4">[Newell et al., 2016a;</ref><ref type="bibr" target="#b1">Cheng et al., 2020]</ref>. They learn the tag embeddings for each person and group keypoints into individuals by clustering the tag.</p><p>Top-down methods estimate poses one by one, without taking the relationship between instances into account. While bottom-up methods detect all poses together, the relationship cues are vague. In contrast, our work pays attention to capturing the hierarchical interactions in the multi-person scenario to promote pose estimation performance. Thus, we provide a novel two-stage framework to focus on modeling part-level and instance-level relationships for MPPE.</p><p>Human Pose Estimation in Crowd. The performance of previous work decrease as the number of human increases. Even though top-down methods achieve higher performance than bottom-up methods, they still fail in crowded scenes due to occlusions. Thus, CrowdPose dataset  and OCHuman dataset  have been proposed to encourage researchers to study this challenging problem. Existing methods <ref type="bibr" target="#b4">Qiu et al., 2020]</ref> attempt to decompose coupled poses in crowd.</p><p>In this paper, however, the proposed Inter-Human Relation Module is designed to conquer this dilemma by leveraging the relationships between instances. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head Head</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-Human</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Vision Transformer</head><p>Transformer is an attention-based model, and has been stud- , 2021] utilize Transformer to calculate relations, yet they aim to solve 3d motion prediction or reconstruction where the relationships between different persons are intuitive. In contrast, our work is the first to incorporate multi-person correlations in single image 2D Pose Estimation, where the relationship is hard to build and we propose a novel module to solve this. Moreover, instead of using a well-designed framework for only building inter-human relationships, we propose a flexible, lightweight, and general module for this purpose that can be easily integrated into current or future single-person pose estimation frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of I 2 R-Net</head><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the framework takes the detected and cropped human image patches in the image as input. In the first stage, Intra-Human Relation Network (Section 3.2) extracts the feature of each human patch separately and preserves relatively high-resolution feature representation. In the second stage, Inter-Human Relation Module (Section 3.3) exchanges information of the per-person features extracted by the first stage, and the per-person features should be downsampled beforehand to reduce computation cost. Finally, the two stages are stacked with a skip-connection to fuse the high-resolution intra-human information and the lowresolution inter-human information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intra-Human Relation Module</head><p>As the goal of Intra-Human Relation Module is to learn semantic representations of the human patches and capture the dependencies between body parts, it operates at patches of cropped human images. It can be an arbitrary single-person human pose estimation method. One key factor is the input and output resolutions, which is a trade-off between performance and computational consumption. Following the recent works <ref type="bibr" target="#b6">Yuan et al., 2021]</ref>, we adopt two settings, i.e., 256 ? 192 of input and 64 ? 48 of output as default, and 384 ? 288 of input and 96 ? 72 of output for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inter-Human Relation Module</head><p>For the task of Multi-Human Pose Estimation, we devise an Inter-Human Relation Module to collaboratively infer the pose of each person via explicitly modeling cross-human correlation. The Inter-Human Relation Module is lightweight yet effective. We illustrate a vanilla network example in <ref type="figure" target="#fig_3">Figure 3</ref>. Given an image I with N persons, the Intra-Human Relation Module (e.g., HRNet, TransPose) locates all persons and extracts their features P 1 , P 2 , ..., P N from raw patches. These features contain pose information of each person. The Inter-Human Relation Module takes these features as input and exchanges pose information among each instance. To fully capture the correlations between different people, we adopt the recently popular Transformer blocks. It is worth noting that, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>, the Inter-Human relation module not only exchanges information across instances, but also correlates information among parts in each instance. This ensures that even using only an Inter-Human Relation Module (without the Intra-stage, and the input per-person features are extracted by a backbone), it can still achieve state-of-the-art performance in our experiments.</p><p>Concretely, suppose the output feature maps of first stage is P i ? R h?w?d , i ? [1, N ], where h ? w is the feature map resolution, d is the channel dimension, N is the number of persons. We use the max pooling operator with kernel size and stride set to R to operate on P i and get resulted downsampled feature map P i ? R h/R?w/R?d . Then we flatten all the features in P i and concatenate them to get a sequence with a length of L = h/R ? w/R ? N :</p><formula xml:id="formula_0">P i = Downsample(P i ) (1) X = Concat[Flatten(P i )], i ? [1, N ]<label>(2)</label></formula><p>where X ? R L?d . Subsequently, X is fed into M (set to 6 for vanilla network) stacked Transformer blocks, aiming to compute correlations between different instances. The formulation of attention operation is given as follows,</p><formula xml:id="formula_1">Attention(X) = Softmax (W q X)(W k X) T ? d W v X (3) where W q , W k , W v ? R d?d are learnable parameters.</formula><p>Finally, we use a standard deconvolution layer to upsample the output embeddings X to higher resolution and employ a keypoint head to predict K types of keypoint heatmaps?, i.e.,? = Head(Upsample(X )).</p><p>Following the previous methods, we adopt the MSE loss to supervise predicted keypoints heatmaps? with the groundtruth H, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K k=1</head><p>? k ? H k 2 . In our framework, the Intra-Human loss L S serves as an intermediate supervision.</p><p>Thus, after summing up instance-level stage loss L M , the total loss is L = L S +?L M . In our experiments, we set ? = 1.</p><p>Our proposed Inter-Human Relation Module is general and flexible, and it can be instantiated with different attention methods (e.g., Non-local networks, Transformer Layers) and implementation specifics. Input of Inter-Human Relation Module. To reduce the computation complexity, we downsample the output feature map of the Intra-Human Relation module with the ratio R and restrict the number of persons to N . Suppose the input feature map size is h ? w, the downsampled feature map is h ? w , then we flatten and concatenate all these elements together. After that, we perform self-attention calculations on all these feature elements (shown as <ref type="figure" target="#fig_3">Figure 3)</ref>. The correlations between poses of different persons are captured. Since N is an important hyper-parameter for the computation complexity. We set a max person number N for each input image differently on different datasets. Specifically, we adopt N = 6 for CrowdPose, N = 3 for OCHuman and N = 4 for COCO, respectively. Experimental results prove that our input setting of the Inter-Human Relation Module achieves a strong performance while introducing little extra computation cost. Person Selection Mechanism. In fact, the number of people usually varies in images. To this end, we introduce the person selection mechanism. As mentioned above, we set N as the maximum number of person patches.</p><p>During the training phase, we use the patch selection mechanism to facilitate the network training. For cases with persons less than N in an input image, we simply calculate correlations between these persons or even a single person. But for parallel computing, we pad noise features to increase the person number to N and mask them out to prevent self-attention calculation in Transformer blocks. When the number exceeds N , we randomly select a person as the target and fill the patches with the N ? 1 nearest neighbors.</p><p>In the testing stage, we apply the sliding window strategy for the inter-human relation module (for the analysis of the maximum person number N , please see the ablation study). The sliding window divides all persons in the image into multiple N -person groups, and then we apply the inter-human relation module within each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>CrowdPose  is one of the most commonly used datasets in multi-person pose estimation task. The images in this dataset are heavily crowded and challenging. It contains 10K, 2K and 8K images for training set, validation set and testing set, respectively. Following <ref type="bibr" target="#b1">[Cheng et al., 2020;</ref><ref type="bibr" target="#b2">Geng et al., 2021]</ref>, we train our models on the training set and validation set, and evaluate on the testing set.</p><p>OCHuman  is a recently proposed benchmark to examine the limitations of MPPE in highly occluded scenarios. It consists of 4731 images, including 2500 images for validation and 2231 images for testing. Following <ref type="bibr" target="#b4">[Qiu et al., 2020]</ref>, we train models on the validation set and report the performance on the testing set.</p><p>COCO <ref type="bibr" target="#b4">[Lin et al., 2014]</ref> is one popular benchmark for keypoint detection. Although it contains few crowded scenes, we use it to validate the generation ability of our method for non-crowded scenes. We train the models on the training set and report the results on the validation set.</p><p>Evaluation Metrics. We follow the standard evaluation procedure and use OKS-based metrics for pose estimation <ref type="bibr" target="#b4">[Lin et al., 2014]</ref>. We report average precision (AP), average recall (AR) with different thresholds. Following , on CrowdPose, we also report AP easy , AP medium and AP hard , which refers to the performance under different crowd index.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use Adam optimizer and the cosine annealing learning rate decay from 1e-4 to 1e-5. We also adopt the coordinate decoding strategy in  to reduce the quantisation error when decoding from downscaled heatmaps. For the training batch size, we choose 64 for I 2 R-Net at 256?192 resolution, and 32 at 384 ? 288 resolution. The length of each token channel d is 96. The number of transformer layers M = 6 in the vanilla I 2 R-Net, and M = 2 for other models. Each experiment takes 8? 32G-V100 GPUs for training.    Results on the COCO pose estimation validation set. As shown in <ref type="table" target="#tab_2">Table 2</ref>, I 2 R-Net achieves 77.3% AP, which is competitive against state-of-the-art methods. The performance gain on COCO is not that large compared with OCHuman and CrowdPose. This is because that COCO contains around 48% single-person images. Hence, our method for multiperson scenarios is not able to give full play. But this result demonstrates that our method also works well in noncrowded scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head><p>Effectiveness of Inter-Human Relation Module. On the CrowdPose and OCHuman, I 2 R-Net outperforms all existing methods at the same resolution. On COCO datasets, I 2 R-Net also achieves competitive performance against state-of-theart methods, especially outperforming on AP M and AP L . As shown in <ref type="table">Table 1</ref>, under our two-stage framework, TransPose can significantly surpass HRNet ? with Inter-Human Relation Module even at lower resolution. This demonstrates that our Inter-Human Relation Module can bring significant performance improvements.</p><p>As is discussed in Section 3.3, the relation modeling capability of inter-human relation module is not limited to crossinstance, but can also relates parts in-instance. This can be evidenced in <ref type="table">Table 1</ref>, 2 and 3, where the vanilla Inter-Human Relation (without intra-stage, and extract per-person feature using a backbone) achieves comparable performance with state-of-the-art methods. Instance patch number. An important factor of I 2 R-Net is the number of instances fed into the Inter-human relation module. We compare the results of different instance numbers on CrowdPose in <ref type="table" target="#tab_5">Table 4a</ref>. We can observe that the AP value tends to be saturated when N is 4 and reaches the highest when N is 6. A higher or lower number of patches will cause a drop in AP value. This is because too few patches may lose information between persons, while too many patches introduce unnecessary cross-human correlations. Effectiveness of Intermediate-supervision. In our framework, we use MSE loss in the first stage as intermediatesupervision (denoted as IS). As shown in the second and last rows in <ref type="table" target="#tab_5">Table 4b</ref>, the intermediate-supervision improves the performance. Position encoding. We also try to add position encoding into I 2 R-Net. As shown in <ref type="table" target="#tab_5">Table 4b</ref>, neither using sinusoidal mode nor encoding the global position as a positional encoding can improve the results. One possible reason is that the Intra-Human Relation Module has learned the local correlations between parts of each person, and the global interactions between instances are position-independent. Person selection mechanism. We compare two strategies for person selection. One is the nearest neighbor strategy, which randomly selects a target person and selects N ? 1 closest neighbors; the other is to completely randomly select N persons. As shown in <ref type="table" target="#tab_5">Table 4c</ref>, there are no significantly difference between them, which means the multi-person interaction is position-free. Token size setting. In <ref type="table" target="#tab_5">Table 4d</ref>, the setting of 16 ? 12 ? N achieves a better result. It shows that our method can learn enough inter-human relations at low resolution, but 1 ? 1 ? N leads to low precision due to the lack of spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Analysis</head><p>In <ref type="figure">Figure 4</ref>, we visualize our results compared with two competitive state-of-the-art methods (TransPose and HRFormer). The proposed method is able to handle the heavily crowded cases and localize the key points correctly. Even though in occluded scenes, we can also infer plausible poses.</p><p>By visualizing the attention maps between persons, the performance gain can be justified. As shown in <ref type="figure">Figure 5</ref>, the incorrect localized keypoints are corrected by highly related regions of the other person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Size and Computation Cost</head><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, Inter-Human Relation Module is lightweight yet effective. In the two-stage framework,   for instance, HRFormer with Inter-Human Relation Module merely increased the parameters by 0.5M and FLOPS by 0.6G. This demonstrates that our Inter-Human Relation Module cost a few and acceptable extra computations but achieves significant improvement for multi-person estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Failure Cases</head><p>Although our approach outperforms the state-of-the-art methods, we still suffer from several extremely challenging cases. Like previous work, our method may fail in cases where only part of the person is visible <ref type="figure" target="#fig_5">(Figure 6(a)</ref>) or the inter-human relationships are insignificantly correlated <ref type="figure" target="#fig_5">(Figure 6(b)</ref>). All these cases with poor relations may lead to failure predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks</head><p>This work investigates and evidences that collaboratively estimating poses via modeling multi-person correlation promotes pose estimation. We propose a flexible and efficient framework to integrate intra-human and inter-human information, leading to superior results than state-of-the-art methods. Inspired by the idea of this work, promising future work is to extend the inter-human relation from the spatial dimension to the temporal dimension. Another interesting direction is to explore the performance gain via correlating the humanobject relationships in various vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustrations of Intra-and Inter-Human Relations. (a) the intra-human relation is marked as the green arrow (left-right pairs and two keypoints of the same limb), while the inter-human relation is denoted as the yellow arrow (the same type of keypoints in different individuals).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>I 2 R-Net Framework. The framework includes an Intra-Human Relation Module and an Inter-Human Relation Module. The Intra-Human Relation Module can apply current single-person pose estimation methods flexibly. The Inter-Human Relation Module is lightweight by downsampling the feature from Intra-Human Relation Modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ied in Neural Language Processing in recent years, like BERT [Devlin et al., 2018] and GPT [Radford et al., 2019]. With the effectiveness of Transformer, recently, an increasing number of works [Parmar et al., 2018; Bello et al., 2019; Touvron et al., 2021] use Vision Transformer for computer vision tasks. Several previous works apply Transformer to 2D pose estimation. For example, Transpose [Yang et al., 2021] proposes a pose estimator with attention layers to capture and reveal the spatial dependencies between 2D heatmaps. HRFormer [Yuan et al., 2021] learns high-resolution representations via self-attention schema. However, none of them take human relations into account. Although recent work [Wang et al., 2021b; Mihai et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A Vanilla Inter-Human Network. The vanilla network uses Inter-Human Relation Module only with a conventional CNNbased model to extract features of each person cropped from the input image. The extracted features are downsampled, flattened, and concatenated before being passed into Inter-Human Relation Module. The output features of the Inter-Human Relation Module are upsampled to higher resolution, and then fed into a keypoint prediction head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visualization Comparisons. From left to right, each example shows the results of TransPose, HRFormer, and I 2 R-Net. The incorrect regions are marked with white circles. Inter-Human Attention Visualization. The incorrect region (marked in the red box) is corrected with the aid of higher response regions, which are indicated by the orange arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Failure cases. The incorrect regions are marked with white circles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>MethodAP AP 50 AP 75 AR AR 50 AR 75 AP easy AP medium AP hardTable 1: Comparisons with state-of-the-art methods on the CrowdPose dataset. The default input resolution for experiments is 256 ? 192, while ? denotes the input resolution is 384 ? 288. AP 75 AP M AP L AR AR M AR L</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Bottom-up</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>OpenPose [Cao et al., 2019]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.7</cell><cell>58.7</cell><cell>32.3</cell></row><row><cell>HigherHRNet [Cheng et al., 2020]</cell><cell cols="2">65.9 86.4</cell><cell>70.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.3</cell><cell>66.5</cell><cell>57.9</cell></row><row><cell cols="3">HigherHRNet Multi-scale [Cheng et al., 2020] 67.6 87.4</cell><cell>72.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.8</cell><cell>68.1</cell><cell>58.9</cell></row><row><cell>SPM [Nie et al., 2019]</cell><cell cols="2">63.7 85.9</cell><cell>68.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>70.3</cell><cell>64.5</cell><cell>55.7</cell></row><row><cell>DEKR [Geng et al., 2021]</cell><cell cols="2">65.7 85.7</cell><cell>70.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>73.0</cell><cell>66.4</cell><cell>57.5</cell></row><row><cell>DEKR Multi-scale [Geng et al., 2021]</cell><cell cols="2">67.0 85.4</cell><cell>72.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.5</cell><cell>68.0</cell><cell>56.9</cell></row><row><cell>PINet [Wang et al., 2021a]</cell><cell cols="2">68.9 88.7</cell><cell>74.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>75.4</cell><cell>69.6</cell><cell>61.5</cell></row><row><cell>PINet Multi-scale [Wang et al., 2021a]</cell><cell cols="2">69.9 89.1</cell><cell>75.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.4</cell><cell>70.5</cell><cell>62.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Top-down</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mask R-CNN [He et al., 2017]</cell><cell cols="2">57.2 83.5</cell><cell cols="3">60.3 65.9 89.5</cell><cell>69.4</cell><cell>69.4</cell><cell>57.9</cell><cell>45.8</cell></row><row><cell>AlphaPose [Fang et al., 2017]</cell><cell cols="2">61.0 81.3</cell><cell cols="3">66.0 67.6 86.7</cell><cell>71.8</cell><cell>71.2</cell><cell>61.4</cell><cell>51.1</cell></row><row><cell>Simple baseline [Xiao et al., 2018]</cell><cell cols="2">60.8 81.4</cell><cell cols="3">65.7 67.3 86.3</cell><cell>71.8</cell><cell>71.4</cell><cell>61.2</cell><cell>51.2</cell></row><row><cell>CrowdPose [Li et al., 2019]</cell><cell cols="2">66.0 84.2</cell><cell cols="3">71.5 72.7 89.5</cell><cell>77.5</cell><cell>75.5</cell><cell>66.3</cell><cell>57.4</cell></row><row><cell>OPEC-Net [Qiu et al., 2020]</cell><cell cols="2">70.6 86.8</cell><cell>75.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HRNet [Sun et al., 2019]</cell><cell cols="2">71.3 91.1</cell><cell>77.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.5</cell><cell>71.4</cell><cell>62.5</cell></row><row><cell>HRNet ? [Sun et al., 2019]</cell><cell cols="2">72.8 92.1</cell><cell>78.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.3</cell><cell>73.3</cell><cell>65.5</cell></row><row><cell>TransPose-H [Yang et al., 2021]</cell><cell cols="2">71.8 91.5</cell><cell cols="3">77.8 75.2 92.7</cell><cell>80.4</cell><cell>79.5</cell><cell>72.9</cell><cell>62.2</cell></row><row><cell>HRFormer-B [Yuan et al., 2021]</cell><cell cols="2">72.4 91.5</cell><cell cols="3">77.9 75.6 92.7</cell><cell>81.0</cell><cell>80.0</cell><cell>73.5</cell><cell>62.4</cell></row><row><cell>I 2 R-Net (Vanilla version, 1st stage: HRNet)</cell><cell cols="2">72.3 92.4</cell><cell cols="3">77.9 76.5 93.2</cell><cell>81.9</cell><cell>79.9</cell><cell>73.2</cell><cell>62.8</cell></row><row><cell>I 2 R-Net (1st stage: TransPose-H)</cell><cell cols="2">76.3 93.5</cell><cell cols="3">82.2 79.1 94.0</cell><cell>84.4</cell><cell>83.2</cell><cell>77.0</cell><cell>67.4</cell></row><row><cell>I 2 R-Net (1st stage: HRFormer-B)</cell><cell cols="2">77.4 93.6</cell><cell cols="3">83.3 80.3 94.5</cell><cell>85.5</cell><cell>83.8</cell><cell>78.1</cell><cell>69.3</cell></row><row><cell cols="10">Method AP AP 50 HRNet-W32 [Sun et al., 2019] Input size #Params FLOPs 256?192 28.5M 7.1G 74.4 90.5 81.9 70.8 81.0 78.9 75.7 85.8</cell></row><row><cell>HRNet-W48 [Sun et al., 2019]</cell><cell>256?192</cell><cell>63.6M</cell><cell></cell><cell>14.6G</cell><cell cols="5">75.1 90.6 82.2 71.5 81.8 80.4 76.2 86.4</cell></row><row><cell cols="2">I 2 R-Net (Vanilla version, 1st stage:HRNet-W48-S) 256?192</cell><cell>18.0M</cell><cell></cell><cell>10.2G</cell><cell cols="5">75.3 90.2 81.9 71.7 82.4 80.5 76.1 86.8</cell></row><row><cell>TransPose-H [Yang et al., 2021]</cell><cell>256?192</cell><cell>17.5M</cell><cell></cell><cell>21.8G</cell><cell cols="5">75.8 90.1 82.1 71.9 82.8 80.8 76.4 87.2</cell></row><row><cell>I 2 R-Net (1st stage:TransPose-H)</cell><cell cols="9">256?192 18.0(+0.5)M 22.7(+0.9)G 75.8 90.4 82.1 72.0 82.9 80.9 76.6 87.3</cell></row><row><cell>HRFormer-B [Yuan et al., 2021]</cell><cell>256?192</cell><cell>43.2M</cell><cell></cell><cell>12.2G</cell><cell cols="5">75.6 90.8 82.8 71.7 82.6 80.8 76.5 87.2</cell></row><row><cell>I 2 R-Net (1st stage:HRFormer-B )</cell><cell cols="9">256?192 43.7(+0.5)M 12.8(+0.6)G 76.4 90.8 83.2 72.3 83.7 81.4 76.9 88.1</cell></row><row><cell>HRFormer-B [Yuan et al., 2021]</cell><cell>384?288</cell><cell>43.2M</cell><cell></cell><cell>26.8G</cell><cell cols="5">77.1 90.9 83.5 69.8 80.5 82.0 77.7 88.3</cell></row><row><cell>I 2 R-Net (1st stage:HRFormer-B )</cell><cell cols="9">384?288 43.7(+0.5)M 29.6(+2.8)G 77.3 91.0 83.6 73.0 84.5 82.1 77.7 88.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on the COCO pose estimation validation set. The results of methods marked with are obtained by running the official code and model. Following TransPose, HRNet-W48-S only uses early 3 stages of HRNet-W48.</figDesc><table><row><cell>Method</cell><cell>AP AP 50 AP 75</cell></row><row><cell>Bottom-up</cell><cell></cell></row><row><cell>SPM [Nie et al., 2019]</cell><cell>47.6 67.5 53.2</cell></row><row><cell>HigherHRNet [Cheng et al., 2020]</cell><cell>27.7 66.9 15.9</cell></row><row><cell>DEKR [Geng et al., 2021]</cell><cell>52.2 69.9 56.6</cell></row><row><cell>PINet [Wang et al., 2021a]</cell><cell>59.8 74.9 65.9</cell></row><row><cell>Top-down</cell><cell></cell></row><row><cell>Mask R-CNN [He et al., 2017]</cell><cell>20.2 33.2 24.5</cell></row><row><cell>SimplePose [Xiao et al., 2018]</cell><cell>24.1 37.4 26.8</cell></row><row><cell>CrowdPose [Li et al., 2019]</cell><cell>27.5 40.8 29.9</cell></row><row><cell>OPEC-Net [Qiu et al., 2020]</cell><cell>29.1 41.3 31.4</cell></row><row><cell>HRFormer-B [Yuan et al., 2021]</cell><cell>62.1 81.4 67.1</cell></row><row><cell>ThansPose-H [Yang et al., 2021]</cell><cell>62.3 82.7 67.1</cell></row><row><cell cols="2">I 2 R-Net (Vanilla version, 1st stage: HRNet) 64.3 85.0 69.2</cell></row><row><cell>I 2 R-Net (1st stage: HRFormer-B)</cell><cell>66.5 83.8 71.4</cell></row><row><cell>I 2 R-Net (1st stage:TransPose-H)</cell><cell>67.8 85.0 72.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparisons with state-of-the-art methods on the OCHuman testing set after training on OCHuman validation set.</figDesc><table><row><cell>4.3 Compare with the State-of-the-art Methods</cell></row><row><cell>Results on the CrowPose testing set. As shown in Ta-</cell></row><row><cell>ble 1, thanks to our Inter-human relation module, the I 2 R-</cell></row><row><cell>Net (1st stage: TransPose-H) achieves 76.3% AP, which out-</cell></row><row><cell>performs TransPose-H. Moreover, the I 2 R-Net (1st stage:</cell></row><row><cell>HRFormer-B) achieves 77.4% AP at 256 ? 192 resolution,</cell></row><row><cell>even surpassing the performance of HRNet ? at resolution of</cell></row><row><cell>384?288. This demonstrates that our method is very suitable</cell></row><row><cell>for crowded scenes, and performs better than other methods</cell></row><row><cell>even without bells and whistles.</cell></row><row><cell>Results on the OCHuman testing set. On OCHuman</cell></row><row><cell>dataset, our method also obtains superior performance. The</cell></row><row><cell>results are shown in Table 3. The vanilla I 2 R-Net achieves</cell></row><row><cell>64.3% AP on OCHuman test set, which is comparable with</cell></row><row><cell>state-of-the-art methods. Furthermore, it is clear in Ta-</cell></row><row><cell>ble 3 that, equipped with the Inter-Human Relation Module,</cell></row><row><cell>HRFormer-B and TransPose-H are significantly improved,</cell></row><row><cell>reaching 66.5% and 67.8% AP, respectively.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of our I 2 R-Net with different settings.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by NSFC (No. 62072382)   and Yango Charitable Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Openpose: realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Attention augmented convolutional networks. intelligence</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<idno type="arXiv">arXiv:1810.04805</idno>
	</analytic>
	<monogr>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<editor>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
	</analytic>
	<monogr>
		<title level="m">Alexandre Sablayrolles, and Herv? J?gou</title>
		<editor>Wang et al., 2021a] Dongkai Wang</editor>
		<meeting><address><addrLine>Matthieu Cord, Matthijs Douze, Francisco Massa</addrLine></address></meeting>
		<imprint>
			<publisher>Hugo Touvron</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transpose: Keypoint localization via transformer</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hrformer: High-resolution transformer for dense prediction. NeurIPS, 2021</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Song-Hai Zhang, Ruilong Li, Xin Dong, Paul Rosin, Zixi Cai, Xi Han, Dingcheng Yang, Haozhi Huang, and Shi-Min Hu</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
