<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Engineering Research Center for Multimedia Software</orgName>
								<orgName type="department" key="dep2">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhua</forename><surname>Liu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">School of Printing and Packaging</orgName>
								<orgName type="department" key="dep2">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengxiang</forename><surname>He</surname></persName>
							<email>hefengxiang@jd.com</email>
							<affiliation key="aff3">
								<orgName type="institution">JD Explore Academy</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyue</forename><surname>Wang</surname></persName>
							<email>chaoyue.wang@sydney.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">Faculty of Engineering</orgName>
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
							<email>dubo@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">National Engineering Research Center for Multimedia Software</orgName>
								<orgName type="department" key="dep2">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep3">School of Computer Science</orgName>
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://github.com/adeline-cs/GTR.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing Scene Text Recognition (STR) methods typically use a language model to optimize the joint probability of the 1D character sequence predicted by a visual recognition (VR) model, which ignore the 2D spatial context of visual semantics within and between character instances, making them not generalize well to arbitrary shape scene text. To address this issue, we make the first attempt to perform textual reasoning based on visual semantics in this paper. Technically, given the character segmentation maps predicted by a VR model, we construct a subgraph for each instance, where nodes represent the pixels in it and edges are added between nodes based on their spatial similarity. Then, these subgraphs are sequentially connected by their root nodes and merged into a complete graph. Based on this graph, we devise a graph convolutional network for textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR can be easily plugged in representative STR models to improve their performance owing to better textual reasoning. Specifically, we construct our model, namely S-GTR, by paralleling GTR to the language model in a segmentation-based STR baseline, which can effectively exploit the visual-linguistic complementarity via mutual learning. S-GTR sets new state-of-the-art on six challenging STR benchmarks and generalizes well to multi-linguistic datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene Text Recognition (STR) remains a fundamental and active research topic in computer vision for its wide applications <ref type="bibr" target="#b40">(Zhang and Tao 2020</ref>). However, this task remains challenging in real-world deployment, since the recognition results are highly influenced by various factors, such as complex background, irregular shapes, diverse textures.</p><p>Existing methods mainly treat STR as a visual recognition (VR) task and perform character-level recognition on input images, including visual-text sequence translationbased methods <ref type="bibr" target="#b37">(Yang et al. 2017;</ref><ref type="bibr" target="#b28">Shi et al. 2018;</ref><ref type="bibr" target="#b0">Baek et al. 2019;</ref><ref type="bibr" target="#b19">Li et al. 2019;</ref><ref type="bibr" target="#b21">Litman et al. 2020</ref>) and semantic segmentation-based methods   GTR performs textual reasoning based on the visual semantics generated by VR to address the irregular and blurry text. The ground truth label is "crocs" and wrong predictions are marked in red. 2020a). Although these methods obtain reasonable performance on identifying individual characters, they ignore vital global textual representations, making it extremely hard to give robust recognition outcomes in real-world scenarios. For global textual modeling, existing efforts <ref type="bibr" target="#b25">(Qiao et al. 2020;</ref><ref type="bibr" target="#b38">Yu et al. 2020;</ref><ref type="bibr" target="#b7">Fang et al. 2021)</ref> have been made to leverage a language model (LM) <ref type="bibr" target="#b12">(Jaderberg et al. 2014a)</ref> to optimize the joint probability of the character sequence predicted by the VR model. Though this strategy can correct mistaken predictions with linguistic context, it is hard to generalize to arbitrary texts and ambiguous cases. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, for the irregular and blurry text, even LM could not make correct predictions. Other than linguistic cues, spatial context could also contribute to global textual modeling of character sequences but few methods explore in this direction. Hence, existing models have difficulty in producing satisfactory results on irregular texts in diverse fonts and shapes as well as with blur and occlusions.</p><p>In this paper, we fill this gap with a novel Graph-based Textual Reasoning (GTR) model for introducing spatial context into the text reasoning stage. Given the character instances recognized by the VR model as well as the derived order relations between them, we first set up a twolevel graph to establish the local-to-global dependency. In the first level, we construct a subgraph for pixels within each character instance based on their spatial similarity. And for the second level, 1-st level subgraphs are merged into a complete graph by linking their root nodes, which represent the geometric center of all nodes within each subgraph. Accordingly, we devise a graph convolutional neural network for context reasoning and optimizing the joint probability of the character sequence.</p><p>Our proposed GTR is an easy-to-plug-in module and can seamlessly function with other context modalities. Specifically, we put GTR parallel to the LM to produce joint features for text reasoning (as shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>). To produce high-quality cross-modality representations, we design a mutual learning protocol to enforce the consistency between predictions from LM and GTR and employ a dynamic fusion strategy <ref type="bibr" target="#b39">(Yue et al. 2020)</ref> to deeply combine visual and linguistic features. Based on these designs, GTR can largely boost the text reasoning performance comparing to existing representative methods with LM only.</p><p>We incorporate all aforementioned designs into a segmentation-based STR baseline and propose S-GTR, a unified framework of Segmentation baseline with GTR. We evaluate S-GTR on multiple datasets with both regular and irregular text material in different languages. Experimental results show that our S-GTR outperforms previous methods and obtains state-of-the-art performance on six challenging benchmarks. In summary, the contribution of this work is threefold:</p><p>? We propose a novel graph-based textual reasoning model named GTR to refine coarse text sequence predictions with spatial context. It is a complementary design to the popular reasoning fashion with LM only in existing representative methods, and can further improve their performance by acting as an easy-to-plug-in module. ? To make GTR work with LM compatibly, we further employ a mutual learning protocol and propose a dynamic fusion strategy to produce consistent linguistic and visual representations and high-quality joint prediction. ? We put all our designs in a unified framework (S-GTR) of segmentation baseline with GTR. Extensive experimental results indicate our S-GTR successfully sets new state-of-the-art for regular and irregular text recognition tasks as well as shows a superiority on both English and Chinese text materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Arbitrary-shaped Scene Text Recognition. Existing STR methods for recognizing texts of arbitrary shapes can be divided into two main categories, i.e., rectificationbased methods and segmentation-based methods. The former methods <ref type="bibr" target="#b8">(Gao et al. 2018;</ref><ref type="bibr" target="#b37">Yang et al. 2017;</ref><ref type="bibr" target="#b5">Cheng et al. 2018</ref>) use the spatial transformer network <ref type="bibr" target="#b14">(Jaderberg et al. 2015)</ref> to normalize text images into the canonical shape, i.e., horizontally aligned characters of uniform heights and widths. These methods, however, are limited by the predefined text transformation set and hard to generalize to real-world examples. The latter methods <ref type="bibr" target="#b30">Wan et al. 2020a</ref>) follow the common processing fashion in the text detection task <ref type="bibr" target="#b37">(Ye et al. 2021</ref>) and formulate the recognition task as an instance segmentation problem, where texts are explicitly modeled into instance masks. In this way, it can efficiently represent irregular texts in different fonts, scales, orientation, and shapes, as well as with occlusions. For this reason, we choose to build our GTR model upon a base instance segmentation-based recognition model. In addition, since the segmentation probability maps embed useful semantics and spatial context, the propose GTR model can efficiently exploit them for text reasoning.</p><p>Semantic Context Reasoning. To further enhance the text recognition performance, some methods resort to linguistic context to improve raw outputs from the VR model. For example, <ref type="bibr" target="#b4">(Cheng et al. 2017</ref>) employ CNN to yield bags of Ngrams of text string for output reasoning. Some later methods <ref type="bibr" target="#b35">Wojna et al. 2017</ref>) leverage RNN to strengthen context dependencies between characters. Recently, some methods adopt semantic context reasoning to achieve high performance. SEED <ref type="bibr" target="#b25">(Qiao et al. 2020</ref>) proposes to use word embedding from FastText <ref type="bibr" target="#b2">(Bojanowski et al. 2017)</ref>, which relies on semantic meaning of a word instead of its visual appearance. SRN <ref type="bibr" target="#b38">(Yu et al. 2020</ref>) uses transformer-based models where global semantic information as well as long-range word dependency is modelled by self-attention. It it computationally efficient due to the parallel nature of transformer architecture like (Xu et al. 2021), but their non-differentiable semantic reasoning block imposes a significant limitation. Based on SRN, ABINet <ref type="bibr" target="#b7">(Fang et al. 2021</ref>) adopts the iterative correction for enhancing semantic reasoning. Beyond semantic reasoning, we propose a graph-based context reasoning model that supplements the language model to exploit both visual spatial context and linguistic context to improve the visual recognition results.</p><p>Graph-structure Data Reasoning. Considerable efforts have been made to design graph convolutional neural networks (GCN) for modelling graph-structured data <ref type="bibr" target="#b17">(Kipf and Welling 2017;</ref><ref type="bibr" target="#b3">Chen et al. 2019;</ref><ref type="bibr" target="#b34">Wang et al. 2019)</ref>. For example, in the text detection task, ) adopts a GCN to link characters that belong to the same word. GTC <ref type="bibr" target="#b11">(Hu et al. 2020</ref>) utilizes a GCN to guide CTC <ref type="bibr" target="#b9">(Graves et al. 2006)</ref> for scene text recognition. Specifically, it models the sliced visual features as the graph nodes, captures their dependency, and merges features of the same instance for prediction. PREN2D <ref type="bibr" target="#b36">(Yan et al. 2021</ref>) adopts a meta-learning framework to extract visual representations via GCN. In this paper, we devise a two-level graph network based on GCN to perform spatial context reasoning within and between character instances to refine the visual recognition results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>The full framework of S-GTR is shown in the <ref type="figure" target="#fig_1">Figure  2</ref>, which comprises a segmentation-based VR model, an LM, and our proposed GTR. Given the input image X ? R H?W ?3 , the segmentation-based VR first produces a segmentation map M ? R H?W ?C , where C is the number of character classes. The segmentation map M is decoded  to a preliminary text sequence prediction T ? R T ?C and further processed by LM for generating linguistic context vector L ? R T ?C . T is the pre-defined maximum length of output sequence.</p><p>Our proposed GTR is stacked in parallel with LM, taking the segmentation map M as input. Firstly, we transform the map M with a feature ordering module to build an ordered feature tensor V ? R T ?H?W ?C , which comprises T attention maps and represents the relations between geometry features and text order information. Next, we build a subgraph for each attention map and then connect all sub-graphs sequentially into a full graph. The graph is then deeply encoded with a GCN to produce the spatial context vector S ? R T ?C . Finally, the coarse sequence prediction T, the linguistic context L and the spatial context S are combined via dynamic fusion and the refined text is predicted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GTR: Graph-based Textual Reasoning</head><p>Given the segmentation map M, we employ a fully convolutional network (FCN) to obtain a series of attention maps related to the character order and use them to attend M via element-wise multiplication to get the ordered feature tensor V ? R T ?H?W ?C , as shown in the bottom left part of <ref type="figure" target="#fig_1">Figure 2</ref>. Based on V, GTR firstly builds sub-graphs for all character instances and connects them sequentially. Then, the graph is encoded with a GCN and pooling operation to produce spatial context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Generation</head><p>We build the two-level graph from the ordered feature tensor V to model the local-to-global dependency. We first connect pixels belonging to the same character in the 1-st level sub-graph. Specifically, for the i-th ordered feature map V i ? R H?W ?C , we first choose pixels having the same estimation to the i-th character in the text sequence predicted by VR. These pixels are collected as a set P i = {(x, y, R) j }, where R is the C-dim feature vector in the position (x, y) of V i and j is the pixel index. Note that we also add a root node with average x, y and R to the set. Then we construct the node feature vector X i,j by embedding x, y and R with three different 1 ? 1 convolutions and i with sine and cosine functions. The four parts of embeddings are concatenated to form node features.</p><p>Next, the adjacent matrix is built according to node similarities. We compute the node similarity as a product between the position similarity E p and the feature similarity E f , which is defined as:</p><formula xml:id="formula_0">E p (p, q) = 1 ? D ((x p , y p ), (x q , y q )) max (H, W ) ,<label>(1)</label></formula><formula xml:id="formula_1">E f (p, q) = R p ? R q R p , R q ,<label>(2)</label></formula><formula xml:id="formula_2">E(p, q) = E p (p, q) ? E f (p, q),<label>(3)</label></formula><p>where p and q are two nodes from the set P i . The position similarity E p is negatively proportional to the Euclidean dis-tance between two pixels whereas the feature similarity E f is the cosine similarity between pixel features. The product of E p (p, q) and E f (p, q) is the overall similarity E between node p and q. Then, we use the 1-hop rule  to build the adjacent matrix A i . Specifically, we connect each node in V i to other nodes that have the top-8 largest similarity and delete the connections to the nodes outside the 1-hop cluster.</p><p>After constructing sub-graphs G i (X i , A i ), we connect them into the 2-level full graph by linking their root nodes in sequence order. The full graph is denoted as G(X, A).</p><p>Spatial Context Reasoning Given a graph G(X, A), we try to use the graph convolutional network to perform twostage spatial context reasoning by following <ref type="bibr" target="#b17">Kipf and Welling 2017)</ref>.</p><p>The first stage is spatial reasoning. After obtaining the feature matrix X and the adjacency matrix A, we use a graph convolutional network to output a transformed node feature matrix Y . This process can be described as follows: represents concatenation. W l is a layer-specific trainable weight matrix. ? denotes a non-linear activation function. K is an aggregation matrix of size N ? N , which is computed according to <ref type="bibr" target="#b17">(Kipf and Welling 2017)</ref>. Note that X l+1 = Y l , i.e., the output feature matrix Y l is used as the input of the l + 1th layer.</p><formula xml:id="formula_3">Y l = ?([X l ; KX l ]W l ), l = 1...L,<label>(4)</label></formula><formula xml:id="formula_4">K =D ?1 2?D ?1 2 . (5) Here, l denotes the layer index, L = 2, X(l) ? R N ?di , Y (l) ? R N</formula><p>After spatial reasoning, we perform the contextual reasoning. Denoting the output graph feature matrix from the aforementioned graph convolutional network as X l c , we compute a new adjacency matrix A c based on X l c . Then, we calculate G according to Eq. (5) based on A c . Next, we use a graph convolutional network to output a transformed node feature matrix Y l c as follows:</p><formula xml:id="formula_5">Y l c = ?((GX l c )W l c ), l = 1...L,<label>(6)</label></formula><p>where W l c is a layer-specific trainable weight matrix. Then, we perform root node check to make sure the selected root node is the underlying reliable root node, i.e., the center of the character instance. In this way, it achieves the balance between the edges with near easy nodes and distant hard nodes by satisfying the following criterion:</p><formula xml:id="formula_6">G iou = G r ? G s G r ? G s &lt; ?,<label>(7)</label></formula><p>where G r and G s are two subgraphs for a same character, given that s is a randomly selected node as the root node while r is always the center of character. G r ? G s and G r ? G s are the intersection and the union of 1-hop neighbors of G r and G s , respectively. In our experiments, ? is set to 0.75. Next, we use a readout layer like <ref type="bibr" target="#b18">(Lee, Lee, and Kang 2019)</ref> to aggregate node features to a fixed-size representation. The output feature of this layer is calculated as follows:</p><formula xml:id="formula_7">x * i = [x i ; max(x * j |j ? N (x i ))],<label>(8)</label></formula><p>where x * j is the updated feature at jth node, which is also calculated according to Eq. (8), i.e., x * i is calculated in a recursive manner. N (x i ) denotes the neighboring node set of node i. After we obtain the updated node features, we discard 50% nodes that are most distant to the root node, i.e., pooling the graph into a smaller new one. We iteratively repeat the feature update and pooling process until only a node exists in the subgraph, resulting in a node sequence. Finally, the feature representations of the node sequence are passed to a linear layer for classification. We adopt the softmax cross-entropy loss for optimizing graph convolutional neural network. Similar to , we only backpropagate the gradient for nodes in the 1-hop neighborhood during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">S-GTR: A Simple Baseline for STR</head><p>We incorporate our GTR to a popular segmentation-based VR model with LM, resulting in a simple baseline for STR, i.e., S-GTR, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Specifically, the VR model is designed following <ref type="bibr" target="#b31">(Wan et al. 2020b)</ref>, and the LM is based on SRN. We devise manifold training strategies to make GTR better support the the STR task.</p><p>Context Consistency Since we have two different types of reasoning features, namely linguistic context and spatial context. To prevent S-GTR from overly relying on one of them and avoid inconsistent reasoning cues to provide ambiguous results, we propose a mutual learning strategy to enforce the consistency between the two types of context features. Specifically, we compute the Kullback Leibler (KL) divergence between L from LM and S from GTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Fusion</head><p>Following <ref type="bibr" target="#b39">(Yue et al. 2020</ref>) that uses a dynamic fusion module to fuse information from multiple domains, we extend it in S-GTR to combine three different text sequences from VR, LM and GTR.Formally,</p><formula xml:id="formula_8">Q i = Sigmoid(W 0 [T i ; L i ; S i ]), Z i = Q i (W 1 [T i ; L i ; S i ]),<label>(9)</label></formula><p>where T i , L i , S i are prediction vectors for the i-th character. W 0 and W 1 are two learnable linear transformations and indicates the element-wise multiplication operation. Z i is the final output of S-GTR for the i-th character.</p><p>Mean Teacher-based Syn-to-Real Adaptation To mitigate the domain shift issue when using both synthetic and real datasets for training, we adopt the popular mean teacher framework <ref type="bibr" target="#b29">(Tarvainen and Valpola 2017)</ref> in the area of domain adaptation. Specifically, a teacher network with the identical architecture as the segmentation-based VR model (i.e., student network) is built and its weights are the exponential moving average of those of the student network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Function</head><p>The overall loss contains three parts, including sequence prediction loss L Seq , the LM-GTR consistency loss L CC , and the mean-teacher training loss L MT :</p><formula xml:id="formula_9">L = ? Seg * L Seg + ? CC * L CC + ? MT * L MT .<label>(10)</label></formula><p>L Seg contains a cross-entropy loss for character classification and a smooth L1 loss for order segmentation. L CC is the  <ref type="table">Table 1</ref>: Results of our S-GTR, SOTA methods and their variants with our GTR on six regular and irregular STR datasets. "R" denotes the real datasets. "*" means using character-level annotations during training. " ?" means the batch size is set to 384 for a fair comparison. The superscripts in the second group of rows denote the type of different methods, i.e., "CTC": CTC-based method, "1DATT": 1D attention-based method, "2DATT": 2D attention-based method, and "Transformer": Transformer-based method. Details can be found in Section 4.2.</p><p>KL loss for context consistency. L MT is the MSE loss on the segmentation maps from teacher and student networks. ? Seg and ? CC are both set to 1.0. ? MT is set to 1.0 when using synthetic datasets for training. After getting accurate feature representations, it is reduced to 0 gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets Following <ref type="bibr" target="#b38">(Yu et al. 2020)</ref>, we use two public synthetic datasets , i.e., SynthText (ST) <ref type="bibr" target="#b10">(Gupta, Vedaldi, and Zisserman 2016)</ref> and MJSynth (MJ) <ref type="bibr" target="#b12">(Jaderberg et al. 2014b</ref><ref type="bibr" target="#b13">(Jaderberg et al. , 2016</ref>) and a real datasets (R) <ref type="bibr" target="#b1">(Baek, Matsui, and Aizawa 2021)</ref> for training. We test the trained model on six benchmarks including three regular scene-text datasets, i.e., IC-DAR2013 <ref type="bibr" target="#b16">(Karatzas et al. 2013</ref>), IIIT5K (Mishra, Alahari, and Jawahar 2012), SVT <ref type="bibr" target="#b32">(Wang, Babenko, and Belongie 2011)</ref>, and three irregular text datasets, i.e., ICDAR2015 <ref type="bibr" target="#b15">(Karatzas et al. 2015)</ref>, SVTP <ref type="bibr" target="#b24">(Phan et al. 2013</ref>) and CUTE <ref type="bibr" target="#b26">(Risnumawan et al. 2014)</ref>. The evaluation metric is the standard word accuracy.</p><p>Implementation Details We train the model with ADAM optimizer on two synthetic datasets for 6 epochs and then transferred to the real dataset for the other 2 epochs. The total batch size is 256, equally distributed on four NVIDIA V100 GPUs. For the pre-training stage on synthetic datasets, the learning rate is set to 0.001 and divided by 10 at the 4-th and 5-th epochs. Then, we utilize the mean teacher training framework on the real dataset for the remaining 2 epochs. The detailed training setting for this stage is deferred to the supplementary material. Our model recognize 63 types of characters, including "0-9", "a-z", and "A-Z". The max decoding length of the output sequence T is set to 25. We follow the standard image pre-processing that randomly resizing the width of original images into 4 scales, i.e., 64, 128, 192 and 256, and then padding the images to the resolution of 64 ? 256. We adopt multiple data augmentation strategies including random rotation, perspective distortion, motion blur, and adding Gaussian noise to the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Performance Analysis</head><p>Comparison with State-of-the-Art We compare the proposed S-GTR with state-of-the-art methods, and the results are summarized in <ref type="table">Table 1</ref>, where the inference speed as well as the number of model parameters are also reported. As can be seen, the proposed S-GTR achieves the highest recognition accuracy and 3? faster inference speed compared with the second best method PREN2D <ref type="bibr" target="#b36">(Yan et al. 2021</ref>). In addition, when real data is utilized for training, S-  <ref type="figure">Figure 4</ref>: Results on some test images. Each image is along with three texts, which are predicted by VR, the Segbaseline, and the proposed S-GTR model, respectively.</p><p>GTR achieves more impressive results on all the six benchmarks, validating the effectiveness of the proposed GTR for textual reasoning and the benefit of real data.</p><p>Plugging GTR in Different Models To further verify the effectiveness of GTR, we plug our GTR module into four representative types of STR methods, including CTCbased method (e.g., CRNN <ref type="bibr" target="#b27">(Shi, Bai, and Yao 2016)</ref>), 1D attention-based method (e.g., TRBA <ref type="bibr" target="#b0">(Baek et al. 2019)</ref>), 2D attention-based method (e.g., Base2D <ref type="bibr" target="#b36">(Yan et al. 2021</ref>)), and transformer-based methods (e.g., SRN <ref type="bibr" target="#b38">(Yu et al. 2020</ref>) and ABINet-LV <ref type="bibr" target="#b7">(Fang et al. 2021)</ref>). For the 1D attention-based method, the prediction result of VR is a 1D semantic vector. Therefore, we adopt the 2D feature map from the layer before the prediction layer as input of GTR after feature ordering. The results are shown in the second group of rows in <ref type="table">Table 1</ref>. As can be seen, after using GTR, the performance of all these models can be improved further. For example, the average recognition accuracy on all the available test sets is increased by 3.77%, 3.20%, 2.78%, 1.69%, and 1.65% for CRNN, TRBA, SRN, Base2D, and ABINet-LV, respectively. These results demonstrate the compatibility of our GTR to typical models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Ablation Study Results of S-GTR All the models in this ablation study have the same training configurations as used in S-GTR. To investigate the impact of different modules in S-GTR, we first train a baseline VR model, which utilizes neither LM and nor GTR. As shown in <ref type="table" target="#tab_4">Table 2</ref>, without LM and GTR, the baseline VR model observes a significant performance drop. Compared with the baseline, a gain of 3.45% can be observed by using LM, since it introduces the global linguistic textual cues for textual reasoning and corrects some linguistically implausible predictions. The proposed GTR module exploits the visual-spatial context information to refine the output of the VR model and increases the average accuracy by 4.06%. When using both LM and GTR together, the best average performance of 90.96% can be obtained. These two modules both contribute to the improvement of S-GTR over the baseline, demonstrating that the linguistic cues and spatial context from visual semantics are complementary to each other. It is also noteworthy that the GTR module brings more gains than LM.</p><p>Note that there is no use of mutual learning in the experiments in <ref type="table" target="#tab_4">Table 2</ref>. After comparing the results in its last row (i.e., S-GTR without mutual learning) with the results of S-GTR in <ref type="table">Table 1</ref>, which is also trained on the "ST+MJ" datasets but with mutual learning, we can find that mutual   <ref type="table">Table 3</ref>: Ablation study of GTR. "#" means the number of GCN layers in the first stage of GTR. "Adj" is the value type of adjacency matrix, i.e., discrete value 0 or 1 and continuous value in [0,1], respectively. "Average" denotes employing average pooling on graph feature rather than the graph pooling described in Section 3.2.</p><p>learning contributes to a better average recognition accuracy of 91.92%. It demonstrates that enforcing the consistency between the context features from LM and GTR is necessary to better exploit the complementary between these two different types of textual reasoning. For the qualitative analysis of different models, we present some test images and their corresponding text predictions from the basic VR model (top), Seg-baseline with LM (middle), and the proposed S-GTR (bottom) in <ref type="figure">Figure 4</ref>. We can see that LM can correct some mistaken predictions from the basic VR model by exploiting the global linguistic context. However, it is still challenging to generalize to arbitrary texts and some ambiguous cases. Compared to it, S-GTR produces satisfactory results on irregular texts in different fonts, scales, orientations, and shapes, owing to its better textual reasoning ability by exploiting both linguistic cues and spatial context from visual semantics.</p><p>Influence of Different Settings in GTR Although the proposed GTR module has shown its effectiveness in improving the STR performance on multiple benchmarks, we would also like to analyze the influence of different settings in GTR. In this section, we evaluate the performance of GTR variants with respect to different numbers of GCN layers in the first stage, different value types of adjacency matrix, and different pooling strategies. As shown in <ref type="table">Table 3</ref>, with the increase of the number of GCN layers, the recognition accuracy, the number of parameters, and inference time all increase as well. To achieve a trade-off between recognition accuracy and model complexity, we choose 2 layers as the default setting. Besides, we find that there is almost no performance gain when using continuous values in the adjacency matrix compared to discrete values, while the inference time increases by 7.98%. Therefore, we choose the discrete value as the default value type. We further compare the graph pooling with the average pooling in the stage of contextual reasoning. The results show that graph pooling outperforms average pooling significantly since it can capture   the local-to-global dependency for reasoning. Therefore, we choose it as the default pooling strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Fusion Strategy</head><p>We also investigate the impact of fusion strategy in S-GTR when fusing the linguistic context from LM and spatial context from GTR. In addition to the proposed dynamic fusion, we consider other two choices, i.e., element-wise sum and concatenation. The results are reported in <ref type="table">Table 4</ref>. As can be seen, while the concatenation fusion strategy performs better than elementwise addition, it still falls behind the proposed dynamic fusion strategy. We suspect that the benefit may come from the learnable fusion weights which are absent in the other two non-parametric cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Further Visualization and Analysis</head><p>Visual Inspection Result For the qualitative analysis, we visualize the feature maps from the penultimate layer in VR, GTR and S-GTR. As shown in <ref type="figure" target="#fig_4">Figure 5</ref>, compared to the feature maps from VR, the feature maps from GTR are more strongly activated on the target characters owing to the textual reasoning of spatial context. Besides, the feature maps from S-GTR cover the target characters more precisely than GTR. These results imply that the S-GTR can learn more discriminative features by attending to the target characters and discard irrelevant information. In addition, we present the visualization of the node similarity matrix in <ref type="figure" target="#fig_5">Figure 6</ref> for better understanding the graph generation process.    <ref type="table">Table 6</ref>: Results of different methods on MLT-17. "NED" is short for Normalized Edit Distance.</p><p>Compatibility of GTR to Different LMs To further investigate the compatibility of GTR to LMs, we apply GTR in a basic VR model with two different LMs, i.e., FastText <ref type="bibr" target="#b2">(Bojanowski et al. 2017</ref>) and BERT <ref type="bibr" target="#b6">(Devlin et al. 2019)</ref>. As shown in <ref type="table" target="#tab_8">Table 5</ref>, GTR contributes to consistent gains on both FastText and BERT Language Model. In addition, we also find that using a better LM together with GTR can further improve text recognition performance.</p><p>Chinese Scene Text Recognition Like English text recognition, the Chinese scene text recognition task offers an alternative way to assess the capability of STR models. The Chinese STR task is more challenging as it requires model to handle a larger vocabulary and more complex data associations. In addition to the recognition accuracy, we also report the Normalized Edit Distance (NED) of different methods following the ICDAR-2019 ReCTS . As shown in <ref type="table">Table 6</ref>, S-GTR outperforms other methods significantly on the multi-language dataset MLT-2017 <ref type="bibr" target="#b23">(Nayef et al. 2017)</ref>. It demonstrates that GTR is still very effective for textual reasoning of Chinese text materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose the idea of performing textual reasoning based on visual semantics from a basic visual recognition (VR) model for the Scene Text Recognition (STR) task. We implement it as a graph-based textual reasoning (GTR) module, which can act as an easy-to-plug-in module in existing representative methods. It is shown to be very effective in improving STR performance while being complementary to the common practice, i.e., language modelbased linguistic context reasoning. Experimental results on six challenging STR benchmarks demonstrate that GTR can be plugged in different types of state-of-the-art STR models and improve their recognition performance further. GTR also shows good compatibility with different language models. Based on a simple segmentation-based VR model, we construct a simple S-GTR baseline for STR, which sets state-of-the-art on both English and Chinese regular and irregular text materials. We hope this work can provide a new perceptive to study textual reasoning in the STR task and inspire more follow-up work in the future, such as efficient design for spatial context-based reasoning as well as the way of effective fusion of different reasoning results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of different STR pipelines. (a) The VR model. (b) The VR model with an LM. (c) The proposed pipeline by adding GTR in parallel with LM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed S-GTR model. It consists of a VR model, a LM, and the proposed GTR. GTR is stacked on the top of the VR model and in parallel with the LM. The detailed structure of GTR as well as a pre-processing step, i.e., feature ordering, are also shown in the bottom part of this figure. More details can be found in Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the subgraph building process, including node feature generation and edge matrix compute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>?do , d i and d o are the dimension of input and output node features, and N is the number of nodes. [; ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>4: Empirical study of the fusion strategy in S-GTR. "Add": element-wise addition. "Concat": Concatenation. "D-fuse": Dynamic fusion. Visualization of feature maps from VR, GTR and S-GTR (from the second column to the last column).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Visualization of a node similarity matrix, which is calculated according to Eq. (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.. ... ...</figDesc><table><row><cell>Input Image</cell><cell></cell><cell cols="3">Segmentation Map</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Context Vector</cell><cell>Prediction</cell></row><row><cell>HxWx3</cell><cell>Recognition Model Seg-based Visual</cell><cell>HxWxC</cell><cell>...</cell><cell cols="3">Ordered Features Prediction</cell><cell cols="2">Language Model</cell><cell>TxC</cell><cell cols="2">...</cell><cell cols="2">KL(P1|P2)</cell><cell>Dynamic Fusion</cell><cell>"BAKER"</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Feature Ordering</cell><cell>... HxWxC</cell><cell>... ... ...</cell><cell>xT</cell><cell>GTR</cell><cell></cell><cell>...</cell><cell></cell><cell cols="2">KL(P2|P1)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv1</cell></row><row><cell>Segmentation Map</cell><cell cols="2">Ordered Features</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Conv2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Conv2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pooling</cell></row><row><cell>Receptive Field ... ... ... ...</cell><cell>FCN</cell><cell>?</cell><cell></cell><cell>Node feature Root Node</cell><cell cols="2">Pooling GCN</cell><cell>X2</cell><cell></cell><cell>Update Adj-Matrix</cell><cell>GCN GCN</cell><cell cols="3">c Conv1 Conv2 c</cell><cell>Pooling</cell><cell>... Node Check Node Check</cell><cell>Readout</cell><cell>Linear</cell></row><row><cell></cell><cell>Feature Ordering</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Top Rank Select Node</cell><cell></cell><cell></cell><cell cols="3">c Conv1</cell><cell>c</cell><cell>Conv2</cell><cell>Pooling</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>GCN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Graph-based Textual Reasoning Model(GTR)</cell><cell></cell><cell></cell><cell></cell></row></table><note>.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the components in S-GTR.</figDesc><table><row><cell># GCN</cell><cell>Adj</cell><cell>Pool</cell><cell cols="3">IIIT5k SVT IC15 CUTE</cell><cell cols="2">Params Time (?10 6 ) (ms)</cell></row><row><cell>2</cell><cell>{0,1}</cell><cell>Graph</cell><cell>95.8</cell><cell>94.1 84.6</cell><cell>92.3</cell><cell>42.1</cell><cell>18.8</cell></row><row><cell>1</cell><cell>{0,1}</cell><cell>Graph</cell><cell>94.3</cell><cell>92.9 82.5</cell><cell>90.8</cell><cell>39.5</cell><cell>16.1</cell></row><row><cell>3</cell><cell>{0,1}</cell><cell>Graph</cell><cell>96.0</cell><cell>94.0 84.8</cell><cell>92.6</cell><cell>44.9</cell><cell>22.5</cell></row><row><cell>2</cell><cell>[0,1]</cell><cell>Graph</cell><cell>95.9</cell><cell>94.2 84.9</cell><cell>92.4</cell><cell>42.2</cell><cell>20.3</cell></row><row><cell>2</cell><cell cols="2">{0,1} Average</cell><cell>94.8</cell><cell>93.2 82.3</cell><cell>89.8</cell><cell>38.1</cell><cell>15.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results of S-GTR with different language models. FastText and BERT are two pretrained language models.</figDesc><table><row><cell></cell><cell cols="5">CRNN ASTER TextScanner ABINet S-GTR</cell></row><row><cell>Acc (%)</cell><cell>59.2</cell><cell>57.4</cell><cell>64.1</cell><cell>68.4</cell><cell>72.2</cell></row><row><cell>NED</cell><cell>0.68</cell><cell>0.69</cell><cell>0.75</cell><cell>0.79</cell><cell>0.82</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by National Natural Science Foundation of China under Grants (No.62076186,  No.61822113), and in part by Science and Technology Major Project of Hubei Province (Next-Generation AI Technologies) under Grant (No.2019AEA170). The numerical calculations in this paper have been done on the supercomputing system in the Supercomputing Center of Wuhan University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3113" to="3122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Graph-based global reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="433" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Focusing attention: Towards accurate text recognition in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5076" to="5084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aon: Towards arbitrarily-oriented text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5571" to="5579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06495</idno>
		<title level="m">Read Like Humans: Autonomous, Bidirectional and Iterative Language Modeling for Scene Text Recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Recurrent Calibration Network for Irregular Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1812.07145: arXiv-1812</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning, ICML &apos;06</title>
		<meeting>the International Conference on Machine Learning, ICML &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gtc: Guided training of ctc towards efficient and accurate scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page" from="11005" to="11012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition. ArXiv, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5903</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2227</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Deep structured output learning for unconstrained text recognition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference on Neural Information Processing Systems</title>
		<meeting>the Annual Conference on Neural Information Processing Systems<address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ICDAR 2015 competition on robust reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition<address><addrLine>Nancy, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Las</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-Attention Graph Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<idno>abs/1904.08082</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Show, attend and read: A simple and strong baseline for irregular text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Honolulu, Hawaii,USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8610" to="8617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Scene text recognition from two-dimensional perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>Honolulu, Hawaii,USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8714" to="8721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scatter: selective context attentional scene text recognizer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Anschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsiper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Litman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mazor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11962" to="11972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Surrey, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nayef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bizid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rigaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chazalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1454" to="1459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision<address><addrLine>Sydney,Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Seed: Semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13528" to="13537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Mean Teachers are Better Role Models: Weight-Averaged Consistency Targets Improve Semi-supervised Deep Learning Results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1196" to="1205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Textscanner: Reading characters in order for robust scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12120" to="12127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On vocabulary reliance in scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11425" to="11434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Endto-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision<address><addrLine>Barcelona,Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Decoupled Attention Network for Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<idno>abs/1912.10205</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Linkage based face clustering via graph convolution network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1117" to="1125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention-based extraction of structured information from street view imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="844" to="850" />
		</imprint>
	</monogr>
	<note>Thirty-Fifth Conference on Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Primitive Representation Learning for Scene Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to Read Irregular Text with Attention Mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.01343</idno>
	</analytic>
	<monogr>
		<title level="m">Intra-and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the International Joint Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Towards accurate scene text recognition with semantic reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12113" to="12122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RobustScanner: Dynamically Enhancing Positional Clues for Robust Text Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision<address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="7789" to="7817" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Icdar 2019 robust reading challenge on reading chinese text on signboard</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 international conference on document analysis and recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep relational reasoning graph network for arbitrary shape text detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9699" to="9708" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
