<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwook</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inha University Incheon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inha University Incheon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byung</forename><forename type="middle">Cheol</forename><surname>Song</surname></persName>
							<email>bcsong@inha.ac.kr</email>
							<affiliation key="aff2">
								<orgName type="institution">Inha University Incheon</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For a long time, anomaly localization has been widely used in industries. Previous studies focused on approximating the distribution of normal features without adaptation to a target dataset. However, since anomaly localization should precisely discriminate normal and abnormal features, the absence of adaptation may make the normality of abnormal features overestimated. Thus, we propose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes sophisticated anomaly localization using features adapted to the target dataset. CFA consists of (1) a learnable patch descriptor that learns and embeds target-oriented features and (2) scalable memory bank independent of the size of the target dataset. And, CFA adopts transfer learning to increase the normal feature density so that abnormal features can be clearly distinguished by applying patch descriptor and memory bank to a pretrained CNN. The proposed method outperforms the previous methods quantitatively and qualitatively. For example, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in anomaly localization of MVTec AD benchmark. In addition, this paper points out the negative effects of biased features of pre-trained CNNs and emphasizes the importance of the adaptation to the target dataset. The code is publicly available at https://github.com/ sungwool/CFA_for_anomaly_localization</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection is a well-known computer vision task to detect anomalous feature(s) in a given image. The human visual system (HVS) can easily recognize unexpected patterns in images, i.e., anomalies, regardless of feature complexity. With the rapid development of CNNs, machine vision systems can recognize anomalies by learning abstract features. In addition to the image-level anomaly detection, anomaly localization, that is, pixel-level anomaly detection, has also been actively studied. Anomaly localization provides a heatmap indicating the location of an outlier as well as the presence or absence of the outlier. Note that the heatmap can be the starting point for explaining the cause of the anomaly.</p><p>Meanwhile, anomaly localization algorithms cannot consider all possible outliers in learning. In other words, they cannot build a dataset that includes all outliers. So, distinguishing abnormal samples by learning the distribution of normal samples has been the mainstream approach. For example, unsupervised learning-based approaches such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">19]</ref> utilized the characteristic that a generator trained with only normal features cannot successfully reconstruct abnormal features. Self-supervised learningbased approaches such as <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b23">21,</ref><ref type="bibr" target="#b25">23]</ref> synthesized noise and used them as abnormal samples in learning. Recently, <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">16]</ref> designed memory banks using pre-trained CNNs with large datasets such as ImageNet <ref type="bibr" target="#b6">[6]</ref> and achieved stateof-the-art (SOTA) performance. This memory bank-based approach extracts sufficiently generalized features from a pre-trained CNN without learning the target dataset and then stores them into a memory bank. Finally, it determines whether an input sample is abnormal by matching the input features with the memorized features.</p><p>However, industrial images generally have a different distribution from ImageNet. So, the pre-trained CNN extracts only unfitted features from new industrial images. This can be a fatal problem in anomaly localization, which requires a precise distinction between normal and abnormal features. <ref type="bibr" target="#b17">[16]</ref> pointed out the mismatch problem caused by pre-trained CNNs extracting biased features. It only used mid-level features with relatively small biases, but did not fundamentally solve the mismatch problem.</p><p>The performance of anomaly localization depends on the size of the memory bank. Conventional methods stored as many normal features of the target dataset as possible in a memory bank to accommodate unfitted features, that is, to understand the distribution of normal features. So, the size of the memory bank was determined in proportion to that of the target dataset. However, a great number of unfitted features in the memory bank may cause the risk of overestimated normality of abnormal features. Furthermore, a large capacity memory bank increases the inference time.</p><p>To obtain discriminative normal features, we propose a novel approach to produce target-oriented features with reduced bias by applying transfer learning to a pre-trained CNN. First, we define a novel loss function based on softboundary regression that searches a hypersphere with a minimum radius to densely cluster normal features. The proposed loss function helps the learnable patch descriptor extract discriminative features by utilizing several memorized features that form a coupled-hypersphere. Next, to reduce the inference time, we present a scalable memory bank. Since the scalable memory bank is independent of the size of the target dataset, it not only alleviates the risk of overestimated normality of abnormal features, but also achieves the efficiency of spatial complexity. Therefore, the proposed method can effectively localize anomalies by extracting appropriately target-oriented features to the target dataset and constructing a down-scaled memory bank to have core normal features.</p><p>We evaluated the proposed method using MVTec AD benchmark <ref type="bibr" target="#b0">[1]</ref>, which is a popular industrial image dataset for visual inspection. The proposed method showed a performance of 99.5% in terms of anomaly detection performance index, i.e., image-level AUROC (I-AUROC), and accomplished SOTA performance of 98.5% in terms of anomaly localization performance index, i.e., pixel-level AUROC (P-AUROC). In particular, it is worth noting that the proposed method provides better performance than conventional methods while decreasing the activations of about 99.9% of the memory bank <ref type="bibr" target="#b7">[7]</ref>.</p><p>Contributions. The contributions of this paper are summarized as follows: 1) We discover the negative effects of biased features from pre-trained CNNs on anomaly localization, and propose an adaptation to the target dataset as a solution. 2) We propose a new approach to acquire discriminative features through metric learning, and experimentally verify that the features enable very sophisticated anomaly localization. 3) A memory bank that is compressed independently of the size of the target dataset through feature adaptation achieves SOTA performance despite its significantly reduced capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In general, the acquisition of outlier samples requires a lot of costs and it is also impossible to consider all types of outliers. So, a memory bank-based approach that acquires normal features by inferring the target dataset using pretrained CNNs has emerged. <ref type="bibr" target="#b3">[4]</ref> obtained normal features from the feature maps and stored them in a memory bank. And during the test time, it calculated anomaly scores by computing the Euclidean distance between the normal features from the memory bank and the patch features from a test sample. <ref type="bibr" target="#b4">[5]</ref> defined a memory bank by modeling the normal distribution at each location of the feature map. To further consider the inter-feature correlation, it adopted Mahalanobis distance metric for computing anomaly scores. <ref type="bibr" target="#b17">[16]</ref> used only mid-level feature maps to mitigate biased features and maximized nominal information by considering the neighbor features of each normal feature. In addition, it proposed greedy coreset subsampling to lighten the memory bank and the time/space complexity. However, the above-mentioned methods have in common that they use features biased on a large dataset without adaptation. Also, the size of the memory bank is still proportional to that of the target dataset, and there is a problem that the memory bank cannot be adjusted to an arbitrary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>This paper proposes a so-called Coupled-hyperspherebased Feature Adaptation (CFA) that performs transfer learning on the target dataset as a solution to alleviate the bias of pre-trained CNNs. The patch descriptor of CFA learns the patch features obtained from normal samples of a target dataset to have a high density around the memorized features. Thus, CFA solves the problem that the normality of abnormal features is overestimated when using a pre-trained CNN.</p><p>As in <ref type="figure" target="#fig_0">Fig. 1</ref>, CFA acquires feature maps of various scales by inferring samples of the target dataset based on a pretrained CNN with a large dataset, that is, a biased CNN. Since the feature maps sampled at each depth of CNN have different spatial resolutions, they are interpolated to have the same resolution and then concatenated, as in <ref type="bibr" target="#b4">[5]</ref>. As a result, patch features F ? R D?H?W are generated. Here, H and W mean the height and width of the largest features map, respectively, and D indicates the sum of dimensions of the sampled feature maps. Since each pixel location of F has a predetermined receptive field, patch feature p t?{1,...,HW} ? R D can be considered as semantic information at the pixel location. Next, p is input to the patch descriptor ?(?) : R D ? R D . Here, ?(?) is an auxiliary network with learnable parameters, which converts p t into target-oriented features ?(p t ) ? R D . Here, D means the dimension of ?(p t ) embedded by ?(?).</p><p>Meanwhile, all initial target-oriented features acquired from the train set consisting of only normal samples are stored in the memory bank C according to a specific modeling procedure. In <ref type="figure" target="#fig_0">Fig. 1</ref>, the dotted line indicates that it is performed only in the initialization step (c.f. section 3.2). In the train phase, CFA performs contrastive supervision based on the superimposed hyperspheres created with the memorized features c ? C as the centers, that is, the socalled coupled-hypersphere. Note that ?(p t )s trained to be densely clustered in the train phase, i.e., normal features, are very useful for distinguishing abnormal features. In the test phase, CFA matches p t obtained from the arbitrary sample of the test set with the nearest neighbor c t searched in the memory bank, and generates heatmaps representing the degree of anomality. Finally, a score map for anomaly localization from the heatmaps is calculated by a specific scoring function (c.f. section 3.3). Note that the score map shows the refined region of abnormal features. This paper is organized as follows: Section 3.1 defines ?(?) and explains how to train it, and section 3.2 defines C. Finally, section 3.3 shows the process of calculating anomaly scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Coupled-hypersphere-based Feature Adaptation</head><p>This section describes how to learn ?(?) attached to a pre-trained CNN through transfer learning based on a memory bank for more sophisticated target-oriented anomaly localization. Previous studies <ref type="bibr" target="#b19">[18]</ref> and <ref type="bibr" target="#b25">[23]</ref> that learned the distribution of target datasets by introducing the hypersphere concept still had a problem in not clearly understanding normal features because they did not use a memory bank. Therefore, we present a method for effectively learning ?(?). The proposed method can solve the bias problem of pre-trained CNNs by fusing a hypersphere-based loss function and a memory bank. The specific process is as follows:</p><p>To obtain a feature space that can clearly detect abnormal features, we extract clustered normal features so that ?(?) has a high density. First, the k-th nearest neighbor c k t is searched through the NN search of ?(p t ) and C. Next, CFA supervises ?(?) so that p t is embedded close to c k t . Specifically, ?(?) makes it possible to form a high concentration between normal features by supervising p t to embed it inside a hypersphere of radius r created with c k t as the center. So, L att for attracting by adding a penalty to ?(p t ) as far as r away from c k t is described by</p><formula xml:id="formula_0">L att = 1 T K T t=1 K k=1 max{0, D(?(p t ), c k t ) ? r 2 } (1)</formula><p>where a hyperparameter K is the number of nearest neighbors matching with ?(p t ) and T = h ? w is the number of ps obtained from a single sample. D(?, ?) is a predefined distance metric, i.e., Euclidean distance in this paper. Eq <ref type="formula">(1)</ref> induces ?(p t ) to gradually approach the hypersphere created with c k t as the center. So, CFA enables feature adaptation by optimizing the parameters of ?(?) to minimize L att through transfer learning. As such, if ?(p) is densely clustered through feature adaptation using L att , it will be easy to distinguish from abnormal features.</p><p>However, the ambiguous ?(p) belonging to multiple hyperspheres at the same time still leaves room for the normality of abnormal features to be overestimated. To address this, we additionally use hard negative features to perform contrastive supervision to obtain a more discriminative ?(p t ). Hard negative features are defined as the K+j-th nearest neighbor c j t of p t matched through NN search with C. Thus, we define L rep that supervises ?(?) contrastively so that the hypersphere created with c j t as the center repels p t as follows.</p><formula xml:id="formula_1">L rep = 1 T J T t=1 J j=1 max{0, r 2 ? D(?(p t ), c j t ) ? ?} (2)</formula><p>where the hyperparameter J is the total number of hard negative features to be used for contrastive supervision and the hyperparameter ? is used to control the balance between L att and L rep . As a result, CFA optimizes the parameters of ?(?) through transfer learning using Eqs. (1) and (2) together:</p><formula xml:id="formula_2">L CFA = L att + L rep<label>(3)</label></formula><p>If the distance between c j t and c k t matched with p t is closer than r, L CFA of Eq. 3 directly supervises p t based on the coupled-hypersphere. Thus, p t can be embedded by ?(?) so that the hypersphere of c k t has a higher density through contrastive supervision using c j t . Note that we named the process of obtaining target-oriented features from the patch descriptor through transfer learning using L CFA 'Coupledhypersphere-based Feature Adaptation'.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Bank Compression</head><p>Transfer learning through the proposed CFA requires a memory bank for effective adaptation to target dataset. However, as seen in <ref type="table">Table 1</ref>, the complexity of the modeling process or memory bank space in the previous methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">16]</ref> tends to increase in proportion to the size of the target dataset, i.e., |X |. To mitigate this phenomenon, this section presents a compression scheme to construct an efficient memory bank.</p><p>The compression process of the memory bank is described by algorithm 1. First, an initial memory bank C 0 is constructed by applying K-means clustering to all ? 0 (p t?{1,...,T } ) obtained from the first normal samples x 0 of the train set X . The process of updating the memory bank after C 0 is as follows: Infer the i-th normal sample x i and search for the set of nearest patch features C NN i from the i-1th memory bank C i-1 . Next, the i-th memory bank of the next state C i is calculated by exponential moving average (EMA) of C NN i and C i-1 . The final memory bank C is obtained by repeating the above process |X | times for all normal samples of the train set.</p><p>The upper part of <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the process in which memorized features are updated for each sample of the target dataset. Unfortunately, C 0 initialized through the K-NN search does not represent X as a whole. However, if C i-1 is updated through EMA iteratively along ? i (p t ), the final C can store the core normal features representing X .</p><p>Since the proposed algorithm 1 updates C in every state, the modeling process requires the space complexity as much as O(HW D ). Also, C has ?(p) of feature dimension D as many as the number of cluster centers, so it has as Algorithm 1 Memory Bank Modeling. <ref type="table">Table 1</ref>. Complexity estimates of memory bank modeling and memory bank size.</p><formula xml:id="formula_3">Require:Patch descriptor ?, dataset X , EMA parameter ? Initialization: C0 ? KMeans?0(p) for i ? {1, . . . , |X |} do C NN i ? {} for j ? {1, . . . , |C|} do Y ? (?i(p) ? C NN i ) ? (C NN i ) c C NN i ? arg miny?Y y ? C j i?1 2 end for Ci ? (1 ? ?) ? Ci?1 + ? ? C NN i end for C ? C |X | return C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modeling</head><formula xml:id="formula_4">Memory Bank SPADE O(|X |HW D) G ? R |X |?H?W ?D PaDiM O(|X |HW D 2 ) N (?, ?) ? R H?W ?D 2 PatchCore O(|X |HW D ) M ? R |X |??(H?W )?D Ours O(HW D ) C ? R ?(H?W ?D)</formula><p>much spatial complexity as O(?(HW )D ). Here, the compression ratio ? indicates the ratio of T , i.e., the number of ps obtained from F and the number of cluster centers. Therefore, C of the proposed method is not affected by |X | as in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scoring Function</head><p>In Section 3.1, the distance between ?(p t ) and c k t was calculated using D(?, ?). min k D(?(p t ), c k t ) means the minimum distance between ?(p t ) and the memorized features C, that is, the degree of anomality of ?(p t ). So, we can define the anomaly score naively using D(?(p t ), c k t ) as it is, as shown below:</p><formula xml:id="formula_5">S t = min k D(?(p t ), c k t )<label>(4)</label></formula><p>However, since normal features are continuously distributed, the boundaries between clusters are not clear. So, it is difficult to discriminate abnormal features with the naive anomaly score precisely. In detail, it can be uncertain which memorized features will match ?(p t ). In this case, even though ?(p t ) is a normal feature, it exists in the middle of the memorized features, resulting in a large distance. So, the naive anomaly score based only on distance risks underestimates the normality of normal features. Thus, we propose a novel scoring function that considers the certainty of ?(p t ).</p><p>The clearer ?(p t ) is matched, the closer the distance to a specific memorized feature is compared to other memo- rized features. Thus, we use softmin to measure how close the nearest c is compared to the other c, and define it as certainty. As a result, the problem of underestimated normality is figured out by multiplying S k t with the certainty of ?(p t ). The formulation is described as follows:</p><formula xml:id="formula_6">A t = e ?St K k=1 e ?D(?(pt),c k t ) ? S t<label>(5)</label></formula><p>Finally, in the test phase of CFA, the anomaly score map, which is the final output of anomaly localization, is obtained from the heatmaps. Note that the heatmaps are generated from naive anomaly scores, which is illustrated in the lower part of <ref type="figure" target="#fig_1">Fig. 2</ref>. Briefly, the k-th heatmap H k = {D(?(p t ), c k t )|1 ? t ? T } is generated and rearranged so that H k has spatial information. Then, Eq. (5) is calculated at all pixel locations to obtain the final output of CFA, i.e., anomaly score map A. Here, in order to output the anomaly score map with the same resolution as the input samples, A is properly interpolated, and Gaussian smoothing of ? = 4 is applied as post-processing.</p><p>In summary, CFA performs transfer learning for targetoriented anomaly localization using the proposed patch descriptor and memory bank. Then, CFA generates heatmaps from task-oriented features and computes sophisticated anomaly scores from them. Therefore, CFA solves the problem that the normality of abnormal features caused by the biased features of the pre-trained CNN is overestimated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section presents various experimental results to evaluate the anomaly detection and localization performance of CFA. All the experiments were performed on MVTec AD benchmark <ref type="bibr" target="#b0">[1]</ref>, that is, the most famous dataset in the anomaly localization field. To verify the robustness of the proposed method, we also presented the performance for Rd-MVTec AD dataset which randomly rotated and cropped MVTec AD dataset. As an evaluation metric, we adopted Area Under the Receiver Operator Curve (AU-ROC), and then evaluated the performance of the proposed method in terms of anomaly detection (I-AUROC) and localization (P-AUROC). In some experiments, we used Area Under the Per-Region-Overlap curve (P-AUPRO) <ref type="bibr" target="#b1">[2]</ref> which can evaluate anomaly localization more precisely.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>This section describes the configurations set up for experiments in this paper. All CNNs used in the experiments were pretrained with ImageNet. To secure multi-scale features on pre-trained CNN, we extract feature maps corresponding to {C 2 , C 3 , C 4 } from intermediate layers as in <ref type="bibr" target="#b13">[12]</ref>. The spatial resolution of each extracted feature map is 1/4, 1/8, and 1/16 of the input sample. Exceptionally, for Ef-ficientNet, which has a very small channel dimension, several feature maps were used for each scale, which were divided into channel dimension values. A 1 ? 1 CoordConv. layer <ref type="bibr" target="#b14">[13]</ref> was used as a Patch descriptor, and its parameters are initialized to He's initializer <ref type="bibr" target="#b10">[9]</ref>. To optimize parameters of patch descriptor, the AdamW <ref type="bibr" target="#b15">[14]</ref> was used, and amsgrad <ref type="bibr" target="#b16">[15]</ref> was applied. Here, the learning rate was set to 1e-3 without any scheduler and weight decay was set to 5e-4. The batch size was set to 4. Patch descriptor was trained for 30 epochs which take about 10 minutes per sub-class. As hyperparameters of CFA, r and ? were set to 1e-5 and 1e-1, respectively. As the number of nearest neighbors for each patch feature, K and J were equally set to 3. The GPU was Quadro RTX 5000, and the CPU was Intel(R) Xeon(R) CPU E5-2650 v4 @ 2.20 GHz to measure the throughput of the proposed method.</p><p>The MVTec AD dataset used in the experiment is the largest dataset consisting of 5354 industrial samples, of which 1725 are test samples. It is divided into 15 subclasses, and we perform transfer learning independently for each class. For pre-processing, each sample of the dataset is resized into 256?256, and is center-cropped into 224?224. And we use the RD-MVTec AD dataset to further consider unaligned samples, which are more difficult to detect outliers. Each sample of the RD-MVTec AD dataset is rotated randomly within ?10 ? . After a random rotation, each sample is resized to 256 ? 256 and then randomly cropped to 224 ? 224. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Results</head><p>This section investigates the quantitative performance of CFA. <ref type="table" target="#tab_0">Table 2</ref> shows I-AUROC and P-AUROC of CFA on MVTec AD dataset when used WRN50-2 pretrained with ImageNet. Here, CFA++ refers to a case that ensembles the results when using cropped images and using only resized samples. The proposed method provided SOTA performance for both texture classes and object classes in terms of I-AUROC, which evaluates the image-level anomaly detection performance. For instance, CFA++ shows 0.4 % better I-AUROC than PatchCore <ref type="bibr" target="#b17">[16]</ref> which has shown SOTA performance so far. Also, even in the aspect of P-AUROC, which is an evaluation metric for pixel-level anomaly localization, the proposed method achieved SOTA performance for object classes. But, the proposed method has slightly lower performance than CFLOW <ref type="bibr" target="#b8">[8]</ref> in terms of P-AUROC when all classes are considered. Nonetheless, while the conventional methods show strength only in a one of anomaly detection and localization, the proposed method guarantees excellent performance in both scenarios. Also, note that the proposed method achieves outstanding performance through feature adaptation despite using a memory bank with a smaller spatial complexity compared to SPADE, PaDiM and PatchCore. We show the performance of each architecture in <ref type="table">Table 4</ref> to demonstrate the performance improvement effect of the proposed method more clearly. We can find that CFA++ provides the highest performance in most classes. In particular, the worst performance of both CFA and CFA++ is just 97.3%, which is much higher than most other techniques. This tendency is because CFA has generalized performance to various classes due to the effect of the proposed feature adaptation. <ref type="table" target="#tab_1">Table 3</ref> shows the performance of CFA on RD-MVTec AD dataset. The RD-MVTec AD dataset consists of the same samples as the MVTec AD dataset, but is not aligned. So, the performance for this dataset is generally lower than that for MVTec AD. Comparing with <ref type="table" target="#tab_1">Table 3</ref>, for example, SPADE's I-AUROC fell by 9%. This means that SPADE is very vulnerable to wild dataset. On the other hand, even in unaligned samples, I-AUROC of CFA++ showed marginal performance degradation of 0.8%, which indicates that the proposed method can distinguish normal features as robustly as HVS. Also, compared to SPADE and PaDiM, CFA++ showed 11.5% and 6.6% higher I-AUROC scores, respectively. A similar trend is also observed in terms of P-AUPRO for evaluating sophisticated detection. For example, CFA++ showed 10.1% higher P-AUPRO score than PaDiM. <ref type="table">Table 5</ref> shows the effect of feature adaptation on anomaly localization. First, take a look at the case of using only the biased features of the pre-trained CNN. Nonadapted pre-trained CNN showed low performance due to the biased features even though they have rich features obtained from large dataset. This is because the normality of normal features was underestimated due to biased features, which negatively affected the anomaly localization. At this time, when only L att was used, CFA improved I-AUROC and P-AUROC scores up to 14.1% and 5.4% in the case of ResNet18. For WRN50-2, I-AUROC and P-AUROC scores were increased by 13.2% and 4.3%, respectively, thanks to L att . This is because normal features are more densely clustered around memorized features. However, a problem remains that they are not discriminative as it is still uncertain which hypersphere they belong to. Therefore, using L rep introduced to obtain discriminative features, further performance improvement is expected. In fact, in ResNet18, I-AUROC and P-AUROC scores improved by 1.1% and 0.3%, respectively, and in WRN50-2, they improved by 0.4% and 0.2%, respectively. By inducing normal features to be clustered more discriminatively, abnormal features were more precisely differentiated. On the other hand, it is interesting that ResNet18 shows a greater performance improvement than WRN50-2. Since ResNet18 uses a relatively small feature dimensions which may increase ambiguity, the problem of hypersphere overlapping can be severed. L rep effectively solved this problem. <ref type="table">Table 6</ref> shows P-AUROC scores when the memory bank is compressed by additionally employing feature dimension reduction ratio ? d and patch reduction ratio ? c . First, note that the memory bank of CFA has a size independent of the target dataset. For example, in the Bottle class consisting of 209 samples, CFA was compressed to a size of approximately <ref type="bibr" target="#b0">1</ref> |X bottle | or 0.5%. That is, the compression ratio ? of each sub-class is calculated as ? d ?c |X | . Even in the memory bank compressed from 25% to about 2%, the P-AUROC score of CFA was slightly decreased by 0.08%. The use of such a lightweight memory bank has a positive effect on the increase in throughput. The throughput of CFA considers inference times of forward pass through pre-trained CNN and patch descriptor. We can observe that if the memory bank is further compressed in the same experimental environment, the throughput increases up to 3.6 times. For example, looking at the 3rd row of <ref type="table">Table 6</ref>, even if the activation of the memory bank is reduced by about 99.9%, the CFA performance hardly decreases and the throughput rather increases up to 2.8 times. This is because the memory bank is compressed to extract only the core features of X , and adaptation is performed so that these features are densely clustered. <ref type="table" target="#tab_3">Table 7</ref> shows the performance of anomaly detection and localization according to pre-trained CNNs. Here, CFA and previous methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b18">17]</ref> was compared for VGG19 <ref type="bibr" target="#b22">[20]</ref>, EffiNet-B5 <ref type="bibr" target="#b24">[22]</ref> and ResNet18 <ref type="bibr" target="#b11">[10]</ref>, which are most popu- larly used for anomaly localization. CFA showed superior I-AUROC scores by 2.4%, 0.9%, and 2.1% than three pretrained CNNs, respectively. Thus, <ref type="table" target="#tab_3">Table 7</ref> supports the universality of the proposed method. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the anomaly score of the patch features per sample according to feature adaptation and scoring function. Here, redness means anomaly score, a dotted circle means the area of abnormal features, and a triangle means memorized features. When a feature biased to a large dataset is used before adaptation, the normality of the normal feature is underestimated and has a score similar to that of the abnormal feature (see the second column of <ref type="figure" target="#fig_2">Fig. 3</ref>). It is difficult to distinguish the two features because the boundary is ambiguous in terms of score. This induces a negative effect that abnormal features cannot be precisely distinguished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Results</head><p>On the other hand, when target-oriented features after feature adaptation are used, they are well clustered. So the normal features of the easy case and abnormal features are clearly distinguished (see the third column of <ref type="figure" target="#fig_2">Fig. 3</ref>). Still, clustering alone cannot precisely score the uncertain abnormal features of the hard case. The proposed scoring function determines the anomaly score by considering the certainty, so even the abnormal features of the hard case can be distinguished precisely, as shown in the last column of <ref type="figure" target="#fig_2">Fig. 3</ref>. As a result, each step of the proposed method effectively improves the anomaly localization performance.</p><p>Next, <ref type="figure">Fig. 4</ref> shows results of anomaly localization that indicate the abnormal areas. The anomaly score map obtained through CFA is interpolated to have the spatial resolution of the input sample and Gaussian filtered with ? = 4 for smooth boundaries. Also, min-max scaling is performed for the normalized anomaly score. The threshold for segmentation result is obtained by calculating the F1-score for all anomaly scores of each sub-class. Experimental results prove that the proposed method can localize abnormal areas well even in rather difficult cases. In addition, we can find that the proposed method has consistent performance in both object and texture classes. As a result, the proposed method performs qualitatively as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we pointed out the bias problem caused by pre-trained CNNs in anomaly localization that mainly uses industrial images. To solve this problem, we proposed Coupled-Hypersphere-based Feature Adaptation (CFA) to obtain target-oriented features. CFA consists of a learnable patch descriptor used with a pre-trained CNN and a memory bank storing memorized features. Through transfer learning and the feature adaptation of patch descriptor associating with a predetermined memory bank, CFA achieved successful target-oriented anomaly localization. CFA showed SOTA performance on the MVTec AD benchmark, the most representative dataset composed of industrial images. Then, the effectiveness of feature adaptation to the target dataset was examined qualitatively/quantitatively through extensive experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overall structure of our proposed method (CFA).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>(Upper) The process of initially modeling the memory bank (lower) the process of generating heatmaps through feature matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of anomaly score of each patch feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Image/Pixel-level AUROC (%) of anomaly localization methods on MVTec AD dataset.</figDesc><table><row><cell cols="2">Model</cell><cell cols="8">SPADE Patch SVDD PaDiM CutPaste CFLOW PatchCore CFA CFA++</cell></row><row><cell></cell><cell>Textures</cell><cell>96.6</cell><cell>94.5</cell><cell>95.3</cell><cell>98.4</cell><cell>98.7</cell><cell>99.0</cell><cell>99.6</cell><cell>99.8</cell></row><row><cell>I-AUROC</cell><cell>Objects</cell><cell>96.0</cell><cell>90.8</cell><cell>95.3</cell><cell>94.1</cell><cell>98.0</cell><cell>99.1</cell><cell>99.2</cell><cell>99.4</cell></row><row><cell></cell><cell>All</cell><cell>96.2</cell><cell>92.1</cell><cell>95.3</cell><cell>95.5</cell><cell>98.3</cell><cell>99.1</cell><cell>99.3</cell><cell>99.5</cell></row><row><cell></cell><cell>Textures</cell><cell>92.9</cell><cell>93.7</cell><cell>95.3</cell><cell>96.9</cell><cell>98.5</cell><cell>97.5</cell><cell>97.2</cell><cell>97.5</cell></row><row><cell>P-AUROC</cell><cell>Objects</cell><cell>97.6</cell><cell>96.7</cell><cell>95.3</cell><cell>97.8</cell><cell>98.7</cell><cell>98.3</cell><cell>98.6</cell><cell>98.9</cell></row><row><cell></cell><cell>All</cell><cell>96.0</cell><cell>95.7</cell><cell>97.5</cell><cell>97.5</cell><cell>98.6</cell><cell>98.2</cell><cell>98.2</cell><cell>98.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Image-level AUROC (%) and Pixel-level AUPRO (%) of anomaly localization methods on RD-MVTec AD dataset.</figDesc><table><row><cell cols="2">Model</cell><cell cols="2">Textures Objects</cell><cell>All</cell></row><row><cell>VAE</cell><cell>I-AUROC</cell><cell>54.7</cell><cell>65.8</cell><cell>62.1</cell></row><row><cell cols="2">(ResNet18) P-AUPRO</cell><cell>23.1</cell><cell>30.2</cell><cell>27.8</cell></row><row><cell>CFA++</cell><cell>I-AUROC</cell><cell>98.6</cell><cell>95.5</cell><cell>96.5</cell></row><row><cell cols="2">(ResNet18) P-AUPRO</cell><cell>81.1</cell><cell>82.2</cell><cell>81.8</cell></row><row><cell>SPADE</cell><cell>I-AUROC</cell><cell>84.6</cell><cell>88.2</cell><cell>87.2</cell></row><row><cell cols="2">(WRN50-2) P-AUPRO</cell><cell>75.6</cell><cell>65.8</cell><cell>69.0</cell></row><row><cell>PaDiM</cell><cell>I-AUROC</cell><cell>92.4</cell><cell>92.1</cell><cell>92.1</cell></row><row><cell cols="2">(WRN50-2) P-AUPRO</cell><cell>77.8</cell><cell>70.8</cell><cell>73.1</cell></row><row><cell>CFA++</cell><cell>I-AUROC</cell><cell>99.7</cell><cell>98.3</cell><cell>98.7</cell></row><row><cell cols="2">(WRN50-2) P-AUPRO</cell><cell>82.2</cell><cell>83.7</cell><cell>83.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .Table 6 .</head><label>456</label><figDesc>Performance comparison of image-level AUROC (%) on each class of MVTec AD dataset. Red, blue, and bold stand for the first, second, and third places Image/Pixel-level AUROC (%) of the proposed method according to Latt and Lrep on MVTec AD dataset. Pixel-level AUROC (%) of the proposed method with additional memory bank compression on MVTec AD dataset.</figDesc><table><row><cell></cell><cell>Class</cell><cell></cell><cell cols="6">SPADE Patch SVDD PaDiM CutPaste CFLOW PatchCore</cell><cell>CFA</cell><cell>CFA++</cell></row><row><cell></cell><cell>Bottle</cell><cell></cell><cell>-</cell><cell>98.6</cell><cell>-</cell><cell>98.2</cell><cell>100</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>Cable</cell><cell></cell><cell>-</cell><cell>90.3</cell><cell>-</cell><cell>81.2</cell><cell>97.6</cell><cell>99.5</cell><cell>99.8</cell><cell>99.8</cell></row><row><cell></cell><cell>Capsule</cell><cell></cell><cell>-</cell><cell>76.7</cell><cell>-</cell><cell>98.2</cell><cell>97.7</cell><cell>98.1</cell><cell>97.3</cell><cell>99.2</cell></row><row><cell></cell><cell>Carpet</cell><cell></cell><cell>-</cell><cell>92.9</cell><cell>-</cell><cell>93.9</cell><cell>98.7</cell><cell>98.7</cell><cell>97.3</cell><cell>99.5</cell></row><row><cell></cell><cell>Grid</cell><cell></cell><cell>-</cell><cell>94.6</cell><cell>-</cell><cell>100.0</cell><cell>99.6</cell><cell>98.2</cell><cell>99.2</cell><cell>99.9</cell></row><row><cell cols="2">Hazelnut</cell><cell></cell><cell>-</cell><cell>92.0</cell><cell>-</cell><cell>98.3</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>Leather</cell><cell></cell><cell>-</cell><cell>90.9</cell><cell>-</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell cols="2">Metal nut</cell><cell></cell><cell>-</cell><cell>94.0</cell><cell>-</cell><cell>99.9</cell><cell>99.3</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>Pill</cell><cell></cell><cell>-</cell><cell>86.1</cell><cell>-</cell><cell>94.9</cell><cell>96.8</cell><cell>96.6</cell><cell>97.9</cell><cell>97.9</cell></row><row><cell></cell><cell>Screw</cell><cell></cell><cell>-</cell><cell>81.3</cell><cell>-</cell><cell>88.7</cell><cell>91.9</cell><cell>98.1</cell><cell>97.3</cell><cell>97.3</cell></row><row><cell></cell><cell>Tile</cell><cell></cell><cell>-</cell><cell>97.8</cell><cell>-</cell><cell>94.6</cell><cell>99.9</cell><cell>98.7</cell><cell>99.4</cell><cell>100.0</cell></row><row><cell cols="2">Toothbrush</cell><cell></cell><cell>-</cell><cell>100.0</cell><cell>-</cell><cell>99.4</cell><cell>99.7</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell cols="2">Transistor</cell><cell></cell><cell>-</cell><cell>91.5</cell><cell>-</cell><cell>96.1</cell><cell>95.2</cell><cell>100.0</cell><cell>100.0</cell><cell>100.0</cell></row><row><cell></cell><cell>Wood</cell><cell></cell><cell>-</cell><cell>96.5</cell><cell>-</cell><cell>99.1</cell><cell>99.1</cell><cell>99.2</cell><cell>99.7</cell><cell>99.7</cell></row><row><cell></cell><cell>Zipper</cell><cell></cell><cell>-</cell><cell>97.9</cell><cell>-</cell><cell>99.9</cell><cell>98.5</cell><cell>99.4</cell><cell>99.6</cell><cell>99.6</cell></row><row><cell></cell><cell>Average</cell><cell></cell><cell>96.2</cell><cell>92.1</cell><cell>95.3</cell><cell>95.5</cell><cell>98.3</cell><cell>99.1</cell><cell>99.3</cell><cell>99.5</cell></row><row><cell cols="6">Backbone Latt Lrep I-AUROC P-AUROC</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>83.7</cell><cell>92.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet18</cell><cell></cell><cell></cell><cell>97.8</cell><cell>97.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>98.9</cell><cell>98.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>85.9</cell><cell>94.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WRN50-2</cell><cell></cell><cell></cell><cell>99.1</cell><cell>98.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>99.5</cell><cell>98.5</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>? d</cell><cell>?c</cell><cell cols="3">P-AUROC Throughput</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>1</cell><cell>98.45</cell><cell>48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>WRN50-2</cell><cell cols="2">1/2 1/2 1/4 1/4</cell><cell>98.44 98.44</cell><cell>93 (1.9x) 132 (2.8x)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">1/8 1/8</cell><cell>98.36</cell><cell>172 (3.6x)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>Image/Pixel-level AUROC (%) of the proposed method with various pretrained CNNs on MVTec AD dataset.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell cols="2">I-AUROC P-AUROC</cell></row><row><cell>VGG19</cell><cell>DFR CFA++</cell><cell>93.8 96.2</cell><cell>95.5 95.3</cell></row><row><cell>EffiNet-B5</cell><cell>PaDiM CFA++</cell><cell>97.9 98.8</cell><cell>97.5 98.0</cell></row><row><cell>ResNet18</cell><cell>CFLOW CFA++</cell><cell>96.8 98.9</cell><cell>98.1 98.1</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Improving unsupervised defect segmentation by applying structural similarity to autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sindy</forename><surname>L?we</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02011</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sub-image anomaly detection with deep pyramid correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niv</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Padim: a patch distribution modeling framework for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelique</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Audigier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="475" to="489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast and accurate model scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="924" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cflow-ad: Real-time unsupervised anomaly detection with</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Gudovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Ishizaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Kozuka</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<idno type="arXiv">arXiv:2107.12571</idno>
		<title level="m">Visualization of results of anomaly localization for object classes in MVTec AD benchmark. localization via conditional normalizing flows</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cutpaste: Self-supervised learning for anomaly detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9664" to="9674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piero</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felipe</forename><forename type="middle">Petroski</forename><surname>Such</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sergeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03247</idno>
		<title level="m">An intriguing failing of convolutional neural networks and the coordconv solution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09237</idno>
		<title level="m">On the convergence of adam and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Towards total recall in industrial anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Latha</forename><surname>Pemula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joaquin</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08265</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Same same but differnet: Semi-supervised defect detection with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1907" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ursula</forename><surname>Sebastian M Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Anoseg: Anomaly segmentation network using self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jouwon</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyeongbo</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye-In</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Gyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suk-Ju</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03396</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Patch svdd: Patch-level svdd for anomaly detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungroh</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
