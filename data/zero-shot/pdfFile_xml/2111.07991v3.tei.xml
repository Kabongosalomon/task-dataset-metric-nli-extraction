<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiT : Zero-Shot Transfer with Locked-image text Tuning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basil</forename><surname>Mustafa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Google Research</orgName>
								<orgName type="institution" key="instit2">Brain Team</orgName>
								<address>
									<settlement>Z?rich</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LiT : Zero-Shot Transfer with Locked-image text Tuning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents contrastive-tuning, a simple method employing contrastive training to align image and text models while still taking advantage of their pre-training. In our empirical study we find that locked pre-trained image models with unlocked text models work best. We call this instance of contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model to read out good representations from a pre-trained image model for new tasks. A LiT model gains the capability of zero-shot transfer to new vision tasks, such as image classification or retrieval. The proposed LiT is widely applicable; it works reliably with multiple pre-training methods (supervised and unsupervised) and across diverse architectures (ResNet, Vision Transformers and MLP-Mixer) using three different imagetext datasets. With the transformer-based pre-trained ViTg/14 model, the LiT model achieves 85.2% zero-shot transfer accuracy on the ImageNet test set, and 82.5% on the challenging out-of-distribution ObjectNet test set. 1 arXiv:2111.07991v3 [cs.CV] 22 Jun 2022 sults [31,46] and supervised fine-tuning results [13,69]. The best LiT model also sets new state-of-the-art on several outof-distribution (OOD) ImageNet test variants, compared to previous supervised and unsupervised methods. For example, it achieves 82.5% accuracy on the challenging Object-Net test set [1], outperforming the previous state-of-the-art method [46] by 10.2%.</p><p>We believe the reason that LiT works well lies in its decoupling of data sources and techniques for learning image descriptors and vision-language alignment. Image-text data can be great for learning correspondences between natural language and the visual world, but, at the same time, it may not be precise and clean enough to result in state-of-the-art image descriptors. In this paper we carefully investigate this hypothesis and support it with empirical evidence.</p><p>The proposed LiT works with both supervised and self-supervised pre-trained models. We verify LiT across three image-text datasets, with Vision Transformer [21], ResNet <ref type="bibr" target="#b32">[33]</ref>, and MLP-Mixer [61] architectures. We also show that with a self-supervised pre-trained model, i.e. DINO [5]  or , LiT achieves better performance compared to from-scratch contrastive-learning.</p><p>Another contribution of this paper is the proposed recipe for high-performance zero-shot models that can be trained using only modest computational resources and public datasets. By re-using already pre-trained models (e.g. publicly released in the literature), the computational resources used to train the image models can be amortized. Furthermore, we explore publicly available datasets such as YFCC100m [60] and CC12M <ref type="bibr" target="#b5">[6]</ref>. Combined with the computational efficiency, we hope to facilitate contributions from a wider audience to research in zero-shot transfer. 2</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Transfer learning <ref type="bibr" target="#b44">[45]</ref> has been a successful paradigm in computer vision <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b42">43]</ref>. Zero-shot learning <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b66">66]</ref> is an alternative approach aiming to develop models that can handle a new task without task-specific data or adaptation protocols. Recently it was demonstrated that web-sourced paired image-text data can be used to pre-train strong models for zero-shot transfer <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>. Zero-shot transfer differs from classical zero-shot learning in that the transfer setup may see relevant supervised information during pretraining; it is zero-shot insofar as no supervised examples are used during the transfer protocol. GPT-3 <ref type="bibr" target="#b3">[4]</ref> explored a similar zero-shot transfer setup using model prompting via natural language.</p><p>In <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref> authors propose a contrastive learning framework where an image model (or image tower) is trained simultaneously with a text model (or text tower). Both towers are trained to minimize a contrastive loss, which encourages Right: results on privately gathered data, LiT halves the gap between previous fromscratch methods CLIP <ref type="bibr" target="#b45">[46]</ref>, ALIGN <ref type="bibr" target="#b30">[31]</ref> and supervised finetuning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b69">69]</ref>.</p><p>representations of paired images and texts to be similar, and representations of non-paired images and texts to be dissimilar. At test time, the resulting model can be used for zero-shot image classification by comparing the image embedding with embeddings of textual class descriptions. In this paper, we adopt a contrastive learning framework and propose a more data-and compute-efficient strategy named contrastive-tuning. The key idea is to tune the text tower using image-text data, while using a pre-trained, strong image model as the image tower. During training, both towers' weights can be locked or unlocked, leading to different design choices that are illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. Specifically, we find that locking the image tower works best, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. We call this specific instance of contrastive-tuning "Locked-image Tuning" (LiT), which just teaches a text model to read out suitable representations from a pre-trained image model. LiT achieves better results compared with the from-scratch CLIP <ref type="bibr" target="#b45">[46]</ref> or ALIGN <ref type="bibr" target="#b30">[31]</ref> models. With the pre-trained model ViT-g/14 <ref type="bibr" target="#b69">[69]</ref>, LiT achieves 85.2% zero-shot transfer accuracy on ImageNet, halving the gap between previous best zero-shot transfer re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>This work is closely related to a vast amount of literature on transfer learning in vision <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b59">59]</ref>. The main idea of transfer learning is to leverage already pre-trained models to solve a new task better and faster, as opposed to less efficient training from-scratch. This paradigm is usually implemented as a two-step procedure: (1) pre-train (once) an initial model on a large dataset of images that are (weakly)labeled or using self-supervised losses and (2) fine-tune the pre-trained model for a task of interest using supervised data. In the context of modern deep learning, many earlier works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref> used supervised pre-training to learn transferrable feature representations, with the Vision Transformer revisiting and improving this approach <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b69">69]</ref>. It was shown that scaling up model and dataset sizes simultaneously leads to dramatic improvements in transfer effectiveness <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b69">69]</ref> and robustness <ref type="bibr" target="#b17">[18]</ref>. Crucially, large pre-trained models exhibit outstanding capabilities in learning in the low-data (few-shot) regime <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Still, collecting task-specific data and fine-tuning large pre-trained models remains time-consuming and potentially costly in many realistic scenarios. Zero-shot transfer is an alternative paradigm that sidesteps the fine-tuning stage entirely and performs classification solely based on a description of the target classes. Early works demonstrated how to train zero-shot classifiers based on attributes <ref type="bibr" target="#b35">[36]</ref> or numerical descriptors <ref type="bibr" target="#b36">[37]</ref>. Another approach, which we adopt in this work, is to learn an alignment between image and text embedding spaces <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b71">71]</ref>. This approach has demonstrated that with modern architectures, contrastive learning, and large data sources it is possible to obtain performance that is competitive with the classical two-step approach that involves fine-tuning on the downstream data <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>. Other efforts in this direction explore image-text alignment or masked language (or image region) modeling <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b37">38]</ref>. The models have been applied to diverse downstream tasks, including visual question answering <ref type="bibr" target="#b23">[24]</ref>, visual commonsense reasoning <ref type="bibr" target="#b68">[68]</ref> and image captioning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b56">56]</ref>.</p><p>Contrastive learning techniques are another closelyrelated research direction. The high-level idea of a contrastive loss is to simplify the learning task by requiring the model to select the correct answers out of a finite set of carefully designed options. Intuitively, this simplification of the task may encourage the model to focus on highlevel information in an image instead of generic information, resulting in high quality learned representations. Early works that investigate very specific instances of this idea include <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44]</ref>. More recently, contrastive learning was formulated and studied in more general settings <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b62">62]</ref>, leading to very promising results. Finally, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref> use contrastive learning for learning from image-text data and derive state-of-the-art zero-shot image classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive pre-training</head><p>Collections of images (potentially noisily) paired with free-form text descriptions have emerged as a powerful resource for training visual models. The key advantage therein is that it is not limited by a finite set of predefined categories and instead describes images using open-ended natural language. As a result, models learned from this data can serve as zero-shot learners for a wide range of tasks, e.g. classification and image/text retrieval.</p><p>Contrastive pre-training is one particularly effective approach for training models from image-text data, which was recently proven to work well in practice <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref>. We take a closer look at this approach and propose a simple, yet highly effective recipe to significantly enhance contrastive  pre-training from image-text data.</p><p>The key idea behind the contrastive pre-training approach is to learn two embedding models: an image model and a text model, both of which produce representations of the same dimensionality. These models are trained using a contrastive loss. This loss encourages corresponding image-text pairs to have similar embeddings and, conversely, encourages non-corresponding pairs to have distinct embeddings. See <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b71">71]</ref> for the detailed discussion of the contrastive loss function.</p><p>An important detail of this loss function is whether the loss is computed on each accelerator device independently and then accumulated or computed jointly across all devices. We ablate this design choice (Appendix F) and confirm that the latter <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref> consistently results in better performance. We therefore use the global loss in all our experiments and ablations.</p><p>After image and text towers are trained, they can be readily used for zero-shot classification: class names or descriptions are embedded with the text model. Then, for a given image the label is selected that has the embedding closest to the embedding of the image. This approach also works for image-text retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Contrastive-tuning</head><p>Contrastive pre-training can be viewed as learning two tasks at the same time: (1) learning an image embedding and (2) learning a text embedding to align with the image embedding space. While contrastive pre-training on imagetext data works well for solving both of these tasks simultaneously, it may be not the optimal approach.</p><p>When not using contrastive pre-training on image-text data, a standard approach to learning image embeddings is to use a large and relatively clean dataset of (semi)manually labeled images. Large scale and and high quality of such data result in state-of-the-art image embeddings. Some dataset choices for learning powerful image embeddings are ImageNet-21k <ref type="bibr" target="#b14">[15]</ref>, JFT-300M <ref type="bibr" target="#b57">[57]</ref>.</p><p>However, this common approach has a clear weakness: it is limited to a predefined set of categories and, thus, the resulting models can only reason about these categories. In contrast, image-text data does not have this limitation, as it learns from the free-form text that potentially spans a broad range of real-life concepts. On the other hand, image-text data that is available may be of lower quality (for learning image embeddings) than carefully curated datasets.</p><p>We propose contrastive-tuning to combine advantages of both sources of data. One specific way of doing this is to initialise the contrastive pre-training with an image model that was already pre-trained using cleaner (semi-)manually labeled data. This way the image-text alignment is learned independently of image embedding, enabling benefit from both data sources.</p><p>Beyond using supervised pre-trained image models, the proposed contrastive-tuning is also flexible enough to integrate any models that can produce meaningful representations. We verify this in our experiments using selfsupervised pre-trained image models.</p><p>Similar lines of reasoning can also be applied to the text tower, as there are many powerful pretrained models that use text-specific data sources and learning techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Design choices and Locked-image Tuning</head><p>Introducing pre-trained image or text models into the contrastive learning setting involves several design choices. First, each tower (image and text) can independently be initialized randomly or from a pre-trained model. For a pretrained model there are at least two variants: we can lock (freeze) it or allow fine-tuning. Note that there are many choices between these two extremes (e.g. partial freezing of selected layers, or custom learning rates), but they are not investigated in this paper.</p><p>Pre-trained image-text models may have different representation sizes, while the contrastive loss expects representations of the same size. To compensate, we add an optional linear projection (head) to each tower, which maps the representations to a common dimensionality. Preliminary investigations with tried MLP-based heads did not yield significant improvements over such a simple linear head.</p><p>We introduce a two-character notation to discuss the potential design choices outlined above (see <ref type="figure" target="#fig_2">Figure 2</ref>). Each character encodes the setting chosen for the image model and the text model (in this order). We define three potential settings: L (locked weights, a initialized from pre-trained model), U (unlocked/trainable weights, initialized from a pre-trained model) and u (unlocked/trainable weights, randomly initialized). For example, the notation Lu means locked pre-trained image model, and unlocked (trainable) randomly initialized text model. Previous works training models from scratch <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref> are uu. In our experiments we find the Lu setting to work particularly well, so we explicitly name it as Locked-image Tuning (LiT ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Image-text datasets</head><p>CC12M. The Conceptual Captions dataset <ref type="bibr" target="#b51">[52]</ref> extracts, filters &amp; transforms image &amp; alt-text pairs from web pages. We use the latest 12 million image-text pair version, i.e. CC12M <ref type="bibr" target="#b5">[6]</ref>. Due to expired URLs, only 10 million imagetext pairs were used for our experiments.</p><p>YFCC100m. The Yahoo Flickr Creative Commons dataset <ref type="bibr" target="#b60">[60]</ref> contains 100 million media objects. Of these, 99.2 million are photos that come with rich metadata including camera info, timestamp, title, description, tags, geolocation, and more. <ref type="bibr" target="#b45">[46]</ref> defines and uses a subset of 15 million images that have been filtered for English text of high quality, which we call YFCC100m-CLIP. A detailed investigation of this dataset and how best to use it, including whether to filter it, is presented in Appendix E.</p><p>Our dataset. We collect 4 billion image and alt-text pairs following the same process as ALIGN <ref type="bibr" target="#b30">[31]</ref>, with the same image-based filtering but simpler text-based filtering. Appendix L shows that reducing text filtering does not harm performance. To avoid misleading evaluation results, we remove from our dataset near-duplicate images of all splits from all datasets we evaluate on. We do not consider the creation of our dataset a main contribution of this paper; we just simplify the data collection process in ALIGN <ref type="bibr" target="#b30">[31]</ref> to demonstrate the efficacy of our methods at scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we first compare LiT to state-of-the-art image-text models. We consider two scenarios: (1) only using public datasets for model training and (2) using privately gathered data. We then present learnings from experimental evaluations of contrastive tuning design choices with various training settings &amp; datasets. We generally perform evaluation on 0-shot ImageNet classification ("0-shot") and MSCOCO image ("T?I") and text ("I?T") retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison to the previous state-of-the-art</head><p>In this section, we present LiT results on our dataset. The image tower is initialized with a ViT-g/14 model 3 pre-trained on JFT-3B <ref type="bibr" target="#b69">[69]</ref>, which has been de-duplicated against the downstream tasks. We use 32k batch size, and tune for 18 billion image-text pairs seen (roughly 550k steps). See Appendix C for details. <ref type="bibr" target="#b2">3</ref> An earlier version of this paper reported slightly lower numbers with the ViT-g/14 model, e.g. ImageNet accuracy was 84.5% vs 85.2%. We fixed a model loading bug with ViT-g/14 in this version. Other results are not affected. We compare the LiT method with the previous state-ofthe-art methods, including CLIP <ref type="bibr" target="#b45">[46]</ref> and ALIGN <ref type="bibr" target="#b30">[31]</ref>. In <ref type="table">Table 1</ref>, we report zero-shot classification results on the ImageNet dataset, five out-of-distribution test variants and seven VTAB-natural tasks <ref type="bibr" target="#b70">[70]</ref>. Our model significantly outperforms the previous state-of-the-art methods at Ima-geNet zero-shot classification. The 9% and 8.8% improvement over CLIP and ALIGN, respectively, halves the gap between zero-shot transfer results and supervised fine-tuned results <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b69">69]</ref>.</p><p>Robustness. We evaluate robustness on ImageNet-v2 <ref type="bibr" target="#b48">[49]</ref>, -R <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b64">64]</ref>, -A <ref type="bibr" target="#b27">[28]</ref>, -ReaL <ref type="bibr" target="#b1">[2]</ref>, and ObjectNet <ref type="bibr" target="#b0">[1]</ref>, following CLIP and ALIGN. On all of the OOD variants, our model consistently outperforms the previous models. Notably, the LiT model sets a new state-of-the-art 82.5% accuracy on the ObjectNet test set. The pre-trained ViT-g/14 model <ref type="bibr" target="#b69">[69]</ref>, achieves 70.5% accuracy on the ObjectNet test set when fine-tuned on ImageNet. This model gets more than 10% improvement when instead locked-image tuned (LiT) on our image-text dataset.</p><p>Diverse downstream tasks. We evaluate the LiT models on VTAB, consisting of 19 diverse tasks. We report averaged results on seven VTAB-natural tasks in <ref type="table">Table 1</ref>. The LiT models achieve promising zero-shot results, comparing to the supervised fine-tuned ResNet50 baseline. In Appendix I.2, we present zero-shot transfer details on VTAB, as well as more results and analysis on the specialized tasks and structured tasks.</p><p>Data &amp; compute efficiency. <ref type="figure" target="#fig_0">Figure 1</ref> shows more results when tuning with fewer seen image-text pairs. With LiT the model achieves 81.7% top-1 accuracy on 0-shot ImageNet transfer, with only 300M image-text pairs seen. In comparison, it took the from-scratch method (i.e. CLIP)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>ImgNet ImgNet-v2 Cifar100 Pets 12.8B image-text pairs seen, i.e. 40 times more data pairs, to reach 76.2% top-1 accuracy. With a pre-trained image model, the proposed setup converges significantly faster than the standard from-scratch setups reported in the literature. LiT provides a way to reuse the already pre-trained models in the literature, amortizing the computational resources used to re-generate the image models.</p><p>Results on public datasets. Given high data efficiency of LiT, we investigate how well it performs when using only smaller, publicly available models and datasets. Specifically, we tune an ImageNet-21k pre-trained ViT-L/16 model <ref type="bibr" target="#b55">[55]</ref> on the union of the YFCC100m-CLIP and CC12M datasets. More details of the training setup are provided in Appendix D. As a result we achieve unprecedented 75.7% zero-shot transfer on ImageNet, an absolute improvement of 30.9% over the previously reported stateof-the-art result <ref type="bibr" target="#b28">[29]</ref> that uses only public data sources. We also obtain strong results on a wide range of robustness datasets and the VTAB-natural tasks, see <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Evaluation of design choices</head><p>Small-scale thorough investigation. We first perform an in-depth study on various combinations of the image and text towers being initialized with pre-trained weights and locked (L) or unlocked (U) or being randomly initialized and unlocked (u). We train each setting many times on the YFCC100m-CLIP dataset, varying the total number of steps from 2 500 to 60 000 in order to understand the setting's trajectory, and sweeping over learning-rates and weightdecays to avoid being misled. Details can be found in Appendix D. <ref type="figure" target="#fig_4">Figure 3</ref> shows the best result for each setting for each duration, i.e. each point on the curves is a separate full run for that duration. It is evident that locking the image tower almost always works best and using a pre-trained image tower significantly helps across the board, whereas using a pre-trained text tower only marginally improves performance, and locking the text tower does not work well.</p><p>This still holds in the near-infinite data regime. One may hypothesize that locking the pre-trained image tower only helps because the YFCC100m-CLIP dataset is relatively small (15 million images, compared to 400M <ref type="bibr" target="#b45">[46]</ref> or 1.8B <ref type="bibr" target="#b30">[31]</ref>), and that a randomly initialized image tower will eventually outperform a locked one on much larger imagetext datasets. The trajectory of the Uu and UU settings in   Maybe surprisingly, experimental results show that this is not the case, and locking the image tower provides benefits even when contrastively tuning on a very large dataset of image-text pairs. <ref type="table">Table 2</ref> shows results of contrastive tuning on our dataset of 4 billion images in three settings: Lu, Uu, and uu. Implementation details can be found in Appendix C. The from-scratch method uu unsurprisingly achieves better performance than with smaller datasets such as CC12M and YFCC100m-CLIP.</p><p>Initializing the image tower from a pre-trained model provides even better performance and is a relatively straightforward extension of CLIP/ALIGN. Perhaps surprisingly, the frozen setup Lu, achieves even better results. While potentially counter-intuitive, another perspective is that LiT simply learns a text tower that extracts knowledge from a strong image embedder. This flexible &amp; performant setup can turn existing vision backbones into a zero-shot learners, by attaching a text-embedding tower.</p><p>Why is locked (L) better than unlocked (U)? It is somewhat surprising and counter-intuitive that locking the image tower works better than allowing it to adapt during the contrastive-tuning; <ref type="figure" target="#fig_7">Figure 4</ref> gives hints as to why.</p><p>The first row shows that locking the image tower leads to substantially worse (contrastive) loss on the dataset used for LiT, while the loss of the locked image variant is substan-  tially better on out-of-distribution datasets such as COCO captions (middle row).</p><p>We also measure the representation quality of the image model (bottom row) via the performance achieved by a few-shot linear regression on its pre-logits, as is commonly done in the self-supervised representation learning literature. Taken together, these figures reveal that the image representation of a pre-trained image model generalizes very well, but contrastively fine-tuning it worsens the generality of the visual representation, leading it to be better on the contrastive dataset, but worse everywhere else. This indicates that locking the image tower during tuning, i.e. LiT, leads to a text model that is well aligned to an already strong and general image representation, as opposed to an image-text model that is well aligned but specialized to the dataset used for alignment.</p><p>Intermediate variants, such as first locking and later unlocking the image tower or separating learning-rates are explored in Appendix H; we did not find a strictly better setup than LiT and leave this as an open research question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">LiT works better for more generally pretrained models</head><p>One may believe that LiT only works because the image tower is initialized with a backbone that was supervisedly pre-trained for classification, and hence remains a supervised classifier, as opposed to becoming an image-text model. We design a controlled experiment to verify whether that is the case. We find that on the contrary, more generally  <ref type="table">Table 3</ref>. The role of pre-training method for the image model: as long as it is general, it does not matter. The background coloring denotes whether a value is similar or far away from the others in that column.</p><p>pre-trained models are better suited for LiT.</p><p>We select a set of image models that all use the same ViT-B/16 architecture but were pre-trained in various ways: supervised (AugReg <ref type="bibr" target="#b55">[55]</ref>) on ImageNet (IN), on the large but narrow Places <ref type="bibr" target="#b38">[39]</ref> dataset, on the much broader ImageNet-21k (IN21k), or fully unsupervised (DINO and MoCo-v3). All but the Places model achieve similar ImageNet top-1 accuracies of around 77% as reported in their respective publications, and can thus be considered similarly good models. <ref type="table">Table 3</ref> shows model performance without LiT (Im-ageNet 10-shot, and accuracy when fully fine-tuned on ImageNet) alongside achieved performance with LiT on YFCC100m-CLIP (zero-shot ImageNet classification and MS Coco retrieval).</p><p>From these results, we conclude that models which are pre-trained in a generic way (e.g. on large amounts of data, or in an unsupervised way) and have similar representation quality, become similarly good image-text models after locked-image tuning (LiT). However, this also shows that a narrowly pre-trained model (AugReg-IN and AugReg-Places) will perform misleadingly well on its narrow task (0-shot IN for AugReg-IN), but significantly fall behind on more general image-text tasks (MSCOCO captions). These findings highlight the importance of a generally pre-trained model and varied set of evaluation tasks.</p><p>Is this specific to ViT image models? No. Here we fixed the architecture to avoid confounders, but Appendix A explores other architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Which text model to use?</head><p>While related work has so far focused on the image model, the text model plays an important yet underexplored role in contrastive image-text learning. We consider four possible transformer-based text models <ref type="bibr" target="#b63">[63]</ref>-the transformer from ViT-B <ref type="bibr" target="#b20">[21]</ref> which also resembles that used in CLIP <ref type="bibr" target="#b45">[46]</ref>, T5-base <ref type="bibr" target="#b46">[47]</ref>, mT5-base <ref type="bibr" target="#b67">[67]</ref>, and the classic BERT-base <ref type="bibr" target="#b16">[17]</ref>-and whether to initialise them randomly, or from a pre-trained checkpoint. BERT uses a WordPiece (WP) tokenizer <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b65">65]</ref>, and all others use the Sentence-Piece (SP) tokenizer <ref type="bibr" target="#b34">[35]</ref>, a component which we also ablate with the ViT model. <ref type="table" target="#tab_3">Table 4</ref> shows the results of LiT using an AugReg-ViT-B/32 on YFCC100M-CLIP and our dataset using the base sized variant of these text models. We sweep over various learning-rates and weight-decays separately for each combination to avoid being misled. Our observations differ slightly between the relatively small YFCC100m-CLIP dataset, and our much larger dataset, we first discuss the former. First, we see a small but consistent improvement by initializing the text model with pre-trained weights. Second and somewhat unexpectedly, we find that the BERT model performs significantly better than others, especially for retrieval. In order to disentangle the contribution of the architecture from the tokenizer, we further apply LiT using a ViT text encoder paired with BERT's WordPiece tokenizer and see no improvement. We believe that small differences in the architecture, such as initialization and LayerNorm placement, are responsible for the slightly better generalization of BERT that we observe. However, we also found the BERT model to be less stable to train. For the large-scale experiments on our dataset, we do not observe this improvement anymore, and favor sticking with the more stable ViT SentencePiece combination.</p><p>What about model capacity? Previous works used relatively low-capacity text models. We show in Appendix B that increasing the text tower's capacity consistently improves performance. The same is true, and more pronounced, for the image tower.  <ref type="table">Table 5</ref>. Results on various de-duplication setups. #tune images are removed from the LiT dataset due to #eval images in the evaluation datasets. We report results averaged across three runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Do duplicate examples matter for LiT?</head><p>One relevant question in the context of large-scale training is the role of duplicate examples between upstream datasets and downstream datasets. We answer this question by performing experiments on three different upstream deduplication setups: (1) no de-duplication; (2) de-duplicate against downstream test splits only; (3) de-duplicate against downstream train and test splits. We conduct experiments using the Lu setup on our dataset. We use a B/32 image model pre-trained on the JFT-3B dataset <ref type="bibr" target="#b69">[69]</ref>, which has been de-duplicated against downstream train and test splits.</p><p>In <ref type="table">Table 5</ref>, we show the number of duplicate samples found between upstream datasets and downstream datasets during de-duplication. In the de-duplication process, a downstream image may have multiple upstream duplicate examples, e.g. due to image copies on the web. As a result, the number of duplicate examples on the upstream dataset is significantly larger than the number on the downstream datasets. The downstream number indicates how many downstream images had a duplicate detected, while the upstream number indicates how many images are removed from the image-text dataset.</p><p>We apply LiT on the three setups, and the zero-shot transfer results vary little. More results with larger backbone can be found in Appendix K, with consistent conclusions. It indicates that the duplication of examples here does not influence the results strongly. This observation is also consistent with previous conclusions <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b45">46]</ref>. A possible interpretation is that with a large upstream dataset, the model may not memorize those duplicate examples.</p><p>Throughout this paper, we report results using the strictest setup (3) with proper de-duplication against downstream train splits and test splits, to avoid data leakage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Technical advantages of locked image models</head><p>Besides potential modelling advantages previously explored, using a locked image tower has several more benefits. First, the training is significantly sped-up and memory use reduced as no gradients are computed for the image tower. Second, if no augmentations are used, such as in our large-data experiment, the image model's embeddings can be precomputed once, further reducing compu-en ru tr es fa fr de ja vi zh ar Prompting language 0% 20% 40% 60% ImageNet 0-shot CLIP subset Lu T5 LU T5 Lu mT5 LU mT5 <ref type="figure">Figure 5</ref>. Including non-English data unlocks multilingual zeroshot models without hurting English performance. In such a regime, multilingual text pre-training can be more useful for lowresource languages.</p><p>tation time and memory requirements. Appendix G shows concrete measurements. Taken together, these implementation features unlock the use of enormous models at very large batch-sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Preliminary multilingual experiments</head><p>It is currently common practice <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b45">46]</ref> to filter imagetext datasets to English language data only. We believe that removing this restriction has the potential to benefit a larger part of the world's population. Concurrent work <ref type="bibr" target="#b29">[30]</ref> has relied on additional translated text pairs for training the text encoder. In contrast, we do not require any translations and purely rely on the pre-trained, locked image model to bridge the language barrier. In this section, we report preliminary experiments that show the promise of LiT for multilingual image-text models.</p><p>We apply LiT on an AugReg-i21k ViT-B/32 with the T5 <ref type="bibr" target="#b46">[47]</ref> and mT5 <ref type="bibr" target="#b67">[67]</ref> base encoders, both with and without the pre-trained checkpoints. We do this on both the full YFCC100m dataset, and the reduced English-only CLIP subset, and we use all available text as supervision signal (See Appendix E). We evaluate the resulting model's multilingualism in two ways, both of which have limitations discussed in Appendix J. First, we translate the ImageNet prompts into the most common languages using an online translation service and perform zero-shot classification in each of them; this evaluation is shown in <ref type="figure">Figure 5</ref>. Second, we use the Wikipedia based Image Text (WIT) dataset <ref type="bibr" target="#b53">[54]</ref> to perform T ? I retrieval across more than a hundred languages. <ref type="figure" target="#fig_8">Figure 6</ref> gives a summary of this evaluation; a more detailed variant is provided in Appendix J.</p><p>The high-level conclusions are consistent across both evaluations: training on the full dataset improves performance on non-English languages much more than on English, using a multilingual tokenizer (as in mT5) significantly helps languages that do not use the Latin script, and starting from a pre-trained multilingual text model can  further help. The combination of all three improvements barely has any effect when evaluated in English, but significantly improves performance on the long tail of languages. This is a promising result for unlocking multimodal models for low-resource languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Limitations. This work explores only classification and retrieval as zero-shot transfer tasks. We leave evaluating zero-shot transfer to a broader set of tasks such as detection, segmentation, visual question answering, and image captioning as future work in order to limit our scope.</p><p>On cross-modal retrieval tasks, we have not observed as clear a benefit of the Lu setup compared to Uu or UU <ref type="figure" target="#fig_4">(Figure 3)</ref>. For very long tuning schedules, Uu or UU sometimes overtake Lu on these tasks. Our results suggest that the proposed Lu setup can still save computational cost within a fixed budget, but with a large enough budget, it may be useful to also consider the Uu setup if zero-shot classification is not the primary end goal.</p><p>Societal impact. This work shows how one can easily add a text-tower to a pre-trained image model. While there are many useful applications, like most research, it is a double-edged sword: the technique also makes it simpler to create malicious, offensive, or obscene text tower pendants to existing image models. Further research is needed on how to best equip open-world image-text models with the behaviour we desire.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We present a simple method named contrastive-tuning that allows transferring any pre-trained vision model in a zero-shot fashion. More specifically, the proposed LiT setup leads to substantial quality improvements on zeroshot transfer tasks. It halves the gap between the fromscratch contrastive learning setup, and the per-task supervised fine-tuning setup. LiT makes it possible to turn publicly available models into zero-shot classifiers using pub-licly available data, and rival the performance of previous works which rely on more, proprietary data.</p><p>We hope that this work motivates future research on how to smartly re-use and adapt already pre-trained models for different research problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Is this specific to ViT image models?</head><p>No. In the main paper, we only used ViT models for all experiments. Could it be that LiT only works with ViT models, or is in some way specific to the Transformer architecture?</p><p>In order to verify that this is not the case, we applied the same recipe to comparably-sized models of different families. <ref type="table">Table 6</ref> shows the zero-shot performance with LiT on the CC12M dataset for ViT <ref type="bibr" target="#b20">[21]</ref>, Mixer <ref type="bibr" target="#b61">[61]</ref>, and ResNet <ref type="bibr" target="#b32">[33]</ref>; all pre-trained on ImageNet21k. Following <ref type="bibr" target="#b13">[14]</ref>, we report parameter count, inference speed, and FLOPs to indicate our attempt to match the "model size". The results show that LiT works for different model families, but also confirm the finding of <ref type="bibr" target="#b45">[46]</ref> that ViT models do seem more amenable to learning image-text mappings than other architectures of similar size.  <ref type="table">Table 6</ref>. LiT with different model families. Showing zero-shot top-1 accuracy on ImageNet in comparison to fine-tuning (column "Adapt"). Inference "Speed" is in images per second per core.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Larger model capacity yields better results</head><p>Increasing the model capacity of the pre-trained imagetower improves zero-shot ImageNet accuracy more than increasing the capacity of the text-tower. <ref type="figure" target="#fig_10">Figure 7</ref> shows substantial gains in the private data setup when the image tower capacity is increased from B/32 and base text tower (74.5%) to g/14 and huge text tower (81.2%). We take the pretrained image towers from <ref type="bibr" target="#b69">[69]</ref>, and the text towers were trained from scratch.</p><p>The improvements in the public CC12M data setup range from 61.1% with a B/32 image tower and base text tower up to 67.6% with the L/16 model combined with a large text tower. In this setup, we used pre-trained BERT text towers <ref type="bibr" target="#b16">[17]</ref> and pre-trained image models from <ref type="bibr" target="#b55">[55]</ref> (using the "recommended checkpoints"). Note that in this case the increase from B/16 to L/16 is more modest (from 66.9% to 67.6% with the large text tower), and we see a similar improvement in ImageNet zero-shot performance when increasing the text tower size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Tuning details on our dataset</head><p>We use the pre-trained transformer models from <ref type="bibr" target="#b69">[69]</ref>. ViT-B/32 was used for most of the ablation tests, and the larger ViT-B/16, ViT-L/16 and ViT-g/14 models are used in Section B for capacity impact evaluations. For our best Lu results, we adopt the ViT-g/14 model pre-trained in <ref type="bibr" target="#b69">[69]</ref>.</p><p>During contrastive-tuning, we use the AdaFactor optimizer <ref type="bibr" target="#b52">[53]</ref> following <ref type="bibr" target="#b69">[69]</ref>. We use 0.001 learning rate, and the default ? 1 = 0.9 and ? 2 = 0.999 for AdaFactor optimizer. We use batch size 16384 by default, unless otherwise noted. Input image is simply resized to 224?224 resolution (apart from 288 ? 288 resolution for "g/14*" model). No weight decay is used during tuning. We use cosine learning rate schedule with a linear learning rate warmup of 10k steps. We train our models for 55k steps by default, which equals to about 900 million seen image-text pairs during tuning. For our best runs, we scale up the training schedule to 18 billion seen image-text pairs. We use 128 TPU cores by default for the above experiments, and 256 TPU cores for our best run with 18 billion seen image-text pairs.</p><p>In the Lu setup, we do not attach the optional linear head on the image tower. We observe a very small quality improvement without using the image linear head, thus we remove it for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Tuning details on CC12m</head><p>We use pre-trained ViT models from <ref type="bibr" target="#b55">[55]</ref> (unless otherwise noted, we used the "recommended checkpoints" from that repository). On the text side, we use BERT-base and BERT-large from <ref type="bibr" target="#b16">[17]</ref> for most experiments. In section 5.4 we use T5-base from <ref type="bibr" target="#b46">[47]</ref> and mT5-base from <ref type="bibr" target="#b67">[67]</ref>.</p><p>We use the Adam optimizer (? 1 = 0.9, ? 2 = 0.999) for all models, except for models with Large text tower that were trained with a modified version of AdaFactor from <ref type="bibr" target="#b69">[69]</ref> (same settings as described in Section C). The learning rate is set to 0.001, and the weight decay to 0.0001 (using "decoupled" weight decay as described in <ref type="bibr" target="#b39">[40]</ref>). Gradients are clipped at global norm 1.</p><p>For training, the images are pre-processed by Inceptionstyle cropping <ref type="bibr" target="#b58">[58]</ref> to a size of 224 pixels. For evaluation, the images are resized to 224 pixels with bi-linear interpolation without cropping.</p><p>When tuning on the CC12M dataset, we train for 20 <ref type="bibr" target="#b15">16</ref>    <ref type="figure">Figure 8</ref>. Ablations for YFCC100m. Top: even though the description field can be long, the potential benefit of using more than 16 tokens does not outweigh the increased memory and computation cost. Middle: When using all text signals, sticking to the CLIP subset is better according to the standard benchmarks, however see also Section 5.7. Bottom: Using all three text signals simultaneously for all examples works better than sampling one per image or per batch. epochs (200 million seen image-text pairs), which corresponds to 12k steps with a batch size of 16384. The first 50k image-text pairs are used as minival validation set. The learning rate is ramped up linearly for the first 2k steps and then follows a cosine decay. Unless otherwise noted, we use the LU setup with a linear head on the text tower only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. How to use YFCC100m?</head><p>This section is an exploratory analysis of the YFCC100m dataset and provides guidance on what is a good setup for LiT. For each experiment we run, we try three learning-rates (0.001, 0.0008, 0.0003) and two weight-decays (0.0001 and 0.00001) and report the best result, this allows avoiding biasing conclusions due to sub-optimal hyper-parameters. We perform the exploration using the small ViT-B/32 Au-gReg <ref type="bibr" target="#b55">[55]</ref> image tower and a BERT base <ref type="bibr" target="#b16">[17]</ref> text tower and run tuning for 60 000 steps, although the same conclusions and similar scores are already reachable after 30 000 steps of tuning.</p><p>The YFCC100m dataset comes with a rich set of annotations for each image, including camera settings and geolocation. Out of all the annotations, three of them are potential candidates for learning image-text pairings: the image's ti-tle, a description, and a set of free-form tags. However, only partially overlapping subsets of 60 M, 30 M, and 65 M images come with a title, description, or tags, respectively. We first explore which supervision signal is most useful. For the description, we simply tokenize the provided text; for the title, we perform basic filtering and remove titles that start with DSC, IMG, Picture, consist of only the word image or consist of more than half digits; for the tags, we randomly shuffle their order, and join them with a random space, newline, or basic punctuation character in order to get a string which we then tokenize. The texts vary dramatically in length, we thus try maximum sequence lengths of 16 and 32 tokens. The first row of <ref type="figure">Figure 8</ref> shows the result of this experiment. The difference between a maximum sequence length of 16 and 32 is small, however the memory savings are substantial and we thus restrict the sequence length to 16 tokens in all further experiments.</p><p>In terms of supervision signal, there is no single clear winner. We thus explore three ways of learning from all signals and so also make use of the full 100 M images. We can either jointly optimize them by summing up three contrastive losses for each image, or we can randomly sample one of the three sources for each image or for a whole minibatch. As can be seen in the bottom row of <ref type="figure">Figure 8</ref>, jointly using all signals consistently works better, although it requires triple the amount of passes through the text tower.</p><p>Finally, the authors of CLIP <ref type="bibr" target="#b45">[46]</ref> provide a curated subset of roughly 15 M images, which contain high quality annotations in English. We refer to this subset as YFCC CLIP . In the middle row of <ref type="figure">Figure 8</ref>, we compare how using the Full YFCC100m for LiT compares to using the CLIP subset of it. Both seem to perform roughly on par for all signals for classification, but when using only titles or tags and performing image-text retrieval, it is better to apply LiT on the full YFCC100m dataset.</p><p>Overall, we obtain the best results with LiT using all text signals jointly on the YFCC CLIP subset. However, this investigation was performed with the small ViT-B/32 model, it is likely that a larger model may perform better when using the full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Effective batch size for contrastive loss</head><p>In this section, we study the impact of the effective batch size for contrastive loss. We use the Lu setup with a pre-trained B/32 image model, tuned for 900 million seen image-text pairs. In <ref type="figure">Figure 9</ref>, we see a clear improvement when using global contrastive loss. It has increased the effective batch size for contrastive learning, thus introducing more hard negatives and improving model quality. Interestingly, we found that larger batch size leads to better performance consistently. We leave extremely large batch size exploration to future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Pre-computation for locked image models</head><p>In LiT method, the locked image model generates identical embeddings given the same image. Based on this characteristic, we use pre-computed image embeddings during tuning. It allows faster iterations and fitting larger text models in memory, as the image representations are extracted only once and no image models are loaded. <ref type="figure" target="#fig_0">Figure 10</ref> left shows how training speeds up as the number of epochs grows. When training no more than a single epoch, pre-computation keeps a constant speed ratio over re-computation, which increases from one (same speed) to larger than one (speedup) as image model size grows. After one epoch, pre-computation clearly accelerates training due to reused image representations. The speedup ratio becomes more visible as either the number of epochs or the  <ref type="table">Table 7</ref>. Pre-computation details. Max speed and Max batch describe metrics collected by maximum speed (img/sec/core) and batch size, respectively, corresponding to <ref type="figure" target="#fig_0">Figure 10</ref>. Pre and Non are metrics with and without pre-computation respectively; Inf describes pre-computation inference speed, which is only affected by image models. All experiments are run on 8 TPU v3 cores.</p><p>image model size grows. For experiments with pre-computed image embeddings, we count both pre-computation inference cost and tuning cost. Pre-computation will be performed on at most a single epoch on the image-text dataset. In practice, the precomputed embeddings can be shared across different experiments, as long as the image tower is identical. As a result, the actual cost is even lower than our estimation. For experiments without pre-computed image embeddings, we count the actual contrastive-tuning cost.</p><p>Pre-computation eliminates loading the image model to memory during training, thus allowing larger batch sizes for contrastive loss. We search maximum batch sizes on each combination of image and text models with and without precomputation, and show the results in <ref type="figure" target="#fig_0">Figure 10</ref> right. We search for the maximum batch size for each model with a unified setup. We report the maximum batch size that the model can fit on 8 TPU v3 cores.</p><p>However, if image augmentations are enabled during training, we may not benefit much from pre-computation. The model sees different augmented images in multiple epochs. Nevertheless, the memory benefits still hold. All metric details are in <ref type="table">Table 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Learning rate schedules</head><p>For most of the experiments, weights were either completely locked, or trained with the same learning rate schedule (linear warmup and cosine decay). We experimented with different learning rate schedules <ref type="figure" target="#fig_0">(Figure 11)</ref>, mainly varying how the image tower was updated. We observed that training the image tower with a smaller learning rate and/or delaying training of the image tower resulted in better retrieval metrics <ref type="figure" target="#fig_0">(Figure 12</ref>). The default schedules (LU and UU) have the best and worst ImageNet 0-shot accuracy of all tried learning rate schedules. Compared to UU, both ITR/VTAB metrics and ImageNet 0-shot accuracy improve modestly, when the image learning rate is only scheduled for the second half of the training ("delay"). The ImageNet 0-shot accuracy improves more but the VTAB accuracy drops when the learning rate is set to a smaller value ("lr=1e-4"). Combining the delay with the smaller learning rate ("lr+dl") further improves both ITR/VTAB metrics and ImageNet 0-shot accuracy. A similar result is achieved by multiplying the learning rate in the UU setting with a sigmoid function ("sigmoid"). Alternating between freezing image tower and rext tower ("two cycles") finally performs somewhere between "lr+dl" and "lr=1e-4" schedules.</p><p>I. Zero-shot transfer details I. <ref type="bibr" target="#b0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Classification</head><p>We follow CLIP <ref type="bibr" target="#b45">[46]</ref> for the zero-shot transfer evaluation. We use the identical ImageNet class label names and the same 80 prompt templates as in CLIP. During evaluation of private LiT models, we first resize the test image and then central crop with 0.875 aspect ratio to the target resolution. More specifically, we use 224 ? 224 target resolution for CIFAR dataset and 288 ? 288 target resolution for the remaining datasets. For all the public LiT models, we resize all test images to 224 ? 224 for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2. VTAB Evaluation</head><p>The Visual Task Adaptation benchmark <ref type="bibr" target="#b70">[70]</ref> consists of 19 diverse visual tasks. We refer readers to the original publication for details about each dataset; here we just mention that they are split into three categories:</p><p>? Natural: These tasks contain classical "natural" realworld images obtained with a camera, such as vehicles, pets, scenery and household objects.</p><p>? Specialized: These are datasets of arguably "natural" images which were captured with specialised photographic equipment, such as satellite photographs and medical images.</p><p>? Structured: These assess understanding of scenes structure in some way, predominately from synthetic environments. Example tasks include 3D depth estimation and counting.</p><p>Note that there is significant overlap with the datasets assessed in <ref type="bibr" target="#b45">[46]</ref>, but it is not guaranteed that the same data splits were used. Evaluation protocol. Previous works <ref type="bibr" target="#b45">[46]</ref> define taskspecific prompts and class names, but it is not clear exactly how an optimal set of prompts for a given task was chosen.</p><p>For VTAB, we define a search space of image preprocessing, prompt templates and classes, where the latter two are often per-task (e.g. using a satellite photo of ... or an overhead photo of ... for tasks involving satellite imagery). All such settings are tried on a small validation set of 800 images, and the optimal setting is then run on the official VTAB test set.</p><p>We note this is arguably not zero-shot transfer, but believe it is a principled and reproducible approach.</p><p>Prompts used. For all tasks, we considered 3 default sets of prompts 1. A photo of a CLASS 2. CLASS 3. The 6 CLIP prompts used for ImageNet <ref type="bibr" target="#b3">4</ref> We also consider some task specific prompts/class name settings. Note that these two degrees of freedom are orthogonal, and a text setting is defined by both. They are shown in <ref type="table" target="#tab_13">Table 11</ref> and <ref type="table">Table 12</ref>. Not all of these prompts were equally useful; some are redundant, providing equal performance gain as other settings, and some do not provide performance gains at all. We show the performance delta comparing only the default prompts versus including a given text variant as well, to give a rough idea of how beneficial it was.</p><p>Can we assess zero-shot performance using VTAB? The strength of such a diverse benchmark is in the variety of its label spaces. ImageNet classes, though very fine-grained, are fairly generic. However, VTAB also includes structured tasks which are designed to assess the model's competence at tasks which aren't object recognition, such as counting and assessing distances and angles. This presents interesting difficulties for solving in a zeroshot natural language grounded manner. <ref type="figure" target="#fig_0">Figure 13</ref> shows the zero-shot performance of many models developed for this paper. Their detailed performance is not important here -the gray lines show what a "random guesser" would achieve on each VTAB category. It is not an obvious number, as performance across categories is an average of all the constituent datasets, which have varying numbers of classes. It is clear from this figure that the structured performance does not significantly deviate from random guessing, despite extensive efforts in prompt engineering. We leave it as an open -and very interesting -research direction to figure how to make such models count and assess distances. Furthermore, though contrastive image-text training on the web can largely match supervised models on natural tasks, further improvements are needed on more specialist tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3. Cross-modal retrieval</head><p>We compute retrieval metrics on MSCOCO captions <ref type="bibr" target="#b9">[10]</ref>, reporting the numbers on the test set (5 000 images, 25 010 captions). For the image to text retrieval, we rank all texts by decreasing cosine similarity of their embedding with the image embedding, and then report the fraction of images that ranks the correct text within the first (1, 5, 10) positions as the Recall@1, Recall@5, Recall@10 metrics. For the text to image retrieval, we compute the same metric, but ranking images and averaging over all texts. When showing a single number, we always refer to the Recall@1 metric. <ref type="bibr" target="#b3">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Multilingual details and limitations</head><p>Extra results. <ref type="table">Table 8</ref> shows the English zero-shot ImageNet classification performance of different English and multilingual T5 models, with LiT on YFCC CLIP vs. YFCC100m. We note that training on the larger, more diverse, multilingual set does not come at the expense of English performance.</p><p>Wiki-Image Text as an evaluation benchmark. We noted qualitatively that, as one may expect from Wikipedia, a large proportion of examples are about entities such as people, places, or art. When translated to other languages, proper nouns are usually kept as is -especially if the two languages share an alphabet. This makes it an imperfect dataset to benchmark multilinguism as monolingual models will score higher than they should.</p><p>Tokenization subtleties. The sentencepiece tokenizers, when faced with unknown vocabulary, will default to byte encoding. This is not a perfect catch-all; in such circumstances models cannot take advantage of pre-training, and the resultantly very long sequences will not fit in the 16token maximum length used in this paper. It is nevertheless better than the [UNK] tokens produced by BERT's Word-  <ref type="figure" target="#fig_0">Figure 14</ref>. Fully detailed evaluation of the multilingual models on WIT.</p><p>Piece tokenizer; with SentencePiece, even with an imperfect vocabulary, the model has a chance to adapt. This explains why even with an ill-suited English-only vocabulary, the T5 models can still learn decent representations of non-English languages. Translation of prompts. One obvious factor worth noting is that, in our setup, non-English languages may be impacted by imperfect translations. This likely means non-English performance is underestimated.</p><p>More subtly, we note that many languages -especially those with Latin alphabets -often use the English word for very niche or specific items. For example, at the time of writing, the Vietnamese translation of I took a photo of an airship contains the word airship verbatim. The contrastive model can in principle pick out the word airship, ignore all the Vietnamese, and retain decent performance despite not understanding Vietnamese at all.</p><p>Backtranslation as data augmentation. Backtranslation <ref type="bibr" target="#b50">[51]</ref> -translating to a language and back again, in order to generate slightly different versions of a given text -is a common augmentation in NLP. We run some experiments to see whether it works for contrastive image-text training. We again use an online translation service to translate the texts in CC12M to and from 9 different languages. This probability is shared across the languages i.e. a backtranslation probability of 0.5 with 5 different backtranslate candidates means there is a 50% chance of picking the original ground truth and a 10% chance each of picking one of the backtranslated candidates. <ref type="figure" target="#fig_0">Figure 15</ref> shows the effect of this augmentation on LiT using an AugReg ImageNet21k pretrained ViT-B/16 model. Backtranslation is fairly useful up to certain point, with 10% giving a good trade-off which improves all metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. More de-duplication results</head><p>We present more ablation test results using larger architectures. We aim to check whether larger architectures benefit more from duplicates, while small architectures do not have enough capacity to overfit to the duplications. More   <ref type="bibr" target="#b69">[69]</ref>, and from-scratch L size text model. <ref type="table">Table 9</ref> shows the experimental results. We find that the conclusions are consistent with the runs using the ViT-B/32 image model discussed in Section 5.5. This is further evidence suggesting that duplications are not the root cause for good zero-shot transfer results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. Image-text dataset comparison</head><p>Using simpler text filters for our dataset leads to a larger dataset size compared to the ALIGN dataset: The ALIGN dataset contains 1.8B image-text pairs, while our data set contains 3.6B image-text pairs.</p><p>In We vary the training schedule from 900M seen images, to 18B seen images. We use 18B images to make sure that the training process is long enough to benefit from a larger dataset. We find that the difference between the two datasets are small when the model is trained for a short period, i.e. less than a single epoch. As the training becomes longer, the impact of the dataset size becomes more visible. Overall, the above results indicate that larger dataset with simpler filters slightly outperforms a smaller dataset with more filters. We leave the thorough exploration of this topic to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M. Qualitative examples</head><p>Though strong classification &amp; retrieval performance is promising, it arguably probes understanding of very simple concepts. Are LiT models really zero-shot learners capable of understanding open vocabularies?</p><p>We touch here on a few qualities these models should ideally have, but note that these are not to be considered representative; benchmarks that investigate more than simply fine grained visual classification should be used to more thoroughly understand these phenomena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.1. Private LiT model</head><p>In this section, we present model predictions with manually constructed image-text pairs input. Results from private LiT model are shown in <ref type="figure" target="#fig_0">Figure 16</ref>. We believe that with LiT, we successfully made a pre-trained image model to a zero-shot learner, that supports classification and retrieval with open vocabularies instead of a fixed label set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.2. Multi-lingual model</head><p>Thanks to LiT on the multilingual dataset, the model also supports inputs using different languages. In <ref type="figure" target="#fig_0">Figure 17</ref>, we show results both in Thai and Chinese. The model recognized the "Songkran" event in Thai, and the "Chinese Spring Festival" event in Chinese; it nonetheless also ranks English translations or transliterations quite highly, which is likely reflective of the data distribution. Multilingual capability makes our models more inclusive and accessible to non-English speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M.3. Model failures</head><p>We present model failures in <ref type="figure" target="#fig_0">Figure 18</ref>. We show examples of how one can slightly change the text candidastes to manipulate the model output; one can easily force a desired answer by tuning other text candidates to rank lower.        Dataset describes the dataset that was used (see Section 4), with "Mixed" referring to alternating batches between CC12M and YFCC100m.</p><p>Images is the number of images seen during contrastive-tuning. Default batch size was 16 384 (only exception model "g/14*" with 32 768).</p><p>Cfg first letter refers to image tower, second letter to text tower (Section 5.2). H describes whether a linear head was added to the image tower (note that the text tower always has a linear head). Image describes the image tower (all models use 224px input resolution apart from "g/14*" that uses 288px), for details on models see <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b69">69]</ref>. Text describes the text tower, for details see <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b67">67]</ref>. Tok describes whether a SentencePiece or WordPiece tokenizer was used. Inits describes the initializations of the image/text towers (AR refers to AugReg "recommended checkpoints" <ref type="bibr" target="#b55">[55]</ref>). Optim is the optimizer, using default Adam or Adafactor <ref type="bibr" target="#b52">[53]</ref>. LR is the base learning rate (with linear ramp-up and cosine decay). WD is the weight decay (using "decoupled" weight decay <ref type="bibr" target="#b39">[40]</ref>). INet describes zero-shot top-1 accuracy on Imagenet. T?I and I?T describe retrieval recall @1 on the MSCOCO test set. Vn, Vsp, Vst VTAB <ref type="bibr" target="#b70">[70]</ref> results for "natural", "specialized", and "structured" subsets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>equal technical contribution, ? equal advising Comparison to the previous SOTA methods. Left: results on public YFCC100m subset, with from-scratch, fine-tuned from a pre-trained image model, and LiT with a pre-trained image model. The proposed LiT improves over 30% ImageNet zero-shot transfer accuracy on YFCC100m subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Design choices for contrastive-tuning on image-text data. Two letters are introduced to represent the image tower and text tower setups. L stands for locked variables and initialized from a pre-trained model, U stands for unlocked and initialized from a pre-trained model, u stands for unlocked and randomly initialized. Lu is named as "Locked-image Tuning" (LiT).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>An in-depth study of the possible locking and initialization settings of LiT on the YFCC100m-CLIP dataset. A pretrained image tower works best, while pre-training of the text tower only helps a little. These are not training curves; each point is the final value reached by a training run of that duration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3</head><label>3</label><figDesc>may seem to support this expectation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Comparing the loss on the dataset used for LiT (top row) to the loss on out-of-distribution (zero-shot) datasets (middle row) and the "representation quality" as measured by linear few-shot evaluation on the pre-logits (bottom row). This reveals how the different settings behave, see text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Image retrieval performance over 100 languages reveals that unfiltered data and a multilingually pre-trained text model can significantly increase long-tail performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>79.1 41.3 25.0 197 M 2855 12 G Mixer-B/32 57.1 75.9 37.5 22.9 169 M 4208 9 G BiT-M-R50 55.2 77.6 37.3 23.9 134 M 2159 11 G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>ImageNet zero-shot accuracy [%] with varying model capacity. Incremental improvemments due to larger text towers (base ? large ? huge) are shown as stacked bars.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Impact of batch sizes for contrastive loss, including both global contrastive loss and local contrastive loss. Left: Pre-computing image embeddings accelerates LiT, when tuning for more than a single epoch. Right: Precomputing image embeddings in LiT allows larger batch size in memory.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Different learning rate schedules. Note that the default LR schedule is shown in black in the lower part of the figure. ITR and VTAB metrics as a function of ImageNet 0shot accuracy for different LR schedules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 15 .</head><label>15</label><figDesc>Backtranslating data as a form of data augmentation improves performance across most metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>(a) Nuanced context: The model can understand information such as actions or implied symptoms.(b) Richer information: The model correctly handles colours, background buildings and even car brands.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(c) Counting: The model does a reasonable job at counting, though prompts like "bunch of cats" are preferred.(d) Esoteric examples:The model has no problems at identifying rare concepts, like a cow on a beach, or an astronaut alien.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 16 .</head><label>16</label><figDesc>Various model predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 17 .</head><label>17</label><figDesc>Training on multilingual data allows the model to recognise concepts in multiple languages, including visual concepts which do not directly exist in English.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 18 .</head><label>18</label><figDesc>Qualitative failures. In the left example, the model ranks the wrong grinning face before the ground truth yawning face. However, by removing the grinning face and adding emoji prompt, the model prefers emoji yawn.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig 7 CC12M 4 FigFig 5 6 Fig</head><label>7456</label><figDesc>200M LU n vit-B/16 bert-large WP AR,Bert Adaf 1e-3 1e-4 66.9 28.3 44.8 58.6 45.4 13.5 Fig 7 CC12M 200M LU n vit-L/16 bert-large WP AR,Bert Adaf 1e-3 1e-4 67.6 26.9 42.6 57.8 50.3 13.0 Fig 7 CC12M 200M LU n vit-B/16 bert-base WP AR,Bert Adam 1e-3 1e-4 66.1 28.2 45.3 59.0 50.6 14.0 Fig 7 CC12M 200M LU n vit-L/16 bert-base WP AR,Bert Adam 1e-3 1e-4 66.8 26.6 44.3 58.6 45.6 12.7 Fig 7 CC12M 200M LU n vit-B/32 bert-large WP AR,Bert Adaf 1e-3 1e-4 61.7 25.4 41.4 56.4 49.9 13.6 Fig 7 CC12M 200M LU n vit-B/32 bert-base WP AR,Bert Adam 1e-3 1e-4 61.1 24.9 40.9 56.8 49.6 15.YFCC CLIP 983M LU y vit-B/32 mt5-base SP AR,mt5 Adam 3e-4 1e-4 58.4 15.6 25.1 54.5 36.7 12.3 Fig 5 YFCC CLIP 983M LU y vit-B/32 t5-base SP AR,t5 Adam 3e-4 1e-4 58.5 17.2 29.1 54.7 40.4 13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>+1.4) 29.4 (+1.6) 17.2 (+1.2) mT5 SP 58.1 (+1.2) 28.3 (+0.4) 16.4 (+1.0) BERT WP 58.8 (+0.7) 35.2 (+1.1) 20.0 (+0.7)The effect of different text encoders on zero-shot performance. The main numbers show performance achieved when the text tower is randomly initialised; the numbers in brackets are the further improvement achieved when the text tower is initialized with a pre-trained language model. The Tok column indicates whether a SentencePiece or WordPiece tokenizer was used.</figDesc><table><row><cell></cell><cell cols="2">Model Tok INet 0shot</cell><cell>I?T</cell><cell>T?I</cell></row><row><cell>YFCC-CLIP</cell><cell cols="2">ViT T5 SP 57.8 (ViT SP 57.2 WP 56.4</cell><cell>29.7 28.2</cell><cell>16.9 17.3</cell></row><row><cell>Ours</cell><cell>ViT ViT</cell><cell>SP 68.8 WP 68.8</cell><cell>43.6 45.4</cell><cell>28.5 29.7</cell></row><row><cell></cell><cell cols="2">BERT WP 65.8</cell><cell>43.8</cell><cell>28.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>https://github.com/openai/CLIP/blob/main/data/prompts.md</figDesc><table><row><cell></cell><cell>80% 90%</cell><cell cols="2">random guess</cell><cell cols="2">supervised R50</cell><cell></cell></row><row><cell></cell><cell>70%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>30% 40% 50% 60% Average accuracy</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>10%</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0%</cell><cell>overall</cell><cell cols="3">natural specialized structured Task category</cell><cell></cell></row><row><cell cols="7">Figure 13. Performance of zero-shot classification models across</cell></row><row><cell cols="7">different VTAB categories. Each dot is a zero-shot model evalua-</cell></row><row><cell>tion.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">CLIP subset</cell><cell></cell><cell>Full</cell><cell></cell></row><row><cell></cell><cell cols="6">ImgNet T?I I?T ImgNet T?I I?T</cell></row><row><cell>T5</cell><cell>58.9</cell><cell>14.5</cell><cell>22.6</cell><cell>62.4</cell><cell>19.6</cell><cell>34.3</cell></row><row><cell>+ pt</cell><cell>58.5</cell><cell>17.2</cell><cell>29.1</cell><cell>62.3</cell><cell>20.1</cell><cell>34.5</cell></row><row><cell>mT5</cell><cell>58.7</cell><cell>14.4</cell><cell>23.1</cell><cell>62.1</cell><cell>18.5</cell><cell>32.6</cell></row><row><cell>+ pt</cell><cell>58.4</cell><cell>15.6</cell><cell>25.1</cell><cell>62.6</cell><cell>18.9</cell><cell>33.6</cell></row><row><cell cols="7">Table 8. Training on the full YFCC100m data significantly im-</cell></row><row><cell cols="7">proves all metrics compared to the CLIP subset. Gray rows are</cell></row><row><cell cols="3">with text pre-training.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>table 10</head><label>10</label><figDesc>, we show the results from training a baseline ViT-B/32 model on both datasets, with the same schedules.</figDesc><table><row><cell>Task</cell><cell>Pairs Seen</cell><cell>our</cell><cell>ALIGN</cell><cell>Diff.</cell></row><row><cell>ImageNet</cell><cell>900M</cell><cell>70.1</cell><cell>69.8</cell><cell>0.3</cell></row><row><cell>ImageNet</cell><cell>3.6B</cell><cell>72.0</cell><cell>71.5</cell><cell>0.5</cell></row><row><cell>ImageNet</cell><cell>7.2B</cell><cell>72.4</cell><cell>71.8</cell><cell>0.6</cell></row><row><cell>ImageNet</cell><cell>18B</cell><cell>72.9</cell><cell>72.2</cell><cell>0.7</cell></row><row><cell cols="5">Table 10. Comparing the ALIGN data with our data, which uses</cell></row><row><cell cols="2">simpler text filters.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 .</head><label>11</label><figDesc>Prompts swept over for VTAB tasks. Performance deltas are shown as mean test accuracy improvement per-task compared to just using the default three prompts. The default class names from TensorFlow Dataset (TFDS) are used in this table. TFDS versions are given alongside task names. YFCC CLIP 983M LU y vit-B/32 bert-base WP AR,Bert Adam 8e-4 1e-4 63.6 22.1 37.6 59.3 35.0 12.7 Fig 1 YFCC CLIP 983M UU y vit-B/32 bert-base WP AR,Bert Adam 3e-4 1e-5 53.3 23.4 37.6 54.9 44.4 14.1 Fig 1 YFCC CLIP 983M uu y vit-B/32 bert-base WP -,-Adam 8e-4 1e-4 42.1 17.9 31.1 45.8 49.8 14.3 Bert Adam 8e-4 1e-4 55.5 18.2 33.4 51.5 45.4 14.8 Tab 3 YFCC CLIP 246M LU y mocov3-B/16 bert-base WP vit,Bert Adam 8e-4 1e-4 55.4 17.6 33.5 50.8 40.5 12.8</figDesc><table><row><cell>Dataset</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 13 .</head><label>13</label><figDesc>Detailed configuration and metrics for a selection of models. Ref describes the Figure/Table where the model is mentioned.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Public LiT models available at https : / / github . com / google-research/vision_transformer#lit-models. We provide pre-training code in the big vision codebase<ref type="bibr" target="#b2">[3]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Matthias Minderer and Josip Djolonga for help on robustness evaluations; Chao Jia and Zhen Li for discussions on the image-text dataset; Ting Chen for feedback on the initial version of the paper; Jordi Pont-Tuset for help on the image-text retrieval evaluation; Jeremiah Harmsen for inspirations on the title; Jakob Uszkoreit for discussions on data augmentations; Krishna Srinivasan for discussions on the Wikipedia based image text dataset; Beer Changpinyo for discussions on conceptual captions dataset; Maxim Neumann for help on zeroshot eval and T5 text models; the Google Brain team at large for providing a supportive research environment.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mayo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Alverio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gutfreund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Katz</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Xiaohua Zhai, and A?ron van den Oord. Are we done with imagenet? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><forename type="middle">J</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<idno>abs/2006.07159, 2020. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Big vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/big_vision" />
		<imprint>
			<biblScope unit="volume">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning the best pooling strategy for visual semantic embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft COCO captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1504.00325</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised vision transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">UNITER: universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CoAtNet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The efficiency misnomer. CoRR, abs/2110.12894</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">VirTex: Learning visual representations from textual annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On robustness and transferability of convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Romijnders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander D&amp;apos;</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Moldovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16?16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VSE++: Improving visual-semantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In BMVC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">DeViSE: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom?s</forename><surname>Mikolov</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">The many faces of robustness: A critical analysis of out-of-distribution generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurav</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Dorundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samyak</forename><surname>Parajuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>CoRR, abs/2006.16241, 2020. 4</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Jacob Steinhardt, and Dawn Song. Natural adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Taori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Achal</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenCLIP. Zenodo</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MURAL: Multimodal, multitask representations across languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aashi</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sneha</forename><surname>Kudugunta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11" />
			<biblScope unit="page" from="3449" to="3463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Big transfer (BiT): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Do better imagenet models transfer better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hannes Nickisch, and Stefan Harmeling. Learning to detect unseen object classes by betweenclass attribute transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zerodata learning of new tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">VisualBERT: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Semantic-aware scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>L?pez-Cifuentes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcos</forename><surname>Escudero-Vi?olo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s</forename><surname>Besc?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garc?a-Mart?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">107256</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">ViL-BERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-task vision and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedanuj</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Sinno Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno>21:140:1-140:67</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Do ImageNet classifiers generalize to Im-ageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Japanese and Korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaisuke</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACL</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">WIT: wikipedia-based image text dataset for multimodal multilingual machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Raman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiecao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/2103.01913</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">How to train your ViT? Data, augmentation, and regularization in vision transformers. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">VL-BERT: pre-training of generic visual-linguistic representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A survey on deep transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenchang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICANN</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">YFCC100M: the new data in multimedia research</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">MLP-Mixer: An all-MLP architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songwei</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alch?-Buc, Emily B. Fox, and Roman Garnett</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="10506" to="10518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno>abs/1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Zero-shot learning -A comprehensive evaluation of the good, the bad and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqin</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2251" to="2265" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Aditya Barua, and Colin Raffel. mT5: A massively multilingual pre-trained text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Scaling vision transformers. CoRR, abs/2106.04560</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ruyssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Susano</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Tschannen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<idno>abs/1910.04867</idno>
		<title level="m">The visual task adaptation benchmark. CoRR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuhide</forename><surname>Miura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><forename type="middle">P</forename><surname>Langlotz</surname></persName>
		</author>
		<title level="m">Contrastive learning of medical visual representations from paired images and text</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">3 ? ten objects ? 10 shapes ? ten shapes Delta</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>+1.2%</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Prompts and customized class names swept over for VTAB tasks. Performance deltas are shown as mean test accuracy improvement per-task compared to just using the default three prompts</title>
		<imprint/>
	</monogr>
	<note>Table 12</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
