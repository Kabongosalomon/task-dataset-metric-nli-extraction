<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bidirectional LSTM-CRF Models for Sequence Tagging</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
							<email>huangzhiheng@baidu.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baidu</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baidu</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">Yu</forename><surname>Baidu</surname></persName>
						</author>
						<title level="a" type="main">Bidirectional LSTM-CRF Models for Sequence Tagging</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a variety of Long Short-Term Memory (LSTM) based models for sequence tagging. These models include LSTM networks, bidirectional LSTM (BI-LSTM) networks, LSTM with a Conditional Random Field (CRF) layer (LSTM-CRF) and bidirectional LSTM with a CRF layer (BI-LSTM-CRF). Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. We show that the BI-LSTM-CRF model can efficiently use both past and future input features thanks to a bidirectional LSTM component. It can also use sentence level tag information thanks to a CRF layer. The BI-LSTM-CRF model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, it is robust and has less dependence on word embedding as compared to previous observations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Sequence tagging including part of speech tagging (POS), chunking, and named entity recognition (NER) has been a classic NLP task. It has drawn research attention for a few decades. The output of taggers can be used for down streaming applications. For example, a named entity recognizer trained on user search queries can be utilized to identify which spans of text are products, thus triggering certain products ads. Another example is that such tag information can be used by a search engine to find relevant webpages.</p><p>Most existing sequence tagging models are linear statistical models which include Hidden Markov Models (HMM), Maximum entropy Markov models (MEMMs) <ref type="bibr" target="#b14">(McCallum et al., 2000)</ref>, and Conditional Random Fields (CRF) <ref type="bibr" target="#b13">(Lafferty et al., 2001)</ref>. Convolutional network based models <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> have been recently proposed to tackle sequence tagging problem. We denote such a model as Conv-CRF as it consists of a convolutional network and a CRF layer on the output (the term of sentence level loglikelihood (SSL) was used in the original paper). The Conv-CRF model has generated promising results on sequence tagging tasks. In speech language understanding community, recurrent neural network <ref type="bibr" target="#b16">(Mesnil et al., 2013;</ref> and convolutional nets <ref type="bibr" target="#b31">(Xu and Sarikaya, 2013)</ref> based models have been recently proposed. Other relevant work includes <ref type="bibr" target="#b8">(Graves et al., 2005;</ref><ref type="bibr" target="#b9">Graves et al., 2013)</ref> which proposed a bidirectional recurrent neural network for speech recognition.</p><p>In this paper, we propose a variety of neural network based models to sequence tagging task. These models include LSTM networks, bidirectional LSTM networks (BI-LSTM), LSTM networks with a CRF layer (LSTM-CRF), and bidirectional LSTM networks with a CRF layer (BI-LSTM-CRF). Our contributions can be summarized as follows. 1) We systematically compare the performance of aforementioned models on NLP tagging data sets; 2) Our work is the first to apply a bidirectional LSTM CRF (denoted as BI-LSTM-CRF) model to NLP benchmark sequence tagging data sets. This model can use both past and future input features thanks to a bidirectional LSTM component. In addition, this model can use sentence level tag information thanks to a CRF layer. Our model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets; 3) We show that BI-LSTM-CRF model is robust and it has less dependence on word embedding as compared to previous observations <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>. It can produce accurate tagging performance without resorting to word embedding.</p><p>The remainder of the paper is organized as follows. Section 2 describes sequence tagging mod-els used in this paper. Section 3 shows the training procedure. Section 4 reports the experiments results. Section 5 discusses related research. Finally Section 6 draws conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Models</head><p>In this section, we describe the models used in this paper: LSTM, BI-LSTM, CRF, LSTM-CRF and BI-LSTM-CRF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LSTM Networks</head><p>Recurrent neural networks (RNN) have been employed to produce promising results on a variety of tasks including language model <ref type="bibr" target="#b17">(Mikolov et al., 2010;</ref><ref type="bibr" target="#b18">Mikolov et al., 2011)</ref> and speech recognition <ref type="bibr" target="#b8">(Graves et al., 2005)</ref>. A RNN maintains a memory based on history information, which enables the model to predict the current output conditioned on long distance features. <ref type="figure" target="#fig_1">Figure 1</ref> shows the RNN structure <ref type="bibr" target="#b4">(Elman, 1990)</ref> which has an input layer x, hidden layer h and output layer y. In named entity tagging context, x represents input features and y represents tags. <ref type="figure" target="#fig_1">Figure 1</ref> illustrates a named entity recognition system in which each word is tagged with other (O) or one of four entity types: Person (PER), Location (LOC), Organization (ORG), and Miscellaneous (MISC). The sentence of EU rejects German call to boycott British lamb . An input layer represents features at time t. They could be one-hot-encoding for word feature, dense vector features, or sparse features. An input layer has the same dimensionality as feature size. An output layer represents a probability distribution over labels at time t. It has the same dimensionality as size of labels. Compared to feedforward network, a RNN introduces the connection between the previous hidden state and current hidden state (and thus the recurrent layer weight parameters). This recurrent layer is designed to store history information. The values in the hidden and output layers are computed as follows:</p><formula xml:id="formula_0">h(t) = f (Ux(t) + Wh(t ? 1)),</formula><p>(1)</p><formula xml:id="formula_1">y(t) = g(Vh(t)),<label>(2)</label></formula><p>where U, W, and V are the connection weights to be computed in training time, and f (z) and g(z)</p><p>are sigmoid and softmax activation functions as follows. In this paper, we apply Long Short-Term Memory <ref type="bibr" target="#b11">(Hochreiter and Schmidhuber, 1997;</ref><ref type="bibr" target="#b8">Graves et al., 2005)</ref> to sequence tagging. Long Short-Term Memory networks are the same as RNNs, except that the hidden layer updates are replaced by purpose-built memory cells. As a result, they may be better at finding and exploiting long range dependencies in the data. <ref type="figure">Fig. 2</ref> illustrates a single LSTM memory cell <ref type="bibr" target="#b8">(Graves et al., 2005)</ref>. LSTM memory cell is implemented as the following:</p><formula xml:id="formula_2">f (z) = 1 1 + e ?z ,<label>(3)</label></formula><formula xml:id="formula_3">i t = ?(W xi x t + W hi h t?1 + W ci c t?1 + b i ) f t = ?(W xf x t + W hf h t?1 + W cf c t?1 + b f ) c t = f t c t?1 + i t tanh(W xc x t + W hc h t?1 + b c ) o t = ?(W xo x t + W ho h t?1 + W co c t + b o ) h t = o t tanh(c t )</formula><p>where ? is the logistic sigmoid function, and i,  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bidirectional LSTM Networks</head><p>In sequence tagging task, we have access to both past and future input features for a given time, we can thus utilize a bidirectional LSTM network ( <ref type="figure" target="#fig_4">Figure 4</ref>) as proposed in <ref type="bibr" target="#b9">(Graves et al., 2013)</ref>. In doing so, we can efficiently make use of past features (via forward states) and future features (via backward states) for a specific time frame. We train bidirectional LSTM networks using backpropagation through time (BPTT)(Boden., 2002). The forward and backward passes over the unfolded network over time are carried out in a similar way to regular network forward and backward passes, except that we need to unfold the hidden states for all time steps. We also need a special treatment at the beginning and the end of the data points. In our implementation, we do forward and backward for whole sentences and we only need to reset the hidden states to 0 at the begging of each sentence. We have batch implementation which enables multiple sentences to be processed at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">CRF networks</head><p>There are two different ways to make use of neighbor tag information in predicting current tags. The first is to predict a distribution of tags for each time step and then use beam-like decoding to find optimal tag sequences. The work of maximum entropy classifier <ref type="bibr" target="#b22">(Ratnaparkhi, 1996)</ref> and Maximum entropy Markov models (MEMMs) <ref type="bibr" target="#b14">(McCallum et al., 2000)</ref> fall in this category. The second one is to focus on sentence level instead of individual positions, thus leading to Conditional Random Fields (CRF) models <ref type="bibr">(Lafferty et al., 2001) (Fig. 5</ref>). Note that the inputs and outputs are directly connected, as opposed to LSTM and bidirectional LSTM networks where memory cells/recurrent components are employed. It has been shown that CRFs can produce higher tagging accuracy in general. It is interesting that the relation between these two ways of using tag information bears resemblance to two ways of using input features (see aforementioned LSTM and BI-LSTM networks), and the results in this paper confirms the superiority of BI-LSTM compared to LSTM. </p><formula xml:id="formula_4">O B?ORG O B?MISC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EU rejects German call</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">LSTM-CRF networks</head><p>We combine a LSTM network and a CRF network to form a LSTM-CRF model, which is shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. This network can efficiently use past input features via a LSTM layer and sentence level tag information via a CRF layer. A CRF layer is represented by lines which connect consecutive output layers. A CRF layer has a state transition matrix as parameters. With such a layer, we can efficiently use past and future tags to predict the current tag, which is similar to the use of past and future input features via a bidirectional LSTM network. We consider the matrix of scores f ? ([x] T 1 ) are output by the network. We drop the input [x] T 1 for notation simplification. The element [f ? ] i,t of the matrix is the score output by the network with parameters ?, for the sentence [x] T 1 and for the i-th tag, at the t-th word. We introduce a transition score [A] i,j to model the transition from i-th state to jth for a pair of consecutive time steps. Note that this transition matrix is position independent. We now denote the new parameters for our network as</p><formula xml:id="formula_5">? = ? ? {[A] i,j ?i, j}. The score of a sentence [x] T 1</formula><p>along with a path of tags [i] T 1 is then given by the sum of transition scores and network scores:</p><formula xml:id="formula_6">s([x] T 1 , [i] T 1 ,?) = T t=1 ([A] [i] t?1 ,[i]t + [f ? ] [i]t,t ).</formula><p>(5) The dynamic programming <ref type="bibr" target="#b21">(Rabiner, 1989)</ref> can be used efficiently to compute [A] i,j and optimal tag sequences for inference. See <ref type="bibr" target="#b13">(Lafferty et al., 2001)</ref> for details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">BI-LSTM-CRF networks</head><p>Similar to a LSTM-CRF network, we combine a bidirectional LSTM network and a CRF network to form a BI-LSTM-CRF network <ref type="figure" target="#fig_7">(Fig. 7)</ref>.  whole training data to batches and process one batch at a time. Each batch contains a list of sentences which is determined by the parameter of batch size. In our experiments, we use batch size of 100 which means to include sentences whose total length is no greater than 100. For each batch, we first run bidirectional LSTM-CRF model forward pass which includes the forward pass for both forward state and backward state of LSTM. As a result, we get the the output score f ? ([x] T 1 ) for all tags at all positions. We then run CRF layer forward and backward pass to compute gradients for network output and state transition edges. After that, we can back propagate the errors from the output to the input, which includes the backward pass for both forward and backward states of LSTM. Finally we update the network parameters which include the state transition matrix [A] i,j ?i, j, and the original bidirectional LSTM parameters ?.</p><p>Algorithm 1 Bidirectional LSTM CRF model training procedure 1: for each epoch do 2:</p><p>for each batch do 3:</p><p>1) bidirectional LSTM-CRF model forward pass: 4:</p><p>forward pass for forward state LSTM 5:</p><p>forward pass for backward state LSTM 6:</p><p>2) CRF layer forward and backward pass 7:</p><p>3) bidirectional LSTM-CRF model backward pass:</p><p>8: backward pass for forward state LSTM 9:</p><p>backward pass for backward state LSTM 10: 4) update parameters 11: end for 12: end for 4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We test LSTM, BI-LSTM, CRF, LSTM-CRF, and BI-LSTM-CRF models on three NLP tagging tasks: Penn TreeBank (PTB) POS tagging, CoNLL 2000 chunking, and CoNLL 2003 named entity tagging. <ref type="table" target="#tab_3">Table 1</ref> shows the size of sen-tences, tokens, and labels for training, validation and test sets respectively.</p><p>POS assigns each word with a unique tag that indicates its syntactic role. In chunking, each word is tagged with its phrase type. For example, tag B-NP indicates a word starting a noun phrase. In NER task, each word is tagged with other or one of four entity types: Person, Location, Organization, or Miscellaneous. We use the BIO2 annotation standard for chunking and NER tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Features</head><p>We extract the same types of features for three data sets. The features can be grouped as spelling features and context features. As a result, we have 401K, 76K, and 341K features extracted for POS, chunking and NER data sets respectively. These features are similar to the features extracted from Stanford NER tool <ref type="bibr" target="#b5">(Finkel et al., 2005;</ref><ref type="bibr" target="#b30">Wang and Manning, 2013)</ref>. Note that we did not use extra data for POS and chunking tasks, with the exception of using Senna embedding (see Section 4.2.3). For NER task, we report performance with spelling and context features, and also incrementally with Senna embedding and Gazetteer features 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Spelling features</head><p>We extract the following features for a given word in addition to the lower case word features.</p><p>? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Word embedding</head><p>It has been shown in <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> that word embedding plays a vital role to improve sequence tagging performance. We downloaded 2 the embedding which has 130K vocabulary size and each word corresponds to a 50-dimensional embedding vector. To use this embedding, we simply replace the one hot encoding word representation with its corresponding 50-dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Features connection tricks</head><p>We can treat spelling and context features the same as word features. That is, the inputs of networks include both word, spelling and context features. However, we find that direct connections from spelling and context features to outputs accelerate training and they result in very similar tagging accuracy. <ref type="figure" target="#fig_8">Fig. 8</ref> illustrates this network in which features have direct connections to outputs of networks. We will report all tagging accuracy using this connection. We note that this usage of features has the same flavor of Maximum Entropy features as used in <ref type="bibr" target="#b18">(Mikolov et al., 2011)</ref>. The difference is that features collision may occur in <ref type="bibr" target="#b18">(Mikolov et al., 2011)</ref> as feature hashing technique has been adopted. Since the output labels in sequence tagging data sets are less than that of language model (usually hundreds of thousands), we can afford to have full connections between features and outputs to avoid potential feature collisions.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We train LSTM, BI-LSTM, CRF, LSTM-CRF and BI-LSTM-CRF models for each data set. We have two ways to initialize word embedding: Random and Senna. We randomly initialize the word embedding vectors in the first category, and use Senna word embedding in the second category.</p><p>For each category, we use identical feature sets, thus different results are solely due to different networks. We train models using training data and monitor performance on validation data. As chunking data do not have a validation data set, we use part of training data for validation purpose. We use a learning rate of 0.1 to train models. We set hidden layer size to 300 and found that model performance is not sensitive to hidden layer sizes. The training for three tasks require less than 10 epochs to converge and it in general takes less than a few hours. We report models' performance on test datasets in <ref type="table" target="#tab_4">Table 2</ref>, which also lists the best results in <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>, denoted as Conv-CRF. The POS task is evaluated by computing per-word accuracy, while the chunk and NER tasks are evaluated by computing F1 scores over chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison with Cov-CRF networks</head><p>We have three baselines: LSTM, BI-LSTM and CRF. LSTM is the weakest baseline for all three data sets. The BI-LSTM performs close to CRF on POS and chunking datasets, but is worse than CRF on NER data set. The CRF forms strong baselines in our experiments. For random category, CRF models outperform Conv-CRF models for all three data sets. For Senna category, CRFs outperform Conv-CRF for POS task, while underperform for chunking and NER task. LSTM-CRF models outperform CRF models for all data sets in both random and Senna categories. This shows the effectiveness of the forward state LSTM component in modeling sequence data. The BI-LSTM-CRF models further improve LSTM-CRF models and they lead to the best tagging performance for all cases except for POS data at random category, in which LSTM-CRF model is the winner. The numbers in parentheses for CoNLL 2003 under Senna categories are generated with Gazetteer features.</p><p>It is interesting that our best model BI-LSTM-CRF has less dependence on Senna word embedding compared to Conv-CRF model. For example, the tagging difference between BI-LSTM-CRF model for random and Senna categories are 0.12%, 0.33%, and 4.57% for POS, chunking and NER data sets respectively. In contrast, the Conv-CRF model heavily relies on Senna embedding to get good tagging accuracy. It has the tagging difference of 0.92%, 3.99% and 7.20% between random and Senna category for POS, chunking and NER data sets respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Model robustness</head><p>To estimate the robustness of models with respect to engineered features (spelling and context features), we train LSTM, BI-LSTM, CRF, LSTM-CRF, and BI-LSTM-CRF models with word features only (spelling and context features removed). <ref type="table" target="#tab_5">Table 3</ref> shows tagging performance of proposed models for POS, chunking, and NER data sets using Senna word embedding. The numbers in parentheses indicate the performance degradation compared to the same models but using spelling and context features. CRF models' performance is significantly degraded with the removal of spelling and context features. This reveals the fact that CRF models heavily rely on engineered features to obtain good performance. On the other hand, LSTM based models, especially BI-LSTM and BI-LSTM-CRF models are more robust and they are less affected by the removal of engineering features. For all three tasks, BI-LSTM-CRF models result in the highest tagging accuracy. For example, It achieves the F1 score of 94.40 for CoNLL2000 chunking, with slight degradation </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Comparison with existing systems</head><p>For POS data set, we achieved state of the art tagging accuracy with or without the use of extra data resource. POS data set has been extensively tested and the past improvement can be realized in <ref type="table" target="#tab_6">Table  4</ref>. Our test accuracy is 97.55% which is significantly better than others in the confidence level of 95%. In addition, our BI-LSTM-CRF model already reaches a good accuracy without the use of the Senna embedding. All chunking systems performance is shown in table 5. <ref type="bibr">Kudo et al. won</ref> the CoNLL 2000 challenge with a F1 score of 93.48%. Their approach was a SVM based classifier. They later improved the results up to 93.91%. Recent work include the CRF based models <ref type="bibr" target="#b23">(Sha and Pereira, 2003;</ref><ref type="bibr" target="#b15">Mcdonald et al., 2005;</ref><ref type="bibr" target="#b28">Sun et al., 2008)</ref>. More recent is <ref type="bibr" target="#b25">(Shen and Sarkar, 2005)</ref> which obtained 95.23% accuracy with a voting classifier scheme, where each classifier is trained on different tag representations (IOB, IOE, etc.). Our model outperforms all reported systems except <ref type="bibr" target="#b25">(Shen and Sarkar, 2005)</ref>.</p><p>The performance of all systems for NER is shown in table 6. <ref type="bibr" target="#b6">(Florian et al., 2003)</ref> presented the best system at the NER CoNLL 2003 challenge, with 88.76% F1 score. They used a combination of various machine-learning classifiers. The second best performer of CoNLL 2003 <ref type="bibr" target="#b2">(Chieu., 2003)</ref> was 88.31% F1, also with the help of an external gazetteer. Later, (Ando and Zhang., 2005) reached 89.31% F1 with a semi-supervised approach. The best F1 score of 90.90% was reported in <ref type="bibr" target="#b20">(Passos et al., 2014)</ref> which employed a new form of learning word embeddings that can leverage information from relevant lexicons to improve the representations. Our model can achieve the best F1 score of 90.10 with both Senna embedding and gazetteer features. It has a lower F1 score than <ref type="bibr" target="#b20">(Passos et al., 2014)</ref> , which may be due to the fact that different word embeddings were employed. With the same Senna embedding, BI-LSTM-CRF slightly outperforms Conv-CRF (90.10% vs. 89.59%). However, BI-LSTM-CRF significantly outperforms Conv-CRF (84.26% vs. 81.47%) if random embedding is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussions</head><p>Our work is close to the work of <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> as both of them utilized deep neural networks for sequence tagging. While their work used convolutional neural networks, ours used bidirectional LSTM networks.</p><p>Our work is also close to the work of <ref type="bibr" target="#b10">(Hammerton, 2003;</ref> as all of them employed LSTM network for tagging. The performance in <ref type="bibr" target="#b10">(Hammerton, 2003)</ref> was not impressive. The work in  did not make use of bidirectional LSTM and CRF layers and thus the tagging accuracy may be suffered.</p><p>Finally, our work is related to the work of <ref type="bibr" target="#b30">(Wang and Manning, 2013)</ref> which concluded that non-linear architecture offers no benefits in a highdimensional discrete feature space. We showed that with the bi-directional LSTM CRF model, we consistently obtained better tagging accuracy than a single CRF model with identical feature sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we systematically compared the performance of LSTM networks based models for se-   <ref type="bibr" target="#b29">(Toutanova et al., 2003)</ref> SVM-based tagger <ref type="bibr" target="#b7">(Gimenez and Marquez, 2004)</ref> 97.16 No Bidirectional perceptron learning <ref type="bibr" target="#b24">(Shen et al., 2007)</ref> 97.33 No Semi-supervised condensed nearest neighbor 97.50 Yes <ref type="bibr" target="#b26">(Soegaard, 2011)</ref> CRFs with structure regularization <ref type="bibr" target="#b27">(Sun, 2014)</ref> 97.36 No Conv network tagger <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> 96.37 No Conv network tagger (senna) <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> 97.29 Yes BI-LSTM-CRF (ours) 97.43 No BI-LSTM-CRF (Senna) (ours) 97.55 Yes <ref type="table">Table 5</ref>: Comparison of F1 scores of different models for chunking. System accuracy SVM classifier <ref type="bibr" target="#b12">(Kudo and Matsumoto, 2000)</ref> 93.48 SVM classifier <ref type="bibr" target="#b12">(Kudo and Matsumoto, 2001)</ref> 93.91 Second order CRF <ref type="bibr" target="#b23">(Sha and Pereira, 2003)</ref> 94.30 Specialized HMM + voting scheme <ref type="bibr" target="#b25">(Shen and Sarkar, 2005)</ref> 95.23 Second order CRF <ref type="bibr" target="#b15">(Mcdonald et al., 2005)</ref> 94.29 Second order CRF <ref type="bibr" target="#b28">(Sun et al., 2008)</ref> 94.34 Conv-CRF <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> 90.33 Conv network tagger (senna) <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> 94.32 BI-LSTM-CRF (ours) 94.13 BI-LSTM-CRF (Senna) (ours) 94.46  <ref type="bibr" target="#b6">(Florian et al., 2003)</ref> 88.76 MaxEnt classifier <ref type="bibr" target="#b2">(Chieu., 2003)</ref> 88.31 Semi-supervised model combination <ref type="bibr" target="#b0">(Ando and Zhang., 2005)</ref> 89.31 Conv-CRF <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> 81.47 Conv-CRF (Senna + Gazetteer) <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref> 89.59 CRF with Lexicon Infused Embeddings <ref type="bibr" target="#b20">(Passos et al., 2014)</ref> 90.90 BI-LSTM-CRF (ours) 84.26 BI-LSTM-CRF (Senna + Gazetteer) (ours) 90.10 quence tagging. We presented the first work of applying a BI-LSTM-CRF model to NLP benchmark sequence tagging data. Our model can produce state of the art (or close to) accuracy on POS, chunking and NER data sets. In addition, our model is robust and it has less dependence on word embedding as compared to the observation in <ref type="bibr" target="#b3">(Collobert et al., 2011)</ref>. It can achieve accurate tagging accuracy without resorting to word embedding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>is tagged as B-ORG O B-MISC O O O B-MISC O O, where B-, Itags indicate beginning and intermediate positions of entities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>A simple RNN model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2: A Long Short-Term Memory Cell.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A LSTM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>A bidirectional LSTM network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>A CRF network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6</head><label>6</label><figDesc>: A LSTM-CRF model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>A BI-LSTM-CRF model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>A BI-LSTM-CRF model with MaxEnt features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>f , o and c are the input gate, forget gate, output gate and cell vectors, all of which are the same size as the hidden vector h. The weight matrix subscripts have the meaning as the name suggests. For example, W hi is the hidden-input gate matrix, W xo</figDesc><table><row><cell cols="4">is the input-output gate matrix etc. The weight ma-</cell></row><row><cell cols="4">trices from the cell to gate vectors (e.g. W ci ) are</cell></row><row><cell cols="4">diagonal, so element m in each gate vector only</cell></row><row><cell cols="4">receives input from element m of the cell vector.</cell></row><row><cell cols="4">Fig. 3 shows a LSTM sequence tagging model</cell></row><row><cell cols="4">which employs aforementioned LSTM memory</cell></row><row><cell cols="4">cells (dashed boxes with rounded corners).</cell></row><row><cell>B?ORG</cell><cell>O</cell><cell>B?MISC</cell><cell>O</cell></row><row><cell>forward</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EU</cell><cell cols="3">rejects German call</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>In addition to the past input features and sentence level tag information used in a LSTM-CRF model, a BI-LSTM-CRF model can use the future input features. The extra features can boost tagging accuracy as we will show in experiments.</figDesc><table><row><cell>B?ORG</cell><cell>O</cell><cell>B?MISC</cell><cell>O</cell></row><row><cell>forward</cell><cell></cell><cell></cell><cell></cell></row><row><cell>backward</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EU</cell><cell>rejects</cell><cell>German</cell><cell>call</cell></row><row><cell>3 Training procedure</cell><cell></cell><cell></cell><cell></cell></row><row><cell>All models used in this paper share a generic SGD</cell><cell></cell><cell></cell><cell></cell></row><row><cell>forward and backward training procedure. We</cell><cell></cell><cell></cell><cell></cell></row><row><cell>choose the most complicated model, BI-LSTM-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CRF, to illustrate the training algorithm as shown</cell><cell></cell><cell></cell><cell></cell></row><row><cell>in Algorithm 1. In each epoch, we divide the</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>? letters only, for example, I. B. M. to IBM ? non-letters only, for example, A. T. &amp;T. to ..&amp; For word features in three data sets, we use unigram features and bi-grams features. For POS features in CoNLL2000 data set and POS &amp; CHUNK features in CoNLL2003 data set, we use unigram, bi-gram and tri-gram features.</figDesc><table><row><cell>4.2.2 Context features</cell></row><row><cell>whether start with a capital letter</cell></row><row><cell>? whether has all capital letters</cell></row><row><cell>? whether has all lower case letters</cell></row><row><cell>? whether has non initial capital letters</cell></row><row><cell>? whether mix with letters and digits</cell></row><row><cell>? whether has punctuation</cell></row><row><cell>? letter prefixes and suffixes (with window size</cell></row><row><cell>of 2 to 5)</cell></row><row><cell>? whether has apostrophe end ('s)</cell></row><row><cell>? word pattern feature, with capital letters,</cell></row><row><cell>lower case letters, and digits mapped to 'A',</cell></row><row><cell>'a' and '0' respectively, for example, D56y-3</cell></row><row><cell>to A00a-0</cell></row><row><cell>? word pattern summarization feature, similar</cell></row><row><cell>to word pattern feature but with consecutive</cell></row><row><cell>identical characters removed. For example,</cell></row><row><cell>D56y-3 to A0a-0</cell></row><row><cell>1 Downloaded from http://ronan.collobert.com/senna/</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Size of sentences, tokens, and labels for training, validation and test sets.</figDesc><table><row><cell></cell><cell></cell><cell>POS</cell><cell cols="2">CoNLL2000 CoNLL2003</cell></row><row><cell>training</cell><cell>sentence #</cell><cell>39831</cell><cell>8936</cell><cell>14987</cell></row><row><cell></cell><cell>token #</cell><cell>950011</cell><cell>211727</cell><cell>204567</cell></row><row><cell cols="2">validation sentence #</cell><cell>1699</cell><cell>N/A</cell><cell>3466</cell></row><row><cell></cell><cell>token #</cell><cell>40068</cell><cell>N/A</cell><cell>51578</cell></row><row><cell>test</cell><cell>sentences #</cell><cell>2415</cell><cell>2012</cell><cell>3684</cell></row><row><cell></cell><cell>token #</cell><cell>56671</cell><cell>47377</cell><cell>46666</cell></row><row><cell></cell><cell>label #</cell><cell>45</cell><cell>22</cell><cell>9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison of tagging performance on POS, chunking and NER tasks for various models.</figDesc><table><row><cell>POS CoNLL2000 CoNLL2003</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Tagging performance on POS, chunking and NER tasks with only word features. POS CoNLL2000 CoNLL2003 LSTM 94.63 (-2.66) 90.11 (-2.88) 75.31 (-8.43) BI-LSTM 96.04 (-1.36) 93.80 (-0.12) 83.52 (-1.65) Senna CRF 94.23 (-3.22) 85.34 (-8.49) 77.41 (-8.72) LSTM-CRF 95.62 (-1.92) 93.13 (-1.14) 81.45 (-6.91) BI-LSTM-CRF 96.11 (-1.44) 94.40 (-0.06) 84.74 (-4.09)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of tagging accuracy of different models for POS.</figDesc><table><row><cell>System</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison of F1 scores of different models for NER. System accuracy Combination of HMM, Maxent etc.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://ronan.collobert.com/senna/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework for learning predictive structures from multiple tasks and unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; R</forename><forename type="middle">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2005" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Guide to Recurrent Neural Networks and Back-propagation</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>In the Dallas project</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Named entity recognition with a maximum entropy approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Chieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CoNLL</title>
		<meeting>CoNLL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural Language Processing (Almost) from Scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2011" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
	<note>Collobert et al.2011</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Finkel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Named entity recognition through classifier combination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SVMTool: A general POS tagger generator based on support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">J</forename><surname>Marquez2004</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Framewise Phoneme Classification with Bidirectional LSTM and Other Neural Network Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A. Graves and J. Schmidhuber</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Graves</surname></persName>
		</author>
		<title level="m">Speech Recognition with Deep Recurrent Neural Networks. arxiv</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Named Entity Recognition with Long Short-Term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hammerton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Use of support vector learning for chunk identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Proceedings of CoNLL</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Maximum entropy Markov models for information extraction and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Flexible text segmentation with structured multilabel classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mcdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-EMNLP</title>
		<meeting>HLT-EMNLP</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Investigation of recurrentneural-network architectures and learning methods for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mesnil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IN-TERSPEECH</title>
		<meeting>IN-TERSPEECH</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>INTER-SPEECH</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
		<title level="m">Strategies for Training Large Scale Neural Network Language Models. Proceedings of ASRU</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lexicon Infused Phrase Embeddings for Named Entity Resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Passos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Proceedings of CoNLL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tutorial on hidden Markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A maximum entropy model for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ratnaparkhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shallow parsing with conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Sha and Pereira2003</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Guided learning for bidirectional sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Voting between multiple data representations for text chunking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian AI</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Shen and Sarkar2005</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semi-supervised condensed nearest neighbor for part-of-speech tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soegaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL-HLT</title>
		<meeting>ACL-HLT</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Structure Regularization for Structured Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling Latent-Dynamic in Shallow Parsing: A Latent Conditional Model with Improved Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okanohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsujii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Sun et al.2008</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of HLT-NAACL</title>
		<meeting>HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Effect of Non-linear Deep Architecture in Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of IJCNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Convolutional neural network based triangular CRF for joint intent detection and slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">]</forename><forename type="middle">P</forename><surname>Sarikaya2013</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarikaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Recurrent conditional random fields for language understanding. ICASSP</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Spoken Language Understanding using Long Short-Term Memory Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IEEE SLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
