<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
							<email>siyuanli@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">Wen</forename><surname>Chen</surname></persName>
							<email>changwen.chen@polyu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<email>yingsshan@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
								<address>
									<region>Tencent PCG</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tencent PCG</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>detection problem using a novel query generator and query decoder. Extensive comparisons with existing methods and ablation studies on QVHighlights, Charades-STA, YouTube Highlights, and TVSum datasets demonstrate the effectiveness, superiority, and flexibility of the proposed method under various settings. Source code and pre-trained models are available at https://github.com/TencentARC/UMT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>QUERY 2: A white cat with spots is playing with a feathery toy (No Query) QUERY 1: A person cuts up photos</head><p>Video (Input)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Audio (Input)</head><p>UMT <ref type="figure">Figure 1</ref>. The proposed UMT is a unified and flexible framework which can handle different input modality combinations, and output video moment retrieval and/or highlight detection results (marked by red rectangles and golden stars, respectively). Note that different text queries lead to different outcomes from the same video. Arrows in different colors denote different input-output combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Finding relevant moments and highlights in videos according to natural language queries is a natural and highly valuable common need in the current video content explosion era. Nevertheless, jointly conducting moment retrieval and highlight detection is an emerging research topic, even though its component problems and some related tasks have already been studied for a while. In this paper, we present the first unified framework, named Unified Multi-modal Transformers (UMT), capable of realizing such joint optimization while can also be easily degenerated for solving individual problems. As far as we are aware, this is the first scheme to integrate multi-modal (visual-audio) learning for either joint optimization or the individual moment retrieval task, and tackles moment retrieval as a keypoint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video has already become the major media in content production, distribution, and consumption in our daily lives. It has the unique advantage of being able to include visual, audio, and linguistic information in the same media, in line with our natural experiences. Such an advantage on information richness, however, is also a challenging factor limiting its production and consumption, as it brings about very high costs on satisfying two critical needs. The first one is to find relevant moments in existing videos for producing new content or just getting creation hints from such references. The second one is to glance at large amounts of video content quickly by scanning video highlights rather than watching the entire original videos or video moments at a normal speed, which is needed by both video producers and consumers in such a content explosion era.</p><p>These two critical needs lead to two important research topics: video moment retrieval <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> and video highlight detection <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b43">44]</ref>, respectively. Although one may realize that these two tasks are closely related (especially when a text query is given), they have not yet been explicitly jointly studied until a very recent work <ref type="bibr" target="#b17">[18]</ref> which builds a novel dataset called QVHighlights for this purpose and presents the first model Moment-DETR optimized for jointly solving both problems. Nevertheless, this seminal work has several limitations. It assumes a text query always exists and it has only applied to the visual modality of each video. Moreover, it is still a very basic model called a strong baseline, although it adopts a transformer framework, the latest and fast-rising neural network architecture type.</p><p>This paper goes deeper into designing joint video moment retrieval and highlight detection approaches by mainly exploring two aspects: multi-modal learning and flexibility, as shown in <ref type="figure">Figure 1</ref>. Apart from text and video (i.e. visual information), audio is also treated as an important input. Moreover, a unified yet flexible framework called Unified Multi-modal Transformers (UMT) is proposed to handle different modality reliability situations and combinations. For example, when the text input is unavailable, the task degenerates to highlight detection only. When there is some significant distraction in the text, its reliability will be compromised. Moreover, the audio may also be noisy, which may limit effective exploration. UMT covers all these natural variations which conventionally need to be resolved by different specifically designed models.</p><p>To demonstrate the effectiveness and superiority of the proposed framework, we conduct experiments not only on the QVHighlights dataset <ref type="bibr" target="#b17">[18]</ref>, the only one built for joint video moment retrieval and highlight detection, but also on popular public datasets for moment retrieval (Charades-STA <ref type="bibr" target="#b7">[8]</ref>) and highlight detection (TVSum <ref type="bibr" target="#b30">[31]</ref>, YouTube Highlights <ref type="bibr" target="#b31">[32]</ref>), with or without text guidance. For each case, we compare the proposed scheme with several stateof-the-art approaches. Detailed ablation studies are also carried out to evaluate the essential components of the proposed scheme and to reveal meaningful insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Video Moment Retrieval &amp; Highlight Detection Moment retrieval is a recently studied research topic that focuses on retrieving related moments in a video given a nat-ural language query. Most existing works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref> assume there is only a single moment in a video corresponding to a given text query, and such queries are usually about activities. The recently proposed QVHighlights dataset <ref type="bibr" target="#b17">[18]</ref> goes beyond that by annotating multiple moments in a video for each query and breaks the former moment distribution bias (locating more likely at the beginning of the videos). Video retrieval via text query [24] is a similar task, but it retrieves whole videos rather than video moments. Some works on language grounding <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref> align textual phrases to temporal video segments, which operate at a finer scale than moment retrieval and target at different applications. Highlight detection concerns about detecting interesting or salient video segments (i.e. highlights) in a video. It has a long history of about two decades with a rich literature, covering various domains of videos, including sports <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b36">37]</ref>, social media <ref type="bibr" target="#b31">[32]</ref>, and first-person <ref type="bibr" target="#b43">[44]</ref>. QVHighlights is the only dataset supporting highlight detection conditioned on text-guided moment retrieval results. Video summarization is a closely related task that targets at summarizing a long video with short video segments. It focuses on representativeness, diversity, and storyline, and thus it tends to be considered as a downstream application of highlight detection <ref type="bibr" target="#b43">[44]</ref>. Dynamic video thumbnail generation is another downstream task, which selects attractive video highlights and reforms them into a very short segment <ref type="bibr" target="#b41">[42]</ref> to serve as the thumbnail. Among all these tasks, moment retrieval and highlight detection are two fundamental ones and they get highly correlated when a text query is given. This study follows the seminal work of QVHighlights on modeling both tasks together within a single framework. Different from Moment-DETR <ref type="bibr" target="#b17">[18]</ref>, our model has the flexibility to perform moment retrieval or highlight detection only.</p><p>Text Query Based Models While text query is a must for moment retrieval, it seldom appears in the studies for video highlight detection, though we believe that providing text queries leads to better results as highlights are usually subjective and interest-dependent. An early work <ref type="bibr" target="#b15">[16]</ref> proposes using text to find video highlights, but it is just about using a text ranking algorithm to rank video descriptions in the text domain for providing supervision to video shot ranking, not directly matching text and highlights. The only text-guided highlight detection exists in the very recent work <ref type="bibr" target="#b17">[18]</ref>. In the closely related field of video thumbnail generation, text queries were first investigated in <ref type="bibr" target="#b45">[46]</ref>, where a graph convolutional network is used to model the word-by-clip interactions. Later on, sentence-guided temporal modulation mechanism <ref type="bibr" target="#b27">[28]</ref> is proposed to modulate an encoder-decoder based network. All these works assume the reliability of the text query and have to rely on it, while our proposed framework can easily work without text queries or with unreliable text queries.</p><p>A person is making pancake   Multi-modal Learning Recently, multi-modal learning approaches have been explored for highlight detection by jointly modeling visual and audio modalities. The earliest work seems to be MINI-Net <ref type="bibr" target="#b11">[12]</ref>, which simply concatenates the feature vectors of both modalities. Very recently, two more sophisticated modality fusion models <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45]</ref> have been proposed and significantly boosted the performances. One of them invents a visual-audio tensor fusion mechanism <ref type="bibr" target="#b44">[45]</ref> to learn cross-modality relationships with tensor decomposition and low-rank constraints. The other does the fusion via cross-modal bidirectional attention layers <ref type="bibr" target="#b2">[3]</ref> which extract audio-attended visual features and visually-attended audio features. Though all the three approaches share the same idea with us on learning multi-modal fused representations for highlight detection, only the attention-based work build their model under the same supervised-learning setting as ours (the other two are weakly-supervised and thus not fairly comparable). Moreover, to the best of our knowledge, our approach is the first one to solve joint moment retrieval and highlight detection with multi-modal (visual-audio) learning.</p><formula xml:id="formula_0">{ ! } !"# $ ! { ! } !"# $ ! { ! } !"# $ " { ! } !"# $ ! { ! } !"# $ ! Clip</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retrieved Moments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given an untrimmed video V containing N v clips and a natural language query T with N t tokens, the goal of joint video moment retrieval and highlight detection is to localize all the moments (represented by temporal boundaries b ? R 2 ) in V , in which the visual and/or audio contents are highly relevant to T , while predicting clip-level saliency scores {s i } Nv i=1 for each moment simultaneously. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the overall architecture of our framework derives from the transformer encoder-decoder structure, and can be divided into five parts, i.e. uni-modal encoder, cross-modal encoder, query generator, query decoder, and prediction heads. The input video and text are firstly processed by pre-trained feature extractors. Specifically, we use three different models (E v , E a , and E t ) to extract visual, audio, and textual features, respectively. Each video-text pair can be therefore represented by three collections of feature vectors, namely visual features</p><formula xml:id="formula_1">{v i } Nv i=1 , audio features {a i } Nv i=1 , and textual features {t i } Nt i=1</formula><p>. The visual and audio features are fed into separate uni-modal encoders to be contextualized under global receptive field, then be fused by the cross-modal encoder for visual-audio joint representations {r i } Nv i=1 . These representations, together with textual features, are used to generate clip-level moment queries {q i } Nv i=1 that can be utilized to retrieve moments and highlights from joint representations in the query decoder. After decoding query-guided video features {d i } Nv i=1 , we use two prediction heads to obtain the final moment retrieval and highlight detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Uni-modal Encoder</head><p>Most existing feature extractors for videos and audios <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> are under the sliding window scheme, thus these methods only consider local temporal correlations, without being aware of the global context information, which is of great essence for video understanding tasks. Detecting queried moments and highlights in a video also requires an overall view of the global content. Therefore, to augment the features with global context within each modality, we adopt uni-modal encoder to process the input visual and audio features. This module is constructed by stacking standard transformer encoder layers <ref type="bibr" target="#b35">[36]</ref>, each consisting of a multi-head self-attention block and a feed-forward network. In each attention head, self-attention for either of the visual Multi-Head Attention</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed-Forward Network</head><p>Multi-Head Attention</p><formula xml:id="formula_2">{ ! " } !#$ % ! { ! } !#$ % ! { ! } !#$ % " { ! } !#$ % " { ! " } !#$ % " { ! " } !#$ % " , ,</formula><p>, , <ref type="figure">Figure 3</ref>. The architecture of bottleneck transformer module. We introduce bottleneck tokens for cross-modal feature compression and expansion, largely reducing the computational cost.</p><p>or audio modality x ? {v, a} can be computed as</p><formula xml:id="formula_3">x i = x i + w z Nv j=1 exp(w q x i ? w k x j ) Nv m=1 exp(w q x i ? w k x m ) w v x j (1)</formula><p>where x i and x i are the input and output features of clip i, and w {q,k,v,z} indicates linear transform weights for the query, key, value, and output matrices. More details about self-attention computation are referred in <ref type="bibr" target="#b35">[36]</ref>. The above formula computes the embedded gaussian correlations among clips, and aggregates the global context information into each clip. After aggregating the features, subsequently, a two-layer feed-forward network formed by Linear ? ReLU ? Dropout ? Linear is used to further project the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Cross-modal Encoder</head><p>Previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref> have claimed that jointly modeling multi-modal features can better obtain the overall representations. Hence after the uni-modal encoders, an extra cross-modal encoder is utilized to jointly capture the global correlations across modalities. Here, the exact form of the cross-modal encoder is not crucial. A straightforward approach is to apply cross-modal attention <ref type="bibr" target="#b2">[3]</ref>. However, such a strategy has two weaknesses. First, as typical natural signals, both visual and audio features have heavy spatialtemporal redundancy and noisy information that are use-less for other modalities. Second, the computation of crossmodal attention is costly, with square complexity when calculating clip-to-clip correlations. A recent work <ref type="bibr" target="#b22">[23]</ref> tried to tackle the first problem by introducing attention bottlenecks that can be regarded as the information bridge across modalities. Although promising results have been achieved, this module still suffers from high computational cost since inter-and cross-modal correlations are jointly modeled. In this work, we extend this idea and propose to disentangle these strategies, thus the resulting bottleneck transformer module can be divided into two stages, i.e. feature compression and expansion, shown in <ref type="figure">Figure 3</ref>.</p><formula xml:id="formula_4">Feature Compression Following [23], we introduce bot- tleneck tokens {z i } N b i=1</formula><p>to capture the compressed features from all modalities. Here N b is a number much smaller than the number of video clips N v . The feature compression is realized by several multi-head attentions between bottleneck tokens and the features from different modalities. Since there are only visual and audio modalities in this case, the compression process can be represented by</p><formula xml:id="formula_5">z i = z i + w z Nv j=1 exp(w q z i ? w k x j ) Nv m=1 exp(w q z i ? w k x m ) w v x j (2)</formula><p>where z i and z i are input and output features of bottleneck tokens. Other notations are consistent with Eq. 1. The only difference between Eq. 1 and Eq. 2 is that the query matrix is replaced by z i , aiming to aggregate features into bottleneck tokens. We apply this operation for both visual and audio features, so that multi-modality information is refined and compressed into bottleneck tokens.</p><p>Feature Expansion After compressing the multi-modal information, we expand the features and propagate them into each modality using another multi-head attention. Formally, the computation is as follows.</p><formula xml:id="formula_6">x i = x i + w z Nv j=1 exp(w q x i ? w k z j ) Nv m=1 exp(w q x i ? w k z m ) w v z j (3)</formula><p>Here, x i represents the cross-modality enhanced features of clip i. These features are then fed into feed-forward networks for further projection. Leveraging such a two-stage feature propagation across modalities, visual and audio features are augmented under linear computational complexities, without incorporating noisy information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Query Generator</head><p>As transformers are firstly introduced for language translation tasks, the lengths of the input and output sequences may not be the same, where the length of the output sequence is determined by the query embeddings fed into the decoder. When generalized to vision tasks, query embeddings are randomly initialized and learned during training. Such a scheme may not be suitable for video highlight detection, since the outputs ought to be strictly aligned with the input tokens. Besides, query embeddings shall naturally guide the process of representation decoding. Therefore, we introduce a query generator to adaptively generate temporally aligned moment queries depending on the natural language input. This module is also constructed by a multihead attention layer, in which visual-audio joint representations {r i } Nv i=1 act as query, textual features are key and value. Our hypothesis is that by computing the attention weights between video clips and text queries, each clip can learn whether it contains which of the concepts described in the text, and predict a query embedding that can be used to decode the learned information for different needs. Note that when text queries are not available, joint representations and learnable positional encodings are summed up to serve as moment queries instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Query Decoder and Prediction Heads</head><p>Query decoder takes visual-audio joint representations {r i } Nv i=1 and text-guided moment queries {q i } Nv i=1 as inputs, and decodes the video features for joint moment retrieval and highlight detection. The output sequence of the query decoder has the same length as the encoder input. Such a peculiarity has two advantages: 1) We may obtain the cliplevel saliency (highlight) scores as simple as adding a linear projection layer with sigmoid activation. 2) The dynamic length of output sequence also enables us to define moment retrieval as a keypoint detection problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref>. That is, each moment can be represented by its temporal center and duration (window), where the center point can be estimated by predicting a temporal heatmap and extracting local maxima. The window can be further regressed from features of the center. Note that the points in the heatmap are discrete, which may be misaligned with the real temporal center and inevitably damage the retrieval performances. An extra offset term used to adjust the center should also be predicted. We adopt four linear projection layers to predict the saliencies, centers, windows, and offsets, respectively. During training, the clip-level saliency score prediction is optimized using a binary cross-entropy loss L s . For each ground truth moment with center p ? [1, N v ] and window d, we quantize the center point to p and fill the heatmap H ? [0, 1] Nv using a 1D gaussian kernel H</p><formula xml:id="formula_7">x = exp(? (x? p) 2 2?p 2 )</formula><p>, where x is the temporal coordinate and ? p is the windowadaptive standard deviation. We optimize the center point prediction using the gaussian focal loss <ref type="bibr" target="#b19">[20]</ref> as</p><formula xml:id="formula_8">L c = ? 1 N x (1 ?? x ) ? log(? x ) if H x = 1 (1 ? H x ) ?? x ? log(1 ?? x ) otherwise<label>(4)</label></formula><p>Here, N is the number of moments, ? and ? denote the weighting and the exponent of the modulating factors in the focal loss, which are set to 2.0 and 4.0 in practice. For window and offset regression, we simply adopt L1 losses to optimize the actual values for all ground truth centers as</p><formula xml:id="formula_9">L w = ? 1 N p |w p ?? p |<label>(5)</label></formula><formula xml:id="formula_10">L o = ? 1 N p |(o p ? p) ?? p |<label>(6)</label></formula><p>where w p ,? p , o p , and? p are the ground truth and predicted windows and offsets respectively. The overall training loss would be the weighted sum of all the losses above as</p><formula xml:id="formula_11">L = ? s L s + ? c L c + ? w L w + ? o L o<label>(7)</label></formula><p>where ? {s,c,w,o} are the weights for saliency, center, window, and offset losses, respectively. When testing, the moment boundries are obtained by comining the center, window, and offset terms as introduced in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref>.  We also utilize three more datasets: Charades-STA <ref type="bibr" target="#b7">[8]</ref>, YouTube Highlights <ref type="bibr" target="#b31">[32]</ref>, and TVSum <ref type="bibr" target="#b30">[31]</ref> for further evaluation on the moment retrieval or highlight detection task only, as our model has the flexibility for tasks. Charades-STA contains 16,128 query-moment pairs annotating different actions. YouTube Highlights has 6 domains with 433 videos currently available. TVSum includes 10 domains with 5 videos each. We follow the tradition to work on a random 0.8/0.2 training/test split. Note that the annotators for TVSum were aware of the video titles, so we believe that these titles can serve as noisy text queries. Our model's flexibility to handle this situation is studied.</p><p>Evaluation Metrics We use the same evaluation metrics used in existing works. Specifically, for QVHighlights, Re-call@1 with IoU thresholds 0.5 and 0.7, mean average precision (mAP) with IoU thresholds 0.5 and 0.75, and the average mAP over a series of IoU thresholds [0.5:0.05:0.95] are used for moment retrieval. For highlight detection, mAP and HIT@1 are utilized, where a clip prediction is treated as a true positive if it has the saliency score of Very Good. For Charades-STA, Recall@1 and Recall@5 with IoU thresholds 0.5 and 0.7 are used. For YouTube Highlights and TV-Sum, mAP and Top-5 mAP are adopted, respectively. Implementation Details On QVHighlights, we simply leverage the pre-extracted features using SlowFast <ref type="bibr" target="#b6">[7]</ref> and CLIP <ref type="bibr" target="#b25">[26]</ref>. Official VGG <ref type="bibr" target="#b28">[29]</ref> and optical flow features, as well as GloVe <ref type="bibr" target="#b24">[25]</ref> embeddings, are used for Charades-STA. On YouTube Highlights and TVSum, we obtain cliplevel visual features using an I3D <ref type="bibr" target="#b3">[4]</ref> pre-trained on Kinetics 400 <ref type="bibr" target="#b12">[13]</ref>. Since each feature vector captures 32 consecutive frames, we follow <ref type="bibr" target="#b2">[3]</ref> and consider the feature vector belonging to a clip if their overlap is more than 50%. We also use CLIP to extract the title features in TVSum. Audio features of all the datasets are extracted by a PANN <ref type="bibr" target="#b14">[15]</ref> model pre-trained on AudioSet <ref type="bibr" target="#b9">[10]</ref>. Visual and audio features are temporally aligned at clip level.</p><p>All the models in our experiments contain one uni-modal and cross-modal encoder layer each. The number of decoder layers is set to 3 for QVHighlights and Charades-STA, and 1 for YouTube Highlights and TVSum since they have smaller scales. The number of bottleneck tokens N b is insensitive thus be set to 4. The weights of losses are set as ? s = 3.0, ? c = 1.0, ? w = 0.1, and ? o = 1.0, while ? w and ? o are reduced to 0.05 and 0.5 specially for Charades-STA. Following <ref type="bibr" target="#b17">[18]</ref>, we set the hidden dimensions to 256, with 4? dimension expansions in feed forward networks. Learnable positional encodings, pre-norm style layer normalizations <ref type="bibr" target="#b1">[2]</ref>, 8 attention heads, and 0.1 dropout rates are used in all transformer layers. We also adopt extra pre-dropouts with rate 0.5 for visual and audio inputs, and 0.3 for text inputs. In all experiments, we use Adam <ref type="bibr" target="#b13">[14]</ref> optimizer with 1e-3 learning rate and 1e-4 weight decay. The model is trained with batch size 32 for 200 epochs on QVHighlights, batch size 8 for 100 epochs on Charades-STA, batch size 4 for 100 epochs on YouTube Highlights, and batch size 1 for 500 epochs on TVSum, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on Joint Video Moment Retrieval and Highlight Detection</head><p>We first evaluate our proposed UMT on QVHighlights test split. The results are shown in <ref type="table" target="#tab_1">Table 1</ref>, in comparison with all the other performances ever reported. On both mo-QUERY: They are driving through a somewhat green desert while music plays.  ment retrieval and highlight detection tasks, our proposed model outperforms all the existing approaches, including the previous state-of-the-art method Moment-DETR <ref type="bibr" target="#b17">[18]</ref> under both settings (with or without pre-training with automatic speech recognition captions). <ref type="figure" target="#fig_4">Figure 4</ref> presents some qualitative results of our method on QVHighlights. <ref type="table" target="#tab_2">Table 2</ref> shows the comparison of UMT with some representative methods on Charades-STA test split. Our approach performs better than previous methods under different metrics. We also tried adopting optical flow instead of audio and obtained similar performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Moment Retrieval</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on Highlight Detection</head><p>Highlight detection results on YouTube Highlights and TVSum are presented in <ref type="table" target="#tab_2">Table 2 and Table 3</ref>, respectively. On both datasets, UMT performs better than not only representative methods which only use video, but also existing multi-modal ones that utilize both video and audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>Multi-modality (Visual-Audio) <ref type="table" target="#tab_5">Table 5</ref> shows the performances of all the multi-modal methods when different modalities are used. Note that Moment-DETR + is a multimodal extension of original Moment-DETR <ref type="bibr" target="#b17">[18]</ref> by implementing a similar bottleneck structure as UMT. Clearly, multi-modal learning can significantly boost most methods' performance on all the datasets and tasks in comparison with using a single modality, since it can capture more useful information. Compared with the most similar competitor Moment-DETR + , UMT can better explore the complementary information from different modalities and suppress the possible noise during information transfer.</p><p>Multi-task Co-optimization Given a text query for a video, retrieving the related moments and detecting salient highlights in such moments seem to be highly correlated tasks. Therefore, it is interesting to see how the multi-task co-optimization performs in comparison with training for  each individual task when the same framework and backbone are used. We conduct single-task experiments by turning off the losses corresponding to each task and training the rest of the model. Note that moment retrieval is considered to be a harder task than highlight detection as explained in Section 1. The results in <ref type="table" target="#tab_6">Table 6</ref> show that the co-optimization not only generates results for both tasks simultaneously, but also significantly boosts the performance on moment retrieval. This is clear for both our UMT and Moment-DETR <ref type="bibr" target="#b17">[18]</ref>. Note that when training for moment retrieval only, UMT performs much better than Moment-DETR, indicating its superiority on the model design. Our UMT better models the moment retrieval task as a keypoint detection problem <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref> rather than set prediction or clip classification. Moreover, the inputs to the UMT decoder are clip-aligned text-guided queries instead of positional encodings, which enables more flexible output sequence lengths and may provide stronger query information for each clip. Therefore, we believe that UMT can model the relationship between the two tasks better than Moment-DETR does. Moment Retrieval Losses <ref type="table" target="#tab_8">Table 7</ref> presents the performances of UMT when different combinations of moment retrieval losses are used. Since the center loss L c and the window loss L w are mandatory for representing a moment, only the necessity of the offset loss L o is justified. As the comparison shows, modeling the temporal offset does make the moment boundary prediction more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Justification of Text Queries</head><p>We believe that highlight detection based on text queries is an important setting for highlight detection, as different interests can favor quite different highlights from the same video. <ref type="table" target="#tab_8">Table 8</ref> reports the results of our model, with or without the text queries. It can be seen that when the query is relevant, it does improve the highlight detection performance, and such an improvement is more significant when the relevance is greater.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper introduces a novel and also the first framework for solving joint moment retrieval and highlight detection as well as its individual component problems in a unified way. It is also the first to integrate multi-modal learning into its model for such a purpose. The effectiveness and superiority of the proposal have been demonstrated on diverse and representative public datasets, in comparison with relevant methods under various settings. The framework is robust to modality quality variations and also flexible enough to work under different text query conditions.</p><p>In this document, we provide more descriptions of the model architecture and implementation details to complement the main paper. Additional ablation studies and visualization on QVHighlights <ref type="bibr" target="#b17">[18]</ref> are also incorporated to demonstrate the effectiveness of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Architecture</head><p>Learnable positional encodings with 0.1 dropout rates are adopted in all the encoder and decoder layers. More specifically, in uni-modal encoders, the same positional encodings are added to the Q and K matrices. In cross-modal encoders, they are added to the K matrix during feature compression and the Q matrix during feature expansion. In the query decoder, two independent positional encodings are added to the Q and K matrices, respectively.</p><p>In video-or audio-only schemes, cross-modal encoders are not necessary thus be removed. Their output normalization layers are moved to the end of the corresponding unimodal encoders. When text queries are not available, the query generator simply outputs the visual-audio joint representations {r i } Nv i=1 , which will be further added with learnable positional encodings to construct moment queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Similar to <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref>, during training, each ground truth moment with quantized center p and window d is used to establish a 1D gaussian kernel H x = exp(? (x? p) 2 2?p 2 ) with radius r p on the heatmap H. Here, x indicates the temporal coordinate aligned with clip indices, r p and ? p are windowadaptive parameters that can be computed as</p><formula xml:id="formula_12">r p = ? ? d (A) ? p = ? ? (r p + 1) (B)</formula><p>where ? and ? are hyperparameters controlling the corresponding values. We add 1 to r p in Eq. B to ensure that the output ? p is not too small, preventing extremely large values in the heatmap. We observe that the moment retrieval performance is not sensitive to these hyperparameters, thus both of them are set to 0.2 in all experiments. When testing, we compute the moment retrieval results assuming all the clips are centers to obtain a higher recall. Following <ref type="bibr" target="#b17">[18]</ref>, we also consider the weakly-supervised pre-training on QVHighlights with automatic speech recognition (ASR) captions. During pre-training, the saliency loss is turned off since the supervision is only for moment retrieval. We observe that UMT converges much faster than Moment-DETR due to the novel formulation of moment retrieval, thus we increase the batch size to 2048 and pre-train the model for 100 epochs to prevent overfitting. Other hyperparameters strictly follow the original settings.  <ref type="table" target="#tab_8">Table A</ref> shows the comparison among bottleneck tokens and its baselines for cross-modal feature fusion on multiple datasets. It shows that utilizing the bottleneck transformer rather than simple operations can improve the performance on both moment retrieval and highlight detection. <ref type="table" target="#tab_8">Table B</ref> studies the influence of the number of bottleneck tokens. We observe that the performance is insensitive to the number of tokens, since the feature compression and expansion process eliminate the undesirable noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Visualization</head><p>Figure A displays more qualitative results on QVHighlights <ref type="bibr" target="#b17">[18]</ref>. The results show that visual and audio features contribute to different moment retrieval and highlight detection outcomes. For example, in <ref type="figure">Figure A (b)</ref>, the video-only model fails to refine the retrieved moment given the determiner 'indicating we are listening to his audio', while the audio-only model can not distinguish the moment boundaries. Combining the visual and audio information can effectively improve the performances on both tasks. <ref type="figure">Figure B</ref> presents some failure cases on QVHighlights <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure">Figure B (a)</ref> shows that our model fails to comprehend the time adverbial clause 'after a tiring trip'. Instead, it pays more attention to 'a young mother and her family' and predicts irrelevant moments. In <ref type="figure">Figure B (b)</ref>, the visual appearances of the retrieved moment and the ground truth are similar. There are few visual clues that can be used to separate 'shot' and other actions. <ref type="figure">Figure B</ref> (c) indicates that our model understands nouns, but can not comprehend abstract words well. We argue that most failure cases are caused by the incomplete understanding of text queries, which may be remitted by using a stronger language model. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overall architecture of our framework. When either the video or audio is unavailable, the corresponding uni-modal encoder and cross-modal encoder are deactivated. If text queries are not provided, the model would simply use learnable moment queries instead. Detailed explanations of notations are described in Section 3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>QUERY 1 :QUERY 2 :</head><label>12</label><figDesc>A toddler meets cartoon characters. Mickey mouse and a costume pink bunny are interacting with a baby held by a mother.(c) (a)QUERY: A fluffy white dog is playing near the grill and getting his own meal at the table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results on QVHighlights. The predicted moments and saliency scores are shown by brackets and lines. a) All the highlight clips are presented, indicating that UMT can learn implicit correlations between video content and query semantics. b) Different modality combinations guide the model pay attention to different moments. c) Our model can handle multiple queries in a single video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>marbled dark cat with white boot patterns is eating out of a dish.(b)QUERY: A dog circling in a lawn.QUERY:A girl looks happy to be laying next to her fluffy white cat.(c) QUERY: Group of friends listening to music and enjoying in car. (e) QUERY: Black and white photo of a man indicating we are listening to his audio. (d) Figure A. Visualization results on QVHighlights val split. QUERY: After a tiring trip, a young mother and her family finally reach their hotel with a nice waterfront night view. (a) QUERY: A cat gets a shot. (b) QUERY: Cute conversation between me and my daughter. (c) Figure B. Failure cases on QVHighlights val split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experimental results on QVHighlights test split. MR and HD represent moment retrieval and highlight detection, respectively. w/ PT means pre-training with ASR captions. 30.35 44.63 31.73 32.14 34.49 55.25 XML+ [18] 46.69 33.46 47.89 34.67 34.90 35.38 55.06 Moment-DETR [18] 52.89 33.02 54.82 29.40 30.73 35.69 55.60 Moment-DETR w/ PT 59.78 40.33 60.51 35.36 36.14 37.43 60.17 UMT (Ours) 56.23 41.18 53.83 37.01 36.12 38.18 59.99 UMT (Ours) w/ PT 60.83 43.26 57.33 39.12 38.08 39.12 62.39</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>MR</cell><cell></cell><cell></cell><cell>HD</cell></row><row><cell>Method</cell><cell>R1</cell><cell></cell><cell></cell><cell>mAP</cell><cell></cell><cell cols="2">? Very Good</cell></row><row><cell></cell><cell cols="7">@0.5 @0.7 @0.5 @0.75 Avg. mAP HIT@1</cell></row><row><cell>BeautyThumb [30]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">14.36 20.88</cell></row><row><cell>DVSE [21]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">18.75 21.79</cell></row><row><cell>MCN [1]</cell><cell cols="5">11.41 2.72 24.94 8.22 10.67</cell><cell>-</cell><cell>-</cell></row><row><cell>CAL [6]</cell><cell cols="5">25.49 11.54 23.40 7.65 9.89</cell><cell>-</cell><cell>-</cell></row><row><cell>XML [19]</cell><cell>41.83</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison with representative moment retrieval methods on Charades-STA test split. All the models use the officially released VGG and/or optical flow features of Charades.</figDesc><table><row><cell></cell><cell cols="2">R@1</cell><cell cols="2">R@5</cell></row><row><cell>Method</cell><cell>IoU=0.5</cell><cell>IoU=0.7</cell><cell>IoU=0.5</cell><cell>IoU=0.7</cell></row><row><cell>SAP [5]</cell><cell>27.42</cell><cell>13.36</cell><cell>66.37</cell><cell>38.15</cell></row><row><cell>SM-RL [39]</cell><cell>24.36</cell><cell>11.17</cell><cell>61.25</cell><cell>32.08</cell></row><row><cell>MAN [47]</cell><cell>41.24</cell><cell>20.54</cell><cell>83.21</cell><cell>51.85</cell></row><row><cell>2D-TAN [49]</cell><cell>40.94</cell><cell>22.85</cell><cell>83.84</cell><cell>50.35</cell></row><row><cell>FVMR [9]</cell><cell>42.36</cell><cell>24.14</cell><cell>83.97</cell><cell>50.15</cell></row><row><cell>UMT  ? (Ours)</cell><cell>48.31</cell><cell>29.25</cell><cell>88.79</cell><cell>56.08</cell></row><row><cell>UMT  ? (Ours)</cell><cell>49.35</cell><cell>26.16</cell><cell>89.41</cell><cell>54.95</cell></row><row><cell cols="3">? video + audio,  ? video + optical flow</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experimental results on YouTube Highlights (metric: mAP). Above are the methods using visual features only, the others are using visual-audio features. Dog Gym. Par. Ska. Ski. Sur. Avg.</figDesc><table><row><cell>Method</cell><cell></cell></row><row><cell>RRAE [43]</cell><cell>49.0 35.0 50.0 25.0 22.0 49.0 38.3</cell></row><row><cell>GIFs [11]</cell><cell>30.8 33.5 54.0 55.4 32.8 54.1 46.4</cell></row><row><cell>LSVM [32]</cell><cell>60.0 41.0 61.0 62.0 36.0 61.0 53.6</cell></row><row><cell>LIM-S [40]</cell><cell>57.9 41.7 67.0 57.8 48.6 65.1 56.4</cell></row><row><cell cols="2">SL-Module [41] 70.8 53.2 77.2 72.5 66.1 76.2 69.3</cell></row><row><cell>MINI-Net [12]</cell><cell>58.2 61.7 70.2 72.2 58.7 65.1 64.4</cell></row><row><cell>TCG [45]</cell><cell>55.4 62.7 70.9 69.1 60.1 59.8 63.0</cell></row><row><cell>Joint-VA [3]</cell><cell>64.5 71.9 80.8 62.0 73.2 78.3 71.8</cell></row><row><cell>UMT (Ours)</cell><cell>65.9 75.2 81.6 71.8 72.3 82.7 74.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison with representative highlight detection methods on TV-Sum (metric: Top-5 mAP). Above are the methods using visual features only, the others are using visual-audio features. VT VU GA MS PK PR FM BK BT DS Avg.</figDesc><table><row><cell>Method</cell><cell></cell></row><row><cell>sLSTM [48]</cell><cell>41.1 46.2 46.3 47.7 44.8 46.1 45.2 40.6 47.1 45.5 45.1</cell></row><row><cell>SG [22]</cell><cell>42.3 47.2 47.5 48.9 45.6 47.3 46.4 41.7 48.3 46.6 46.2</cell></row><row><cell>LIM-S [40]</cell><cell>55.9 42.9 61.2 54.0 60.4 47.5 43.2 66.3 69.1 62.6 56.3</cell></row><row><cell>Trailer [38]</cell><cell>61.3 54.6 65.7 60.8 59.1 70.1 58.2 64.7 65.6 68.1 62.8</cell></row><row><cell cols="2">SL-Module [41] 86.5 68.7 74.9 86.2 79.0 63.2 58.9 72.6 78.9 64.0 73.3</cell></row><row><cell>MINI-Net [12]</cell><cell>80.6 68.3 78.2 81.8 78.1 65.8 57.8 75.0 80.2 65.5 73.2</cell></row><row><cell>TCG [45]</cell><cell>85.0 71.4 81.9 78.6 80.2 75.5 71.6 77.3 78.6 68.1 76.8</cell></row><row><cell>Joint-VA [3]</cell><cell>83.7 57.3 78.5 86.1 80.1 69.2 70.0 73.0 97.4 67.5 76.3</cell></row><row><cell>UMT (Ours)</cell><cell>87.5 81.5 88.2 78.8 81.4 87.0 76.0 86.9 84.4 79.6 83.1</cell></row></table><note>Datasets QVHighlights [18] is the only existing public dataset that has ground-truth annotations for both moment retrieval and highlight detection, thus being suitable for evaluating the full version of our proposed model. This dataset contains videos cropped into 10,148 short (150s- long) segments, and had each segment annotated with at least one text query depicting its relevant moments. There are averagely about 1.8 disjoint moments per query, anno- tated on non-overlapping 2s-long clips. In total, there are 10,310 queries with 18,367 annotated moments. We follow the original QVHighlights data splits in all experiments.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>The effectiveness of multi-modal learning on YouTube Highlights, TVSum, and QVHighlights val split. MR and HD denote moment retrieval and highlight detection, respectively.</figDesc><table><row><cell></cell><cell>YouTube</cell><cell>TVSum</cell><cell cols="2">QVHighlights</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>mAP</cell><cell cols="3">Top-5 mAP MR (mAP) HD (mAP)</cell></row><row><cell>MINI-Net  ? [12]</cell><cell>61.38</cell><cell>69.79</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint-VA  ? [3]</cell><cell>70.50</cell><cell>74.80</cell><cell>-</cell><cell>-</cell></row><row><cell>Moment-DETR  ? [18]</cell><cell>-</cell><cell>-</cell><cell>32.20</cell><cell>36.52</cell></row><row><cell>UMT  ? (Ours)</cell><cell>73.48</cell><cell>81.89</cell><cell>37.79</cell><cell>38.97</cell></row><row><cell>MINI-Net  ? [12]</cell><cell>52.23</cell><cell>59.72</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint-VA  ? [3]</cell><cell>67.00</cell><cell>68.70</cell><cell>-</cell><cell>-</cell></row><row><cell>Moment-DETR  ? [18]</cell><cell>-</cell><cell>-</cell><cell>16.69</cell><cell>26.00</cell></row><row><cell>UMT  ? (Ours)</cell><cell>65.61</cell><cell>76.51</cell><cell>13.73</cell><cell>23.91</cell></row><row><cell>MINI-Net [12]</cell><cell>64.36</cell><cell>73.24</cell><cell>-</cell><cell>-</cell></row><row><cell>Joint-VA [3]</cell><cell>71.80</cell><cell>76.30</cell><cell>-</cell><cell>-</cell></row><row><cell>Moment-DETR + [18]</cell><cell>-</cell><cell>-</cell><cell>34.05</cell><cell>37.67</cell></row><row><cell>UMT (Ours)</cell><cell>74.93</cell><cell>83.14</cell><cell>38.59</cell><cell>39.85</cell></row><row><cell cols="4">? video only,  ? audio only, + w/ bottleneck transformer</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Comparison with Moment-DETR using different training task combinations on QVHighlights val split. MR and HD denote moment retrieval and highlight detection, respectively.</figDesc><table><row><cell></cell><cell>Tr. Task(s)</cell><cell>MR</cell><cell>HD</cell></row><row><cell>Method</cell><cell>MR HD</cell><cell>R1 @0.5 @0.7 Avg. R1 mAP</cell><cell>mAP HIT@1</cell></row><row><cell>Moment-DETR [18]</cell><cell></cell><cell cols="2">44.84 25.87 25.05 ---53.94 34.84 32.20 35.65 55.55 --36.52 56.45</cell></row><row><cell>UMT (Ours)</cell><cell></cell><cell cols="2">54.14 33.82 34.02 ---60.26 44.26 38.59 39.85 64.19 --40.22 65.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Effectiveness justification of the offset loss for moment retrieval on QVHighlights val split. Both models are trained using the co-optimization recipe. Lw 62.32 43.23 57.78 38.61 37.36 Lc + Lw + Lo 60.26 44.26 56.70 39.90 38.59 The influence of weakly relevant (TVSum) or highly relevant (QVHighlights) text queries on highlight detection.</figDesc><table><row><cell>R1</cell><cell></cell><cell>mAP</cell></row><row><cell>Losses</cell><cell></cell><cell></cell></row><row><cell cols="3">@0.5 @0.7 @0.5 @0.75 Avg.</cell></row><row><cell>Lc + Text Query TVSum</cell><cell cols="2">QVHighlights</cell></row><row><cell></cell><cell>mAP</cell><cell>HIT@1</cell></row><row><cell>81.42</cell><cell>25.14</cell><cell>33.42</cell></row><row><cell>83.14</cell><cell>39.85</cell><cell>64.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A .</head><label>A</label><figDesc>Comparison of cross-modal fusion methods on YouTube Highlights, TVSum, and QVHighlights val split. MR and HD denote moment retrieval and highlight detection, respectively.Table B. Ablation on number of tokens in the bottleneck transformer on QVHighlights val split (metric: mAP). MR and HD denote moment retrieval and highlight detection, respectively. MR 38.59 37.19 38.29 37.15 37.20 38.18 HD 39.85 39.64 39.26 39.37 39.22 39.26</figDesc><table><row><cell></cell><cell>YouTube</cell><cell></cell><cell>TVSum</cell><cell cols="3">QVHighlights</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>mAP</cell><cell cols="5">Top-5 mAP MR (mAP) HD (mAP)</cell></row><row><cell>Concat</cell><cell>73.32</cell><cell></cell><cell>80.26</cell><cell>37.03</cell><cell></cell><cell>38.74</cell></row><row><cell>Mean</cell><cell>73.29</cell><cell></cell><cell>81.76</cell><cell>37.04</cell><cell></cell><cell>38.91</cell></row><row><cell>Sum</cell><cell>73.53</cell><cell></cell><cell>81.77</cell><cell>37.33</cell><cell></cell><cell>38.88</cell></row><row><cell>Bottleneck</cell><cell>74.93</cell><cell></cell><cell>83.14</cell><cell>38.59</cell><cell></cell><cell>39.85</cell></row><row><cell>#Tokens</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>128</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is supported in part by Key-Area Research and Development Program of Guangdong Province, China with Grant 2019B010155002 and financial support from ARC Lab, Tencent PCG.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint visual and audio learning for video highlight detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taivanbat</forename><surname>Badamdorj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="8127" to="8137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic proposal for activity localization in videos via sentence query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoxiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8199" to="8206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Temporal localization of moments in video collections with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12763</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tall: Temporal activity localization via language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast video moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1523" to="1532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and humanlabeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="776" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video2gif: Automatic generation of animated gifs from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangliang</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1001" to="1009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mini-net: Multiple instance rank-ing network for video highlight detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Words speak for actions: Using text to find video highlights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukanya</forename><surname>Kudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Anoop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Namboodiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<meeting>the IAPR Asian Conference on Pattern Recognition (ACPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="322" to="327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Qvhighlights: Detecting moments and highlights in videos via natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tvr: A large-scale dataset for video-subtitle moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="447" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task deep visual-semantic embedding for video thumbnail selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cherry</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3707" to="3715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised video summarization with adversarial lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behrooz</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<idno>2021. 4</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning joint representations of videos and sentences with web image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayu</forename><surname>Otani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuta</forename><surname>Nakashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Heikkil?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naokazu</forename><surname>Yokoya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="651" to="667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grounding action descriptions in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominikus</forename><surname>Wetzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sentence guided temporal modulation for dynamic video thumbnail generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahesh</forename><surname>Mrigank Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">To click or not to click: Automatic selection of beautiful thumbnails from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Redi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Information and Knowledge Management (CIKM)</title>
		<meeting>the ACM International Conference on Information and Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="659" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tvsum: Summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Vallmitjana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Jaimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ranking domainspecific highlights by analyzing edited videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Detecting highlights in sports videos: Cricket as a test case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ullas</forename><surname>Mehmet Emre Sargin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gargi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting>the IEEE International Conference on Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards surveillance video search by natural language query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Image and Video Retrieval (CIVR)</title>
		<meeting>the ACM International Conference on Image and Video Retrieval (CIVR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sports highlight detection from keyword sequences using hmm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Engsiong</forename><surname>Chng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Multimedia and Expo (ICME)</title>
		<meeting>the IEEE International Conference on Multimedia and Expo (ICME)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="599" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning trailer moments in full-length movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lezi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="300" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Languagedriven temporal activity localization: A semantic matching reinforcement learning model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weining</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="334" to="343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Less is more: Learning highlight detection from video duration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1258" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross-category video highlight detection via set-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhu</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7970" to="7979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Gif thumbnails: Attract more clicks to your videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingxuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longwen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuigeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huyang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised extraction of video highlights via robust recurrent auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minyi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4633" to="4641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Highlight detection with pairwise deep ranking for first-person video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="982" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal cue guided video highlight detection with low-rank audio-visual fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sentence specified dynamic video thumbnail generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia (MM)</title>
		<meeting>the ACM International Conference on Multimedia (MM)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2332" to="2340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Man: Moment alignment network for natural language moment retrieval via iterative graph adjustment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1247" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning 2d temporal adjacent networks for moment localization with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12870" to="12877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
