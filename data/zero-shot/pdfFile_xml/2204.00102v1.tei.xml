<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DYNAMIC MULTIMODAL FUSION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihui</forename><surname>Xue</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Marculescu</surname></persName>
							<email>radum@utexas.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Texas at Austin</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DYNAMIC MULTIMODAL FUSION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Multimodal Learning, Dynamic Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep multimodal learning has achieved great progress in recent years. However, current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data. In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference. DynMM can reduce redundant computations for "easy" multimodal inputs (that can be predicted correctly using only one modality or simple fusion techniques) and retain representation power for "hard" samples by adopting all modalities and complex fusion operations for prediction. Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach. For instance, DynMM can reduce the computation cost by 46.5% with a negligible accuracy loss on CMU-MOSEI sentiment analysis. For RGB-D semantic segmentation on NYU Depth data, DynMM achieves a +0.7% mIoU improvement with over 21% reductions for the depth encoder when compared with strong baselines. We believe this opens a novel direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans perceive the world in a multimodal way, through vision, hearing, touch, taste, etc. Recent years have witnessed great progress of deep learning approaches that leverage data of multiple modalities. Multimodal fusion has boosted the performance of many classical problems, such as sentiment analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, action recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, or semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Despite these advances, how to best combine information characterized by multiple modalities remains a fundamental challenge in multimodal learning <ref type="bibr" target="#b8">[9]</ref>. Various research efforts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3]</ref> have been put into designing new fusion paradigms that effectively fuse multimodal data. These approaches are generally task-and modality-specific and require manual design. Building on the success of Neural Architecture Search (NAS), a few recent works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> have adopted NAS to find effective fusion architectures automatically.</p><p>However, these methods (both manually-designed and NAS-based networks) process all the instances in a single fusion architecture and lack adaptability to diverse multimodal data. Namely, once the fusion network is trained, it performs static inference for each piece of data, without accounting for the differences in inherent characteristics of different multimodal inputs. Thus the computational efficiency, as well as the representation power of a well-designed fusion architecture may be limited by its static nature. As a motivating example, consider two multimodal instances shown in <ref type="figure">Figure 1</ref>. It is relatively easy to classify emotions for the left example: the text modality alone provides strong evidence for a positive emotion. On the other hand, we are unlikely to correctly predict emotions for the right example based on textual information since the sentence is confusing. Audio and visual modality can provide important cues for a multimodal network to make correct decisions. From this example, we can see that multimodal data enables a model to learn from enriched representations (for "hard" inputs), but brings significant redundancy in computations (for "easy" inputs) as well.  <ref type="figure">Figure 1</ref>: Two examples in CMU-MOSEI <ref type="bibr" target="#b20">[21]</ref> for emotion recognition. The left figure shows an "easy" multimodal instance as using textual information is sufficient to predict emotions correctly (this is a positive emotion). The right figure shows a "hard" example where all three modalities are required to make correct predictions (this is a negative emotion). While static multimodal fusion networks process "hard" and "easy" inputs identically, we propose dynamic instance-wise inference that can achieve computational savings for "easy" examples and preserve representation power for "hard" instances. For the left example, DynMM only activates the text path and skips paths corresponding to the other two modalities, thus leads to computational efficiency.</p><p>Inspired by this observation, we propose a novel idea, dynamic multimodal fusion (DynMM) that adaptively fuses input data from multiple modalities during inference. Compared with a static multimodal architecture, DynMM enjoys the benefits of reduced computation, as well as potentially improved performance. More precisely, dynamic fusion leads to computational savings for "easy" inputs that can be correctly predicted using a subset of modalities or simple fusion operations. For "hard" multimodal inputs, DynMM can match the representation power of a static network by adopting all the modalities and complex fusion operations for prediction. In addition, real-world multimodal data may be noisy and contradictory <ref type="bibr" target="#b19">[20]</ref>. In such cases, skipping paths that involve noisy modalities for certain instances can reduce noise and may in turn boost performance.</p><p>Dynamic neural networks have emerged as a promising research topic and gained increasing attention over the past few years <ref type="bibr" target="#b21">[22]</ref>. Indeed, dynamic neural networks have a broad range of applications, such as image recognition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, semantic segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and machine translation <ref type="bibr" target="#b28">[29]</ref>. Motivated by the great success of dynamic inference for unimodal networks, this paper aims at proposing a novel application domain (i.e., multimodal fusion) for this line of research. We draw inspiration from the natural redundancy of multimodal data, which provides a conceptually different angle from existing works. To be specific, we propose dynamic forward paths, both at the modality level and at the fusion level. For the modality level, we train a gating network that decides a subset of input modalities (or using all modalities) for predictions based on each input. For challenging multimodal tasks such as RGB-D semantic segmentation (i.e., a dense prediction problem), we introduce the concept of progressive fusion, where a gating network provides sample-wise decisions on when to stop fusion. By allowing exits at the early fusion stages, DynMM saves computations in later fusion modules. In the meantime, in terms of "hard" multimodal inputs, DynMM can turn on all fusion modules for accurate predictions.</p><p>We conduct experiments on various popular multimodal tasks to verify the efficacy and generalizability of our approach. DynMM strikes a good balance between computational efficiency and learning performance. For instance, for RGB-D semantic segmentation tasks, our proposed DynMM achieves a +0.7% mIoU improvement with over 21% reductions in multiply-add operations (MAdds) for the depth encoder when compared against strong baselines <ref type="bibr" target="#b5">[6]</ref>. Moreover, the reduction ratio for the depth encoder part can be up to 55.1% with a slightly degraded performance (i.e., -0.4% mIoU).</p><p>The remainder of the paper is organized as follows. In Section 2, we discuss prior work. In Section 3, we propose designs of modality-level DynMM and fusion-level DynMM. Experimental results are presented in Section 4. Finally, Section 5 concludes the paper.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dynamic Neural Networks</head><p>As an emerging research topic, dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>, object detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, or semantic segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. While prevalent deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference. This flexibility yields many benefits, including high efficiency, representation power and interpretability <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>. A variety of ideas <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref> have been proposed towards designing dynamic architectures. Namely, the model architecture is adjusted based on each sample during inference time. These methods can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing.</p><p>The idea of dynamic depth is to adjust the network depth based on each sample. Early exiting <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> serves as a popular technique. By providing early exits in shallow layers, one can save computations of executing deep layers for "easy" samples. For dynamic width, the idea is to adapt the network width in a sample-wise manner. To build a network of dynamic width and achieve inference efficiency, previous works have proposed to skip neurons in fully-connected layers <ref type="bibr" target="#b34">[35]</ref>, skip branches in Mixture-of-Experts (MoE) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29]</ref>, or skip channels in Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b35">[36]</ref>. To enable more flexibility, recent works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref> build SuperNets with multiple inference paths. Dynamic routing is thus performed inside a SuperNet to generate data-dependent forward paths during inference. Our proposed modality-level DynMM falls into the category of dynamic width and fusion-level DynMM can be seen as a dynamic routing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multimodal Learning</head><p>Multimodal fusion networks have a clear advantage over their unimodal counterparts on various applications, such as sentiment analysis <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, action recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, or semantic segmentation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. However, how to effectively combine multimodal features to better exploit information remains a challenge. Existing works either propose hand-crafted fusion designs based on domain knowledge <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3]</ref>, or apply NAS to find good architectures automatically <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. However, their scope is limited to static networks.</p><p>Recent work <ref type="bibr" target="#b6">[7]</ref> proposes to dynamically exchange channels between sub-networks of different modalities, which can be unified into the topic of dynamic fusion networks. The emphasis is purely on performance, without accounting for computational efficiency. Gao et al. <ref type="bibr" target="#b38">[39]</ref> use audio as a preview mechanism to eliminate visual redundancies in the temporal domain for efficient action recognition. Their method is specially tailored for this task.</p><p>Despite these early works, a systematic and general formulation of dynamic multimodal fusion is notably lacking, which is the motivation of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We present the key design contributions of our proposed dynamic multimodal fusion networks (DynMM) in this section. First, we introduce new decision making schemes that enable DynMM to generate data-dependent forward paths during inference. Two levels of granularity are considered, i.e., modality-level and fusion-level decision making. Next, we propose new training strategies for DynMM, which consist of (1) a training objective that accounts for resource budgets, and (2) optimization of a non-differentiable gating function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Modality-level Decision</head><p>Assume input data has M modalities, denoted by x = (x 1 , x 2 , ? ? ? , x M ). Following the classical Mixtureof-Experts <ref type="bibr" target="#b39">[40]</ref> framework, we design a set of expert networks as follows. Each expert specializes in a subset of all M modalities. Take M = 3 for example, we can have up to 7 expert networks, denoted by</p><formula xml:id="formula_0">E 1 (x 1 ), E 2 (x 2 ), E 3 (x 3 ), E 4 (x 1 , x 2 ), E 5 (x 2 , x 3 ), E 6 (x 1 , x 2 ), E 7 (x 1 , x 2 , x 3 )</formula><p>. In real applications, the candidate expert networks can be narrowed down with domain expertise. For instance, depth images can provide useful cues when combined with RGB images, but often perform poorly by themselves in semantic segmentation. In such a case, we will not consider adopting an expert network that only takes depth as input. Let B represent the number of expert networks that get selected. We propose a gating function, denoted by G(x), to decide which expert network should be activated. This gating function takes multimodal inputs x to form a global view and then produces a B-dimensional sparse vector g as output. The final output y takes the form of:</p><formula xml:id="formula_1">y = B i=1 g i E i (x i )<label>(1)</label></formula><p>where x i denotes the subset of modalities that the i-th expert takes as input.</p><p>Different from conventional MoEs <ref type="bibr" target="#b39">[40]</ref>, where the output is a weighted summation of expert networks and every branch is executed, in our formulation, the output of the gating function g is a one-hot encoding, i.e., only one branch is Expert Network</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Expert Network</head><p>Expert Network + Gating Network <ref type="figure">Figure 2</ref>: An illustration of Modality-level DynMM, where input data has two modalities, denoted by x 1 and x 2 , and the output is denoted by y. We design a set of expert networks {E i } that specialize in different subsets of modalities and adopt a gating network G(x) to generate data-dependent decisions on which expert network to select.</p><p>selected for each instance. Therefore, the computations required for other expert networks can be saved. Note that since our expert network already covers a broad range of modality combinations, we only select one branch (as opposed to say selecting top K branches) during each forward pass for maximum computational savings. <ref type="figure">Figure 2</ref> provides an illustration of the proposed design with 2 modalities and 3 expert networks (i.e., M = 2 and B = 3).</p><p>The design of a gating network follows two general requirements: (1) it should be computationally cheap to only introduce a small overhead (2) it needs to be sufficiently expressive to make informative decisions on which expert to select. Various gating networks have been proposed previously, and they are usually tailored for specific tasks and network architectures. We design different gating functions (i.e., a multi-layer perceptron (MLP) gate, a transformer gate and a convolutional gate) for three multimodal tasks and provide the detailed description of our gating network architecture in the experiments section.</p><p>One remaining problem is the training of gating network G(x). Due to non-differentiability of discrete decisions given by G(x), the network can not be directly trained with back-propagation. Consequently, we propose reparameterization techniques and discuss them in detail in Section 3.4.</p><p>In addition, this gating function is not restricted to the input-level and can also take intermediate features per modality as inputs. Thus, modality-level DynMM can be plugged into any part of a multimodal network and achieve savings for computations after this gating function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fusion-level Decision</head><p>While the modality-level decisions directly impact the computational efficiency, completely skipping computations of one modality will likely lead to a downgraded performance for some challenging tasks, e.g., dense prediction problems like semantic segmentation. Therefore, we provide a finer-grained formulation of DynNN with fusion-level decisions in this section.</p><p>We first present the design of a fusion cell. Assume input data has M modalities, i.e., x = (x 1 , x 2 , ? ? ? , x M ). Denote a set of fusion operations as {O i }. O i can be implemented as any fusion module; typical choices include identity mapping, addition, concatenation and linear weighting, etc. Let B represent the total number of operations. A gating function G(x) takes multimodal inputs and produces a B-dimensional vector g that decides which operation to execute. The output of the cell h can be represented as:</p><formula xml:id="formula_2">h = B i=1 g i O i (x)<label>(2)</label></formula><p>Following the previous discussion, we adopt hard gates (i.e., g is one-hot) to achieve computational efficiency. <ref type="figure" target="#fig_0">Figure  3</ref> provides the design of a fusion cell with two modalities (i.e., x = (x 1 , x 2 )), where three simple operations are considered i.e.,  Fusion-level DynMM allows decisions at a finer granularity and in a more flexible way by stacking fusion cells to build a dynamic network. We provide an example architecture in <ref type="figure" target="#fig_0">Figure 3</ref> that is adopted in our experiments for semantic segmentation (x 1 and x 2 denote RGB and depth images, respectively). The network consists of four fusion blocks and a global gating function, which allows us to flexibly control the degree of fusion in a sample-wise manner. For instance, for some multimodal inputs, combining low-level features from each modality (i.e., fusing at early stages) is sufficient for good predictions. Thus, the decision given by the gating function would be to choose O 3 for fusion cell 1 and choose O 1 for the other fusion cells. This not only skips complex fusion operations that are not selected within the fusion cell, but also saves unnecessary computations of the feature extraction layer. Since we only adopt features from modality 1 after fusion cell 1, there is no need to further process features from modality 2. Thus, we can skip computations of feature extraction layers for x 2 (i.e., yellow blocks 2-4 in <ref type="figure" target="#fig_0">Figure 3</ref>). On the other hand, for "hard" instances, DynMM provides the option of combining multimodal features in each cell with complex fusion operations for maximum representation power. Note that we adopt a global gating function here that receives multimodal input features and makes global decisions on a sequence of operations to adopt (i.e., 4 operations in this example). Other choices of the gating function (e.g., using a recurrent neural network to provide sequential decisions) are left as future work.</p><formula xml:id="formula_3">O 1 = x 1 , O 2 = x 2 , O 3 = x 1 + x 2 .</formula><p>This paradigm is especially helpful in tasks where the final prediction is mainly based on one dominant modality, while the other auxiliary modalities provide useful cues to improve the prediction. Fusion-level DynMM provides a flexible way to control how and when the auxiliary modality comes in to assist the main prediction process. We call this concept as progressive fusion. Progressive fusion is achieved by our carefully designed fusion cell and dynamic architecture, leading to great computational savings and strong representation power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training Objective</head><p>We notice that in the above design, the computation for each expert network E i (operation O i ) is different. Normally, an expert network (an operation) that is computationally heavy has a strong representation power. If we directly train the network by minimizing a task-specific loss, the gating network is likely to learn a trivial solution that always chooses the branch with heavy computation. To achieve efficient inference, we introduce a resource-aware loss into the training objective. Let C(E i ) denote the computation cost (e.g., MAdds) of executing expert network E i . Similarly, C(O i,j ) represents the computation cost of the i-th fusion operation in the j-th cell. Note that the computation cost can be pre-determined before training and is a constant term. The training objectives are shown below:</p><formula xml:id="formula_4">L = L task + ? B i=1 g i C(E i ) (modality-level DynMM) (3) L = L task + ? F j=1 B i=1 g (j) i C(O i,j ) (fusion-level DynMM)<label>(4)</label></formula><p>where L task denotes the task loss (e.g., cross entropy between the network prediction and true label for classification). g (j) represents the decision vector given by the j-th fusion cell. B is the total number of experts (operations) and F is the number of fusion cells. ? is a hyperparamter controlling the relative importance of the two loss terms.</p><p>The new objective accounts for the computation cost of executing each path and enables DynMM to achieve a desired tradeoff between accuracy and efficiency. We can adjust the value of ? based on the deployment constraints. More precisely, for large ?, DynMM will prioritize lightweight computations for high computational efficiency. For small ?, DynMM can explore those computationally heavy paths more often and potentially yield better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Optimization</head><p>We aim to train DynMM in an end-to-end manner. Since the current gating function provides discrete decisions, the branch selection process is not directly differentiable with respect to the gating network. Gumbel-softmax and reparameterization techniques <ref type="bibr" target="#b40">[41]</ref> are introduced in the training process. Recall that g denotes the desired one-hot B-dimensional decision vector produced by a gating network G(x), i.e., g = one_hot(arg max i G(x) i ). We adopt a real-valued soft vectorg with the following form:</p><formula xml:id="formula_5">g i = exp((logG(x) i + b i )/? ) B j=1 exp((logG(x) j + b j )/? ) i = 1, 2, . . . , B<label>(5)</label></formula><p>where b 1 , b 2 , . . . , b B are samples independently drawn from Gumbel(0, 1) <ref type="bibr" target="#b40">[41]</ref> and ? denotes the softmax temperature. The distribution ofg is more uniform with large ? and resembles a categorical distribution with small ? .g serves as a continuous, differentiable approximation of g. We consider two training techniques: (a) Hard g is replaced with soft g in previous equations to enable back-propagation. During training, we anneal ? so thatg gradually converges to a desired one-hot vector. (b) Following the straight-through technique <ref type="bibr" target="#b40">[41]</ref>, we adopt hard g in the forward pass and soft g in the backward propagation with the gradient approximation ?g ? ?g. In this way, the gating network still outputs a discrete decision during training. Note that we always use hard g during inference for computational benefits. Next, we propose two-stage training of DynMM that jointly optimizes the multimodal network and gating modules.</p><p>Stage I: Pre-training. We find that following sparse decisions of the gating network in the early stage of training can result in a biased optimization. Branches that are rarely selected have fewer and smaller weight updates, and poor performance may result in them getting selected less often (thus never improving). The goal of a pre-training stage is to ensure every branch of DynMM is fully optimized before the gating modules get involved. For modality-level DynMM, we sufficiently train each expert network in this stage. For fusion-level DynMM, we adopt random decisions (i.e., chooses an operation from the set of candidate operations randomly) for each fusion cell so that each path of the dynamic network is optimized uniformly.</p><p>Stage II: Fine-tuning. We incorporate gating networks into our optimization process in this stage. With the reparamterization technique introduced above, we jointly optimize the dynamic network along with gating networks in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>We conduct experiments on three multimodal tasks: (a) movie genre classification on MM-IMDB <ref type="bibr" target="#b41">[42]</ref>; (b) sentiment analysis on CMU-MOSEI <ref type="bibr" target="#b20">[21]</ref>; (c) semantic segmentation on NYU Depth V2 <ref type="bibr" target="#b42">[43]</ref>. To demonstrate the great applicability of our proposed DynMM, we select the above three tasks that include different modalities (i.e., image and text in task (a), video, audio and text in task (b), RGB and depth images in task (c)). We adopt modality-level DynMM for the first two tasks and fusion-level DynMM for the more challenging semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Movie Genre Classification</head><p>MM-IMDB is the largest publicly available multimodal dataset for genre prediction on movies. It comprises 25,959 movie titles, metadata and movie posters. The task is to perform multi-label classification of 27 movie genres from posters (image modality) and text descriptions (text modality). We follow the original data split in <ref type="bibr" target="#b41">[42]</ref>, and use 15,552 data for training, 2,608 for validation and 7,799 for testing. We use the same method as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> to extract text and image features. We adopt two expert networks for this task, namely, a unimodal network E 1 that takes textual features as input and another multimodal network E 2 that adopts late fusion to combine image and text features. We do not consider the use of an image-only network here due to its poor performance on this task. The gating network is a 2-layer MLP with hidden dimension of 128, which takes concatenated image and text features as input and outputs a 2-dimensional vector for expert network selection. We set the temperature of Gumbel-softmax as 1 and adopt straight-through training (i.e., the gating network outputs a one-hot decision vector in the forward propagation). <ref type="table" target="#tab_2">Table 1</ref> provides the comparison of our proposed modality-level DynMM with several static unimodal and multimodal networks. Among these, we adopt the text network as expert E 1 and late fusion network as E 2 in DynMM. We provide results of DynMM under different resource requirements (i.e., use different ? in the loss). From <ref type="table" target="#tab_2">Table 1</ref>, we can see that DynMM achieves a good balance between computational efficiency and performance. Compared to the static E 2 network, DynMM-c improves both MAdds and macro F1 score. DynMM-d provides maximum representation power by using soft gates (which leads to more computation) and achieves best micro and macro F1 scores. On the other hand, DynMM-a involves much less computation, while still maintaining good performance (outperforms E 1 by 1.6% in macro F1). This demonstrates great flexibility and efficacy of DynMM.</p><p>In addition, we vary ? in Equation 3 to control the importance of resource loss during training. The resulting DynMM models have varying computation costs and performance, as shown in <ref type="figure" target="#fig_1">Figure 4 (left figure)</ref>. On one hand, when compared against a multimodal baseline that is computationally heavy, DynMM maintains good performance with much fewer MAdds. On the other hand, DynMM has better representation power than a unimodal network and thus improves the F1 score. The right figure shows the selection ratio of each expert network in DynMM with respect to ?. We observe that as ? increases, DynMM focuses more on reducing computation and thus is more likely to select expert network 1 (E 1 ) with a small computation cost. Note that for the w = 0 case, we adopt soft gates, i.e., every expert network is activated and the output is a weighted combination of predictions given by the two expert networks. Thus, DynMM achieves the best performance at the cost of increased computation. This also demonstrates the flexibility of DynMM, as we can easily adjust ? to target high performance or high inference efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sentiment Analysis</head><p>CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) <ref type="bibr" target="#b20">[21]</ref> is the largest dataset of sentiment analysis and emotion recognition. It contains 3,228 real-world online videos from more than 1000 speakers and 250 topics. Each video is split into short segments of 10-20 seconds. Each segment is annotated for a sentiment from -3 (strongly negative) to 3 (strongly positive). The task is to predict the sentiment scores from video, audio and text.  Following <ref type="bibr" target="#b43">[44]</ref>, we use 16,265 data for training, 1,869 data for validation and 4,643 data for testing. The feature extraction steps are the same as <ref type="bibr" target="#b43">[44]</ref>.</p><p>As text is the best performing modality in this task, we adopt a unimodal network that takes textual features as input to be the expert network E 1 . The second expert network (E 2 ) of our DynMM is selected as a late fusion network that receives inputs from three modalities. The gating network is designed as a lightweight transformer network with hidden dimension equal to 5 and 2 attention heads, followed by a linear layer. The gating network receives concatenated features from three modalities and generates sample-wise decisions on which expert network to activate during inference time. We set temperature of Gumbel-softmax as 1 and adopt straight-through training.</p><p>Results are summarized in <ref type="table" target="#tab_3">Table 2</ref>. We provide three DynMM networks trained with different ?. Compared with the best performing static network, DynMM-a can reduce computations by 46.5% with a slightly decreased accuracy (i.e., -0.47%). By allowing more computation, DynMM-b improves both inference efficiency (i.e., reduce MAdds by 17.8%) and prediction accuracy. Finally, DynMM-c further improves the accuracy by trading off some computation. It achieves best accuracy and smallest mean absolute error with reduced computation cost. These results demonstrate the great advantages of a dynamic network in multimodal learning. Since multimodal data naturally brings redundancy in characterizing information, we observe that many computations can be reduced without loss in accuracy.  Fusion-level DynMM is adopted for this task. We base our dynamic architecture design on a recently proposed (static) efficient architecture, ESANet <ref type="bibr" target="#b5">[6]</ref>. As illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>, we incorporate four fusion cells in the encoder design, where each fusion cell contains two operations. Operation 1 is an identity mapping of RGB features, i.e., O 1 = x 1 . For the second operation, we use channel attention fusion, where features from both modalities are first reweighted with a Squeeze and Excitation module <ref type="bibr" target="#b47">[48]</ref> and then added element-wisely. Two ResNet-50 <ref type="bibr" target="#b48">[49]</ref> are used as feature extraction models for RGB and depth modality; The decoder design is identical to <ref type="bibr" target="#b5">[6]</ref>. The gating network comprises a pipeline of 2 convolution blocks with kernel size 5?5 and stride size 2, a global average pooling and a linear layer. RGB and depth features after the first convolutional layer are concantenated together and passed to the convolutional gate. The gating network outputs a 4-dimensional vector per sample that determines which operation to select for each fusion cell. We experiment with two training strategies: (1) DynMM-a in <ref type="table" target="#tab_5">Table 3</ref> is trained with straight-through technique with Gumbel-softmax temperature ? = 1;</p><p>(2) We obtain DynMM-b in <ref type="table" target="#tab_5">Table 3</ref> by exponentially decaying ? from 1 to 0.0001 during 500 training epochs. <ref type="table" target="#tab_5">Table 3</ref> provides the detailed results of fusion-level DynMM. We report performance of DynMM after first-stage training in the second row; its great performance validates the design of our random gating function in the pre-training stage. This also lends support to our claim that there exist a lot of redundancy in multimodal networks. Utilizing the finding that depth modality plays an auxiliary role in this task, fusion-level DynMM effectively reduces computations of the depth encoder. DynMM-a reduces MAdds by 55.1% with only -0.4% mIoU drop. Furthermore, DynMM-b achieves a mIoU improvement of 0.7% and 21.1% reduction in MAdds at the same time, thus demonstrating the superiority of DynMM over static fusion methods.</p><p>Finally, we provide a comparison of the resulting DynMM-a and DynMM-b with SOTA semantic segmentation methods in <ref type="table" target="#tab_6">Table 4</ref>. For baseline methods, we list mIoU reported in their original papers and calculate MAdds with the Pytorch-OpCounter library <ref type="bibr" target="#b49">[50]</ref>. These results clearly show that our proposed method achieves the best balance    between performance and efficiency. The computation cost of DynMM is similar to a unimodal lightweight RefineNet, yet its performance can match methods that use ResNet-101 as the backbone and involve significantly larger MAdds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Multimodal data enables the model to learn from an enriched representation space, but brings significant redundancy at the same time. Motivated by this observation, we have proposed dynamic multimodal fusion that adaptively fuses inputs during inference. The proposed DynMM saves computation at the modality (fusion) level for "easy" samples and preserves its representation power for "hard" samples. Experimental results on three very different multimodal tasks demonstrate the efficacy and generalizability of DynMM.</p><p>More importantly, this work demonstrates the potential of dynamic multimodal fusion, which opens up a new research direction. Past years have witnessed great progress in static multimodal fusion architecture design. Considering the benefits brought by a dynamic architecture (i.e., reduced computation, improved representation power and flexibility), we believe that developing dynamic networks tailored for multimodal fusion is a topic worth further investigations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>An illustration of Fusion-level DynMM, where input data has two modalities, denoted by x 1 and x 2 . We design a fusion cell with a set of candidate operations {O i } and a gating network G(x) (upper figure). h represents output of the cell. The lower figure provides an example of a dynamic multimodal architecture with stacked fusion cells. We interlace static feature extraction blocks (colored with green and yellow) with dynamic fusion cells. The resulting architecture enjoys high inference efficiency and improved representation power.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of DynMM with varying resource regularization strength (?) on MM-IMDB. Left: comparison of DynMM with static unimodal (UM) and multimodal (MM) baselines. We train DynMM with different ? and report test F1 score and MAdds of the resulting model. DynMM offers a wide range of choices that balance computation and learning behavior well. Right: branch selection ratio in DynMM with respect to ?. We average the choices of DynMM on all test data to get the ratio for each expert network. With increasing ?, DynMM prioritizes efficiency over computation and tends to select the expert with small computations (i.e., the unimodal text network E 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5</head><label>5</label><figDesc>provides results of DynMM with varying resource regularization strength ?. From the left figure, we can see that DynMM achieves a good balance between inference efficiency and accuracy. Moreover, DynMM offers a wide range of choices that can be controlled by ?, showing great flexibility. The right figure shows the branch selection ratio of DynMM for different ?. When ? is small, DynMM focuses more on performance and chooses expert network 2 most of the time. As ? increases, more test samples are routed to the expert network 1 that requires fewer computations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Analysis of DynMM with varying resource regularization strength (?) on CMU-MOSEI. Left: comparison of DynMM with static unimodal (UM) and multimodal (MM) baselines. Right: branch selection ratio in DynMM with respect to ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Text Vision AudioUm I wish we wished that it would have been the guy.I just got finished watching an excellent movie called Mars needs moms</figDesc><table><row><cell>Multimodal</cell><cell>Multimodal</cell></row><row><cell>Fusion</cell><cell>Fusion</cell></row><row><cell>Network</cell><cell>Network</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on MM-IMDB Movie Genre Classification. Modalities I and T denote image and text, respectively. We report micro and macro F1 scores for the multi-label classification problem. The computation cost of each model is measured by MAdds with one image-text pair as the input. M denotes million. DynMM consists of two experts: a text network E 1 and a late fusion network E 2 . They are also listed as the static network baseline for comparison. Results of four DynMM architectures (trained with different ?) are reported. Best results are bolded.</figDesc><table><row><cell>Method</cell><cell cols="4">Modality Micro F1 (%) Macro F1 (%) MAdds (M)</cell></row><row><cell>Image Network</cell><cell>I</cell><cell>39.99</cell><cell>25.26</cell><cell>5.0</cell></row><row><cell>Text Network (E 1 )</cell><cell>T</cell><cell>59.16</cell><cell>47.21</cell><cell>0.7</cell></row><row><cell>Early Fusion [44]</cell><cell></cell><cell>58.45</cell><cell>49.20</cell><cell>5.1</cell></row><row><cell>Late Fusion [44] (E 2 ) LRTF [16]</cell><cell>I+T</cell><cell>59.55 59.18</cell><cell>50.94 49.26</cell><cell>10.3 10.3</cell></row><row><cell>MI-Matrix [45]</cell><cell></cell><cell>58.45</cell><cell>48.36</cell><cell>10.3</cell></row><row><cell>DynMM-a</cell><cell></cell><cell>59.57</cell><cell>48.84</cell><cell>1.6</cell></row><row><cell>DynMM-b DynMM-c</cell><cell>I+T</cell><cell>59.59 59.72</cell><cell>50.42 51.20</cell><cell>7.8 9.8</cell></row><row><cell>DynMM-d</cell><cell></cell><cell>60.35</cell><cell>51.60</cell><cell>12.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on CMU-MOSEI Sentiment Analysis. Modalities V, A, T represent video, audio and text, respectively. Acc 2 denotes binary accuracy (i.e., positive/negative sentiments) and MAE represents mean absolute error.</figDesc><table><row><cell>MAdds are</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>4.4 Semantic SegmentationNYU Depth V2<ref type="bibr" target="#b42">[43]</ref> is an indoor dataset for semantic segmentation. It contains 1,449 RGB-D images with 40-class labels; 795 images are used for training and 654 images are for testing. The two modalities are RGB and depth images.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on RGB-D semantic segmentation. mIoU denotes mean Intersection-over-Union. MAdds are calculated for input size of 3 ? 480 ? 640. G stands for Giga.</figDesc><table><row><cell>Method</cell><cell>mIoU (%)</cell><cell>Depth Enc MAdds (G)</cell><cell>Reduction Ratio</cell></row><row><cell cols="2">ESANet [6] (baseline) 50.3</cell><cell>24.7</cell><cell>-</cell></row><row><cell>DynMM (Stage I)</cell><cell>48.5</cell><cell>11.7</cell><cell>52.6%</cell></row><row><cell>DynMM-a (Stage II)</cell><cell>49.9</cell><cell>11.1</cell><cell>55.1%</cell></row><row><cell>DynMM-b (Stage II)</cell><cell>51.0</cell><cell>19.5</cell><cell>21.1%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with SOTA methods for RGB-D semantic segmentation on NYU Depth V2 test data.</figDesc><table><row><cell>Method</cell><cell>Modality</cell><cell>Backbone</cell><cell cols="2">mIoU (%) MAdds (G)</cell></row><row><cell>LW-RefineNet [46] LW-RefineNet [46]</cell><cell>RGB</cell><cell>ResNet-50 ResNet-101</cell><cell>41.7 43.6</cell><cell>38.5 61.2</cell></row><row><cell>ACNet [47]</cell><cell></cell><cell>ResNet-50</cell><cell>48.3</cell><cell>126.2</cell></row><row><cell>SA-Gate [8] CEN [7]</cell><cell>RGB+D</cell><cell>ResNet-50 ResNet-101</cell><cell>50.4 51.1</cell><cell>147.6 618.3</cell></row><row><cell>ESANet [6]</cell><cell></cell><cell>ResNet-50</cell><cell>50.5</cell><cell>56.9</cell></row><row><cell>DynMM-a DynMM-b</cell><cell>RGB+D</cell><cell>ResNet-50 ResNet-50</cell><cell>49.9 51.0</cell><cell>43.4 52.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis: A survey and comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramandeep</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Kautish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Service Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="38" to="58" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Management, Engineering, and Technology</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Utd-mhad: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roozbeh</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep multimodal feature analysis for action recognition in rgb+ d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1045" to="1058" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient rgb-d semantic segmentation for indoor scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Seichter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Lewandowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Wengefeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst-Michael</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13525" to="13531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep multimodal fusion by channel exchanging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4835" to="4845" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bidirectional cross-modality feature propagation with separation-and-aggregation gate for rgb-d semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwan-Yee</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="561" to="577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention-based multimodal fusion for video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiori</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng-Yok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bret</forename><surname>Harsham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><forename type="middle">K</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhiko</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4193" to="4202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centralnet: a multilayer approach for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Lechervy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention bottlenecks for multimodal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Temporal multimodal fusion for video emotion classification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimodal Interaction</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Learn to combine modalities in multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Natarajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11730</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mmtm: Multimodal transfer module for cnn fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Hamid Reza Vaezi Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuhito</forename><surname>Iuzzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13289" to="13299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient low-rank multimodal fusion with modality-specific factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bharadhwaj Lakshminarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00064</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mfas: Multimodal fusion architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Manuel</forename><surname>P?rez-R?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Vielzeuf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Pateux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moez</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6966" to="6975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep multimodal neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3743" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep rgb-d saliency detection with depthsensitive attention and automatic multi-modal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1407" to="1417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Detect, reject, correct: Crossmodal compensation of corrupted sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bohg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="909" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dynamic neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glance and focus: a dynamic approach to reducing spatial redundancy in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kangchen</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2432" to="2444" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hydranets: Specialized dynamic architectures for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravi Teja Mullapudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kayvon</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fatahalian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8080" to="8089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Skipnet: Learning dynamic routing in convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi-Yi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynamic routing networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3588" to="3597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning dynamic routing for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8553" to="8562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive weighting multi-field-of-view cnn for semantic segmentation in pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroki</forename><surname>Tokunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Teramoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Yoshizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Bise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12597" to="12606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azalia</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adaptive feeding: Achieving fast and accurate detections by adaptively combining object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bin-Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3505" to="3513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adaptive convolution for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3205" to="3217" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Condconv: Conditionally parameterized convolutions for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Luc</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06297</idno>
		<title level="m">Conditional computation in neural networks for faster models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Condensenet: An efficient densenet using learned group convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2752" to="2761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Branchynet: Fast inference via early exiting from deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surat</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Tsung</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2464" to="2469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Adaptive neural networks for efficient inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Listen to look: Action recognition by previewing audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10457" to="10467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mixture of experts: a literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Gated multimodal units for information fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Arevalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><surname>Montes-Y G?mez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">A</forename><surname>Gonz?lez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01992</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Pushmeet Kohli Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Paul Pu Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zetian</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07502</idno>
		<title level="m">Multiscale benchmarks for multimodal representation learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Multiplicative interactions and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Light-weight refinenet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03272</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Acnet: Attention based network to exploit complementary features for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kailun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1440" to="1444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Thop: Pytorch-opcounter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://github.com/Lyken17/pytorch-OpCounter" />
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
