<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">WildQA: In-the-Wild Video Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Castro</surname></persName>
							<email>sacastro@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naihao</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingxuan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Burzo</surname></persName>
							<email>mburzo@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
							<email>mihalcea@umich.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Michigan</orgName>
								<address>
									<settlement>Ann Arbor</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">WildQA: In-the-Wild Video Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Q: What is the man using as tools? A: A saw and handaxe E: 00:00 00:37 00:51 Figure 1: An example from our WildQA dataset, showing a question (Q), an answer (A), and evidence (E) that supports the answer. The corresponding part of the videos is provided as evidence for the question.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing video understanding datasets mostly focus on human interactions, with little attention being paid to the "in the wild" settings, where the videos are recorded outdoors. We propose WILDQA, a video understanding dataset of videos recorded in outside settings. In addition to video question answering (Video QA), we also introduce the new task of identifying visual support for a given question and answer (Video Evidence Selection). Through evaluations using a wide range of baseline models, we show that WILDQA poses new challenges to the vision and language research communities. The dataset is available at https://lit.eecs.umich. edu/wildqa/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: An example from our WildQA dataset, showing a question (Q), an answer (A), and evidence (E) that supports the answer. The corresponding part of the videos is provided as evidence for the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Existing video understanding datasets mostly focus on human interactions, with little attention being paid to the "in the wild" settings, where the videos are recorded outdoors. We propose WILDQA, a video understanding dataset of videos recorded in outside settings. In addition to video question answering (Video QA), we also introduce the new task of identifying visual support for a given question and answer (Video Evidence Selection). Through evaluations using a wide range of baseline models, we show that WILDQA poses new challenges to the vision and language research communities. The dataset is available at https://lit.eecs.umich. edu/wildqa/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Video understanding plays an important role in the development of intelligent AI systems, as it enables the effective processing of different modalities of information <ref type="bibr" target="#b6">(Li et al., 2021a)</ref>. Various tasks have been proposed to examine the ability of models' to understand videos, including video question answering (Video QA), video captioning, and fill-in-*: Equal contribution the-blank tasks <ref type="bibr" target="#b24">Tran et al., 2016;</ref><ref type="bibr">Castro et al., 2022)</ref>. Recent years have witnessed significant progress in video understanding, including new benchmarks <ref type="bibr" target="#b23">(Tapaswi et al., 2016;</ref><ref type="bibr">Grauman et al., 2021)</ref> as well as advanced sophisticated benchmarksmodels <ref type="bibr">(Jin et al., 2019;</ref><ref type="bibr" target="#b17">Radford et al., 2021)</ref>.</p><p>There are however several drawbacks associated with existing video understanding research. First, existing video understanding benchmarks focus on common human activities as typically appearing in cooking videos <ref type="bibr" target="#b42">(Zhu et al., 2017)</ref> or in movies <ref type="bibr" target="#b23">(Tapaswi et al., 2016)</ref>, leading to a limited set of video domains. Second, most video understanding benchmarks adopt a multiple-choice format, where models select an answer from a set of candidates <ref type="bibr">(Jang et al., 2017;</ref><ref type="bibr">Castro et al., 2020)</ref>. Models trained under such a setting cannot be used in real-life applications because candidate answers are not provided <ref type="bibr">(Castro et al., 2022)</ref>. Third, videos included in existing benchmarks are typically short <ref type="bibr">(Kim et al., 2016)</ref>, and the performance of models on longer videos is not well studied.</p><p>We address these challenges in our dataset construction process. First, we propose the WILDQA dataset in which we collect "in the wild" videos that are recorded in the outside world, going beyond  <ref type="bibr" target="#b23">(Tapaswi et al., 2016)</ref>, TVQA <ref type="bibr" target="#b3">(Lei et al., 2018)</ref>, and our WildQA dataset. The previous datasets mostly focus on human interactions in a multiple-choice setting, while ours focus on scenes recorded in the outside world in an open-ended setting. We only list a single answer here for illustration purposes. daily human activities. <ref type="figure" target="#fig_0">Figure 2</ref> shows the difference between the WILDQA dataset and previous question answering datasets. Second, we adopt the challenging answer generation approach, aiming to build a system that can answer questions with an open-ended answer, rather than selecting from a predefined set of candidate answers. Third, the average video length in our dataset is one minute, longer than the video clips in most of the existing datasets in <ref type="table" target="#tab_5">Table 3</ref>, which presents a novel challenge for video understanding algorithms.</p><p>Using the WILDQA dataset, we address two main tasks. First, we address the task of video question answering (Video QA), aiming to generate open-ended answers. Second, we introduce the task of retrieving visual support for a given question and answer (Video Evidence Selection). Finding the relevant frames in a video for a given question-answer pair can help a system in its reasoning process, and is in line with ongoing efforts to build interpretable models <ref type="bibr">(Jacovi and Goldberg, 2020)</ref>. For each of these two tasks, we evaluate several baseline models, including multi-task models that combine the two tasks together. <ref type="figure">Figure 1</ref> shows an example from our dataset, including an example of a question, answer, and supporting video evidence.</p><p>To summarize, the main contributions of this paper are:</p><p>1. We propose WILDQA, a multimodal video understanding dataset where video scenes are recorded in the outside world. 2. We propose two tasks for WILDQA: Video QA and Video Evidence Selection, aiming to build more interpretable systems. 3. We test several baseline models; experimental results show that our dataset poses new challenges to the vision and language research communities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multimodal Question Answering. Two popular and representative tasks are Visual Question Answering (Visual QA) on images, and Video Question Answering (Video QA) on videos. Visual QA has attracted attention for a long time <ref type="bibr" target="#b12">(Malinowski and Fritz, 2014;</ref><ref type="bibr" target="#b21">Ren et al., 2015;</ref>. Recently, much progress has been made in Video QA. Researchers proposed various datasets such as TVQA that contain videos from movies or TV series <ref type="bibr" target="#b23">(Tapaswi et al., 2016;</ref><ref type="bibr" target="#b3">Lei et al., 2018</ref><ref type="bibr" target="#b4">Lei et al., , 2020a</ref> or videos from the Internet spanning from YouTube videos to Tumblr GIFs <ref type="bibr" target="#b36">(Zeng et al., 2017;</ref><ref type="bibr" target="#b29">Ye et al., 2017;</ref><ref type="bibr">Jang et al., 2017;</ref><ref type="bibr" target="#b32">Yu et al., 2019b)</ref>. Other datasets such as MSVD-QA  contain videos from the existing corpus <ref type="bibr">(Chen and Dolan, 2011)</ref> or cartoon videos <ref type="bibr">(Kim et al., 2016)</ref>. Recent Video QA datasets have stronger focuses such as temporal relations <ref type="bibr" target="#b14">(Mun et al., 2017)</ref>, multi-step and non-factoid answers <ref type="bibr">(Colas et al., 2020)</ref>, natural interactions <ref type="bibr" target="#b33">(Zadeh et al., 2019)</ref>, characters in the video <ref type="bibr" target="#b35">(Choi et al., 2021)</ref>, question answering in real life <ref type="bibr">(Castro et al., 2020)</ref>, incorporating external knowledge <ref type="bibr">(Garcia et al., 2020)</ref>, and videos recorded from the egocentric view <ref type="bibr">(Fan, 2019;</ref><ref type="bibr">Grauman et al., 2021)</ref>. To the best of our knowledge, we are the first to collect videos from the outside world.</p><p>Researchers have also developed various methods to handle the Video QA task, including joint reasoning of the spatial and temporal structure of a video <ref type="bibr" target="#b7">Gao et al., 2019;</ref><ref type="bibr" target="#b30">Huang et al., 2020;</ref><ref type="bibr" target="#b22">Jiang et al., 2020)</ref>, integrating memory to keep track of past and future frames <ref type="bibr">(Kim et al., 2017;</ref><ref type="bibr">Gao et al., 2018a;</ref><ref type="bibr" target="#b40">Zhao et al., 2018;</ref><ref type="bibr">Fan et al., 2019;</ref><ref type="bibr" target="#b30">Yu et al., 2020)</ref>, various attention mechanisms <ref type="bibr" target="#b42">(Zhu et al., 2017;</ref><ref type="bibr" target="#b38">Zhang et al., 2019;</ref><ref type="bibr" target="#b7">Li et al., 2019;</ref><ref type="bibr" target="#b31">Yu et al., 2019a;</ref><ref type="bibr">Kim et al., 2018;</ref><ref type="bibr">Jin et al., 2019)</ref>, and others. Recently, pre-trained models have proved to be useful in various visual and language tasks <ref type="bibr" target="#b17">(Radford et al., 2021;</ref><ref type="bibr">Chen et al., 2020;</ref><ref type="bibr" target="#b35">Zellers et al., 2021)</ref>. However, the pretrained visual and language models are typically encoder-only and cannot generate an answer in natural language on their own. Thus, such pre-trained encoder-only models do not fit into the open-end video question answering setting in our task.</p><p>Additionally, previous work has also investigated various reasoning tasks in a multimodal setting <ref type="bibr">(Gao et al., 2016;</ref><ref type="bibr" target="#b28">Yang et al., 2018;</ref><ref type="bibr">Gao et al., 2018b;</ref><ref type="bibr" target="#b34">Zellers et al., 2019)</ref>. Although it is not our focus, some questions in our dataset require a certain level of reasoning ability. Moreover, since our dataset is created by domain experts, there is domain knowledge involved in the questions as well.</p><p>Moment Retrieval. Moment Retrieval is the task of retrieving a short moment from a large video corpus given a natural language query <ref type="bibr">(Escorcia et al., 2019;</ref><ref type="bibr" target="#b5">Lei et al., 2020b)</ref>. Researchers have proposed or adapted various datasets for this task <ref type="bibr">(Krishna et al., 2017;</ref><ref type="bibr">Hendricks et al., 2017;</ref><ref type="bibr">Gao et al., 2017;</ref><ref type="bibr" target="#b5">Lei et al., 2020b)</ref>. The task of retrieving relevant parts in the video given the question (Video Evidence Selection) in our proposed dataset is akin to Moment Retrieval. However, moment retrieval focuses on retrieving the part of videos that the question describes, while Video Evidence Selection is to find parts of videos that can support the answer to the questions as shown in <ref type="figure">Figure 1</ref>. Prior work such as Tutorial-VQA (Colas et al., 2020) also adopt the setting of providing parts of the videos as answers to the question, but they did not include any text answers in their dataset.</p><p>Few-shot Learning. Recently, there is a trend to evaluate neural models in a few-shot learning setting <ref type="bibr">(Huang et al., 2018;</ref><ref type="bibr" target="#b13">Mukherjee and Awadallah, 2020;</ref><ref type="bibr" target="#b22">Sun et al., 2020;</ref><ref type="bibr" target="#b8">Li et al., 2021b;</ref><ref type="bibr">Lee et al., 2021;</ref><ref type="bibr" target="#b16">Pfeiffer et al., 2022)</ref>, where the model is tuned with a small portion of the data and tested against the rest. We adopt the few-shot learning setting for our dataset for both Video QA and Video Evidence Selection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">WildQA Dataset</head><p>Video Selection and Processing. Following <ref type="bibr" target="#b33">Zadeh et al. (2019);</ref><ref type="bibr">Castro et al. (2020)</ref>, we start by collecting videos from YouTube. First, we identify five domains that primarily consist of outdoor scenes and are representative for the outside world, namely, Agriculture, Geography, Human Survival, Natural Disasters, and Military. We then manually collected videos from relevant YouTube channels for each domain.</p><p>Because the raw videos can be as long as an hour, we split the raw videos into short clips using PySceneDetect, 1 and concatenate these short clips so that the output video is approximately one minute. We use the output videos for the annotation process described below. More details for the video selection and video processing steps are discussed in Appendix A.1.</p><p>Question, Answer, and Evidence Annotation. There are two phases in our annotation process, as shown in <ref type="figure">Figure 3</ref>. In Phase 1, annotators watch the video clips and come up with a hypothetical motivation. They ask one or more questions and provide an answer to each of the questions they ask. Annotators are also instructed to provide all the relevant parts in videos as pieces of evidence to support the answer to their question. After this step in the data collection, three of the authors of this paper manually review all the question-answer pairs for quality purposes. Next, in Phase 2, we collect more answers and evidences for each question from Phase 1. Over the entire annotation process, annotators spent a total of 556.81 annotation hours, split into 77.05 hours in Phase 1 and 479.76 in Phase 2. Appendices A.2, A.3, and A.5 present the annotation instructions, annotation interfaces, and reviewing process for question-answer pairs, respectively.</p><p>Because we want to collect questions that domain experts are interested, as opposed to arbitrary questions, domain experts carry out the Phase 1 annotations. To demonstrate the quality difference of questions collected from domain experts versus non-experts, we conduct a pilot study. Appendices A.4 and A.6 discuss the pilot study and the annotators' expertise, respectively. <ref type="table" target="#tab_1">. Tables 1 and 2 present</ref>     of the five domains, along with other relevant statistics. <ref type="figure" target="#fig_1">Figure 4</ref> shows the distribution of question types. Appendix A.7 discusses more statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Statistics</head><p>Dataset Comparison. <ref type="table" target="#tab_5">Table 3</ref> shows the comparison between WILDQA and other existing datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Video Question Answering</head><p>Following <ref type="bibr" target="#b27">Xue et al. (2017)</ref>, we adopt free-form open-ended video question answering for our video question answering (Video QA) task. Given a question q and a video v, the task is to generate an answer a in natural language. We adopt a few-shot learning setting on our dataset, where models are fine-tuned on questionanswer pairs corresponding to 30% of the videos for each domain. The tuned models are tested on data for the remaining 70% videos. The reason is that the time to annotate 30% of the data is around 23 hours, during which there are around 50 data points annotated for each domain, which is acceptable. We hypothesize that it is realistic to have such a setting because the potential endusers could spend around a day or two collecting data, and we can then quickly tune a model using it. Moreover, no repeated videos appear in different splits, following <ref type="bibr" target="#b3">Lei et al. (2018)</ref>. We end up having 264 question-answer pairs for 108 videos in our dev set and 652 pairs for 261 videos in the test set. We adopt BLEU <ref type="bibr" target="#b15">(Papineni et al., 2002)</ref> and ROUGE <ref type="bibr" target="#b9">(Lin, 2004)</ref> as the metrics to measure the quality of the generated answer. We run each MovieQA <ref type="bibr" target="#b23">(Tapaswi et al., 2016)</ref> Movies ? 6.7K 6.4K 203 Manual MC VideoQA (FiB) <ref type="bibr" target="#b42">(Zhu et al., 2017)</ref> Cooking, movies, web 109K 390K 33 Automatic MC MSRVTT-QA  General life videos 10K 243K 15 Automatic OE MovieFIB <ref type="bibr" target="#b11">(Maharaj et al., 2017)</ref> Movies 128K 348K 5 Automatic OE TVQA <ref type="bibr" target="#b3">(Lei et al., 2018)</ref> TV shows ? 21.8K 152K 76 Manual MC ActivityNet-QA <ref type="bibr" target="#b32">(Yu et al., 2019b)</ref> Human activity 5.8K 58K 180 Manual OE TVQA+ <ref type="bibr" target="#b4">(Lei et al., 2020a)</ref> TV shows ? 4.2K 29.4K 60 Manual MC, ES KnowIT VQA <ref type="bibr">(Garcia et al., 2020)</ref> TV shows 12K 24K 20 Manual MC LifeQA <ref type="bibr">(Castro et al., 2020)</ref> Daily life 275 2.3K 74 Manual MC TutorialVQA <ref type="bibr">(Colas et al., 2020)</ref> Instructions ? 76 6.2K -Manual ES NExT-QA <ref type="bibr" target="#b25">(Xiao et al., 2021)</ref> Daily life 5.4K 52K 44 Manual MC, OE FIBER <ref type="bibr">(Castro et al., 2022)</ref> Human actions 28K <ref type="formula">2K</ref>   Q: What type of weather is happening? A: Flooding and rain. The weather is rain and flood. Q: Where is the road at? A: It is in a tundra environment The road zig-zags across the landscape. The road winds through a mountainous landscape. The road is in an elevated area. model 3 times and report the scores of mean ? standard deviation in <ref type="table" target="#tab_6">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>Human Baselines. We report the average BLEU and ROUGE scores by leaving one annotator out in <ref type="table" target="#tab_6">Table 4</ref> (Human).</p><p>Text-only Models. We implement several baselines that only use the question-answer pairs in the dev set. Random randomly chooses answers from the dev set. Common always predicts the most common answer in the dev set; Closest employs embedding produced by a pretrained roberta-base model . In the inference, Closest retrieves the answers for the dev set question whose embedding has the highest cosine similarity to the test question. We also finetune T5 <ref type="bibr" target="#b18">(Raffel et al., 2020)</ref> using question-answer pairs from the dev set (T5 T).</p><p>Text + Visual Models. Following Castro et al.</p><p>(2022), we concatenate the text features with the visual features and input the concatenated features to the T5 model (T5 T+V). We extract I3D (Carreira and Zisserman, 2017) video features and take one feature per second.</p><p>Multi-task Learning. Multi-task learning has proved to be successful in various domains <ref type="bibr">(Collobert and Weston, 2008;</ref><ref type="bibr">Deng et al., 2013;</ref><ref type="bibr">Girshick, 2015)</ref>. Following <ref type="bibr" target="#b2">Caruana (1993)</ref>, we train Multi T+V,SE which combines T5 T+V and T5 SE (the Video Evidence Selection model described in Section 5) with a shared T5 encoder between the tasks of Video Question Answering and Video  Evidence Selection. We also train Multi T+V,IO which combines T5 T+V and T5 IO (another Video Evidence Selection model described in Section 5) in a similar way. The loss function during the fine-tuning is:</p><formula xml:id="formula_0">L = ?L 1 + ?L 2 (1)</formula><p>where L 1 , L 2 are the losses for Video Question Answering and Video Evidence Selection, respectively; ?, ? are the weights for the two tasks. The selection process behind the values of ? and ? are presented in Appendix C. <ref type="table" target="#tab_6">Table 4</ref> reports F1 scores of ROUGE-1 (R1), ROUGE-2 (R2), and ROUGE-L (RL) for our baseline models. For comparison, we also test the outof-box T5 model on our test split under the zeroshot setting (T5 Text 0-shot in <ref type="table" target="#tab_8">Table 5</ref>). T5-based models significantly outperform the random baselines as well as the out-of-box T5 model, which suggests that the T5-based models acquire certain levels of question-answering ability in the tuning stage. However, adding visual features does not improve the model's performance. This might be due to the challenges of attending to the visual features at the corresponding parts in the video, because both models under multi-task learning outperform the text-only baseline, suggesting that attending to the correct part of the video helps the answer generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>All baseline models underperform human baselines on ROUGE scores, especially on ROUGE-1 and ROUGE-L scores, suggesting that there is room for improvement. However, the ROUGE-2 score for human annotators is low because although human annotators tend to use the same word to describe the object that appears in the video, there are large variations in terms of expressing the ideas of their answers. More discussions on the diversity of the answers are in Appendix A.7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Video Evidence Selection</head><p>Similar to <ref type="bibr">Colas et al. (2020)</ref>, given a video v and a question q, the video evidence selection task consists of predicting {(s 1 , e 1 ), (s 2 , e 2 ), . . .}, where (s i , e i ) represents the time for start s and end e of a singles span within the video v. We also adopt the few-shot learning setting as described in Section 4 for the task of Video Evidence Selection. Similar to DeYoung et al. <ref type="formula">(2020)</ref>, we design an Intersection-Over-Union (IOU) metric borrowed from <ref type="bibr">Everingham et al. (2010)</ref>. We define IOU as follows: given two time spans in the video, IOU is defined as the length of their intersection divided by the length of their union. The prediction is counted as a match if it overlaps with any of the ground truth spans by more than the threshold (0.5, following DeYoung et al., 2020). We use these partial matches to calculate an F1 score (IOU-F1 scores). As described in Section 4, we run each model three times and report the scores of mean ? standard deviation in <ref type="table" target="#tab_8">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>As described in Section 4, we compute the average IOU-F1 score on the annotations from one annotator against the remaining annotators; we denote this metric as Human. The Random baseline consists of randomly choosing the start and end of a part within the original video as evidence. Similar to the structure Devlin et al. (2019) experiment on SQuAD <ref type="bibr" target="#b19">(Rajpurkar et al., 2016)</ref>, we build T5 SE; here, we feed the concatenated question embeddings and I3D visual features to the T5 encoder, and the T5 encoder outputs a sequence of the encoded states. We treat the subsequence corresponding to the visual features as the encoded hidden sequence T m ? R H for the video frames (H denotes the dimension of the hidden sequence). We then multiply the sequence with two vectors S, E ? R H . The T i and T j that maximize the likelihood are predicted as the start and the end of the evidence, respectively. During the training, we maximize their joint probability:  being the start and j the end of the evidence, respectively. Inspired by the Inside-Outside-Beginning ("IOB") tagging scheme <ref type="bibr" target="#b20">(Ramshaw and Marcus, 1995)</ref>, we also formulate the evidence finding as a task of tagging whether a video frame is inside ("I") the evidence, or outside ("O") the evidence. We then build T5 IO by feeding the concatenated features to a T5 encoder. Similar to T5 Start End, we have an encoded sequence of T m ? R H corresponding to the video frames. We then multiply the sequence with a vector L ? R H and apply a sigmoid function to the multiplication result. The model predicts the frame as "I" if the value at the corresponding position is greater than or equal to 0.5, otherwise it predicts "O". We test Multi T+V,IO and Multi T+V,SE described in Section 4 on Video Evidence Selection as well. <ref type="table" target="#tab_8">Table 5</ref> shows the performance of the baseline models on the Video Evidence Selection task. All the baseline models perform significantly worse than the human annotators, and sometimes worse than the random baseline. This is understandable because selecting evidence from a long video can be difficult. Additionally, multi-task learning makes the model's performance worse. However, this could be due to the fact that the Video Evidence Selection itself is a hard task, and all the baseline models struggle with such a task. Although multi-task learning does not help Video Evidence Selection, as mentioned in Section 4, training with Video Evidence Selection does help Video QA. Thus, Video Evidence Selection is still an important task to improve a model's ability to answer questions. We include more ablation studies in Appendix D.1.</p><formula xml:id="formula_1">P i P j = e S?T i m<label>e</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Analysis and Discussion</head><p>Model Performance v.s. Question Types. <ref type="table" target="#tab_10">Table 6</ref> shows Multi T+V,SE 's performance on different question types for Video QA and Video Evi-  Interestingly, even if the answers have similar lengths, the model struggles on Motion questions (with a relatively low IOU-F1 score). A possible reason could be that this type of questions provide a very abstract description of the action, which makes the model hard to attend to the relevant part of the video. For instance, an example of a Motion question is "Are there any structure or natural features being affected?". To attend to the corresponding period in the video, the model needs to understand the word "affected" and the objects that are actually affected, which can be very difficult. The model also struggles to attend to the correct places in the video for the Spatial type of question. This might be because there is more than one entity in Spatial type of questions, and the model needs to locate all the objects appearing in various parts of the video, which is similarly complex. For instance, for the question "What effects did the weather have?", the model needs to attend to Model name</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R1</head><p>R2 RL  <ref type="table" target="#tab_13">Table 7</ref>: ROUGE scores for the task of Video Question Answering for few-shot learning setting (the standard setting in our WildQA dataset introduced in Section 4) and zero-shot learning setting ("0-shot" in the superscript). Subscript "TVQA" means pre-training on the TVQA <ref type="bibr" target="#b3">(Lei et al., 2018)</ref> dataset; subscript "TVQA,ours" means first pre-training the model on TVQA, then tuning the model on our WildQA dataset; subscript "ours" means tuning the model directly on our WildQA dataset.</p><formula xml:id="formula_2">T5 T 0-shot 0.8 ? 0.0 0.0 ? 0.0 0.8 ? 0.0 T5 T 0-shot TVQA 9.1 ? 0.0 1.2 ? 0.0 8.8 ? 0.0 T5 TTVQA,</formula><p>"debris in the air", "truck turnover" and "destruction of buildings". For Location type of questions such as "What sorts of terrain is the vegetation present in?", it might be difficult to attend to all the terrains of "forest", "plateaus", "mountainous", "valleys", and "arboreal" and to include them in the answer.</p><p>Domain Adaptation. Furthermore, we tune the Multi T+V,SE model on the dev set data from a single domain, and test it against data from other domains. <ref type="figure" target="#fig_4">Figures 6 and 7</ref> show the model's performance in different tuning and testing domains. Interestingly, the diagonal cells do not always have the darkest color, which indicates that interrelations exist across domains. For instance, the model tuned on Geography performs relatively better for Video QA on Human Survival and Agriculture rather than itself. This suggests that the questions and videos from Geography, Agriculture, and Human Survival exhibit some similarity so that the model tuned on one domain can answer questions from the other domains relatively well. But answering questions from Geography can introduce the domain knowledge, an example of the answer is "Mountainous, temperate forest.", where "temperate forest" is one of the terminologies specific to Geography domain. Training on these terminologies might confuse the model and hurt the performance. Thus, future research might be needed to study how to better incorporate domain knowledge into multimodal question answering.</p><p>As for Video Evidence Selection, the patterns N a tu r a l D is a s te r H u m a n S u r v iv a l generally resemble the pattern in <ref type="figure">Figure 6</ref>, which means that in general, the model answers a question better if it can attend to the relevant part in the video. However, when tuned on Human Survival and tested on Natural Disaster the model performs relatively well on Video QA (with a 28.7 ROUGE-1 score) but less well on Video Evidence Selection (with a 0.7 IOU-F1 score). This might indicate that the model picks up some common patterns in the text rather than reasoning about the video and the question in an expected manner.</p><p>Pre-training on Other Datasets. We also pretrain the T5 T and T5 T+V using TVQA <ref type="bibr" target="#b3">(Lei et al., 2018)</ref>, a large-scale multimodal question answering dataset with videos from TV series. We report the zero-shot learning performances as well as the few-shot learning performances for T5 T and T5 T+V in  training the model on the TVQA dataset reduces the variance of model performance, which suggests that training the model with more data helps the model perform consistently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduced a new and challenging benchmark, WILDQA, to promote domain diversity for video understanding. Specifically, we focused on five domains that involve long videos recorded in the outside world, which can be useful for applications in these domains. Instead of the traditional multiple-choice setting for Video Question Answering, we proposed to generate open-ended answers. We believe open-end answer generation can help construct systems that can answer end users' questions in a more natural way. To help the model attend to the relevant parts in the videos, we also proposed the task of Video Evidence Selection. Through experiments, we showed the feasibility of these tasks, and also showed that jointly training for both Video Question Answering and Video Evidence Selection can improve the models' performance. In addition, we found it is easier to understand models' behavior by knowing which part of the video the model attends to when answering a question. We believe that this is an important step towards a trustworthy, explainable multimodal system. The dataset is available at https: //lit.eecs.umich.edu/wildqa/. ions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of ARC or any other related entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Consideration</head><p>The videos we use are all publicly available on YouTube. The dataset includes a variety of domains, including videos in the Military domain, but we are ensuring that all the videos are only used for asking questions specific to the video content. Moreover, in our manual examination, we make sure that the question-answer pairs collected from our annotators are appropriate without any use of offensive language. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Annotation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Video Selection and Processing</head><p>Video Selection. For the video selection part, as mentioned in Section 3, first, we identify 5 domains, Agriculture, Geography, Human Survival, Natural Disasters, and Military, to collect videos recorded in the outside world. We then identify eight (8) YouTube channels and crawl videos from those channels. During crawling, we manually substitute irrelevant videos such as advertisements with videos that contain scenes mostly recorded in the outside world from the same channel.</p><p>Video Processing. As mentioned in Section 3, we clip the raw videos into short clips by PySceneDetect because the raw videos can be as long as an hour. We then concatenate these short clips so that the output video will be around 1 minute. The output videos are used for the following annotation process. We want to include longer videos because the videos recorded in the outside world usually contain less information compared to the videos about human interactions. Besides, if the concatenated video is at the end of the original video, it is allowed to be shorter than 1 minute. We select the concatenated videos that only contain scenes recorded in the outside world. If none of the concatenated videos satisfies, we manually clip the original videos to get an output video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Annotation Instructions</head><p>As mentioned in Section 3, we have 2 phases in our annotation process as shown in <ref type="figure">Figure 3</ref>. In Phase 1, annotators come up with a hypothetical motivation, ask questions, and provide the corresponding answers with relevant parts of the video as evidence. Phase 2 is to collect answers and evidence for questions we collect in Phase 1. The following are the instructions for these two phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions for Phase 1</head><p>We need help for this Video QA task based on video content (including the audio).</p><p>In this task, we suppose you can hypothetically send a robot to a place that you want, for many hours, so as to collect information that you need. In this hypothetical scenario, you have an objective that you want the robot to learn about. This robot can chart territory and is able to answer questions based on recorded videos. Therefore, after it comes back, you can ask questions to help you satisfy your objective, then this robot will provide you with answers, as well as video evidence clips to support the answers. In this task, to simplify, the provided videos represent places where you could potentially have sent the robot and are much shorter (a few minutes). Given a recorded video, please help us provide one hypothetical objective that makes sense with it, along with questions, answers, and evidence. Specifically, you should pretend to be both the information-seeker and the robot, which means that as the robot, you could watch the recorded video, and you should provide answers and video evidence clips; as the information-seeker, you have an objective, not watch the whole video (because of practical reasons), and you can only ask questions and receive answers and video evidence clips as feedback.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Basic Instructions</head><p>? You will need to propose a hypothetical objective (or topic, intention, motivation), to motivate the questions, that makes sense for the given video. ? You will need to provide as many questions as you need (to satisfy your objective) with regard to the content in the videos and that seek to understand more about the proposed objective. ? You will first watch the video, but when you are providing the objective and questions, please pretend you haven't seen it before. ? You will need to provide at least one question for each video. The more the better. ? You will need to identify the source of your question (whether it is based on the visual scene or the audio) and classify your question accordingly. ? You will need to provide the correct answer to the question you asked, as supported by the content in the video. ? You will need to provide video evidence (video clip) to support your question and answer. ? If one video doesn't make sense at all, or there's no possible objective for this video that makes sense, please comment at the bottom of this annotation page (and fill in the mandatory fields for the corresponding video with placeholder values).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">How To Propose Hypothetical Objective</head><p>? For each video, you need to come up with a hypothetical objective (or intention, motivation, topic) that makes sense for this video, and briefly explain it. ? Your questions should all relate to this objective. ? Example 1:</p><p>-Objective: I want to learn about the water in the territory. -Question 1: How big is the lake? -Question 2: Are there any boats in the lake? -Question 3: Where is the river? -...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>? Example 2:</head><p>-Objective: people/life movement -Question 1: Is there any sign that wildlife has passed this area? -Question 2: How much traffic is there on the road? -...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">How To Ask Your Question</head><p>? Your question should relate to your proposed objective. ? For each video, after you finish one question, you could click the Add one more question for this video button to continue to provide another question for this video. On the contrary, if you want to delete one question, you could click the Delete this Question button. ? Ask one question at a time.</p><p>-E.g., "Are there any people? What are they doing?" is not appropriate. ? When you provide multiple questions for the same video, make sure these questions are independently asked.</p><p>-E.g., "What is growing on pine trees?" and "What is their color?" are not independent. ? The answer should be derived from the video (visual or audio).</p><p>-E.g., "Why do they run every morning?" is not a good question.</p><p>? Ask from the 3rd person point of view.</p><p>-E.g., "What do we have on this farm?" -&gt; "What do They have on this farm?" ? Try to balance the questions such that the answers are not too repetitive (E.g., too many 'yes' answers). ? Ask questions matter-of-factly (as objectively as possible). Stick to what you can see or hear from the video.</p><p>-E.g., "Does it make people feel good here?" is somehow subjective. ? Don't ask questions about how's the video being recorded, the camera-person or the camera itself. Ask about the content itself. Ignore what the camera-person is doing.</p><p>-E.g., "What's the cameraman doing?" / "How fast is the camera moving?" are not good questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">How to identity the Question Category</head><p>We have some basic categories: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">How To Provide Correct Answer</head><p>? Your answer should be written as full sentences (at least one).</p><p>-E.g., "Left" -&gt; "The landspout bends toward the left."</p><p>? The answer should be derived from the video (visual or audio).</p><p>-E.g., "These plants are green because they contain chlorophyll." is not a good answer. ? Provide answers matter-of-factly (as objectively as possible). Stick to what you can see or hear from the video.</p><p>-E.g., "beautiful" is likely not a good word to use within an answer. -E.g., "This takes some bravery to do."</p><p>is somehow subjective. ? Don't answer about how's the video being recorded, the camera-person, or the camera itself. Answer about the content itself. Ignore what the camera-person is doing.</p><p>-E.g., "There are two people, i.e. a running child, and the cameraman." is not a good answer. ? When you enter numbers, please enter digits instead of text.</p><p>-"Seventeen" -&gt; "17" 6. How to provide video evidence ? The video evidence consists of all the parts of the video that support the answer to your given question. ? You need to provide at least one video evidence clip (intervals within the video) for each question. ? You need to provide both the start point and end point for all the video evidence you identify in the video; ? You could use your mouse or ?/? key to click or drag the process bars of start point and end point. When you click or drag the bar, the above video will change accordingly, so you could locate the points according to the video screen. ? For each video evidence clip, the end point should be greater than zero, and the end point should be greater or equal to the start point. ? The video evidence clips (the time gap between the start point and the end point) should be as short as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instructions for Phase 2</head><p>We need help for this Video Question Answering task based on video content (including the audio).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Basic Instructions</head><p>? You will first watch the video, then answer the questions, each question in turn. ? You will need to provide at least one answer for each question (ignoring differences such as upper/lower case, or the article). The more answers the better, but every answer should be correct. ? You will need to identify the source of your answer (whether it is based on the visual scene or the audio).</p><p>? For each answer, you will need to provide video evidence (video clip) to support your answers. See below for additional information. ? If one video or question is not available, please comment at the bottom of this annotation page (and fill the mandatory fields for this video/question with placeholder values). ? There are five questions, you need to finish all five questions according to the content in the video (including audio).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">How To Answer</head><p>? Provide one or more answers for each question. ? Each answer should be written as full sentences (at least one).</p><p>-E.g., "Left" -&gt; "The landspout bends toward the left." ? The answer should be derived from the video (visual or audio).</p><p>-E.g., "These plants are green because they contain chlorophyll." is not a good answer. ? Respond matter-of-factly (as objectively as possible). Stick to what you can see or hear from the video.</p><p>-E.g., "beautiful" is likely not a good word to use within an answer. -E.g., "This takes some bravery to do." is somehow subjective. ? Answer in 3rd person point of view.</p><p>-E.g.,"We raise cattle on this farm." -&gt; "They raise cattle on this farm." ? Don't answer about how's the video being recorded, the camera-person, or the camera itself. Answer about the content itself. Ignore what the camera-person is doing.</p><p>-E.g., "There are two people, i.e. a running child, and the cameraman." / "The camera is moving fast." are not good answers. ? When you enter numbers, please enter digits instead of text.</p><p>-"Seventeen" -&gt; "17" ? Use your best judgment.</p><p>3. How to provide video evidence ? The video evidence consists of all the frame intervals of the video that support the answer to your given question. ? You need to provide at least one video evidence clip (interval within the video) for each question. ? You need to provide both the start point and end point for all the video evidence you identify in the video; ? You can use your mouse or ?/? key to click or drag the process bars of the start point and end point. When you click or drag the bar, the above video will change accordingly, so you could locate the points according to the video screen. ? For each video evidence clip, the end point should be greater than zero, and the end point should be greater or equal to the start point. ? The video evidence clips (the time gap between the start point and the end point) should only cover the actual evidence and not more (in other words, it should be as short as possible).</p><p>A.3 Annotation Interface <ref type="figure" target="#fig_5">Figure 8</ref> shows the annotation interface for Phase 1. <ref type="figure" target="#fig_6">Figure 9</ref> shows the annotation interface for Phase 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Pilot Study Comparison between Annotations from Experts v.s. Non-Expert</head><p>Before the formal annotation, we compare the nonexperts and experts' annotations for both phases. For Phase 1, we randomly selected 45 videos from each domain to be annotated by both the experts and crowdworkers. Following Castro et al. <ref type="formula">(2022)</ref>, we set the AWS annotation qualification as HIT approve rate &gt;92%, the number of HITs approved &gt;1000, the location is either Canada or U.S., and the reward as $6/HIT (around $9/h).   After annotation, two authors of this paper who do not know the source of annotation evaluate and score in terms of Relevance, Interestingness, and Professionality for each annotation from 0 to 3. We define Relevance, Interestingness, and Professionality as follows:</p><p>? Relevance: how relevant a question and an answer are to the video. Good relevance indicates that the question is related to the video and focuses on the major events, objects, or people in the video. A relevant answer should address the question and can be derived from this video.</p><p>? Interestingness: whether the question interests you. In other words, whether you are interested in the question and answer, given a video.</p><p>? Professionality: how detailed and precise the question and answer are. Good professionality can be demonstrated by the precise usage of terminologies and numbers, and accurate description in the answer.</p><p>? Overall Score: the average score of the score for Relevance, Interestingness, and Professionality.</p><p>For each category, the higher score indicates the better the annotation demonstrates that characteristic. <ref type="table" target="#tab_16">Table 8 lists the scores and Table 9</ref> presents some annotation examples. From both the empirical and numerical results, we could see there is a significant quality gap for the annotation from experts versus from crowdworkers. Therefore, we decide to employ domain experts for Phase 1.</p><p>For Phase 2, we randomly select 104 Geography videos and questions from the questions annotated in Phase 1 to be annotated by both experts and crowdworkers. Moreover, we set the reward as $3/HIT(around $9/h) and employ the AWS Master 2 as the crowdworkers. <ref type="table" target="#tab_1">Table 10</ref> lists the results of the pilot study for Phase 2. According to Table 10, crowdworkers perform similarly to experts 2 https://www.mturk.com/worker/help# what_is_master_worker in Phase 2. Considering the annotation efficiency, we decide to employ both experts and crowdworkers to annotate more diversified answers for each question in Phase 2. Note that the ROUGE scores in <ref type="table" target="#tab_1">Table 10</ref> are lower than the scores for the human baselines in <ref type="table" target="#tab_6">Tables 4 and 5</ref>. This is because we only compare the collected answers to a single answer in <ref type="table" target="#tab_1">Table 10, while in Tables 4 and 5</ref>, we calculate the average scores of one annotator against the remaining as described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Question and Answer Correction</head><p>After we collect annotation from Phase 1, the authors of this paper check the quality of the collected question and answers and modify the question and answers accordingly. Specifically, we:</p><p>? Delete the questions that can be answered without watching the video (e.g. Q: "If water can get through the hut's roof; can the wind go through the hut's roof?", A: "Yes the wind can go through the hut's roof.")</p><p>? Modify the question or the answer to 3rd person view (e.g. change Q: "Do we have aircraft that we can do a touch and go landing like a helicopter?" to Q: "Do they have aircraft that can do a touch and go landing like a helicopter.")</p><p>? Exclude the man holding the camera in the answer if it is a first-person view video.</p><p>? Modify questions that are not independently asked (e.g."Where are they?", where "they" refers to the "paved and unpaved roads" in the previous question. Therefore, we change the question to "Where are the roads?" )</p><p>? Split questions that include multiple subquestions into several questions.</p><p>Some of the annotators from Phase 2 do not annotate any evidence (leaving the evidence from the start to the end of the video). Thus, we empirically filter out evidence longer than 1/4 of the video.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Annotator Information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Dataset Analysis</head><p>Figure 10 presents question distributions in terms of words.</p><p>Questions Types. <ref type="table" target="#tab_1">Table 12</ref> examines the frequent words for each domain, which demonstrates the characteristics of the domain. Take Natural Disaster as an example, the 3 most frequent words are used in 20.63% of sentences. Besides, <ref type="figure" target="#fig_1">Figure 4</ref> in Section 3 lists the annotators' self-reported question types. One thing we observe is that questions that start with "What" possess a large proportion of all the questions. Such questions might be hard to classify into certain question typs (Castro et al., 2020), so we allow annotators to choose multiple question types for a single question. Empirically speaking, questions that start with "is(are)"/"where"/"how many" are commonly relevant to "Existence"/"Location"/"Number" questions. In our dataset, their distribution trend ("is(are)": 24.13% &gt; "where": 7.21% &gt; "how many": 4.48%) is akin to the trend of the distribution of the reported question types ("Existence": 45.20% &gt; "Location": 12.23% &gt; "Number": 4.59%). Moreover, although we have "human", "man" and "people" as the most frequent words in some domains, the most frequent words in domains such as Military are "military", and "aircraft", which demonstrates that our dataset does not only focus on human interactions as most of the existing datasets do.    Information Needed. As shown in the left Venn figure in <ref type="figure" target="#fig_9">Figure 11</ref>, generally, most questions are based on the visual (scene). Such a distribution is also justified by the distribution of the question types. The dominant question types we have in <ref type="figure" target="#fig_1">Figure 4</ref> are Motion, Spatial, Existence and Entity, which typically focus on visual information. However, in Agriculture (the right Venn figure in <ref type="figure" target="#fig_9">Figure 11</ref>), the audio-based questions take more portion, because videos in Agriculture usually focus on farming tips, instructions for using tools, etc.</p><p>In this paper, we do not experiment with models that use audio or transcripts from the video. Future research might look into letting models use audio and transcripts on our dataset.   Answer Similarity/Diversity. We have similar and diversified answers collected in our dataset. <ref type="figure" target="#fig_2">Figure 5</ref> gives 2 examples: answers from the upper example are similar to each other; for the lower example, answers diverse a lot between Phase 1 and Phase 2 annotations or even within Phase 2. However, all of the answers are acceptable given the video. The similarity demonstrates the reliability of the Phase 2 annotation. Meanwhile, the diversified answers help to better evaluate models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Annotation Statistics</head><p>Tables 13 and 14 list the statistics for annotation in Phase 1 and Phase 2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of Multi-task Learning</head><p>Tables 15 and 16 report the model performances under different sets of ?, ? for Equation (1). We highlight the rows we report in <ref type="table" target="#tab_6">Table 4</ref> in Section 4.2, <ref type="table" target="#tab_6">Table 4</ref> in Section 4.2, <ref type="table" target="#tab_8">Table 5</ref> in Section 5.2, and <ref type="table" target="#tab_8">Table 5</ref> in Section 5.2.   that ROUGE scores follow a similar trend as mentioned in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Ablation Study on Video Evidence Selection</head><p>To investigate whether the vision part is indeed needed by the baseline models for the Video Evidence Selection task, we conduct an ablation study using T5 IO and T5 SE (introduced in Section 5). We take a random sequence of the same length as the original video sequence and feed the random sequence instead of the original video sequence to the model. <ref type="table" target="#tab_1">Table 17</ref> shows the results of the comparison between these different settings. T5 IO performs roughly the same as T5 IO random , which indicates that the model struggles to utilize visual information. T5 IO even underperforms the random baseline which can achieve an IOU-F1 score of 2.5 ? 0.3 (as shown in <ref type="table">Table tab</ref>:fewshot-evidence-results-10-epochs). However, T5 SE outperforms T5 SE random , suggesting that T5 SE uses visual features to locate the evidence of the question.</p><p>? R1 R2 RL IOU-F1 0.5 33.8 ? 0.8 18.5 ? 0.7 32.5 ? 0.8 3.7 ? 2.4 1.0 32.2 ? 0.7 17.6 ? 0.5 31.0 ? 0.6 1.9 ? 1.7 1.5 33.8 ? 0.3 18.0 ? 0.9 32.5 ? 0.3 1.5 ? 0.1 <ref type="table" target="#tab_1">Table 15</ref>: We set ? = 1 throughout all the experiments, and report the corresponding Multi T+V,SE performances on Video QA (ROUGE scores) and Video Evidence Selection (IOU-F1 scores). We highlight the row we report in <ref type="table" target="#tab_6">Table 4</ref> in Section 4.2 and <ref type="table" target="#tab_6">Table 4</ref> in Section 4.2.</p><p>? R1 R2 RL IOU-F1 0.5 34.0 ? 0.5 18.8 ? 0.7 32.8 ? 0.6 1.2 ? 0.1 1.0 33.4 ? 0.6 18.4 ? 0.2 32.1 ? 0.6 1.4 ? 0.3 1.5 32.8 ? 0.3 18.3 ? 0.3 31.7 ? 0.2 1.0 ? 0.2 <ref type="table" target="#tab_1">Table 16</ref>: We set ? = 1 throughout all the experiments, and report the corresponding Multi T+V,IO performances on Video QA (ROUGE scores) and Video Evidence Selection (IOU-F1 scores). We highlight the row we report in <ref type="table" target="#tab_8">Table 5</ref> in Section 5.2 and <ref type="table" target="#tab_8">Table 5</ref> in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model name IOU-F1</head><p>T5 IOrandom 1.1 ? 0.3 T5 IO 1.1 ? 0.2 T5 SErandom 2.7 ? 1.9 T5 SE 4.5 ? 0.8   <ref type="figure" target="#fig_1">Figure 14</ref>: Multi T+V,SE performance on different question types for Video QA. For each question type, we report ROUGE-1, ROUGE-2, and ROUGE-L scores from left to right. We can see that different ROUGE scores follow similar trends, we only report ROUGE-1 in <ref type="table" target="#tab_10">Table 6</ref> in Section 6.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Examples from MovieQA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Percentage distribution of question types. Because one question might be classified into multiple categories, the scale summation is larger than 100%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Examples of questions (Q) and answers (A) from WildQA. The first answer is collected during Phase 1 of the annotation process; all remaining answers are collected in Phase 2. More analyses in Appendix A.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>34.0 ? 0.5 18.8 ? 0.7 32.8 ? 0.6 MultiT+V,SE 33.8 ? 0.8 18.5 ? 0.7 32.5 ? 0.8 Human 40.8 ? 0.0 18.1 ? 0.0 36.3 ? 0.0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Multi T+V,SE performance (IOU-F1) for Video Evidence Selection when tuned on a single domain (y-axis) and tested against each domain (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Interface for Phase 1 annotation. After watching the video, annotators provide a motivation, ask questions and provide corresponding answers by filling the blank. They provide parts of the videos as evidence to support each of the question-answer pairs by dragging the moving bar.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Interface for Phase 2 annotation. After watching the video and given the question from Phase 1, annotators provide answers with the corresponding evidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Distribution of questions by the first four tokens. The ordering of words starts from the center to outside.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>scene</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Venn diagrams showing whether the question depends on visual (scene) or audio from the original video. The left is for the entire dataset, while the right is for the Agriculture domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figures</head><label></label><figDesc>12 and 13 report Multi-Task model's performance on Video QA by ROUGE-2, and ROUGE-L, respectively.Figure 14demonstrates N a tu r a l D is a s te r H u m a n S u r v iv a</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Multi-Task ROUGE-2 scores for Video QA when tuned on a single domain (y-axis) and tested against each domain (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>Multi-Task ROUGE-L scores for Video QA when tuned on a single domain (y-axis) and tested against each domain (x-axis).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>statistics of the videos and associated questions for each</figDesc><table><row><cell>Expert question annotation objective, question, answer, evidences, etc</cell><cell>Manual review</cell><cell>Expert answer annotation Crowd answer annotation</cell></row><row><cell cols="2">Phase 1</cell><cell>answers, evidences, etc</cell></row><row><cell></cell><cell></cell><cell>Phase 2</cell></row><row><cell cols="3">Figure 3: The two phases of data annotation.</cell></row><row><cell>Domain</cell><cell cols="2">video count question count</cell></row><row><cell>Agriculture</cell><cell>85</cell><cell>109</cell></row><row><cell>Human Survival</cell><cell>95</cell><cell>309</cell></row><row><cell>Natural Disaster</cell><cell>70</cell><cell>187</cell></row><row><cell>Geography</cell><cell>46</cell><cell>110</cell></row><row><cell>Military</cell><cell>73</cell><cell>201</cell></row><row><cell>Total</cell><cell>369</cell><cell>916</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Video and question count for each domain.</figDesc><table><row><cell>Videos</cell><cell>369</cell></row><row><cell>Duration (in seconds)</cell><cell>71.22 ? 26.47</cell></row><row><cell>Questions</cell><cell>916</cell></row><row><cell>Question per video</cell><cell>2.48 ? 1.38</cell></row><row><cell>Question length (#tokens)</cell><cell>7.09 ? 2.60</cell></row><row><cell>Answer per question</cell><cell>2.22 ? 0.69</cell></row><row><cell>Answer length (#tokens)</cell><cell>9.08 ? 8.15</cell></row><row><cell>Evidence per answer</cell><cell>1.18 ? 0.80</cell></row><row><cell>Evidence length (s)</cell><cell>9.64 ? 10.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics for WildQA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Comparison between our WILDQA and other existing datasets. VE?: Whether the dataset provides</cell></row><row><cell>"Video Evidences"?; MC: "Multiple Choice" question answering; OE: "Open Ended" question answering; ES:</cell></row><row><cell>"Evidence Selection". We adapt the comparison table from Zhong et al. (2022).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: ROUGE scores for the task of Video Question</cell></row><row><cell>Answering. For comparison, we test the out-of-box T5</cell></row><row><cell>model under the zero-shot setting (T5 T 0-shot ).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>E?Tm where P i and P j are the probability for the i</figDesc><table><row><cell>Model name</cell><cell>IOU-F1</cell></row><row><cell>Random</cell><cell>2.5 ? 0.3</cell></row><row><cell>T5 IO</cell><cell>1.1 ? 0.2</cell></row><row><cell>T5 SE</cell><cell>4.5 ? 0.8</cell></row><row><cell>MultiT+V,IO</cell><cell>1.4 ? 0.3</cell></row><row><cell>MultiT+V,SE</cell><cell>3.7 ? 2.4</cell></row><row><cell>Human</cell><cell>18.37 ? 0.0</cell></row></table><note>S?Tm e E?T jm e</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>IOU-F1 scores for Video Evidence Selection.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>? 0.3 5.3 ? 0.3 Motion 32.8 ? 0.6 3.1 ? 2.0 Reasoning 33.3 ? 0.4 3.1 ? 1.3 Location 26.2 ? 10.7 4.4 ? 1.4 Entity 33.2 ? 0.7 5.2 ? 0.7</figDesc><table><row><cell>Type</cell><cell>R1</cell><cell>IOU-F1</cell></row><row><cell cols="3">Existence 33.3 Spatial 32.2 ? 0.6 2.4 ? 1.7</cell></row><row><cell>Number</cell><cell cols="2">33.8 ? 0.4 4.5 ? 0.7</cell></row><row><cell>Temporal</cell><cell cols="2">33.8 ? 0.6 3.8 ? 0.5</cell></row><row><cell>Time</cell><cell cols="2">33.1 ? 0.8 5.7 ? 1.0</cell></row><row><cell>Other</cell><cell cols="2">33.2 ? 0.6 5.3 ? 0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Multi T+V,SE performance on different question types for Video QA (ROUGE-1) and for Video Evidence Selection (IOU-F1). dence Selection respectively. Other ROUGE scores for Video QA follow similar trends as shown in Figure 14. According toTable 6, the model achieves good ROUGE-1 scores for Video QA when the model has a good IOU-F1 score for Video Evidence Selection such as its performance on Existence. The model has the highest ROUGE-1 variation on Location question types, with a relatively large variation for IOU-F1. The model's ROUGE-1 score on Spatial questions is relatively low, with the lowest IOU-F1 score. Multi T+V,SE excels at question type Entity and Existence with relatively high IOU-F1 scores. One possible explanation could be that the average length of the answers generated for Entity and Existence are around eight tokens, which might be easier for the model to ground to the relevant part in the video.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>ours 32.4 ? 0.2 17.5 ? 0.2 31.6 ? 0.2 T5 Tours 33.8 ? 0.2 17.7 ? 0.1 32.4 ? 0.3 T5 T+V 0-shot TVQA 20.3 ? 0.0 8.1 ? 0.0 20.1 ? 0.0 T5 T+Vours 33.1 ? 0.3 17.3 ? 0.4 31.9 ? 0.2 T5 T+VTVQA,ours 33.7 ? 0.2 18.3 ? 0.1 32.6 ? 0.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 .</head><label>7</label><figDesc>We can see that pre-training on TVQA for text-only T5 T does not help, which shows that the question styles in our dataset might be different from TVQA. For T5 T+V which uses both text and visual features, pre-training on TVQA does help the model, which suggests that the pretraining helps the model take advantage of the visual features. T5 T+V pre-trained on TVQA</figDesc><table><row><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell></cell><cell>4</cell><cell>5</cell></row><row><cell>Natural Disaster</cell><cell>2.9</cell><cell>2.6</cell><cell>1.8</cell><cell>0.5</cell><cell>2.6</cell></row><row><cell>Human Survival</cell><cell>0.7</cell><cell>2.0</cell><cell>3.4</cell><cell>4.7</cell><cell>4.5</cell></row><row><cell>Military</cell><cell>2.3</cell><cell>5.2</cell><cell>3.5</cell><cell>5.3</cell><cell>3.2</cell></row><row><cell>Agriculture</cell><cell>1.9</cell><cell>2.2</cell><cell>2.7</cell><cell>2.2</cell><cell>2.7</cell></row><row><cell>Geography</cell><cell>2.1</cell><cell>3.5</cell><cell>2.0</cell><cell>3.3</cell><cell>2.1</cell></row><row><cell cols="3">N a tu r a l D is a s te r H u m a n S u r v iv a l</cell><cell>M il it a r y</cell><cell>A g r ic u lt u r e</cell><cell>G e o g r a p h y</cell></row></table><note>underperforms T5 T+V trained together with T5 IO (the Multi T+V,SE model) according to Table 4 and Table 7, suggesting that attending to the rele- vant part in the video helps the model better than training the model on more data. However, pre-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>a challenging video understanding evaluation framework. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2925-2940, Dublin, Ireland. Association for Computational Linguistics. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4443-4458, Online. Association for Computational Linguistics. Victor Escorcia, Mattia Soldan, Josef Sivic, Bernard Ghanem, and Bryan Russell. 2019. Temporal localization of moments in video collections with natural language. ArXiv preprint, abs/1907.12763. Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. 2017. TGIF-QA: toward spatiotemporal reasoning in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1359-1367. IEEE Computer Society.</figDesc><table><row><cell>David Chen and William Dolan. 2011. Collecting highly parallel data for paraphrase evaluation. In Proceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 190-200, Portland, Ore-gon, USA. Association for Computational Linguis-tics. Proceedings of the 56th Annual Meeting of the As-sociation for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 934-945. Association for Com-putational Linguistics. Noa Garcia, Mayu Otani, Chenhui Chu, and Yuta Nakashima. 2020. KnowIT VQA: answering knowledge-based questions about videos. In The Thirty-Fourth AAAI Conference on Artificial Intelli-gence, AAAI 2020, The Thirty-Second Innovative Ap-plications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and York, NY, USA, February 7-12, 2020, pages 10826-10834. AAAI Press. Jingjing Liu. 2020. UNITER: Universal image-text representation learning. In European conference on computer vision, pages 104-120. Springer. Ross B. Girshick. 2015. Fast R-CNN. In 2015 IEEE International Conference on Computer Vision, ICCV</cell><cell>Yunseok Jianwen Jiang, Ziqiang Chen, Haojie Lin, Xibin Mark Everingham, Luc Van Gool, Christopher KI Zhao, and Yue Gao. 2020. Divide and conquer: Williams, John Winn, and Andrew Zisserman. 2010. Question-guided spatio-temporal contextual atten-The PASCAL visual object classes (VOC) challenge. tion for video question answering. In The Thirty-International journal of computer vision, 88(2):303-Fourth AAAI Conference on Artificial Intelligence, 338. AAAI 2020, The Thirty-Second Innovative Appli-cations of Artificial Intelligence Conference, IAAI Chenyou Fan. 2019. EgoVQA -an egocentric video question answering benchmark dataset. In Proceed-2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New ings of the IEEE/CVF International Conference on York, NY, USA, February 7-12, 2020, pages 11101-Computer Vision Workshops, pages 0-0. 11108. AAAI Press.</cell></row><row><cell>2015, Santiago, Chile, December 7-13, 2015, pages</cell><cell>Chenyou Fan, Xiaofan Zhang, Shu Zhang, Wensheng</cell></row><row><cell>1440-1448. IEEE Computer Society.</cell><cell>Wang, Chi Zhang, and Heng Huang. 2019. Het-Weike Jin, Zhou Zhao, Mao Gu, Jun Yu, Jun Xiao, and</cell></row><row><cell>Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. 2021. Ego4D: Around the world in 3,000 hours of egocentric video. ArXiv preprint, abs/2110.07058.</cell><cell>erogeneous memory enhanced multimodal attention model for video question answering. In IEEE Con-ference on Computer Vision and Pattern Recogni-tion, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 1999-2007. Computer Vision Foun-dation / IEEE. Yueting Zhuang. 2019. Multi-interaction network with object relation for video question answering. In Proceedings of the 27th ACM International Con-ference on Multimedia, MM '19, page 1193-1201, New York, NY, USA. Association for Computing Machinery.</cell></row><row><cell>Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan C. Russell. 2017. Localizing moments in video with natural lan-guage. In IEEE International Conference on Com-puter Vision, ICCV 2017, Venice, Italy, October 22-</cell><cell>Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. 2018a. Motion-appearance co-memory networks for video question answering. In 2018 IEEE Confer-ence on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 6576-6585. IEEE Computer Society. K Kim, C Nan, MO Heo, SH Choi, and BT Zhang. 2016. PororoQA: Cartoon video series dataset for story understanding. In Proceedings of NIPS 2016 Workshop on Large Scale Computer Vision System, volume 15.</cell></row><row><cell>29, 2017, pages 5804-5813. IEEE Computer Soci-ety.</cell><cell>Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Neva-tia. 2017. TALL: temporal activity localization via Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and Byoung-Tak Zhang. 2018. Multimodal dual atten-</cell></row><row><cell>2013. New types of deep neural network learning for speech recognition and related applications: An overview. In 2013 IEEE international conference on acoustics, speech and signal processing, pages 8599-8603. IEEE. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deng Huang, Peihao Chen, Runhao Zeng, Qing Du, Mingkui Tan, and Chuang Gan. 2020. Location-aware graph convolutional networks for video ques-tion answering. In The Thirty-Fourth AAAI Con-ference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial In-telligence, EAAI 2020, New York, NY, USA, Febru-ary 7-12, 2020, pages 11021-11028. AAAI Press. Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, and Xiaodong He. 2018. Natural language to structured query generation via meta-learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Compu-deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ-tational Linguistics: Human Language Technolo-gies, Volume 2 (Short Papers), pages 732-738, New Orleans, Louisiana. Association for Computational Linguistics.</cell><cell>language query. In IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, Octo-ber 22-29, 2017, pages 5277-5285. IEEE Computer tion memory for video story question answering. In Proceedings of the European Conference on Com-puter Vision (ECCV), pages 673-688. Society. Lianli Gao, Pengpeng Zeng, Jingkuan Song, Yuan-Fang Li, Wu Liu, Tao Mei, and Heng Tao Shen. 2019. Structured two-stream attention network for video question answering. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Kyung-Min Kim, Min-Oh Heo, Seong-Ho Choi, and Byoung-Tak Zhang. 2017. DeepStory: Video story QA by deep embedded memory networks. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, pages 2016-2022. ijcai.org. Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Intelligence, EAAI 2019, Honolulu, Hawaii, USA, and Juan Carlos Niebles. 2017. Dense-captioning January 27 -February 1, 2019, pages 6391-6398. events in videos. In IEEE International Conference AAAI Press. on Computer Vision, ICCV 2017, Venice, Italy, Oc-tober 22-29, 2017, pages 706-715. IEEE Computer Qiaozi Gao, Malcolm Doering, Shaohua Yang, and Society. Joyce Yue Chai. 2016. Physical causality of action verbs in grounded language understanding. In Pro-Chia-Hsuan Lee, Oleksandr Polozov, and Matthew ceedings of the 54th Annual Meeting of the Associ-ation for Computational Linguistics, ACL 2016, Au-Richardson. 2021. KaggleDBQA: Realistic evalu-</cell></row><row><cell>ation for Computational Linguistics.</cell><cell>gust 7-12, 2016, Berlin, Germany, Volume 1: Long</cell></row><row><cell></cell><cell>Papers. The Association for Computer Linguistics.</cell></row></table><note>Seongho Choi, Kyoung-Woon On, Yu-Jung Heo, Ah- jeong Seo, Youwon Jang, Minsu Lee, and Byoung- Tak Zhang. 2021. DramaQA: Character-centered video story understanding with hierarchical qa. Pro- ceedings of the AAAI Conference on Artificial Intel- ligence, 35(2):1166-1174. Anthony Colas, Seokhwan Kim, Franck Dernoncourt, Siddhesh Gupte, Zhe Wang, and Doo Soon Kim. 2020. TutorialVQA: Question answering dataset for tutorial videos. In Proceedings of the 12th Lan- guage Resources and Evaluation Conference, pages 5450-5455, Marseille, France. European Language Resources Association. Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: deep neural networks with multitask learning. In Ma- chine Learning, Proceedings of the Twenty-Fifth In- ternational Conference (ICML 2008), Helsinki, Fin- land, June 5-9, 2008, volume 307 of ACM Inter- national Conference Proceeding Series, pages 160- 167. ACM. Li Deng, Geoffrey Hinton, and Brian Kingsbury.Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C. Wallace. 2020. ERASER: A benchmark to evaluate rationalized NLP models. InQiaozi Gao, Shaohua Yang, Joyce Yue Chai, and Lucy Vanderwende. 2018b. What action causes this? to- wards naive physical action-effect prediction. InAlon Jacovi and Yoav Goldberg. 2020. Towards faith- fully interpretable NLP systems: How should we de- fine and evaluate faithfulness? In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 4198-4205, Online. As- sociation for Computational Linguistics.ation of text-to-SQL parsers. In Proceedings of the 59th Annual Meeting of the Association for Compu- tational Linguistics and the 11th International Joint Conference on Natural Language Processing (Vol- ume 1: Long Papers), pages 2261-2273, Online. As- sociation for Computational Linguistics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>: Average scores of the pilot study for Phase</cell></row><row><cell>1 (from 0 to 3). R: Relevance; I: Interestingness; P:</cell></row><row><cell>Professionality; Overall:Overall Score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 11</head><label>11</label><figDesc>shows the expertise of each expert, together with their assigned domains of annotation and the number of questions they annotate in their assigned domains in Phase 1. The soldiers are caught on the ship. What they are doing in this video?They caught the ship.</figDesc><table><row><cell>Objective</cell><cell>Question</cell><cell>Answer</cell></row><row><cell>E Precipitation</cell><cell cols="2">What types of precipitation are occurring? Rain and hail.</cell></row><row><cell>C Very like</cell><cell>Nice</cell><cell>Nice</cell></row><row><cell>E I want to learn about the people</cell><cell>What type of weapons are they carrying?</cell><cell>M4's</cell></row><row><cell>C E Storm</cell><cell>Where is the storm?</cell><cell>In a field.</cell></row><row><cell>C Motivation</cell><cell>5</cell><cell>Very amazing</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 9 :</head><label>9</label><figDesc>Examples in pilot study for Phase 1. E: Expert; C: Crowd</figDesc><table><row><cell>Annotator</cell><cell>R1</cell><cell>R2</cell><cell cols="2">RL IOU-F1</cell></row><row><cell>Expert</cell><cell cols="3">23.63 8.05 21.22</cell><cell>12.24</cell></row><row><cell>Crowd</cell><cell cols="3">20.03 3.24 17.69</cell><cell>8.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>ROUGE and IOU-F1 scores for the pilot study in Phase 2. Note that the scores here are lower than the scores for the human baselines inTables 4 and 5. This is because we only compare the collected answers to a single answer here, while inTables 4 and 5we calculate the average scores of one annotator against the remaining as described in Section 4.</figDesc><table><row><cell>Annotator ID</cell><cell>Expertise</cell><cell>Assigned Domains (# Q)</cell></row><row><cell>0</cell><cell>Geography</cell><cell>Geography (94) ; Natural Disaster (187)</cell></row><row><cell>1</cell><cell>Geography</cell><cell>Geography (16) ; Human Survival (74)</cell></row><row><cell>2</cell><cell>Veteran</cell><cell>Military (26) ; Human Survival (146)</cell></row><row><cell>3</cell><cell>Veteran</cell><cell>Military (70) ; Human Survival (89)</cell></row><row><cell>4</cell><cell>Veteran</cell><cell>Military (12)</cell></row><row><cell>5</cell><cell>Veteran</cell><cell>Military (8)</cell></row><row><cell>6</cell><cell>Veteran</cell><cell>Military (85)</cell></row><row><cell>7</cell><cell>Biology</cell><cell>Agriculture (88)</cell></row><row><cell>8</cell><cell>Biology</cell><cell>Agriculture (21)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 11 :</head><label>11</label><figDesc>Information for expert annotators who annotate the questions, together with their assigned domains and number of questions (# Q) in the parentheses.</figDesc><table><row><cell>Domain</cell><cell>top1</cell><cell>top2</cell><cell>top3</cell></row><row><cell>Agriculture</cell><cell>farm</cell><cell cols="2">agricultural understand</cell></row><row><cell cols="3">Natural Disaster weather people</cell><cell>flooding</cell></row><row><cell>Human Survival</cell><cell>man</cell><cell>determine</cell><cell>human</cell></row><row><cell>Geography</cell><cell>people</cell><cell cols="2">topography water</cell></row><row><cell>Military</cell><cell cols="2">military aircraft</cell><cell>determine</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 12 :</head><label>12</label><figDesc>Most common 3 words for each domain after removing stop-words.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 13 :</head><label>13</label><figDesc>Annotation statistics for Phase 1. "#tokens" represent the number of tokens.</figDesc><table><row><cell>Crowd annotated answers</cell><cell>932</cell></row><row><cell>Expert annotated answers</cell><cell>182</cell></row><row><cell>Total</cell><cell>1114</cell></row><row><cell>Answer per question</cell><cell>1.22 ? 0.69</cell></row><row><cell>Answer length (#tokens)</cell><cell>9.45 ? 7.46</cell></row><row><cell>Evidence per answer</cell><cell>0.89 ? 0.72</cell></row><row><cell>Evidence length (s)</cell><cell>10.43 ? 5.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 14 :</head><label>14</label><figDesc>Annotation statistics for Phase 2. "#okens" represents the number of tokens.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 17 :</head><label>17</label><figDesc>Ablation study on Video Evidence Selection. We feed T5 IO random and T5 SE random the question concatenated with a random sequence, while we feed T5 IO and T5 SE the question with the actual video sequence.</figDesc><table><row><cell>10</cell><cell></cell><cell>15</cell><cell></cell><cell>20</cell><cell>25</cell></row><row><cell>Natural Disaster</cell><cell>17.6</cell><cell>14.3</cell><cell>15.9</cell><cell>16.9</cell><cell>16.4</cell></row><row><cell>Human Survival</cell><cell>27.7</cell><cell>28.5</cell><cell>27.7</cell><cell>28.2</cell><cell>27.8</cell></row><row><cell>Military</cell><cell>29.6</cell><cell>29.5</cell><cell>28.6</cell><cell>28.9</cell><cell>28.8</cell></row><row><cell>Agriculture</cell><cell>20.9</cell><cell>21.7</cell><cell>22.1</cell><cell>21.7</cell><cell>22.1</cell></row><row><cell>Geography</cell><cell>6.9</cell><cell>19.1</cell><cell>16.4</cell><cell>17.1</cell><cell>6.1</cell></row><row><cell cols="3">N a tu r a l D is a s te r H u m a n S u r v iv a l</cell><cell>M il it a r y</cell><cell>A g r ic u lt u r e</cell><cell>G e o g r a p h y</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PySceneDetect uses the OpenCV<ref type="bibr" target="#b0">(Bradski, 2000)</ref> to find scene changes in video clips (py.scenedetect.com).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers for their constructive feedbacks. We thank Artem Abzaliev, Do June Min, and Oana Ignat for proofreading and suggestions. We thank William McNamee for the help with the video collection process, and all the annotators for their hard work on data annotation. We thank Yiqun Yao for the helpful discussions during the early stage of the project. This material is based in part upon work supported by the Automotive Research Center ("ARC"). Any opin-</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? A new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.502</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multitask learning: A knowledge-based source of inductive bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Caruana</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1023/A:1007379606734</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TVQA: Localized, compositional video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1167</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1369" to="1379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">TVQA+: Spatio-temporal grounding for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.730</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8211" to="8225" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TVR: A large-scale dataset for videosubtitle moment retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="447" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UNIMO: Towards unified-modal understanding and generation via cross-modal contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guocheng</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.202</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2592" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Beyond rnns: Positional self-attention with co-attention for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianli</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianglong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33018658</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="8658" to="8665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Few-shot semantic parsing for new predicates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1281" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>RoBERTa: A robustly optimized BERT pretraining approach. ArXiv preprint, abs/1907.11692</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher Joseph</forename><surname>Pal</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.778</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="7359" to="7368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A multiworld approach to question answering about realworld scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="1682" to="1690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Uncertainty-aware self-training for few-shot text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Awadallah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21199" to="21212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MarioQA: Answering questions by watching gameplay videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Hongsuck</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilchae</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.312</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2886" to="2894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">xGQA: Cross-lingual visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Geigle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Martin</forename><surname>Steitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="2497" to="2511" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ICML 2021</title>
		<meeting>the 38th International Conference on Machine Learning, ICML 2021</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07" />
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text chunking using transformation-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Marcus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third Workshop on Very Large Corpora</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2953" to="2961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural semantic parsing in low-resource settings with back-translation and meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeyun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i05.6427</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8960" to="8967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">MovieQA: Understanding stories in movies through question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.501</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="4631" to="4640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">VideoMCC: a new benchmark for video comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Bolonkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno>abs/1606.07373</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Next-qa: Next phase of questionanswering to explaining temporal actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xindi</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="9777" to="9786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3123266.3123427</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International Conference on Multimedia, MM &apos;17</title>
		<meeting>the 25th ACM International Conference on Multimedia, MM &apos;17<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
	<note>Xiangnan He, and Yueting Zhuang</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unifying the video and question attentions for openended video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5656" to="5666" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Commonsense justification for action explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaozi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sari</forename><surname>Saba-Sadiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joyce Yue</forename><surname>Chai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1283</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31" />
			<biblScope unit="page" from="2627" to="2637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video question answering via attribute-augmented attention network learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3077136.3080655</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Shinjuku, Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="829" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long-term video question answering via multimodal hierarchical memory attentive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="931" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Compositional attention networks with two-stream fusion for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1204" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ActivityNet-QA: A dataset for understanding complex web videos via question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33019127</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27" />
			<biblScope unit="page" from="9127" to="9134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Social-IQ: A question answering benchmark for artificial social intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edmund</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00901</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Computer Vision Foundation / IEEE</publisher>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="8807" to="8817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From recognition to cognition: Visual commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MERLOT: Multimodal neural script knowledge models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Sung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jize</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Leveraging video descriptions to learn video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuo-Hao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tseng-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Hong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017-02-04" />
			<biblScope unit="page" from="4334" to="4340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.542</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="5014" to="5022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Frame augmented alternating attention network for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanpeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1032" to="1041" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video question answering via hierarchical spatio-temporal attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofei</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2017/492</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19" />
			<biblScope unit="page" from="3518" to="3524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Open-ended long-form video question answering via adaptive hierarchical reinforced networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuwen</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2018/512</idno>
		<ptr target="ijcai.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018<address><addrLine>Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-13" />
			<biblScope unit="page" from="3683" to="3689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.01225</idno>
		<title level="m">Video question answering: Datasets, algorithms and challenges</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Uncovering the temporal context for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/article/10.1007/s11263-017-1033-7</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.540</idno>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
