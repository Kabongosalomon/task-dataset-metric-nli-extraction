<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Computing Technology</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Deng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianpeng</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for Anomaly Detection and Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The essence of unsupervised anomaly detection is to learn the compact distribution of normal samples and detect outliers as anomalies in testing. Meanwhile, the anomalies in real-world are usually subtle and fine-grained in a high-resolution image especially for industrial applications. Towards this end, we propose a novel framework for unsupervised anomaly detection and localization. Our method aims at learning dense and compact distribution from normal images with a coarse-to-fine alignment process. The coarse alignment stage standardizes the pixel-wise position of objects in both image and feature levels. The fine alignment stage then densely maximizes the similarity of features among all corresponding locations in a batch. To facilitate the learning with only normal images, we propose a new pretext task called non-contrastive learning for the fine alignment stage. Non-contrastive learning extracts robust and discriminating normal image representations without making assumptions on abnormal samples, and it thus empowers our model to generalize to various anomalous scenarios. Extensive experiments on two typical industrial datasets of MVTec AD and BenTech AD demonstrate that our framework is effective in detecting various real-world defects and achieves a new state-of-the-art in industrial unsupervised anomaly detection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Image anomaly detection is the identification of unexpected or abnormal image patterns in the dataset, which has wide applications in spotting defects from manufacturing lines <ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref>, analyzing medical images <ref type="bibr">(Seeb?ck et al. 2016)</ref>, and monitoring video streams <ref type="bibr" target="#b31">(Sultani, Chen, and Shah 2018;</ref><ref type="bibr" target="#b17">Liu, Li, and P?czos 2018)</ref>. Different from classical supervised learning tasks that assume an even distribution among classes, the anomalies occur rarely in real-world and are often hard to collect and label. Moreover, the lack of prior knowledge about anomalous patterns imposes a great challenge for designing comprehensive anomaly detection algorithms.</p><p>Due to the scarcity and uncertainty of abnormal images, existing anomaly detection methods usually follow the unsupervised or one-class classification setting. That is, models are provided with only normal data in training. During inference, the anomaly is spotted by the difference between the test data and learned normal features. <ref type="bibr" target="#b33">(Tax and</ref><ref type="bibr">Duin 1999, 2004;</ref><ref type="bibr" target="#b29">Sch?lkopf et al. 1999)</ref>. Existing works <ref type="bibr" target="#b29">(Sch?lkopf et al. 1999;</ref><ref type="bibr" target="#b18">Masci et al. 2011;</ref><ref type="bibr" target="#b10">Golan and El-Yaniv 2018;</ref><ref type="bibr" target="#b25">Ruff et al. 2018;</ref><ref type="bibr" target="#b13">Hendrycks et al. 2019</ref>) are proven to be successful in abstracting semantically rich representation for isolating defect images; nonetheless, they lack the ability to explore the fine-grained structures for anomalies. For example, a common setting in previous works <ref type="bibr" target="#b10">(Golan and El-Yaniv 2018;</ref><ref type="bibr" target="#b30">Sohn et al. 2021</ref>) is to set one category in CIFAR-10 dataset <ref type="bibr" target="#b15">(Krizhevsky, Hinton et al. 2009</ref>) as the normal class and the rest as anomalies. In actual manufacturing or medical industries, however, the difference between normal images and anomalies is more fine-grained and subtle than these object class differences <ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref>.</p><p>We thus designed a novel framework targeting the finegrained anomalous patterns in actual industrial setting, where images are usually taken under a clean background and shared positions and defects are usually subtle. The intuition of our method is inspired by the human inference process. When asked to play the spot the difference game, human beings would usually first roughly align, or find the correspondence, between the global context of two images. Then, they closely examine the detailed local distinction underlying two patterns. Inspirited by this, we design a two-stage coarseto-fine framework that learns robust feature distributions for normal images.</p><p>We first apply a coarse alignment module to roughly extract and align global feature embeddings. The module operates on both pixel-level for the input image and feature-level for each pyramid feature map. In the fine alignment stage, we apply self-supervised learning and propose a novel pretext task for learning the normal representation. The current state-of-the-art  in self-supervised anomaly detection designs augmentations that generate abnormal samples through mixing normal image patches. However, we lack sufficient prior knowledge of the real-world anomaly distributions, so the created defects cannot model the numerous real-life possibilities of anomalies. We thus define a new task for self-supervised learning called non-contrastive learning -using no abnormal samples and only normal images to train a robust feature encoder. By enforcing the similarity among each position's feature from a minibatch, we capture the local fine-grained correspondence in every aligned position of images. The distribution of normal images thus becomes more compact and more semantically meaningful, making the abnormal outliers more salient and easier to detect.</p><p>To summarize, the main contributions of this paper are:</p><p>? We propose a coarse-to-fine anomaly detection paradigm to detect and localize the fine-grained defects in real-world industrial dataset;</p><p>? We propose a novel pretext task named dense noncontrastive learning for self-supervised learning of compact normal features without any assumption of abnormal samples;</p><p>? We provide extensive experimental results and ablation studies to highlight the strength of our method, and the results in MvTec anomaly detection dataset <ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref> show that our method outperforms the previous state-of-the-art anomaly detection methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>The mainstream unsupervised anomaly detection and localization methods are either reconstruction-based or representation-based.</p><p>Reconstruction-based method applies autoencoders <ref type="bibr" target="#b2">(Bergmann et al. 2019;</ref><ref type="bibr" target="#b10">Gong et al. 2019)</ref> or generative adversarial networks <ref type="bibr" target="#b26">(Sabokrou et al. 2018;</ref><ref type="bibr" target="#b22">Pidhorskyi et al. 2018)</ref> to encode and reconstruct the normal data. During inference, an anomaly is spotted when the reconstructed image diverges from the original one. The pixel-wise reconstruction error can be applied to localize anomalies <ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref>, and the image level anomaly score is thus determined by aggregating pixel-wise errors <ref type="bibr" target="#b10">(Gong et al. 2019)</ref>. Despite the high interpretability of reconstruction and comparison, the pixel-wise difference fails to encode the global semantic meaning of images <ref type="bibr" target="#b24">(Ren et al. 2019;</ref><ref type="bibr" target="#b16">Li et al. 2021)</ref>. Also, the autoencoder sometimes generates comparable reconstruction results for the anomalous images too <ref type="bibr" target="#b21">(Perera, Nallapati, and Xiang 2019)</ref>.</p><p>Representation-based method, on the other hand, extracts discriminative feature vectors from normal images <ref type="bibr">Rippel, Mertens, and Merhof 2021)</ref> or normal image patches <ref type="bibr" target="#b3">(Bergmann et al. 2020;</ref><ref type="bibr" target="#b35">Yi and Yoon 2020;</ref><ref type="bibr" target="#b7">Cohen and Hoshen 2020)</ref> and yields more promising results for anomaly detection. The anomaly score is calculated by the distance between the embedding of a test image and the distribution of normal image representations. The normal image distribution is typically characterized by the center of a nsphere for the normal image <ref type="bibr" target="#b35">Yi and Yoon 2020)</ref>, the Gaussian distribution of normal images <ref type="bibr">(Rippel, Mertens, and Merhof 2021;</ref><ref type="bibr" target="#b9">Defard et al. 2020)</ref>, or the kNN for the entire normal image embedding <ref type="bibr" target="#b0">(Bergman, Cohen, and Hoshen 2020;</ref><ref type="bibr" target="#b7">Cohen and Hoshen 2020)</ref>. One of the most recent works, PaDiM <ref type="bibr" target="#b9">(Defard et al. 2020)</ref>, learns the parameters of multivariate Gaussian distribution from different CNN layers. As a concurrent work with ours, PANDAS <ref type="bibr" target="#b23">(Reiss et al. 2021</ref>) also uses non-contrastive learning to achieve feature adaptation to further refine the pretrained CNN backbone. The main difference between our method and PANDA is how to solve the model collapse issue. In PANDA, it suggests three options: simple early stopping, sample-wise early stopping and continual learning to avoid the model collapse. In our method, we use a stop-gradient strategy to alleviate this issue and we adapt the feature in an dense pixel-wise manner.</p><p>To assist the learning of semantic vectors for images, many works <ref type="bibr" target="#b10">(Golan and El-Yaniv 2018;</ref><ref type="bibr" target="#b30">Sohn et al. 2021;</ref><ref type="bibr" target="#b32">Tack et al. 2020</ref>) employed self-supervised learning <ref type="bibr" target="#b11">He et al. 2020;</ref><ref type="bibr" target="#b14">Komodakis and Gidaris 2018;</ref><ref type="bibr" target="#b6">Chen and He 2020)</ref> to discriminate normal data and outliers. Some methods are predicting rotation of images (Golan and El-Yaniv 2018; Bergman and Hoshen 2020) and contrastive learning with usual image augmentation strategies <ref type="bibr" target="#b32">Tack et al. 2020)</ref>. Although these methods well capture the semantic object information in images, they fail to encode the fine-grained local irregularities in anomalies . Thus, several works (DeVries and Taylor 2017; <ref type="bibr" target="#b36">Yun et al. 2019;</ref><ref type="bibr" target="#b16">Li et al. 2021</ref>) created a set of new data augmentations that replicates the local defects in anomalies. The method Cutout (DeVries and Taylor 2017) randomly removes a small rectangular area from images, and CutPaste ) further modifies the algorithm to cut a patch from one image and paste it on the other. However, the representation of created negative irregularities usually does not overlap with real-world anomalies , which limits the generalization potential of these methods in inference processes.</p><p>In our paper, we follow the method of self-supervision in anomaly detection to propose a novel coarse-to-fine task. Earlier self-supervised methods <ref type="bibr" target="#b11">He et al. 2020</ref>) typically employ negative samples to learn diversified representations for different classes. Inspired by the recent innovative self-supervised method SimSiam (Chen and He 2020), we replace the generated anomalous samples that disagree with reality with our non-contrastive learning in a dense pixel-wise self-supervision manner. With the stop-gradient operation (Chen and He 2020) that discards negative samples, we eliminate any prior assumption about the anomalous data in training, and our model can therefore generalize to a variety of anomalies in real world. Furthermore, through our proposed coarse alignment of images and dense supervision of pixel-wise feature learning, we reduce the variances in normal data representation, enabling the learned compact distribution to predict robust distance estimates for outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-to-Fine Non-Contrastive Learning</head><p>In this section, we demonstrate our novel framework to detect and localize fine-grained anomalies. As indicated in <ref type="figure" target="#fig_0">Figure 1</ref>, our method consists of a coarse alignment stage and a fine alignment stage. The coarse alignment module first captures and aligns the global context of images. The standardized image features are then passed to the dense representation encoder in for fine-grained self-supervised learning. We construct compact Gaussian distribution from our dense normal features and spot distribution outliers as anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse Alignment Stage</head><p>Image-Level Coarse Alignment (ICA) The image-level coarse alignment (ICA) aims at regularizing the pixel distribution of normal images: it orients all images in a batch to a similar direction and position for dense comparison. Specif- The coarse alignment module standardizes the image-wise and feature-wise correspondence among pixels, while the fine alignment module densely maximizes the similarity of these pixels in a batch. We train these two modules with non-contrastive learning method in an end-to-end manner.</p><p>ically, we regress the affine transformations T ? (D i ) on an input image D i ? R H?W ?C :</p><formula xml:id="formula_0">x t i y t i = T ? (D i ) = ? 11 ? 12 ? 13 ? 21 ? 22 ? 23 x s i y s i 1<label>(1)</label></formula><p>Inspired by Spatial Transformer Network (Jaderberg et al. 2015), we adopt its similar architecture to our ICA module, which uses a tiny network h T ? to learn the above affine mapping from the original image (denoted by {(x s i , y s i )}) to a globally aligned representation ({(x t i , y t i )}). To train ICA module, we randomly pair the images in a batch and then minimize the 2 distance between them:</p><formula xml:id="formula_1">L ICA (D; ? h , T ? ) = A,B?D H?1 i=0 W ?1 j=0 h T ? (A i,j )?h T ? (B i,j ) 2 .</formula><p>(2) Note that we do not assign any standard orientation or alignment for our module to regress, so it learns a unified position in a self-supervised manner. Each time we random select two images A, B ? D, the 2 loss in Equation 2 supervises the alignment of A and B toward the reduction of entropy in this system. Thus, given enough iterations, the system reaches a consensus on alignment and the entropy is thus reduced to a local minimum. The results of roughly unified positions are displayed by the coarsely-aligned images in <ref type="figure" target="#fig_3">Figure 2</ref>.</p><p>Feature-Level Coarse Alignment (FCA) For transformed images in <ref type="figure" target="#fig_3">Figure 2</ref>, their positions are not strictly aligned to a unified representation. We thus introduce a feature-level coarse alignment (FCA) module to further adjust the high-level image representations. We use a pre-trained ResNet-18 <ref type="bibr" target="#b12">(He et al. 2016</ref>) as the feature extractor. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, we insert the FCA between succeeding layers Output Image of ICA with GT Mask</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image with GT Mask</head><p>Anomaly Localization Heatmap without ICA Anomaly Localization Heatmap with ICA <ref type="figure" target="#fig_3">Figure 2</ref>: The visualization results for our coarse image alignment module, the top two rows are the results for screws without using ICA and the bottom two rows are the alignment results and anomaly localization results for the same screws with ICA.</p><p>to align the embedding distribution with feature-level affine transformation. FCA has a similar implementation as ICA; nonetheless, this module enforces the backbone to extract generalizable features to align positions in a global highlevel embedding. The FCA module is only governed by selfsupervised loss in the fine alignment stage. The implementation details of image-level and feature-level alignment are in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine Alignment Stage: Pixel-wise Non-contrastive Learning</head><p>Non-contrastive learning is the training of a semantically meaningful normal image representation without leveraging its distance with anomalies. To detect the fine-grained anoma- </p><formula xml:id="formula_2">z1, z2 = f(x1), f(x2) # encodeing p1, p2 = g(z1), g(z2) # prediction L = D(p1, z2)/2 + D(p2, z1)/2 # loss L.backward() # back-propagate update(f, g) # SGD update def D(p, z): # negative cosine similarity z = z.detach() # stop gradient return -cosine_similarity(p, z, dim=-1) .mean()</formula><p>lies, we propose the pixel-wise alignment module, which maximizes the feature similarity across every embedding position for all normal images. As indicated by the Algorithm 1, we first randomly shuffle the minibatch and sample two feature maps. Let W, V ? R H ?W ?C denote the two encoded image feature map belonging to two different images from the last FCA. Then, for every position 0 ? i &lt; H , 0 ? j &lt; W in these two features, we extract the corresponding feature vectors w ij ? W and v ij ? V . We aim at encoding a unique vector representation for each position in the feature map, as well as narrowing its distribution for all normal images. Thus, we use 1 ? 1 convolutional operator instead of fully-connected layers in the feature extractor. The vectors are passed to a shared 3-layer 1?1conv encoder f . Only w ij is processed through a 2-layer 1?1conv predictor g to project its feature to the vector space of v ij . Given the two output vectors from the encoder m ij f (w ij ) and n ij f (v ij ), we minimize their negative cosine similarity:</p><formula xml:id="formula_3">L ij (m ij , n ij ; ? g , ? f ) = ? g(m ij ), n ij m ij 2 ? n ij 2 ,<label>(3)</label></formula><p>where ? g and ? f are the parameters of encoder and predictor, respectively. We conduct the above minimization for w and v at all positions i, j respectively to densely supervise the positionally-aligned feature distribution.</p><p>To avoid model collapsing (Wang and Isola 2020) when training with normal data only, we introduce the stop-gradient operation from <ref type="bibr" target="#b6">(Chen and He 2020)</ref>. That is, ?L ij is only allowed to descent backward through the upper branch of the network w.r.t. m ij , and it updates no information of n ij to the encoder f . A symmetry operation is applied to further supervise the learning of robust and generalizable features. The aggregated loss for every position thus becomes:</p><formula xml:id="formula_4">L F AS (m, n; ? g , ? f ) = D i,j 1 2 L ij (m ij , stop grad(n ij ))+ 1 2 L ij (n ij , stop grad(m ij )).</formula><p>(4) Its implementation is detailed in Algorithm 1. We train the above stages in an end-to-end manner and adjust the weight between coarse and fine alignment with ? 1 and ? 2 . Hence, the final loss in our framework is:</p><formula xml:id="formula_5">L total ( ? ; ? h,f,g , T ? ) = ? 1 ? L ICA + ? 2 ? L F AS . (5)</formula><p>L F AS is the dominant loss function supervising all parameters and L ICA is the auxiliary loss function used to guarantee the convergence of ICA. By optimizing the coarse and fine alignment module collectively, we allow the network to selfadjust and learn meaningful correlations of normal image embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anomaly Score Computation in Inference</head><p>With the densely extracted features, we model the representation of normal images with the Gaussian distribution for every pixels on feature map following <ref type="bibr" target="#b9">(Defard et al. 2020)</ref>. We extract the normal image representation at position (i, j) by concatenating the three pyramid layers of features of CNN at (i, j). Let X ij ? R (C1+C2+C3)?N denote the aggregated feature from the CNN for all images of training set. We model a distinctive Gaussian distribution N (? ij , ? ij ) for each pixel (i, j) on the feature map by:</p><formula xml:id="formula_6">? ij = 1 N N k x k ij ; ? ij = 1 N ? 1 N k (x k ij ?? ij )(x k ij ?? ij ) T</formula><p>(6) During inference, we compute the anomaly score by taking the Mahalanobis distances at every pixel between the test images and the normal distribution:</p><formula xml:id="formula_7">D(x ij ) = (x ij ? ? ij ) T ? ?1 ij (x ij ? ? ij )<label>(7)</label></formula><p>Then, the distance matrix D is an anomaly map with dense pixel-wise anomaly scores. A greater score indicates a severer anomalous signal. We thus use the maximum anomaly score map to represent the anomaly score for the entire image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Datasets and Metrics</head><p>We perform experiments on two industrial anomaly detection datastes MVTec AD dataset <ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref> and BeanTech AD dataset <ref type="bibr" target="#b19">(Mishra et al. 2021)</ref>. MVTec AD dataset consists of 5354 real-world images with 15 categories, among which 10 of them are objects and the rest 5 are texture classes. BeanTech AD dataset has 3 categories of 2540 images. In both datasets, the training set consists of only normal images, while the testing set has a mixture of both normal and abnormal images. These datasets provides both anomaly   types and anomaly masks as test image labels for evaluation. As mentioned previously, the anomalies in these datasets are more fine-grained than those in the academic dataset settings, e.g. the CIFAR-10 <ref type="bibr" target="#b15">(Krizhevsky, Hinton et al. 2009</ref>) dataset whose anomaly is defined as different object classes. Under the one-class classification protocol, we train a model for each category with its respective normal images. The implementation details are listed in Appendix. During inference, we evaluate our method with image-level AUC and pixel-level AUC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anomaly Detection and Localization for MVTec</head><p>In <ref type="table" target="#tab_2">Table 1,</ref>   ) under the metrics of image-level AUC and pixel-level AUC, we give the results of the mean and standard deviation of 5 repeated experiments. With our proposed coarse-to-fine non-contrastive learning method, we achieve the best result among all existing works and make notable improvements on both of texture and object defects. Our method surpasses the current state-of-the-art by a margin of 2.1, yieding 97.7 image-level AUC and 98.2 pixel-level AUC. Some results of anomaly localization are visualized in <ref type="figure" target="#fig_2">Fig 3 for</ref> our method, and more comprehensive results of defect localization are provided in Appendix. We can observe that not only does the anomaly heatmap highlights the object with defect, it also displays intense and fine-grained attention on the small anomalous regions. This proves how our framework focuses precisely on the anomalies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anomaly Detection and Localization for BeanTech</head><p>In <ref type="table" target="#tab_3">Table 2</ref>, we compare our method with the anomaly detection approaches reported in <ref type="bibr" target="#b19">(Mishra et al. 2021</ref>) on the BeanTech AD dataset. They applied auto-encoder with MSE loss, auto-encoder with MSE and SSIM loss, and VT-ADL in anomaly localization. We give the results of ResNet-18 and we additionally report the image-level results. Our method achieves the best result among all existing and surpasses the current state-of-the-art by a margin of 7.0, yielding 98.1 image-level AUC and 97.0 pixel-level AUC. This result shows our method's potential to generalize to new anomalous detection scenarios, where the anomalous data has varied distribution and needs close scrutinization of details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance for Disturbed MVTec</head><p>Considering that the current MVTec AD dataset does not contain products with multiple appearances but only has spatially aligned products, we build a new dataset called Disturbed MVTec through various augmentations. We aim at simulating more challenging real life detection situations and verify the robustness and effectiveness of our framework. Specifically, we build two disturbed scenarios with color and spatial transformation, respectively. For the color transformation, we apply random brightness contrast enhancement and limited adaptive histogram equalization in MVTec's train and test images. For the spatial transformation, we apply random zoom in, zoom out, rotation and flip in the train and test images in MVTec. The examples of these two disturbed datasets and augmentation details are documented in our Appendix. Corresponding experimental results wth ResNet-18 are reported in <ref type="table">Table 3</ref>. It should be noted that some categories are suitable  <ref type="table">Table 3</ref>: Anomaly detection and localization performance on Disturbed MVTec AD dataset with ResNet-18, "det" is image-level AUC and "loc" is pixel-level AUC.</p><p>for color augmentation, especially for the texture classes. For instance, areas of different colors will be treated as anomalies on tile, wood and capsule, so we did not perform color augmentation on them. Moreover, some categories may not suitable do some special kinds of spatial augmentation. For instance, we can not apply vertical flip or rotation in transistor. We can observe that our method performs generally better than the baseline <ref type="bibr" target="#b9">(Defard et al. 2020</ref>) under these color and spatial transformations, which shows our framework's adaptability to complicated settings that simulate the real-world disturbances in industrial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Analysis on Distribution</head><p>To quantify the improvement of distribution compactness in our work, we compare the distance variance score of our method to the baseline (ImageNet pre-trained model). Given a class of total N normal image representations X ? R N ?H?W ?C , the distance variance score is calculated by first computing each pixel's distance to its alignment center:</p><formula xml:id="formula_8">? ij = N n x nij N , ? 0 ? i &lt; H, 0 ? j &lt; W (8) D nij = (x nij ? ? ij ) T ? ?1 ij (x nij ? ? ij )<label>(9)</label></formula><p>Then, we get the distance map D ? R N ?H?W at every location for all normal images. We compute the distance variance score as the aggregated variance for all distance embedding:</p><formula xml:id="formula_9">S cls = 1 H ? W i,j N n (D nij ? D ij ) 2 N ,<label>(10)</label></formula><p>where D ij is the average distance for all images at position (i, j). If the distribution of image representations becomes more compact, then S class would decrease, since the variation in distances between feature vectors to their center should be smaller. We finally aggregate the distance score for all 15 categories in our dataset:</p><formula xml:id="formula_10">S total = 1 N cls cls S cls<label>(11)</label></formula><p>The S total for the baseline is 0.95, and our method achieves a score of 0.84, which is a 11.6% decrease in the variance score. It thus demonstrates that our method effectively shrinks the feature distributions, while it also keeps the meaningful and discriminating variations in encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component-wise Analysis</head><p>We investigate the contributions of the main components for our method in <ref type="table" target="#tab_6">Table 4</ref>. "Baseline" only uses the ImageNet pre-trained ResNet-18 to model the Gaussian distribution in Equation 6 in inference. "Non-contrastive Learning" is the fine alignment stage to densely maximize the similarity of all images in a batch with the stop-gradient operation. "Coarse Alignment" is the initial image-level and feature-level alignment modules for distribution regularization. The baseline gives less AUC and PRO results comparing to other methods, such as U-S <ref type="bibr" target="#b3">(Bergmann et al. 2020)</ref> and PaDiM <ref type="bibr" target="#b9">(Defard et al. 2020)</ref>. Adding a single non-contrastive learning block improves the image-level AUC to 95.7 and pixel-level AUC to 96.8, which surpasses all previous works. This demonstrates the effectiveness of our designed non-contrastive learning module, because it eliminates the abnormal samples in training and shrinks the distribution of normal samples. Then, adding the coarse alignment module further enhances our advantage over the current state-of-the-art. This is very intuitive, since without first aligning the coarse locations of images, the densely minimized distance among pixels may not be correctly associated. Hence, the ablation study shows the additive effect of each module and the comprehensiveness of our framework.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effects of Coarse Alignment Stage</head><p>We give the qualitative results of a specific class to show the effects of the coarse alignment stage in <ref type="table" target="#tab_7">Table 5</ref>. We can observe that the coarse align stage can improve most of the categories in MVTec AD dataset, especially for "screw", and we will discuss this improvement is achieved below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Level Coarse Alignment module</head><p>To visualize the result of image-level coarse alignment, we choose the most disordered category screw. As indicated in <ref type="figure" target="#fig_3">Figure 2</ref>, the screws in dataset have different orientations and positions.</p><p>The localization heatmap has sparse and distributed attention over the screws, which cannot accurately localize the defects. After passing them through ICA, they are all roughly aligned to the straight-up direction. The heatmap also becomes more focused on the specific defect locations, instead of spreading in different orientations and positions. That is to say, the ICA module narrows the distribution of normal samples and reduces the difficulties in feature learning, which improves the  anomaly detection and localization performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature-Level Coarse Alignment Module</head><p>We investigate the effects of the position of the feature-level coarse alignment module in <ref type="table" target="#tab_9">Table 6</ref>. The baseline is the clean backbone without feature-level alignment. We find that inserting FCA after a single feature layer has limited improvement over the baseline, while inserting it in all three layers gives a thorough boost in AUC and PRO. We speculate that although adding FCA in a single layer enables the network to adjust the feature's positions, it limits the flexibility and reception field of the alignment to a single scope. Adding them to all three layers successively reinforces the feature alignment process and allows the self-supervised signal in Equation 4 to descent backward without information loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose a coarse-to-fine non-contrastive learning framework for unsupervised anomaly detection. The key to our success is the dense non-contrastive learning with coarse alignment and fine alignment module, which encourages the model to learn and narrow down the distribution of normal patterns. Our method achieves high performance on the industrial defect dataset and surpasses the state-of-the-art in both anomaly detection and localization tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material for Focus Your Distribution: Coarse-to-Fine Non-Contrastive Learning for AnomalyDetection and Localization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details on Experiments Implementation Details of two Coarse Alignment modules</head><p>Our implementation of the Image-level Coarse Alignment module (ICA) and Feature-level Coarse Alignment module (FCA) is inspired by <ref type="bibr">(Jaderberg et al. 2015)</ref>. As shown in <ref type="figure">Figure 4</ref>, ICA contains two 3 ? 3 Convolution layers, two max-pooling layers, and two fully-connected layers. The final FC layer outputs the angle for rotation transformation matrix of input images, which then aligns them to a unified direction. We develop a self-supervised learning task to train the ICA. Specifically, the ICA loss minimizes the 2 distance between each paired images on one batch in a pixel-wise manner. The pixels are thus enforced to learn shared, representitive position alignment information. We give the details of this self-supervised learning task and its loss function in Algorithm 2.</p><p>FCA shares the same the basic structure with ICA, and the only difference between them is the transformation matrix M. In ICA, the matrix M is only used to rotate the image with a single rotation angle, while the M in FCA contains 6 parameters for the affine transformation, including scale, rotation, and translation. Moreover, different from ICA who has a distinctive 2 similarity loss, the supervision signal for FCA is from the non-contrastive learning loss in the fine alignment stage. Therefore, the feature-wise fine alignment gives a full set of high-level alignment, which facilitates the adjustment of features in its downstream non-contrastive learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details of Training</head><p>We use the first three blocks of ImageNet pre-trained ResNet-18 <ref type="bibr" target="#b12">(He et al. 2016</ref>) as our backbone network. We train our model on 224 ? 224 image with one GPU. We update the parameters using momentum SGD with the learning rate of 0.01 for the ICA and 0.0001 for the others. The momentum is set to 0.9, and the batch size is 32. Moreover, we use a single cycle of cosine learning rate decay schedule and L2 weight regularization with a coefficient of 0.00001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details of Disturbed MVTec</head><p>For the color transformation, we apply random brightness contrast enhancement and limited adaptive histogram equalization in the train and test images in MVTec. See the examples in <ref type="figure" target="#fig_4">Figure 5</ref>. We train our model without ICA and FCA in this new color-transformed dataset, the results are reported in the below table. It should be noted that some categories may not suitable do color augmentation, especially for the texture classes. For instance, areas of different colors will be treated as anomalies on tile, wood and capsule, so we did not experiment on them.</p><p>For the spatial transformation, we apply random zoom in/out/rotation/flip in the train and test images in MVTec.</p><p>See the examples in <ref type="figure">Figure 6</ref>. We train our model without ICA and FCA in this new spatial-transformed dataset, the results are reported in the below table. It should be noted that some categories may not suitable do some special kinds of augmentation. For instance, we can not apply flip or rotation in transistor, because transistors are required to be arranged in the specified direction and position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization Hypothesis in Fine Alignment Stage</head><p>We introduce the stop-gradient operation from <ref type="bibr" target="#b6">(Chen and He 2020)</ref> to allow training with only normal data and avoid collapsing. We now provide a hypothesis for the mechanism in this non-contrastive learning framework, especially how it helps our model to fit and align compact distribution of normal images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization Problem Formulation</head><p>We hypothesize that our non-contrastive learning implicitly defines a nonparametric clustering algorithm, and the stopgradient operation thus becomes a decent procedure in the optimization steps. The purpose of our non-contrastive learning module is to regress a representative embedding for a class of normal images along the pixel dimension. This can then be viewed as a single-mode seeking problem for all normal data, from which we identify the most typical features as the mode. At the best case, the mode is the ground truth representation ? A for all normal objects in class A. A natural algorithm for mode seeking is the mean-shift method proposed by <ref type="bibr" target="#b8">(Comaniciu and Meer 2002)</ref>. That is, let x i ? R D , i = 1, 2, ...N denote N independent random 6 *&amp;+) , 6 *&amp;+) , (6 (6 * 5 67? -&amp; .? / , + ) ? /0? / 60.</p><p>-' <ref type="figure">Figure 4</ref>: The architecture of Image-level Coarse Alignment module. ICA, supervised by similarity loss, learns the rotation transformation parameters from input images and then applies the affine matrix to them to get batch images in same pose.</p><p>variables, the mean-shift updates x through a fixed-point iteration:</p><formula xml:id="formula_11">x i = m(x i ) = N j x j K(x i , x j ) N j K(x i , x j ) ,<label>(12)</label></formula><p>where K(x i , x j ) is the kernel that weighs the distance between x i and x j . The update continues as m(x) converges. In our setting, let m ij ? M ? R H?W ?C and n ij ? N ? R H?W ?C denote the two corresponding pixels sampled from the feature map M and N . For each iteration t, we define the loss as:</p><formula xml:id="formula_12">L = min ? D(m ij , n ij ) = D(g (t) (f (t) (m ij )), f (t) (n ij )),<label>(13)</label></formula><p>where ? are parameters for the predictor g and encoder f . We use negative cosine similarity as D(m ij , n ij ) in our paper, but it can be relaxed to any distance measurements. To regress the global minimizer where g * (f * (m ij )) = f * (n ij ) for all m ij , n ij , we perform the gradient descent update of L:</p><formula xml:id="formula_13">? (t+1) = ? (t) + ? ?L ?m ij<label>(14)</label></formula><p>With the stop-gradient operation, n ij is treated as a constant and does not back-propagate its gradient to L. We can thus interpret f (t) (n ij ) as the optimizing target for f and g to regress toward. Therefore, at each iteration t, we update new parameters by treating previous f (t) (n ij ) as the approximation for the ground truth representation ? A :</p><formula xml:id="formula_14">? (t+1) g , ? (t+1) f ? min D( ? , f (t) (n ij ))<label>(15)</label></formula><p>As we aggregate the updates for a batch of images from our random shuffle algorithm, the process of aligning each pair of m ij and n ij toward f (t) ( ? ) is equivalent to:</p><formula xml:id="formula_15">g (t+1) (f (t+1) ( ? )) E Z [f (t) (Z)],<label>(16)</label></formula><p>where Z is the random variable for normal image feature maps in a batch. Since we perform the pairing of images by random and assign equal weights to all images, Eq (16) becomes:</p><formula xml:id="formula_16">g (t+1) (f (t+1) ( ? )) E Z [f (t) (Z)] = i f (t) (z i ) |B| ,<label>(17)</label></formula><p>where |B| denotes the batch size. We can relax Eq <ref type="formula" target="#formula_0">(17)</ref> to Eq (12) by adding a flat kernel:</p><formula xml:id="formula_17">K( ? , z i ) = 1, z i ? B 0, otherwise ,<label>(18)</label></formula><p>where B represents all feature maps in a batch. Then,</p><formula xml:id="formula_18">g (t+1) (f (t+1) ( ? )) E Z [f (t) (Z)] = i f (t) (z i )K( ? , z i ) i K( ? , z i )<label>(19)</label></formula><p>Our non-contrastive learning thus becomes an implicitlydefined mean shift algorithm. At each iteration, among all normal images in the training set, the parameters of embedding only shift toward the mean of randomly sampled feature maps from a single batch. By the convergence of fixed-point iteration <ref type="bibr" target="#b4">(Burden and Faires 2011)</ref>, our method regresses to a mode representation of all normal images given enough training epoches. The scattered feature vectors for embedding thus shrink into a compact distribution where their feature encodings are shared and aligned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results of PRO Metric</head><p>The pixel-level AUC score favors over large anomalies. To resolve this, Bergmann et al <ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref> introduced the PRO (per-region overlap) metric. For each connected component in the anomaly mask, it plots the mean correctly classified pixel rates over the false positive rate (FPR). The PRO score is the normalized value of the integral of this curve from 0 to 0.3 FPR. A greater PRO score indicates better performance in localizing both obvious and subtle anomalies. We compare our method with existing works under per-regionlevel in <ref type="table" target="#tab_11">Table 7</ref>. We can observe that our method surpasses the state-of-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Application to Semantic Outlier Detection</head><p>We conduct the semantic anomaly detection experiment on CIFAR-10 <ref type="bibr" target="#b15">(Krizhevsky, Hinton et al. 2009</ref>) following the protocol in (Golan and El-Yaniv 2018; <ref type="bibr" target="#b30">Sohn et al. 2021)</ref>, where a single class is treated as normal and the remaining 9 classes are anomalies. We achieves 66.7 AUC, which surpasses some previous works, including OCSVM <ref type="bibr" target="#b28">(Sch?lkopf et al. 2001)</ref>, KDE <ref type="bibr" target="#b20">(Parzen 1962)</ref>, AnoGAN <ref type="bibr" target="#b27">(Schlegl et al. 2017)</ref>, DeepSVDD ) and OCGAN <ref type="bibr" target="#b21">(Perera, Nallapati, and Xiang 2019)</ref>. Still, its performance lags behind some other algorithms that specifically designed for imagelevel anomaly detection <ref type="bibr" target="#b21">(Perera, Nallapati, and Xiang 2019)</ref>. This result highlights the difference between fine-grained anomaly detection and image-level defect detection. While the image-level detection focuses on the overall semantic information, our fine-grained anomaly detection aims at detecting subtle anomalies that are usually indiscernible in global semantic context. Such difference suggests that these two detection tasks need different algorithms to target different aspects for the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Anomaly Detection and Localization for Shanghai Tech Campus Dataset</head><p>To further investigate the generalization performance of our method, we conduct experiments on Shanghai Tech Campus Dataset. Considering Shanghai Tech is used for video anomaly detection which has 13 sences, we choice 8 sences from them which don't require the correlation information of the front and rear frames of the video to detect anomaly. We draw images every 5 frames and use all the extracted single frame for training. The results are shown in <ref type="table" target="#tab_12">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Anomaly Localization Visualizations</head><p>From <ref type="figure">Figure 7</ref> to <ref type="figure" target="#fig_0">Figure 21</ref>, we show localization visualizations examples of 10 object and 5 texture categories. We not only show successful cases, but also some failure cases.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The whole architecture of the proposed approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>we compare our method with the state-of-theart one-class anomaly detection approaches in MVTec AD dataset, including deep one-class classifier (DOCC) (Ruff et al. 2021), FCDD (Liznerski et al. 2020), uninformed student (U-S) (Bergmann et al. 2020), patch SVDD (Yi and Yoon 2020), SPADE (Cohen and Hoshen 2020), PaDiM (Defard et al. 2020), Cut Paste</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Defect localization on toothbrush, transistor, pill, bottle, zipper, and wood classes of MVTec AD datasets. From top to bottom, input images with ground-truth localization area labeled in red and anomaly localization heatmaps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 :</head><label>2</label><figDesc>Pixel-wise Image Coarse Alignment Pseudocode, Pytorch-like # x: features of a minibatch of images ( NxCxHxW) # I: ICA network for x in loader: # load a minibatch x of N samples x1 = x.clone() # copy x to get x1 x2 = x[rand_permutation(N)] # shuffle the batch to generate reordered images as x2 z1, z2 = I(x1), I(x2) # coarse align L = l2(z1, z2)# pixel-wise l2 loss L.backward() # back-propagate update(I) # SGD update</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Some image examples of color transformation in MVTec AD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Some image examples of spatial transformation in MVTec AD dataset. Anomaly localization on bottle class of MVTec AD. From top to bottom, input images, those with ground-truth localization area in red, heatmaps predicted by our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>99.5) (100.0?0.0, 98.9?0.2) (100.0?0.0, 99.2?0.</figDesc><table><row><cell cols="2">Category</cell><cell>DOCC</cell><cell>FCDD</cell><cell>U-S</cell><cell>P-SVDD</cell><cell>SPADE (WR50)</cell><cell>PaDiM (WR50)</cell><cell>Cut Paste (R18)</cell><cell>Ours (R18)</cell><cell>Ours (WR50)</cell></row><row><cell></cell><cell>carpet</cell><cell>(90.6, -)</cell><cell>(-, 96)</cell><cell cols="2">(95.3, -) (92.9, 92.6)</cell><cell>(-, 97.5)</cell><cell>(-, 98.9)</cell><cell>(93.1, 98.3)</cell><cell>(98.3?0.4, 98.4?0.1)</cell><cell>(98.8?0.2, 98.5?0.1)</cell></row><row><cell></cell><cell>grid</cell><cell>(52.4, -)</cell><cell>(-, 91)</cell><cell cols="2">(98.7, -) (94.6, 96.2)</cell><cell>(-, 93.7)</cell><cell>(-, 94.9</cell><cell>(99.9, 97.5)</cell><cell>(97.4?1.4, 95.7?0.4)</cell><cell>(98.9?0.8, 96.8?0.3)</cell></row><row><cell>texture</cell><cell>leather tile</cell><cell>(78.3, -) (96.5, -)</cell><cell>(-, 98) (-, 91)</cell><cell cols="2">(93.4, -) (90.9, 97.4) (95.8, -) (97.8, 91.4)</cell><cell>(-, 97.6) (-, 87.4)</cell><cell>(-, 99.1) (-, 91.2)</cell><cell cols="3">(100.0, 1) (93.4, 90.5) (95.4?1.1, 93.7?0.7) (98.8?0.9, 96.8?0.5)</cell></row><row><cell></cell><cell>wood</cell><cell>(91.6, -)</cell><cell>(-, 88)</cell><cell cols="2">(95.5, -) (96.5, 90.8)</cell><cell>(-, 88.5)</cell><cell>(-, 93.6)</cell><cell>(98.6, 95.5)</cell><cell>(99.8?0.2, 94.2?0.1)</cell><cell>(99.4?0.4, 99.6?0.2)</cell></row><row><cell></cell><cell>average</cell><cell>(81.9, -)</cell><cell>(-, 93)</cell><cell cols="2">(95.7, -) (94.5, 93.7)</cell><cell>(-, 92.9)</cell><cell>(-, 95.6)</cell><cell>(97.0, 96.3)</cell><cell>(98.2?0.8, 96.2?0.3)</cell><cell>(99.2?0.5, 98.2?0.2)</cell></row><row><cell></cell><cell>bottle</cell><cell>(99.6, -)</cell><cell>(-, 97)</cell><cell cols="2">(96.7, -) (98.6, 98.1)</cell><cell>(-, 98.4)</cell><cell>(-, 98.1)</cell><cell cols="3">(98.3, 97.6) (100.0?0.0, 98.3?0.0) (100.0?0.0, 98.3?0.0)</cell></row><row><cell></cell><cell>cable</cell><cell>(90.9, -)</cell><cell>(-, 90)</cell><cell cols="2">(82.3, -) (90.3, 96.8)</cell><cell>(-, 97.2)</cell><cell>(-, 95.8)</cell><cell>(80.6, 90.0)</cell><cell>(94.3?0.5, 96.7?0.2)</cell><cell>(95.3?0.4, 97.5?0.2)</cell></row><row><cell></cell><cell>capsule</cell><cell>(91.0, -)</cell><cell>(-, 93)</cell><cell cols="2">(92.8, -) (76.7, 95.8)</cell><cell>(-, 99.0)</cell><cell>(-, 98.3)</cell><cell>(96.2, 97.4)</cell><cell>(93.2?1.8, 98.5?0.6)</cell><cell>(92.5?2.0, 98.6?0.8)</cell></row><row><cell></cell><cell>hazelnut</cell><cell>(95.0, -)</cell><cell>(-, 95)</cell><cell cols="2">(91.4, -) (92.0, 97.5)</cell><cell>(-, 99.1)</cell><cell>(-, 97.7)</cell><cell>(97.3, 97.3)</cell><cell>(99.8?0.2, 98.3?0.1)</cell><cell>(99.9?0.1, 98.7?0.1)</cell></row><row><cell></cell><cell cols="2">metal nut (85.2, -)</cell><cell>(-, 94)</cell><cell cols="2">(94.0, -) (94.0, 98.0)</cell><cell>(-, 98.1)</cell><cell>(-, 96.7)</cell><cell>(99.3, 93.1)</cell><cell>(99.9?0.1, 97.1?0.2)</cell><cell>(99.9?0.1, 98.2?0.1)</cell></row><row><cell>object</cell><cell>pill</cell><cell>(80.4, -)</cell><cell>(-, 81)</cell><cell cols="2">(86.7, -) (86.1, 95.1)</cell><cell>(-, 96.5)</cell><cell>(-, 94.7)</cell><cell>(92.4, 95.7)</cell><cell>(94.9?1.2, 97.2?0.3)</cell><cell>(94.5?1.5, 97.3?0.5)</cell></row><row><cell></cell><cell>screw</cell><cell>(86.9, -)</cell><cell>(-, 86)</cell><cell cols="2">(87.4, -) (81.3, 95.7)</cell><cell>(-, 98.9)</cell><cell>(-, 97.4)</cell><cell>(86.3, 96.7)</cell><cell>(89.7?0.8, 98.7?0.5)</cell><cell>(90.1?0.5, 98.7?0.4)</cell></row><row><cell></cell><cell cols="2">toothbrush (96.4, -)</cell><cell>(-, 94)</cell><cell cols="2">(98.6, -) (100.0, 98.1)</cell><cell>(-, 97.9)</cell><cell>(-, 98.7)</cell><cell>(98.3, 98.1)</cell><cell cols="2">(99.9?0.1, 98.9?0.0) (100.0?0.0, 98.9?0.0)</cell></row><row><cell></cell><cell cols="2">transistor (90.8, -)</cell><cell>(-, 88)</cell><cell cols="2">(83.6, -) (91.5, 97.0)</cell><cell>(-, 94.1)</cell><cell>(-, 97.2)</cell><cell>(95.5, 93.0)</cell><cell>(99.7?0.1, 98.6?0.1)</cell><cell>(99.2?0.3, 98.1?0.3)</cell></row><row><cell></cell><cell>zipper</cell><cell cols="4">(92.4, -) (-, 95.1) (95.8, -) (97.9, 95.1)</cell><cell>(-, 96.5)</cell><cell>(-, 98.2)</cell><cell>(99.4, 99.3)</cell><cell>(97.0?0.5, 97.8?0.4)</cell><cell>(97.5?0.5, 98.2?0.3)</cell></row><row><cell></cell><cell>average</cell><cell>(90.9, -)</cell><cell>(-, 91)</cell><cell cols="2">(90.9, -) (90.8, 96.7)</cell><cell>(-, 97.6)</cell><cell>(-, 97.3)</cell><cell>(94.3, 95.8)</cell><cell>(96.8?0.5, 98.0?0.2)</cell><cell>(97.0?0.5, 98.3?0.3)</cell></row><row><cell cols="2">average</cell><cell>(87.9, -)</cell><cell>(-, 92)</cell><cell cols="5">(92.5, -) (92.1, 95.7) (85.5, 96.5) (95.3, 96.7) (95.2, 96.0)</cell><cell>(97.3?0.5, 97.4?0.2)</cell><cell>(97.7?0.4, 98.2?0.3)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Anomaly detection and localization performance on MVTec AD dataset<ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref> with the format of (Image-level AUC, Pixel-level AUC). "WR50" means Wide-ResNet50?2 and "R18" indicates ResNet-18.</figDesc><table><row><cell>Det)</cell></row></table><note>Categories AE MSE AE MSE+SSIM VT-ADL Ours (Loc) Ours (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Anomaly detection and localization performance on BeanTech AD dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study for effects of each component.</figDesc><table><row><cell></cell><cell>carpet</cell><cell>grid</cell><cell>leather</cell><cell>tile</cell><cell>wood</cell><cell>bottle</cell><cell cols="2">cable capsule</cell></row><row><cell>w/o</cell><cell>99.3</cell><cell>96.3</cell><cell>99.5</cell><cell>95.9</cell><cell>98.8</cell><cell>100.0</cell><cell>89.4</cell><cell>90.4</cell></row><row><cell>w</cell><cell>98.3</cell><cell>97.4</cell><cell>100.9</cell><cell>95.4</cell><cell>99.8</cell><cell>100.0</cell><cell>94.3</cell><cell>93.2</cell></row><row><cell></cell><cell cols="8">hazelnut pill metal nut screw toothbrush transistor zipper average</cell></row><row><cell>w/o</cell><cell>96.1</cell><cell>99.1</cell><cell>91.8</cell><cell>74.2</cell><cell>99.4</cell><cell>94.8</cell><cell>89.5</cell><cell>94.3</cell></row><row><cell>w</cell><cell>99.8</cell><cell>99.9</cell><cell>94.9</cell><cell>89.7</cell><cell>99.9</cell><cell>99.7</cell><cell>97.0</cell><cell>97.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>The class-wise image-level AUC for our method with or without coarse alignment module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Effects of the insert location of feature-level coarse alignment module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Anomaly localization performance (PRO) on MVTec AD dataset<ref type="bibr" target="#b2">(Bergmann et al. 2019)</ref>. "WR50" means Wide-ResNet50?2.</figDesc><table><row><cell>Sence</cell><cell>01</cell><cell>02</cell><cell>03</cell><cell>06</cell><cell>09</cell><cell>10</cell><cell>11</cell><cell>average</cell></row><row><cell cols="8">Pixel-level AUC 98.8 99.0 97.5 99.4 96.0 98.1 93.9</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Anomaly localization performance (Pixel-level AUC) on Shanghai Tech Campus Dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3 3 6 3 3 213 3 ! "3 24###$1%440%3 6&amp;'3 ( 3 0102) 143 5 67? 9 2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10445</idno>
		<title level="m">Deep nearest neighbor anomaly detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">MVTec AD -A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9592" to="9600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4183" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Burden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Faires</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Numerical analysis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.10566</idno>
		<title level="m">Exploring Simple Siamese Representation Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02357</idno>
		<title level="m">Sub-image anomaly detection with deep pyramid correspondences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mean shift: A robust approach toward feature space analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Defard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Setkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Audigier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.08785</idno>
		<title level="m">PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<idno>arXiv:1805.10917</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1705" to="1714" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Improved regularization of convolutional neural networks with cutout</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using self-supervised learning can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadavath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.12340</idno>
		<idno>arXiv:1506.02025</idno>
	</analytic>
	<monogr>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">CutPaste: Self-Supervised Learning for Anomaly Detection and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04015</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classifier Two Sample Test for Video Anomaly Detections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>P?czos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liznerski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Franks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01760</idno>
	</analytic>
	<monogr>
		<title level="m">BMVC, 71</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Explainable deep oneclass classification</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stacked convolutional auto-encoders for hierarchical feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial neural networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="52" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fornasier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Piciarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Foresti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10036</idno>
		<title level="m">VT-ADL: A Vision Transformer Network for Image Anomaly Detection and Localization</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ocgan: Oneclass novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2898" to="2906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Generative probabilistic novelty detection with adversarial autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pidhorskyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Almohsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Adjeroh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Doretto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">PANDA: Adapting Pretrained Features for Anomaly Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2806" to="2814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling the distribution of normal data in pre-trained deep features for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fertig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Depristo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mertens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kauffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6726" to="6733" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3379" to="3388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised anomaly detection with generative adversarial networks to guide marker discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schlegl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seeb?ck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Waldstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt-Erfurth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Langs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on information processing in medical imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Estimating the support of a high-dimensional distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1443" to="1471" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identifying and categorizing anomalies in retinal imaging data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00686</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Support vector method for novelty detection</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning and Evaluating Representations for Deep One-class Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Csi: Novelty detection via contrastive learning on distributionally shifted instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08176</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition letters</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1191" to="1199" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Support vector domain description</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support vector data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9929" to="9939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Patch SVDD: Patch-level SVDD for Anomaly Detection and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
