<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Free-Form Deformation for 3D Face Reconstruction from In-The-Wild Images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harim</forename><surname>Jung</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeong-Seok</forename><surname>Oh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
						</author>
						<title level="a" type="main">Learning Free-Form Deformation for 3D Face Reconstruction from In-The-Wild Images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-3D face reconstruction</term>
					<term>Free-form deforma- tion</term>
					<term>3D morphable model</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The 3D Morphable Model (3DMM), which is a Principal Component Analysis (PCA) based statistical model that represents a 3D face using linear basis functions, has shown promising results for reconstructing 3D faces from single-view in-the-wild images. However, 3DMM has restricted representation power due to the limited number of 3D scans and global linear basis. To address the limitations of 3DMM, we propose a straightforward learning-based method that reconstructs a 3D face mesh through Free-Form Deformation (FFD) for the first time. FFD is a geometric modeling method that embeds a reference mesh within a parallelepiped grid and deforms the mesh by moving the sparse control points of the grid. As FFD is based on mathematically defined basis functions, it has no limitation in representation power. Thus, we can recover accurate 3D face meshes by estimating the appropriate deviation of control points as deformation parameters. Although both 3DMM and FFD are parametric models, deformation parameters of FFD are easier to interpret in terms of their effect on the final shape. This practical advantage of FFD allows the resulting mesh and control points to serve as a good starting point for 3D face modeling, in that ordinary users can fine-tune the mesh by using widely available 3D software tools. Experiments on multiple datasets demonstrate how our method successfully estimates the 3D face geometry and facial expressions from 2D face images, achieving comparable performance to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The task of inferring the 3D face geometry and appearance from 2D images has been extensively explored in computer vision and graphics research, as it is essential for many facerelated tasks and applications such as face recognition, antispoofing, tracking, virtual and augmented reality, animation, and gaming. Recovering a 3D face mesh from a single unconstrained in-the-wild image is especially challenging, due to large head pose variations, extreme expressions, occlusions, lighting conditions, and complex backgrounds.</p><p>Research in computer vision has quickly advanced and has been widely used in various applications <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Recently, considerable improvement has been made in 3D face reconstruction, with the help of Convolutional Neural Networks (CNNs). Previous research on 3D face reconstruction can be ms oh@korea.ac.kr mainly categorized into model-based and model-free methods. The statistical PCA-based face model, so-called the 3D Morphable Model (3DMM), has established the foundations of model-based methods. 3DMM is a globally linear model, where the face shape is represented as a linear combination of basis meshes obtained from a set of collected 3D face scans. Lately, many face reconstruction methods began to employ CNNs to regress 3DMM parameters <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b8">[9]</ref>.</p><p>On the other hand, model-free methods do not rely on a predefined face model but directly regress 3D vertices using volumetric representations <ref type="bibr" target="#b9">[10]</ref> or UV position maps <ref type="bibr" target="#b10">[11]</ref> for example. The idea of nonlinear 3DMM <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> was also introduced, where the nonlinear decoder of a deep neural network maps the shape and texture parameters to the 3D shape and texture. Since the decoder forms the final mesh through direct vertices regression rather than the parameters, it can be considered as model-free. While 3DMM has a model space restricted to the distribution of a specific set of 3D face scans, model-free methods do not have limited representation power. Nevertheless, they tend to be computationally inefficient due to the high degrees of freedom.</p><p>In order to address the limitations of model-based and model-free methods, we propose a learning-based 3D face reconstruction method that uses Free-Form Deformation (FFD) <ref type="bibr" target="#b13">[14]</ref>, for the first time to the best of our knowledge. FFD is a well-known geometric modeling technique. It embeds a reference mesh within a parallelepiped grid and deforms the mesh by shifting the control points of the grid. Since our FFD-based method does not have limited representation power nor excessively high degrees of freedom, it can be seen to fall into the category between model-based and modelfree methods. It is "model-free" in the sense that it does not reflect actual face scans as in 3DMM. Our method tends to have relatively more parameters to learn than 3DMM-based methods for this reason. On the other hand, it is "modelbased" in the sense that it does not directly use vertices to represent a mesh but lower dimensional control points and mathematically defined basis functions. Thus, it requires less parameters than direct vertices regression. Our goal is to train the network to find the proper deviation of control points, which deforms the reference mesh to be similar to the target face. We summarize our main contributions as follows:</p><p>? We explore how FFD can be applied to 3D face meshes in the context of deep learning. Our method attempts to discover the appropriate number of control points, their distribution, and their range of influence over vertices. ? While 3DMM-based methods tend to be restricted in the spanned model space of linear bases, our method has no limit in representation power and generalizes well for unseen faces, as FFD is based on mathematically defined basis functions rather than PCA basis functions. ? Our method can be readily utilized for practical purposes, since the reconstructed mesh and control points can provide a solid starting point for 3D face modeling and can be easily modified for detailed adjustments by using widely available 3D software tools. ? Our method either outperforms or achieves comparable results to existing 3D face reconstruction methods, both in quantitative and qualitative experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. 3D Face Reconstruction from a Single Image</head><p>Since Blanz et al. <ref type="bibr" target="#b14">[15]</ref> proposed the 3DMM, it has played a dominant role in 3D face modeling and reconstruction. Later on, more advanced variants of 3DMM were designed utilizing larger 3D scan databases and higher dimensional basis <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b18">[19]</ref>. Most recent methods use CNNs to regress the 3DMM parameters in a supervised manner, which are used to reconstruct 3D faces [5]- <ref type="bibr" target="#b6">[7]</ref>. Moreover, 3DMM parameters may be found through unsupervised methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref> without the help of training data. Sanyal et al. <ref type="bibr" target="#b7">[8]</ref> proposed a novel shape consistency loss that induces the face shape to be similar for images of the same person and dissimilar for different people.</p><p>Model-based methods, however, have limited representation power and model-free methods <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b19">[20]</ref> were proposed to overcome this limitation. Jackson et al. <ref type="bibr" target="#b9">[10]</ref> proposed to map image pixels to a volumetric representation, while Feng et al. <ref type="bibr" target="#b10">[11]</ref> developed a UV position map to represent the 3D shape. In a similar fashion, Deng et al. <ref type="bibr" target="#b19">[20]</ref> directly regressed 3D vertex coordinates in the image space. Tran et al. <ref type="bibr" target="#b12">[13]</ref> first introduced the concept of nonlinear 3DMM, where the decoders act as nonlinear models that map the parameters to the actual 3D shape and texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D Shape Reconstruction using Free-Form Deformation</head><p>There have been previous works that attempted to learn FFD for 3D shape reconstruction for rigid objects. Kuryenkov et al. <ref type="bibr" target="#b20">[21]</ref> first searched for the nearest shape template from a database and applied FFD to manipulate the template to match the target image. Jack et al. <ref type="bibr" target="#b21">[22]</ref> proposed a similar approach where they learned to deform points sampled from high-quality meshes.</p><p>To the best of our knowledge, FFD has never been applied to learning-based 3D face reconstruction, which is fairly different from general object reconstruction. The object reconstruction methods <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref> incorporated FFD using Bernstein basis functions, which is appropriate for rigid shapes, as its control points impose global influence on vertices. This method is not necessarily suitable for deformable shapes such as the human face, so we further experiment with FFD using B-spline basis functions. Moreover, they did not consider the pose of objects by using images obtained from the same viewpoint. However, considering the head Reference face mesh with frontal pose embedded in the 3D parallelepiped grid of 700 control points (l = 6, m = 19, n = 4). Each point on the intersections represents a control point and P 0 lmn refers to the last control point, where l, m, n are the number of divisions along the S, T,U axes respectively. The control points are displaced from their original positions to deform the shape of the embedded reference mesh to match the target face.</p><p>orientation is important in the face domain, since human faces tend to have more obvious changes in pose. For this reason, we train our model to not only regress deformation parameters but also camera projection parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deforming 3D Face Mesh with Free-Form Deformation</head><p>The goal of our method is to reconstruct the dense 3D face geometry from a single in-the-wild image. Since there are numerous vertices in a mesh, using all vertices as free variables to represent a mesh would be computationally inefficient. The more fundamental problem with this approach, however, is that these variables are not all free because the vertices should be constrained by each other to construct a mesh. Thus, we need to define new variables whose degrees of freedom are just enough to represent a mesh. For this purpose, we use FFD to represent and manipulate 3D meshes.</p><p>FFD is a shape modification method that has been widely used for geometric modeling in computer graphics and is supported by almost all 3D softwares. It embeds a reference mesh in a parallelepiped grid and deforms it by moving the control points of the grid. Each vertex of a deformed mesh is computed by a linear combination of control points by means of coefficient basis functions <ref type="bibr" target="#b13">[14]</ref>. Originally, FFD was implemented by using Bernstein polynomials as basis functions <ref type="bibr" target="#b22">[23]</ref>, which we refer to as Bernstein FFD. In this approach, each vertex of the mesh is influenced by all control points of the grid. Therefore, we further experiment with FFD using B-spline basis functions <ref type="bibr" target="#b13">[14]</ref>, in which each vertex is influenced by only a small number of neighbor control points. We refer to this method as B-spline FFD.</p><p>An arbitrary volume v(s,t, u) can be represented by taking a linear combination of control points using B-spline polynomials as coefficients, defined as follows:</p><formula xml:id="formula_0">Fig. 2.</formula><p>Overview of our method. The Deformation Parameter Regressor includes the ResNet-50 backbone, which regresses the pose parameters T (s, R,t) and the deformation parameters ?P of 700 control points. The grid on the right of ?P illustrates the possible deviation of control points in all directions of the red arrows and as an example, the left-bottom control point P i jk could be moved to the position of P i jk , which would effect the reference mesh accordingly. The Free-Form Deformation part takes in the predicted ?P, which is added to the original control points P 0 of the reference mesh, and the displaced control points are multiplied to B 0 , to obtain the deformed mesh in the world coordinate system, as in Eq. (2). T (s, R,t) is a 3D scaled orthographic projection (3 ? 4 affine transformation), which is multiplied to the deformed mesh to output a transformed mesh in the camera coordinate system. R,t are the rotation and translation parameters and s is the scale factor applied to all x, y, z directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>v(s,t, u)</head><formula xml:id="formula_1">= l ? i=0 m ? j=0 n ? k=0 B i jk (s,t, u)P i jk , B i, j,k (s,t, u) = l ? i=0 m ? j=0 n ? k=0 B i,p (s)B j,p (t)B k,p (u), where 0 ? s,t, u ? 1.<label>(1)</label></formula><p>Here B i,p (s) is a B-spline basis function of degree p defined over the knot spans dividing the range of s. Basis functions B j,p (t) and B k,p (u) are similarly defined. In this work, we use B-spline functions of degree 3, that is, p = 3. The number of control points are (l + 1), (m + 1), and (n + 1) in each direction. The control points P i jk are distributed in a lattice structure where the space between the control points is nonuniform in general, but we particularly use a uniform grid with different dimensions on each axis.</p><p>We choose one of the mesh data from 300W-LP <ref type="bibr" target="#b4">[5]</ref> as the reference mesh, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>. Note that the reference mesh can be any face mesh in the world coordinate system, since the goal is to find the appropriate deformation to reach the target mesh. To represent the reference mesh in the context of FFD, we need to define a parametric representation of the mesh by obtaining the values of the parameters (s q ,t q , u q ) corresponding to each vertex (x q , y q , z q ). Considering the relationship between each vertex and the parameters expressed in the following Eq. (2), we can obtain these parameters by solving the nonlinear equations:</p><formula xml:id="formula_2">(x q , y q , z q ) = v(s q ,t q , u q ) = l ? i=0 m ? j=0 n ? k=0 B 0 i jk (s q ,t q , u q )P 0 i jk , for each (x q , y q , z q ) ? RefMesh,<label>(2)</label></formula><p>where RefMesh refers to the reference mesh and P 0 refers to the control points of the undeformed, initial grid embedding the reference mesh. B 0 i jk (s q ,t q , u q ) is the B-spline coefficient function computed with respect to the reference mesh, which has the property that given a vertex (x q , y q , z q ), it has a large value if the control point P i jk is close to the vertex and a small value if it is far from the vertex.</p><p>By representing Eq. (2) in a matrix multiplication form, the reference mesh V 0 is represented in terms of the coefficient matrix B 0 and the control points P 0 as follows:</p><formula xml:id="formula_3">V 0 = B 0 P 0 ,<label>(3)</label></formula><p>that is,</p><formula xml:id="formula_4">? ? ? v 0 (s 1 ,t 1 , u 1 ) . . . v 0 (s N ,t N , u N ) ? ? ?= ? ? ? B 0 000 (s 1 ,t 1 , u 1 ) . . . B 0 lmn (s 1 ,t 1 , u 1 ) . . . B 0 000 (s N ,t N , u N ) . . . B 0 lmn (s N ,t N , u N ) ? ? ? ? ? ? P 0 000 . . . P 0 lmn ? ? ? .<label>(4)</label></formula><p>In Eq. (3), V 0 represents the reference mesh with N vertices and B 0 ? R N?M is the B-spline coefficient matrix which expresses the influence of each control point on each vertex of the mesh. P 0 ? R M?3 expresses the 3D coordinates of M control points. Once the coefficient matrix B 0 is computed using the reference mesh V 0 , any mesh V can be represented by deforming the control points as follows:</p><formula xml:id="formula_5">V (?P(I i )) = B 0 (P 0 + ?P(I i )),<label>(5)</label></formula><p>where I i indicates an arbitrary input image. Given a list of training data pairs of image I i and its corresponding ground truth mesh V i , we train the neural network to extract image features and predict the appropriate deformation ?P(I i ) of the control points from those features, so that the deformed mesh V matches the ground truth mesh. All deformation of the mesh happens so that the vertex indices and triangle connectivity remain the same. The overview of our proposed method is shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Applying Camera Projection to 3D Face Mesh</head><p>Our model regresses pose parameters, as well as deformation parameters. It is a natural approach to consider the shape deformation in the world coordinate system and the camera projection separately. T (s, R,t) indicates a 3D scaled orthographic projection (3 ? 4 affine transformation). The scale factor s is applied to x, y, z directions. This projection is called 3D in that it preserves the depths (z-values) of vertices, scaling them in the same factor as in the x and y directions. The transformation matrix T (s, R,t) is multiplied to the deformed mesh to output the final transformed deformed mesh in the camera coordinate system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Functions 1) Vertex Loss</head><p>Since the ground truth mesh and the predicted mesh share the same topology, it is possible to calculate the distance between the vertices that share the same indices. The Mean Squared Error (MSE) of the vertex loss is defined as:</p><formula xml:id="formula_6">L Vertex = 1 N N ? i=0 (T (V (?P(I i ))) ? T g (V i )) 2 ,<label>(6)</label></formula><p>where N refers to the number of data, V i refers to the ground truth mesh of the training image I i , and V refers to Eq. (5) which outputs the deformed mesh from the predicted ?P. T is a 3 ? 4 transformation matrix and T g is the ground truth pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Landmark Region Loss</head><p>There exists a set of predefined vertex indices corresponding to 68 facial landmarks. If we simply use MSE, each facial region would be weighted depending on the number of landmarks in each region. In order to control the importance of each region, we divide the 68 landmarks into 9 different regions and form a weighted loss. The regions are divided into the left and right eyebrow, left and right eye, upper and lower nose, upper and lower lip, and contour. The loss of each landmark region, abbreviated as LMRegion, can be computed by sampling the mesh with landmark region indices, abbreviated as LMIndex in the following equation.</p><formula xml:id="formula_7">L LMRegion = 1 N N ? i=0 1 M ? j?LMIndex (T (V j (?P(I i ))) ? T g (V i j )) 2 ,<label>(7)</label></formula><p>where N refers to the number of data, M refers to the number of landmarks of a specific region, and V j refers to the sampled vertices using LMIndex. The total loss can be computed as the weighted average of L Vertex defined in Eq. (6) and all L LMRegion defined in Eq. <ref type="bibr" target="#b6">(7)</ref>. In our implementation, the weight of the vertex loss is set to 0.46, and the weight of each landmark region loss is set to 0.06.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Datasets and Protocols 1) 300W-LP 300W-LP <ref type="bibr" target="#b4">[5]</ref> is composed of synthesized large-pose face images. The dataset consists of pairs of face images and 3DMM parameters, which were found by fitting a 3DMM built from a combination of the Basel Face Model <ref type="bibr" target="#b16">[17]</ref> and FaceWarehouse <ref type="bibr" target="#b15">[16]</ref>. We used an augmented version of 300W-LP <ref type="bibr" target="#b23">[24]</ref> for training, which has a more variety of extreme poses. Although the ground truth face meshes are based on 3DMM, our method has the capacity to generate deformed meshes beyond the linearly spanned space of 3DMM because the reconstructed mesh is not represented by 3DMM parameters, but by the deformation parameters.</p><p>2) AFLW2000-3D AFLW2000-3D <ref type="bibr" target="#b4">[5]</ref> contains the first 2,000 images of AFLW <ref type="bibr" target="#b24">[25]</ref>, with ground truth 3DMM parameters defined in a consistent manner with 300W-LP <ref type="bibr" target="#b4">[5]</ref> and 68 3D landmarks. The dataset consists of images with large pose variations of yaw angles ranging from ?90?to 90?and with variations of illumination conditions and facial expressions. Thus, it is often used to evaluate the 3D face reconstruction performance on challenging in-the-wild images.</p><p>Quantitative evaluation of 3D face reconstruction for inthe-wild images is challenging due to the lack of pairs of 2D images and 3D models collected in an unconstrained setting. As an alternative, evaluation of facial landmarks can be indicative of the overall 3D face reconstruction accuracy, especially when considering both visible and invisible landmarks. We evaluated our method using facial landmark data of AFLW2000-3D. The protocol followed <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref> to calculate the Normalized Mean Error (NME) normalized by the bounding box size. The result is reported by the range of yaw angles, which is divided into [0?, 30?], [30?, 60?] and [60?, 90?]. Following the works of <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b10">[11]</ref>, we randomly sampled 696 faces to balance the distribution so that each pose range has the same number of data.</p><p>3) CelebA CelebA <ref type="bibr" target="#b25">[26]</ref> is a large-scale face dataset with over 200K in-the-wild images of celebrities. The images have large variations in pose and background clutter. We use the provided aligned images resized to 218 ? 178 as our test data.</p><p>We use CelebA to conduct a qualitative evaluation of our model. As shown in <ref type="figure">Fig. 3</ref>, reconstructed 3D face mesh is rendered on top of the input image using scaled orthographic projection. The rendered result shows the resemblance between the original face of the 2D image and the deformed face mesh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>The training and experiments were implemented based on Pytorch <ref type="bibr" target="#b26">[27]</ref>. We employed a ResNet-50 architecture <ref type="bibr" target="#b1">[2]</ref> as the backbone of our network. At the end, we implemented a fully connected layer with 2,112 nodes to regress the deformation of 700 control points in 3D coordinates and 12 pose parameters for the transformation matrix. We divided the grid uniformly but with different dimensions on each axis (l = 6, m = 19, n = 4).</p><p>For training, we used an augmented version of the 300W-LP dataset <ref type="bibr" target="#b23">[24]</ref> with 687,854 images, which were obtained through applying random perturbations to pitch, yaw, and roll angles. Then, the images were cropped around the facial <ref type="figure">Fig. 3</ref>. Qualitative results of our method on CelebA compared to 3DDFA <ref type="bibr" target="#b4">[5]</ref>, PRNet <ref type="bibr" target="#b10">[11]</ref> and 3DDFA V2 <ref type="bibr" target="#b5">[6]</ref>. Our results of two separate cases were reported, each using Bernstein FFD and B-spline FFD. B-spline FFD showed better results on the mouth area, since only considering neighbor control points and imposing local influence on vertices allow for finer control. B-spline FFD also outperformed other existing 3D face reconstruction methods. region and resized to 120 ? 120. Finally, the images were normalized by the RGB mean and standard deviation. We constructed the mesh data with 35,709 vertices covering the face area, excluding the ears and neck, using the provided 3DMM parameters. Our model was trained with the Adam optimizer using a learning rate of 0.001 and weight decay of 0.0005, for 50 epochs with a mini-batch size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison between Bernstein FFD and B-spline FFD</head><p>We have experimented with FFD using both Bernstein [23] and B-spline basis functions <ref type="bibr" target="#b13">[14]</ref>, where the control points of Bernstein FFD have global influence on vertices while those of B-spline FFD have local influence. Empirically, we observed that one of the most challenging issues is properly controlling the mouth area, mostly due to the opened space between the lips. In fact, the qualitative results shown in <ref type="figure">Fig. 3</ref> demonstrate that Bernstein FFD could hardly control the lips. We observed B-spline FFD yielded better results, since the human face is a deformable shape and requires local control. Furthermore, <ref type="table" target="#tab_0">Table I</ref> shows that B-spline FFD achieved higher accuracy on predicting facial landmarks in general. For fair comparison, both methods were tested with the same number of control points and grid dimensions. <ref type="table" target="#tab_0">Table II</ref> reports the NME of facial landmarks compared to existing methods including SDM <ref type="bibr" target="#b27">[28]</ref>, 3DDFA <ref type="bibr" target="#b4">[5]</ref>, DeFA <ref type="bibr" target="#b6">[7]</ref>, Yu et al. <ref type="bibr" target="#b28">[29]</ref>, 3DSTN <ref type="bibr" target="#b29">[30]</ref>, 3D-FAN <ref type="bibr" target="#b30">[31]</ref>, PRNet <ref type="bibr" target="#b10">[11]</ref>, CMD <ref type="bibr" target="#b11">[12]</ref>, and 3DDFA V2 <ref type="bibr" target="#b5">[6]</ref>. Our method, B-spline FFD, showed better accuracy than most of the compared methods and achieved comparable performance to 3DDFA V2 <ref type="bibr" target="#b5">[6]</ref>. It is worth noting that the landmarks are inherently a subset of 3DMM fitted meshes, so 3DMM-based methods tend to show better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative Results</head><p>Our reconstruction results of Bernstein FFD and B-spline FFD on the images of the CelebA dataset can be viewed at <ref type="figure">Fig. 3</ref>, in comparison to 3DDFA <ref type="bibr" target="#b4">[5]</ref>, PRNet <ref type="bibr" target="#b10">[11]</ref>, and 3DDFA V2 <ref type="bibr" target="#b5">[6]</ref>. The qualitative evaluation demonstrates that our model is robust to large poses, not only frontal poses, as illustrated in the last row. In addition, it can reconstruct 3D meshes even when part of the face is occluded by glasses, as shown in the first and third row. Overall, this experiment validated that our model works well for in-the-wild images with various lighting, expressions, poses, and occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we proposed a new approach to learningbased 3D face shape reconstruction through B-spline FFD. FFD can be considered as both model-based and modelfree; the mesh is represented by low dimensional control points and polynomial basis functions, but still has no limit in expressiveness or model space. Moreover, the regressed deformation parameters are intuitive and self-explanatory. This further allows one to readily utilize the estimated mesh and control points for detailed adjustment and further deformation. Lastly, our model is meaningful in that it achieved comparable results to state-of-the-art methods of 3D face reconstruction from a single in-the-wild image.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>This work was supported by Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate School Program (Korea University)). 1 H. Jung and S.-W. Lee are with the Department of Artificial Intelligence, Korea University, Seoul 02841, South Korea. hr jung@korea.ac.kr and sw.lee@korea.ac.kr 2 M.-S. Oh is with the Department of Computer and Radio Communications Engineering, Korea University, Seoul 02841, South Korea.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Reference face mesh with frontal pose embedded in the 3D parallelepiped grid of 700 control points (l = 6, m = 19, n = 4). Each point on the intersections represents a control point and P 0 lmn refers to the last control point, where l, m, n are the number of divisions along the S, T,U axes respectively. The control points are displaced from their original positions to deform the shape of the embedded reference mesh to match the target face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I COMPARISON</head><label>I</label><figDesc>BETWEEN BERNSTEIN FFD AND B-SPLINE FFD ON AFLW2000-3D. THE NME (%) OF FACIAL LANDMARKS WITH DIFFERENT YAW ANGLES ARE REPORTED.</figDesc><table><row><cell>Method</cell><cell cols="4">0?to 30?30?to 60?60?to 90?Mean</cell></row><row><cell>Bernstein FFD</cell><cell>2.97</cell><cell>3.70</cell><cell>4.90</cell><cell>3.86</cell></row><row><cell>B-spline FFD</cell><cell>2.60</cell><cell>3.44</cell><cell>4.50</cell><cell>3.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II PERFORMANCE</head><label>II</label><figDesc>EVALUATION OF 68 LANDMARKS ON AFLW2000-3D. THE NME (%) WITH DIFFERENT YAW ANGLES ARE REPORTED. THE BEST RESULTS ARE HIGHLIGHTED.</figDesc><table><row><cell>Method</cell><cell cols="4">0?to 30?30?to 60?60?to 90?Mean</cell></row><row><cell>SDM [28]</cell><cell>3.67</cell><cell>4.94</cell><cell>9.76</cell><cell>6.12</cell></row><row><cell>3DDFA [5]</cell><cell>3.78</cell><cell>4.54</cell><cell>7.93</cell><cell>5.42</cell></row><row><cell>3DDFA + SDM [5]</cell><cell>3.43</cell><cell>4.24</cell><cell>7.17</cell><cell>4.94</cell></row><row><cell>DeFA [7]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>4.50</cell></row><row><cell>Yu et al. [29]</cell><cell>3.62</cell><cell>6.06</cell><cell>9.56</cell><cell>6.41</cell></row><row><cell>3DSTN [30]</cell><cell>3.15</cell><cell>4.33</cell><cell>5.98</cell><cell>4.49</cell></row><row><cell>3D-FAN [31]</cell><cell>3.15</cell><cell>3.53</cell><cell>4.60</cell><cell>3.76</cell></row><row><cell>PRNet [11]</cell><cell>2.75</cell><cell>3.51</cell><cell>4.61</cell><cell>3.62</cell></row><row><cell>CMD [12]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>3.98</cell></row><row><cell>3DDFA V2 [6]</cell><cell>2.63</cell><cell>3.42</cell><cell>4.48</cell><cell>3.51</cell></row><row><cell>B-spline FFD</cell><cell>2.60</cell><cell>3.44</cell><cell>4.50</cell><cell>3.51</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reconstruction of 3D human body pose from stereo image sequences based on top-down learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3120" to="3131" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translation-, rotationand scale-invariant recognition of hand-drawn symbols in schematic diagrams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Groen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Pattern Recognition and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="25" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Text extraction in MPEG compressed video for content-based indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-K</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Pattern Recognition</title>
		<meeting>the 15th International Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="409" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3D solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards fast, accurate and stable 3D dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="152" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dense face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jourabloo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1619" to="1628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to regress 3D face shape and expression from an image without 3D supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7763" to="7772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1274" to="1283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large pose 3D face reconstruction from a single image via direct volumetric cnn regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1031" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Joint 3D face reconstruction and dense alignment with position map regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="534" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dense 3D face decoding over 2500fps: Joint texture &amp; shape convolutional mesh decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1097" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear 3D face morphable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7346" to="7355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Direct manipulation of free-form deformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaufman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="177" to="184" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3D faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 26th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facewarehouse: A 3D facial expression database for visual computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="413" to="425" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A 3D face model for pose and illumination invariant face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paysan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Knothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romdhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Advanced Video and Signal-based Surveillance</title>
		<meeting>the IEEE International Conference on Advanced Video and Signal-based Surveillance</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A 3D morphable model learnt from 10,000 faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ponniah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dunaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5543" to="5552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A morphable 3D-model of Korean faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>B?lthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics</title>
		<meeting>the IEEE International Conference on Systems, Man, and Cybernetics</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2283" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Retinaface: Single-shot multi-level face localisation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ververas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5203" to="5212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deformnet: Free-form deformation network for 3D shape reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuryenkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="858" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning free-form deformations for 3D object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Pontes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fookes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shirazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="317" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Free-form deformation of solid geometric models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Sederberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Parry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 13th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="151" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">3DDFA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<ptr target="https://github.com/cleardusk/3DDFA" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Global supervised descent method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2664" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning dense facial correspondences in unconstrained images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4723" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster than realtime facial alignment: A 3D spatial transformer network approach in unconstrained poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3980" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2D &amp; 3D face alignment problem? (and a dataset of 230,000 3D facial landmarks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
