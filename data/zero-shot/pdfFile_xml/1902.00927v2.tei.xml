<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Depthwise Convolution is All You Need for Learning Multiple Visual Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhui</forename><surname>Guo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI ? ? University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
							<email>rsferis@us.ibm.com</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI ? ? University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
							<email>lwang@cs.ucf.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI ? ? University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tajana</forename><surname>Rosing</surname></persName>
							<email>tajana@ucsd.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research AI ? ? University of Central Florida</orgName>
								<address>
									<settlement>Orlando</settlement>
									<region>FL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Depthwise Convolution is All You Need for Learning Multiple Visual Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is a growing interest in designing models that can deal with images from different visual domains. If there exists a universal structure in different visual domains that can be captured via a common parameterization, then we can use a single model for all domains rather than one model per domain. A model aware of the relationships between different domains can also be trained to work on new domains with less resources. However, to identify the reusable structure in a model is not easy. In this paper, we propose a multi-domain learning architecture based on depthwise separable convolution. The proposed approach is based on the assumption that images from different domains share cross-channel correlations but have domain-specific spatial correlations. The proposed model is compact and has minimal overhead when being applied to new domains. Additionally, we introduce a gating mechanism to promote soft sharing between different domains. We evaluate our approach on Visual Decathlon Challenge, a benchmark for testing the ability of multi-domain models. The experiments show that our approach can achieve the highest score while only requiring 50% of the parameters compared with the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Deep convolutional neural networks (CNN) <ref type="bibr" target="#b17">(Krizhevsky, Sutskever, and Hinton 2012;</ref><ref type="bibr" target="#b11">He et al. 2016</ref>) have been the state-of-the-art methods for tackling vision tasks. The existing CNN models are powerful but mostly designed for dealing with images from a specific visual domain (e.g. digits, animals, or flowers) <ref type="bibr" target="#b27">Gan et al. 2017;</ref><ref type="bibr" target="#b28">Long et al. 2018</ref>). This limits the applications of current approaches, as each time the network needs to be retrained when new tasks arrive. In sharp contrast to such CNN models, humans can easily generalize to new domains based on the acquired knowledge <ref type="bibr" target="#b6">(Cichon and Gan 2015;</ref><ref type="bibr" target="#b11">Hayashi-Takagi et al. 2015;</ref><ref type="bibr" target="#b15">Kirkpatrick et al. 2017;</ref><ref type="bibr" target="#b18">Li and Hoiem 2017)</ref>. Previous works <ref type="bibr" target="#b23">Rebuffi, Bilen, and Vedaldi 2018)</ref> show that images from Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. * Equal contribution. Work done during internship at IBM Research mentored by Rogerio Feris. ? The authors' work was supported in part by NSF-1741431. different domains may have a universal structure that can be captured via a common parameterization. A natural question then arises:</p><p>Can we build a single neural network that can deal with images across different domains?</p><p>The question motivates the field called multi-domain learning, where we target designing a common feature extractor that can capture the universal structure in different domains and reducing the overhead of adding new tasks to the model. With multi-domain learning, the visual models are vested with the ability to work well on different domains with minimal or no domain-specific parameters.</p><p>There are two challenges in multi-domain learning. The first one is to identify a common structure among different domains. As shown in <ref type="figure" target="#fig_0">Fig 1,</ref> images from different domains are visually different, it is challenging to design a single feature extractor for all domains. Another challenge is to add new tasks to the model without introducing additional parameters. Existing neural network based multi-domain learning approaches <ref type="bibr" target="#b22">Rebuffi, Bilen, and Vedaldi 2017;</ref><ref type="bibr" target="#b10">2018;</ref><ref type="bibr" target="#b24">Rosenfeld and Tsotsos 2017)</ref> mostly focus on the architecture design while ignoring the structural regularity hidden in different domains which leads to sub-optimal solutions.</p><p>In this paper, we propose a multi-domain learning approach based on depthwise separable convolution. Depthwise separable convolution has been proved to be a powerful variation of standard convolution for many applications, such as image classification <ref type="bibr" target="#b5">(Chollet 2017)</ref>, natural language processing <ref type="bibr" target="#b14">(Kaiser, Gomez, and Chollet 2017)</ref> and embedded vision applications . To the best of our knowledge, this is the first work that explores depthwise separable convolution for multi-domain learning. The proposed multi-domain learning model is compact and easily extensible. To promote knowledge transfer between different domains we further introduce a softmax gating mechanism. We evaluate our method on Visual Decathlon Challenge <ref type="bibr" target="#b22">(Rebuffi, Bilen, and Vedaldi 2017)</ref>, a benchmark for testing multi-domain learning models. Our method can beat the state-of-the-art models with only 50% of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary and contributions:</head><p>The contributions of this paper are summarized below:</p><p>? We propose a novel multi-domain learning approach by exploiting the structure regularity hidden in different domains. The proposed approach greatly reduces the number of parameters and can be easily adapted to work on new domains. ? The proposed approach is based on the assumption that images in different domains share cross-channel correlations while having domain-specific spatial correlations. We validate the assumption by analyzing the visual concepts captured by depthwise separable convolution using network dissection <ref type="bibr" target="#b0">(Bau et al. 2017</ref>). ? Our approach outperforms the state-of-the-art results on Visual Decathlon Challenge with only 50% of the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Multi-Domain Learning Multi-domain learning aims at creating a single neural network to perform image classification tasks in a variety of domains.  showed that a single neural network can learn simultaneously several different visual domains by using an instance normalization layer. <ref type="bibr" target="#b22">(Rebuffi, Bilen, and Vedaldi 2017;</ref><ref type="bibr" target="#b10">2018)</ref> proposed universal parametric families of neural networks that contain specialized problem-specific models which differ only by a small number of parameters. <ref type="bibr" target="#b24">(Rosenfeld and Tsotsos 2017)</ref> proposed a method called Deep Adaptation Networks (DAN) that constrains newly learned filters for new domains to be linear combinations of existing ones. Multi-domain learning can promote the application of deep learning based vision models since it reduces engineers' effort to train new models for new images.</p><p>Multi-Task Learning The goal of multi-task learning <ref type="bibr" target="#b2">(Bilen and Vedaldi 2016;</ref><ref type="bibr">Doersch and Zisserman 2017;</ref><ref type="bibr" target="#b16">Kokkinos 2017;</ref><ref type="bibr" target="#b26">Wang, He, and Gupta 2017)</ref> is to extract different features from a single input to simultaneously perform classification, object recognition, edge detection, etc. Various applications can be benefited from a multi-task learning approach since the training signals can be reused among related tasks <ref type="bibr" target="#b4">(Caruana 1997;</ref><ref type="bibr" target="#b30">Zamir et al. 2018)</ref>.</p><p>Transfer Learning The goal of transfer learning is to improve the performance of a model on a target domain by leveraging the information from a related source domain <ref type="bibr">(Pan, Yang, and others 2010;</ref><ref type="bibr" target="#b1">Bengio 2012;</ref><ref type="bibr" target="#b13">Hu, Lu, and Tan 2015)</ref>. Transfer learning has wide applications in a variety of areas, such as computer vision <ref type="bibr" target="#b21">(Raina et al. 2007</ref>), sentiment analysis <ref type="bibr" target="#b8">(Glorot, Bordes, and Bengio 2011)</ref> and recommender systems <ref type="bibr" target="#b20">(Pan et al. 2010;</ref><ref type="bibr" target="#b9">Guo, Wang, and Xu 2015)</ref>. Different from transfer learning, multi-domain learning aims at maximizing the performance of the model across multiple domains rather than focusing on a specific target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Preliminary Problem Definition and Notations</head><p>Consider a set of image domains</p><formula xml:id="formula_0">{D 1 , D 2 , ..., D T }, each domain D i consists of a triplet {X i , Y i , P i }. X i ? R Ci?Hi?Wi is the input image space and Y i ? {1, 2, ..., L i } is the output label space. Let</formula><p>x ? X i and y ? Y i be a pair of objects. The joint probabilistic distribution P i (x, y) describes the frequency of encountering (x, y) in domain D i . For a neural network g i (x): R Ci?Hi?Wi ? {1, 2, ..., L i } and a given loss function l, the risk of g i (x) can be measured as below,</p><formula xml:id="formula_1">R i = E[l(y, g i (x)] = l(y, g i (x))dP i (x, y)<label>(1)</label></formula><p>In multi-domain learning, our goal is to design neural network architectures that can work well on all the domains simultaneously. Let E (Di) be the domain-specific parameters for domain D i and C be the sharable portion of the neural network. For x ? X i , the output of the network can be calculated as,?</p><formula xml:id="formula_2">= (E (Di) ? C)(x)<label>(2)</label></formula><p>The average risk of the neural network across all the domains can be expressed as,</p><formula xml:id="formula_3">R = 1 T T i=1 E[l(y, (E (Di) ? C)(x)]<label>(3)</label></formula><p>The goals of multi-domain learning include: (1) minimize the average risk across different domains;</p><p>(2) maximize the size of sharing part C;</p><p>(3) minimize the size of the domainspecific part E (Di) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depthwise Separable Convolution</head><p>Our proposed approach is based on depthwise separable convolution that factorizes a standard 3 ? 3 convolution into a 3 ? 3 depthwise convolution and a 1 ? 1 pointwise convolution. While standard convolution performs the channelwise and spatial-wise computation in one step, depthwise separable convolution splits the computation into two steps: depthwise convolution applies a single convolutional filter per each input channel and pointwise convolution is used to create a linear combination of the output of the depthwise convolution. The comparison of standard convolution and depthwise separable convolution is shown in <ref type="figure">Fig. 3</ref>. Consider applying a standard convolutional filter K of</p><formula xml:id="formula_4">size W ? W ? M ? N on an input feature map F of size D f ? D f ? M and produces an output feature map O is of size D f ? D f ? N , O k,l,n = i,j,m K i,j,m,n ? F k+i?1,l+j?1,m<label>(4)</label></formula><p>In depthwise separable convolution, we factorize above computation into two steps. The first step applies a 3 ? 3 depthwise convolutionK to each input channel,</p><formula xml:id="formula_5">O k,l,m = i,jK i,j,m ? F k+i?1,l+j?1,m<label>(5)</label></formula><p>The second step applies 1 ? 1 pointwise convolutionK to combine the output of depthwise convolution,</p><formula xml:id="formula_6">O k,l,n = mK m,n ?? k?1,l?1,m<label>(6)</label></formula><p>Depthwise convolution and pointwise convolution have different roles in generating new features: the former is used for capturing spatial correlations while the latter is used for capturing channel-wise correlations.</p><p>Most the previous works <ref type="bibr" target="#b5">(Chollet 2017;</ref><ref type="bibr" target="#b12">Howard et al. 2017;</ref><ref type="bibr" target="#b24">Sandler et al. 2018</ref>) focus on the computational aspect of depthwise separable convolution since it requires less parameters than standard convolution and is more computationally effective. In <ref type="bibr" target="#b5">(Chollet 2017)</ref>, the authors proposed the "Inception hypothesis" stating that mapping crosschannel correlations and spatial correlations separately is more efficient than mapping them at once. In this paper, we provide further evidence to support this hypothesis in the setting of multi-domain learning. We validate the assumption that images from different domains share crosschannel correlations but have domain-specific spatial correlations. Based on this idea, we develop a highly efficient multi-domain learning method. We further analyze the visual concepts captured by depthwise convolution and pointwise convolution based on network dissection <ref type="bibr" target="#b0">(Bau et al. 2017)</ref>. The visualization results show that while having less parameters depthwise convolution captures more concepts than pointwise convolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Approach Network Architecture</head><p>For the experiments, we use the same ResNet-26 architecture as in <ref type="bibr" target="#b23">(Rebuffi, Bilen, and Vedaldi 2018)</ref>. This allows us to fairly compare the performance of the proposed approach with previous ones. This original architecture has three macro residual blocks, each outputting 64, 128, 256 feature channels. Each macro block consists of 4 residual blocks. Each residual block has two convolutional layers consisting of 3 ? 3 convolutional filters. The network ends with a global average pooling layer and a softmax layer for classification.</p><p>Different from <ref type="bibr" target="#b23">(Rebuffi, Bilen, and Vedaldi 2018)</ref>, we replace each standard convolution in the ResNet-26 with depthwise separable convolution and increase the channel size. The modified network architecture is shown in <ref type="figure">Fig.  2</ref>. This choice leads to a more compact model while still maintaining enough network capacity. The original ResNet-26 has over 6M parameters while our modified architecture has only half the amount of parameters. In the experiments we found that the reduction of parameters does no harm to the performance of the model. The use of depthwise separable convolution allows us to model cross-channel correlations and spatial correlations separately. The idea behind our multi-domain learning method is to leverage the different roles of cross-channel correlations and spatial correlations in generating image features by sharing the pointwise convolution across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learning Multiple Domains</head><p>For multi-domain learning, it is essential to have a set of universally sharable parameters that can generalize to unseen domains. To get a good starting set of parameters, we first train the modified ResNet-26 on ImageNet. After we obtain a well-initialized network, each time when a new domain arrives, we add a new output layer and finetune the depth-wise convolutional filters. The pointwise convolutional filters are shared accross different domains. Since the statistics of the images from different domains are different, we also allow domain-specific batch normalization parameters. During inference, we stack the trained depthwise convolutional filters for all domains as a 4D tensor and the output of domain d can be calculated as,</p><formula xml:id="formula_7">O k,l,m,d = i,jK i,j,m,d ? F k+i?1,l+j?1,m,d<label>(7)</label></formula><p>The adoption of depthwise separable convolution provides a natural separation for modeling cross-channel correlations and spatial correlations. Experimental evidence <ref type="bibr" target="#b5">(Chollet 2017)</ref> suggests the decouple of cross-channel correlations and spatial correlations would result in more useful features. We take one step further to develop a multi-domain domain method based on the assumption that different domains share cross-channel correlations but have domain-specific spatial correlations. Our method is based on two observations: model efficiency and interpretability of hidden units in a deep neural network. <ref type="table" target="#tab_0">Table 1</ref> shows the comparison of standard 3 ? 3 convolution, 3 ? 3 depthwise convolution (Dwise) and 1 ? 1 pointwise convolution (Pwise). Clearly, standard convolution has far more parameters than both depthwise convolution (?c 2 ) and pointwise convolution (?9). Typically, pointwise convolution has more parameters than depthwise convolution. In the architecture shown in <ref type="figure">Fig 2,</ref> pointwise convolution accounts for 80% of the parameters in the convolutional layers. The choice of sharing pointwise convolution and adding depthwise convolution induces minimal additional parameters when dealing with new domains. In the experiments we found that only by adding depthwise convolution leads to a network with limited number of free parameters which cannot handle some large datasets. To increase the network capacity, we allow the last convolutional layer to be specific for each domain. Based</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Operator </p><formula xml:id="formula_8">Output Parameters c1 ? h ? w 3 ? 3 Conv2d c2 ? h ? w 3 ? 3 ? c1 ? c2 c1 ? h ? w 3 ? 3 Dwise c1 ? h ? w 3 ? 3 ? c1 c1 ? h ? w 1 ? 1 Pwise c2 ? h ? w 1 ? 1 ? c1 ? c2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Sharing of Trained Depthwise Filters</head><p>In addition to the proposed sharing pointwise filters (crosschannel correlations) for multi-domain learning, we also investigate whether the depthwise filters (spatial correlations) learned from other domains can be transferred to the target domain. We introduce a novel soft sharing approach in the multi-domain setting to allow the sharing of depthwise convolution. We first train domain-specific depthwise filters. Then we stack all the domain-specific filters as in <ref type="figure" target="#fig_2">Fig 4.</ref> During soft-sharing, we train each domain one by one. All the domain-specific depthwise filters and pointwise filters (trained on ImageNet) are fixed during soft sharing. We only train the feedforward network that controls the softmax gate. For a specific target domain, the softmax gate allows a soft sharing of trained depthwise filters with other domains. It can be denoted as follows: for each domain D j , consider a network with L depthwise separable convolutional layers S 1 , ..., S L , the input to the pointwise convolution in layer l is defined as,?</p><formula xml:id="formula_9">l = T i=1 s l i? l i , with T i=1 s l i = 1 (8)</formula><p>where? l i is the output of the depthwise convolution of domain i in the layer l if we use images in domain D j as input. s l i denotes a learned scale for the depthwise convolution of domain i in the layer l. The scales s 1 , ..., s T are the output of a softmax gate. The input to the softmax gate is the convolutional feature map X l?1 ? R C?H?W produced by the previous layer. Similar to (Veit and Belongie 2017), we only consider global channel-wise features. In particular, we perform global average pooling to compute channel-wise means,</p><formula xml:id="formula_10">M c = 1 H ? W H i=1 W j=1 X c,i,j<label>(9)</label></formula><p>The output is a 3-dimensional tensor of size C ? 1 ? 1. To achieve a lightweight design, we adopt a simple feedforward network consisting of two linear layers with ReLU activations to apply a nonlinear transformation on the channelwise means and feed the output to the softmax gate. All the convolutional filters are freezed during soft sharing. The scales s 1 , ..., s T and the parameters of the feedforward networks are learnt jointly via backpropagation.</p><p>It is widely believed that early layers in a convolutional neural network are used for detecting lower level features such as textures while later layers are used for detecting parts or objects. Based on this observation, we partition the network into three regions (early, middle, late) as shown in <ref type="figure">Figure 2</ref> and consider different placement of the softmax gate which allows us to compare a variety of sharing strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>Datasets and evaluation metrics We evaluate our approach on Visual Domain Decathlon Challenge <ref type="bibr" target="#b22">(Rebuffi, Bilen, and Vedaldi 2017)</ref>. It is a challenge to test the ability of visual recognition algorithms to cope with images from different visual domains. There are a total of 10 datasets:</p><p>(1) ImageNet (2) CIFAR-100 (3) Aircraft (4) Daimler pedestrian classification (5) Describable textures (6) German traffic signs (7) Omniglot (8) SVHN (9) UCF101 Dynamic Images (10) VGG-Flowers. The detailed statistics of the datasets can be found at http://www.robots.ox. ac.uk/?vgg/decathlon/.</p><p>The performance is measured in terms of a single scalar score S =</p><formula xml:id="formula_11">10 i=1 ? i max{0, E max i ? E i } ?i ,where E i = 1 D test i (x,y)?D test i 1{y = (E (Di) ? C)(x)}. E i is the aver- age test error of domain D i . E max i</formula><p>is the error of a reasonable baseline algorithm. The exponent ? i is set to be 2 for all domains. The coefficient ? i is 1000(E max i ) ??i then a perfect classifier receives 1000. The maximum score achieved across 10 domains is 10000.</p><p>Baselines We consider the following baselines in the experiments, (a) Individual Network: The simplest baseline we consider is Individual Network. We finetune the pretrained modified ResNet-26 on each domain which leads to 10 models altogether. This approach results in the largest model size since there is no sharing between different domains. (b) Classifier Only: We freeze the feature extractor part of the pretrained modified ResNet-26 on ImageNet and train domain-specific classifier layer for each domain. (c) Depthwise Sharing: Rather than sharing pointwise convolution, we consider an alternative approach of multidomain extension of depthwise separable convolution which shares the depthwise convolution between different domains. (d) Residual Adapters: Residual Adapters <ref type="bibr" target="#b22">(Rebuffi, Bilen, and Vedaldi 2017;</ref><ref type="bibr" target="#b10">2018)</ref> are the state-of-the-art approaches for multi-domain learning which include Serial Residual Adapter <ref type="bibr" target="#b22">(Rebuffi, Bilen, and Vedaldi 2017)</ref> and Parallel Residual Adapter <ref type="bibr" target="#b23">(Rebuffi, Bilen, and Vedaldi 2018)</ref>. Implementation details All networks were implemented using Pytorch and trained on 2 NVIDIA V100 GPUs. For the base network trained on ImageNet we use SGD with momentum as the optimizer. We set the momentum rate to be 0.9, the initial learning rate to be 0.1 and use a batch size of 256. We train the network with a total of 120 epochs and the learning rate decays twice at 80th and 100th epoch with a factor of 10. To prevent overfitting, we use a weight decay (L2 regularization) rate of 0.0001. For the multi-domain extension of depthwise separable convolution, we keep the same optimization settings as training the base network. We train the network with a total of 100 epochs and the learning rate decays twice at 60th and 80th epoch by a factor of 10. We apply weight decay (L2 regularization) to prevent overfitting. Since the size of the datasets are highly unbalanced, we use different weight decay parameters for different domains. Similar to <ref type="bibr" target="#b23">(Rebuffi, Bilen, and Vedaldi 2018)</ref>, higher weight decay parameters are used for smaller datasets. In particular, 0.002 for DTD, 0.0005 for Aircraft, CIFAR100, Daimler pedestrain, Omniglot and UCF101, and 0.0003 for GTSTB, SVHN and VGG-Flowers.</p><p>For soft sharing, we train the network with a total of 10 epochs and the learning rate decays once at the 5th epoch with a factor of 10. Other settings are kept the same as training multi-domain models.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>The results of the proposed approach and the baselines on Visual Decathlon Challenge are shown in <ref type="table" target="#tab_2">Table 2</ref>. Our approach achieves the highest score among all the methods while requiring the least amount of parameters. In particular, the proposed approach improves the current state-ofthe-art approaches by 100 points with only 50% of the parameters. The ResNet-26 with depthwise separable convolution surpasses the performance of the original ResNet-26 by a large margin on ImageNet (63.99 vs 60.32). On other smaller datasets, our approach still achieves better or comparable performance to the baselines. The improvement can be attributed to the sharing of pointwise convolution that has a regularization effect and allows the training signals in Im-ageNet to be reused when training new domains.</p><p>Compared with other variations of the modified ResNet-26, our approach still achieves the highest score. Our approach obtains a remarkable improvement (3507 vs 2756) with only 20% of the parameters compared with Individual Network. One reason for the improvement is that the proposed approach is more robust to overfitting, especially for some small datasets. While only training domain-specific classifier layers leads to the smallest model, the score is about 1000 points lower than the proposed approach. Compared with Depthwise Sharing, the assumption of sharing pointwise convolution leads to a more compact and efficient model (3507 vs 3234). This validates our assumption that it is preferable to share pointwise convolution rather than depthwise convolution in the setting of mutli-domain learning. We provide more qualitative results in the next section to support this claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>This section presents our visualization results of deptwise convolution and pointwise convolution based on network dissection <ref type="bibr" target="#b0">(Bau et al. 2017)</ref>. Network dissection is a general framework for quantifying the interpretability of deep neural networks by evaluating the alignment between individual hidden units and a set of semantic concepts. The accuracy of unit k in detecting concept c is denoted as IoU k,c . If the value of IoU k,c exceeds a threshold then we consider the unit k as a detector for the concept c. The details of calculating IoU k,c is omited due to space limitation.</p><p>In the experiments, we use the individual networks trained on ImageNet and CIFAR100 as examples. We visualize the hidden units in the 18th, 20th, 22th convolutional layers. <ref type="figure">Fig 5 shows</ref> the interpretability of units of the depthwise convolution and pointwise convolution in the corresponding layer. The highest-IoU matches among hidden units of each layer are shown. We observe that the hidden units in depthwise convolution detect higher level concepts than the units in pointwise convolution. The units in the depthwise convolution can capture part or object while the units in pointwise convolution can only detect textures. Moreover, <ref type="figure">Fig 6</ref> shows the number of attributes captured by the units in depth convolution and pointwise convolution. The results demonstrate that depthwise convolution consistently detects more attributes than pointwise convolution. These observations imply that pointwise convolution are mostly used for capturing low level features which can be generally shared across different domains. <ref type="table" target="#tab_3">Table 3</ref> shows the results of soft sharing. Regardless of the different placements of the softmax gate, the base approach without sharing still achieves the highest score on Visual Decathlon Challenge. One possible reason is that the datasets are from very different domains, sharing information between them may not generally improve the performance. However, for some specific datasets, we still observe some improvement. In particular, by sharing early layers we can obtain a slightly higher accuracy on DTD and SVHN. Another observation is that sharing later layers leads to a higher score than other alternatives. This implies that although images in different domain may not share similar low level features, they can still be benefited from each other by transfering information in later layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Sharing of Trained Depthwise Filters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we present a multi-domain learning approach based on depthwise separable convolution. The proposed approach is based on the assumption that images from different domains share the same channel-wise correlation but have domain-specific spatial-wise correlation. We evaluate our approach on Visual Decathlon Challenge and achieve the highest score among the current approaches. We further visualize the concepts detected by the hidden units in depthwise convolution and pointwise convolution. The results reveal that depthwise convolution captures more attributes and higher level concepts than pointwise convolution.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Image examples from different domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>ResNet-26 with depthwise separable convolution.DepthwisePointwise Standard convolution and depthwise separable convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The proposed soft-sharing approach for sharing spatial correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(e) Deep Adaptation Networks (DAN): In (Rosenfeld and Tsotsos 2017) the authors propose Deep Adaptation Networks (DAN) that constrains newly learned filters for new domains to be linear combinations of existing ones via controller modules. (f) PiggyBack: In (Mallya and Lazebnik 2018) the authors present PiggyBack for adding multiple tasks to a single network by learning domain-specific binary masks. The main idea is derived from network quantization (Courbariaux et al. 2016; Guo 2018) and pruning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>A comparison of visual concepts identified by network dissection in ResNet-26 with depthwise separable convolution trained on ImageNet and CIFAR100. The first two rows demonstrate the results on ImageNet and the last two rows demonstrate the results on CIFAR100. The columns show the results in different layers. The highest-IoU matches among hidden units of each layer are shown. The hidden units of the pointwise convolution in the 18th layer detect no visual concepts. Number of attributes captured by the hidden units of depthwise convolution and pointwise convolution in the 18th, 20th and 22th convolutional layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Interpretability While depthwise convolution typical has</cell></row><row><cell>less paramaters, by using the technique of network dissec-</cell></row><row><cell>tion (Bau et al. 2017), we found it captures more visual con-</cell></row><row><cell>cepts than pointwise convolution. Meanwhile, the results in</cell></row><row><cell>the same convolutional layer show that depthwise convolu-</cell></row><row><cell>tion captures higher level concepts such as wheel and grass</cell></row><row><cell>while pointwise convolution can only detect dots or honey-</cell></row><row><cell>combed. This observation suggests that pointwise convolu-</cell></row><row><cell>tion can be generally shared between different image do-</cell></row><row><cell>mains since it is typically used for dealing with lower level</cell></row><row><cell>features.</cell></row></table><note>Comparison of standard 3 ? 3 convolution, 3 ? 3 depthwise convolution (Dwise) and 1 ? 1 pointwise convo- lution (Pwise).on this modification, each new domain averagely introduces 0.3M additional parameters which is 10% of the modified ResNet-26.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2? 59.67 61.87 81.20 93.88 57.13 97.57 81.67 89.62 96.13 50.12 76.89 2621 Parallel Res. Adapt. 2? 60.32 64.21 81.91 94.73 58.83 99.38 84.68 89.21 96.54 50.94 78.07 3412 DAN 2.17? 57.74 64.12 80.07 91.30 56.64 98.46 86.05 89.67 96.77 49.38 77.01 2851 Piggyback 1.28? 57.69 65.29 79.87 96.99 57.45 97.27 79.09 87.63</figDesc><table><row><cell>Model</cell><cell>#par</cell><cell cols="6">ImNet Airc. C100 DPed DTD GTSR</cell><cell>Flwr</cell><cell cols="3">OGlt SVHN UCF</cell><cell>mean</cell><cell>S</cell></row><row><cell># images</cell><cell></cell><cell>1.3m</cell><cell>7k</cell><cell>50k</cell><cell>30k</cell><cell>4k</cell><cell>40k</cell><cell>2k</cell><cell>26k</cell><cell>70k</cell><cell>9k</cell></row><row><cell>Serial Res. Adapt.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>97.24</cell><cell>47.48 76.60 2838</cell></row><row><cell>Individual Network</cell><cell>5?</cell><cell cols="8">63.99 65.71 78.26 88.29 52.19 98.76 83.17 90.04</cell><cell>96.84</cell><cell>48.35 76.56 2756</cell></row><row><cell>Classifier Only</cell><cell>0.6?</cell><cell cols="8">63.99 51.04 75.32 94.49 54.21 98.48 84.47 86.66</cell><cell>95.14</cell><cell>43.75 74.76 2446</cell></row><row><cell>Depthwise Sharing</cell><cell>4?</cell><cell cols="8">63.99 67.42 74.46 95.60 54.85 98.52 87.34 89.88</cell><cell>96.62</cell><cell>50.39 77.91 3234</cell></row><row><cell>Proposed Approach</cell><cell>1?</cell><cell cols="8">63.99 61.06 81.20 97.00 55.48 99.27 85.67 89.12</cell><cell>96.16</cell><cell>49.33 77.82 3507</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top-1 classification accuracy and the Visual Decathlon Challenge score (S) of the proposed approach and baselines. #par is the number of parameters w.r.t. the proposed approach.</figDesc><table><row><cell>Model</cell><cell cols="6">ImNet Airc. C100 DPed DTD GTSR</cell><cell>Flwr</cell><cell cols="3">OGlt SVHN UCF</cell><cell>mean</cell><cell>S</cell></row><row><cell># images</cell><cell>1.3m</cell><cell>7k</cell><cell>50k</cell><cell>30k</cell><cell>4k</cell><cell>40k</cell><cell>2k</cell><cell>26k</cell><cell>70k</cell><cell>9k</cell></row><row><cell>early</cell><cell cols="8">63.99 58.69 81.01 95.44 55.75 98.75 84.90 88.80</cell><cell>96.18</cell><cell cols="2">48.86 77.23 3102</cell></row><row><cell>middle</cell><cell cols="8">63.99 59.11 80.93 95.33 54.74 98.71 85.42 88.93</cell><cell>96.09</cell><cell cols="2">48.91 77.21 3086</cell></row><row><cell>late</cell><cell cols="8">63.99 58.81 80.93 96.63 54.74 98.91 84.79 89.35</cell><cell>96.30</cell><cell cols="2">49.01 77.88 3303</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-1 classification accuracy and the Visual Decathlon Challenge score (S) of different soft sharing strategies.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>Work done during internship at IBM Research. This work is supported in part by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA. This work is also supported by NSF CHASE-CI #1730158.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05796</idno>
		<title level="m">Network dissection: Quantifying interpretability of deep visual representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML Workshop on Unsupervised and Transfer Learning</title>
		<meeting>ICML Workshop on Unsupervised and Transfer Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="235" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Universal representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07275</idno>
	</analytic>
	<monogr>
		<title level="m">The missing link between faces, text, planktons, and cat breeds</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Branch-specific dendritic ca2+ spikes cause persistent synaptic plasticity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cichon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-B</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">520</biblScope>
			<biblScope unit="issue">7546</biblScope>
			<biblScope unit="page" from="180" to="185" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-task selfsupervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02830</idno>
	</analytic>
	<monogr>
		<title level="m">Binarized neural networks: Training deep neural networks with weights and activations constrained to+ 1 or-1</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>The IEEE International Conference on Computer Vision (ICCV)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning</title>
		<meeting>the 28th international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crorank: cross domain personalized transfer ranking for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1204" to="1212" />
		</imprint>
	</monogr>
	<note>Data Mining Workshop</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04752</idno>
		<title level="m">A survey on methods and theories of quantized neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Labelling and optical erasure of synaptic memory traces in the motor cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hayashi-Takagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yagishita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shirai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">I</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Loshbaugh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuhlman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">525</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Deep residual learning for image recognition</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep transfer metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Depthwise separable convolutions for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03059</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5454" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Piggyback: Adding multiple tasks to a single, fixed network by learning to mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06519</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transfer learning in collaborative filtering for sparsity reduction. Pan, S</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">; J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>A survey on transfer learning</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Self-taught learning: transfer learning from unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04228</idno>
		<idno>arXiv:1801.04381</idno>
	</analytic>
	<monogr>
		<title level="m">Incremental learning through deep adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11503</idno>
		<title level="m">Convolutional networks with adaptive computation graphs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transitive invariance for selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Intl Conf. on Computer Vision (ICCV)</title>
		<meeting>of Intl Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vqs: Linking segmentations to questions and answers for supervised attention in vqa and question-focused semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multimodal keyless attention fusion for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How local is the local diversity? reinforcing sequential determinantal point processes with dynamic ground sets for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Taskonomy: Disentangling task transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3712" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
