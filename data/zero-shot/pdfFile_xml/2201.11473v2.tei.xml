<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning Like Program Executors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pi</surname></persName>
							<email>xinyupi2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<settlement>Urbana</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
							<email>liuqian@sea.com</email>
							<affiliation key="aff1">
								<orgName type="department">Sea AI Lab</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Chen</surname></persName>
							<email>beichen@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morteza</forename><surname>Ziyadi</surname></persName>
							<email>morteza.ziyadi@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Azure AI</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqi</forename><surname>Lin</surname></persName>
							<email>zeqi.lin@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Fu</surname></persName>
							<email>qifu@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
							<email>yan.gao@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
							<email>jlou@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
							<email>wzchen@microsoft.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Azure AI</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning Like Program Executors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reasoning over natural language is a longstanding goal for the research community. However, studies have shown that existing language models are inadequate in reasoning. To address the issue, we present POET, a novel reasoning pre-training paradigm. Through pretraining language models with programs and their execution results, POET empowers language models to harvest the reasoning knowledge possessed by program executors via a data-driven approach. POET is conceptually simple and can be instantiated by different kinds of program executors. In this paper, we showcase two simple instances POET-Math and POET-Logic, in addition to a complex instance, POET-SQL. Experimental results on six benchmarks demonstrate that POET can significantly boost model performance in natural language reasoning, such as numerical reasoning, logical reasoning, and multi-hop reasoning. POET opens a new gate on reasoningenhancement pre-training, and we hope our analysis would shed light on the future research of reasoning like program executors. * The first two authors contributed equally. Work done during internship at Microsoft Research Asia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Model Program</head><p>(e.g., SQL query, python code) Program Context (e.g., database, variables in python) Program Executor (e.g., MySQL, python interpreter) Sentence (e.g., question in reading comprehension) Answer Language Model Natural Context (e.g., passage in reading comprehension)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent breakthroughs in pre-training illustrate the power of pre-trained Language Models (LM) on a wide range of Natural Language (NL) tasks. Pretraining on self-supervised tasks, such as masked language modeling <ref type="bibr">(Devlin et al., 2019;</ref><ref type="bibr">He et al., 2021)</ref> using large amounts of NL sentences, boosts the language understanding of models by a large margin <ref type="bibr">(Wang et al., 2018a)</ref>. However, existing pre-training paradigms have primarily focused on language modeling and paid little attention to advanced reasoning capabilities <ref type="table" target="#tab_1">(Table 1)</ref>. As a result, though reaching near-human performance on several tasks, pre-trained LMs are still far behind expectations in reasoning-required scenarios (Rae <ref type="figure">Figure 1</ref>: Given a program context and a program as input, POET pre-trains LMs to output the execution result. After fine-tuning on downstream tasks, POET can boost LMs on reasoning-required scenarios. Explanations about program context, program, program executor and execution result can be found in ? 3. More examples of natural context and sentence are in <ref type="table" target="#tab_1">Table 1</ref>. <ref type="bibr">et al., 2021)</ref>, such as numerical reasoning <ref type="bibr">(Wallace et al., 2019;</ref><ref type="bibr">Ravichander et al., 2019)</ref> and logical reasoning <ref type="bibr">(Yu et al., 2020;</ref><ref type="bibr">Liu et al., 2020)</ref>.</p><p>To alleviate the deficiency, reconciling NL understanding in LMs and reasoning in symbolic representations, i.e., neuro-symbolic reasoning, has been a major area of interest <ref type="bibr" target="#b3">(Besold et al., 2017;</ref><ref type="bibr">Zhang et al., 2021)</ref>. With a hybrid architecture, i.e., symbolic reasoners attached to LMs, neuralsymbolic reasoning shines in a variety of reasoning tasks <ref type="bibr">(Chen et al., 2020c;</ref><ref type="bibr">Tu et al., 2020;</ref><ref type="bibr">Wolfson et al., 2020)</ref>. However, the reasoning mechanism remains in the symbolic reasoner and is not internalized into LMs, making it difficult to reuse the reasoning mechanism on unseen tasks. Meanwhile, neural models are notorious for their reliance on correlations among concrete tokens of a representation system and are usually assumed to be hard to grasp abstract rules of a symbolic reasoner <ref type="bibr">(Helwe et al., 2021;</ref><ref type="bibr">Sinha et al., 2021)</ref>  explore whether symbolic reasoning can be internalized by language models and, especially,</p><p>Can neural language models advance reasoning abilities by imitating symbolic reasoners?</p><p>Motivated by this, we conceive a new pretraining paradigm, POET (Program Executor), to investigate the learnability of language models from symbolic reasoning and transferrability across distinct representation systems. As illustrated in <ref type="figure">Figure 1</ref>, with a program (e.g., SQL query) and its program context (e.g., database) as input, the model receives automatic supervision from an established program executor (e.g., MySQL) and learns to produce correct execution result. By imitating program execution procedures, we believe LMs could potentially learn the reasoning knowledge that humans adopted to create the associated program executor and tackle NL sentences with the learned reasoning capability. This reveals the key hypothesis of POET: program executors are crystallized knowledge of formal reasoning, and such knowledge can be grasped by language models and transferred to NL reasoning via pre-training. In other words, pre-training over natural language might be a contingent condition for LMs to have better reasoning capabilities over natural language.</p><p>This contingency assumption of NL brings POET another great merit in data quality: while it is typically difficult to obtain large amounts of clean natural language sentences containing clear evidence of reasoning, synthesized programs can be made arbitrarily complicated but readily available on any scale, thanks to the artificial and compo-sitional nature of programming languages. These merits greatly facilitate the construction of highquality corpora, addressing most of the unresolved shortcomings in previous reasoning-enhancement pre-training. In other words, POET differs from existing pre-training paradigms relying on noisy NL data. In summary, our contribution is three-fold:</p><p>? We propose POET, a new pre-training paradigm for boosting the reasoning capabilities of language models by imitating program executors. Along with this paradigm, we present three exemplary across-program POET instantiations for various reasoning capabilities.</p><p>? We show with quantitative experiments that the reasoning ability our models obtains from POET pre-training is transferable to broader natural language scenarios. On six reasoningfocused downstream tasks, POET enables general-purpose language models to achieve competitive performance.</p><p>? We carry out comprehensive analytical studies, summarize insightful open questions, and provide insights for future work. We hope these insights would shed light on more research on reasoning like program executors 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Since we focus on reasoning over natural language, our work is closely related to previous work on reasoning skills in NL tasks. Regarding methods to inject reasoning skills into LMs, our method is related to two lines of work contributing to the topic: specialized models and pre-training. Last, our work is also related to program execution since we employ program executors during pre-training.</p><p>Reasoning Skills The literature focuses on reasoning skills, including numerical reasoning <ref type="bibr">(Dua et al., 2019;</ref><ref type="bibr">Li et al., 2022a)</ref>, multi-hop reasoning <ref type="bibr">(Yang et al., 2018)</ref>, reasoning in hybrid context <ref type="bibr" target="#b10">(Chen et al., 2020b;</ref><ref type="bibr">Zhu et al., 2021)</ref> and logical reasoning <ref type="bibr">(Liu et al., 2020;</ref><ref type="bibr">Yu et al., 2020)</ref>. Our work concentrates on improving the above reasoning skills, leaving the other reasoning abilities such as commonsense reasoning <ref type="bibr">(Zellers et al., 2018;</ref><ref type="bibr">Talmor et al., 2019;</ref><ref type="bibr" target="#b4">Bhagavatula et al., 2020)</ref> for future work.</p><p>Reasoning via Specialized Models Early works typically design specialized models and augment them into LMs for different types of questions <ref type="bibr">(Dua et al., 2019;</ref><ref type="bibr" target="#b0">Andor et al., 2019;</ref><ref type="bibr">Hu et al., 2019;</ref><ref type="bibr">Ding et al., 2019)</ref>. Taking Hu et al. <ref type="formula" target="#formula_1">(2019)</ref> as an example, they first predicted the answer type of a given question (e.g., "how many"), and then adopted the corresponding module (e.g., count module) to predict the answer. Although these methods work well on a specific dataset, it is challenging for them to scale to complex reasoning scenarios <ref type="bibr">(Chen et al., 2020c)</ref>. Differently, our work follows the line of reasoning via pre-training, which enjoys better scalability.  <ref type="bibr">et al., 2021)</ref>, generated by a model-based generator <ref type="bibr" target="#b2">(Asai and Hajishirzi, 2020)</ref>, or synthesized via human-designed templates <ref type="bibr">(Geva et al., 2020;</ref><ref type="bibr">Yoran et al., 2022;</ref><ref type="bibr" target="#b6">Campagna et al., 2020;</ref><ref type="bibr">Wang et al., 2022)</ref>. However, large-scale high-quality textual data involving reasoning is difficult to collect <ref type="bibr">(Deng et al., 2021)</ref>. Meanwhile, as the complexity of desired reasoning operations increases, synthesizing high-quality (e.g., fluent) NL sentences becomes more challenging. Different from the above pre-training methods relying on NL data, our pre-training is performed on programs. These programs can be synthesized at any scale with high quality, and thus are much easier to collect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning via</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reasoning in Giant Language Models</head><p>Recent works demonstrate that with proper prompting (e.g., chain-of-thoughts prompting), giant language models (e.g., GPT-3) can perform well on reasoning tasks <ref type="bibr">(Wei et al., 2022;</ref><ref type="bibr">Kojima et al., 2022;</ref><ref type="bibr">Li et al., 2022b)</ref>. For example, <ref type="bibr">Wei et al. (2022)</ref> find that giant language models have the ability to perform complex reasoning step by step with few-shot examples. Although these prompting strategies do not need further fine-tuning, the basic assumptions of them and POET are similar, i.e., it is difficult to obtain large amounts of clean sentences involving complex reasoning. However, these prompting strategies do not work well for non-giant language models, while POET is simultaneously applicable to language models ranging from millions (e.g., BART) to billions (e.g., T5-11B). It is also interesting to investigate how these prompting strategies and POET can be combined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Program Execution</head><p>We present a framework to leverage program executors to train LMs, and thus our work is close to recent work on learning a neural program executor. In this line, the most related work to ours is Liu et al. <ref type="table" target="#tab_4">(2022), which revealed  the possibility of SQL execution on helping table  pre-training. Different from them mainly focusing on table-related tasks, we present a generalized approach to include Math, Logic, and SQL, as</ref> well as their applications on many different natural language downstream tasks. Other related studies include learning program executors on visual question answering <ref type="bibr" target="#b1">(Andreas et al., 2016)</ref>, reading comprehension <ref type="bibr">(Gupta et al., 2019;</ref><ref type="bibr">Khot et al., 2021)</ref>, knowledge base question answering <ref type="bibr">(Ren et al., 2021) and</ref><ref type="bibr">3D rendering (Tian et al., 2019)</ref>. These works mainly focus on learning a neural network to represent the program executor, while ours focuses on transferring the knowledge of program executor to downstream tasks via pre-training. Other lines of research leverage program execution in inference as a reliable sanity guarantee for generated programs by pruning non-executable candidates <ref type="bibr">(Wang et al., 2018b;</ref><ref type="bibr">Chen et al., 2019b</ref><ref type="bibr" target="#b8">Chen et al., , 2021</ref><ref type="bibr">Odena et al., 2020;</ref><ref type="bibr">Ellis et al., 2019;</ref><ref type="bibr">Chen et al., 2019b;</ref><ref type="bibr">Sun et al., 2018;</ref><ref type="bibr">Zohar and Wolf, 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Reasoning Like Program Executors</head><p>Reasoning is the process where deduction and induction are sensibly applied to draw conclusions from premises or facts <ref type="bibr">(Scriven, 1976)</ref>. As a supreme feature of intelligence, humans apply reasoning across modalities. Taking numerical reasoning as an example, humans can tell how many chocolates are consumed from a math word problem description, or from a real-world event where a mother gets off work and finds the choco-can empty, aside standing their guilty-looking kids with brownish stains on their faces. Through detachment of information from their superficial modality and symbolic abstraction, humans manage to unify input formats and condense their numerical reasoning knowledge into one executable symbolic system -This is the origin of an arithmetic program executor. If a model can master these reasoning skills by imitating program executors, we believe in the possibility of transferring those reasoning skills to different modalities. In our case, we expect language models to transfer reasoning to NL-related tasks. Given this motivation, we discuss the fundamental components of POET in the rest of this section and present its instantiations later.</p><p>Program refers to a finite sequence of symbols that can be understood and executed by machines. For example, a program can be a logical form (e.g., Prolog), a piece of code (e.g., Python), or a math expression. Compared with NL sentences, programs are more formal. Each well-established program follows a specific set of grammar rules and can thus be synthesized systematically. The generalizability of POET framework is free from assumption and derived from the set of grammar rules on which a program follows. In POET, as long as a program returns meaningful output to reflect its computational procedure, it is an acceptable program.</p><p>Program Context is the environment in which a program is running, which holds numerous variables accessible to the program. These variables serve as pivot points that anchor the program context with the program. In the same sense, the question and the passage in reading comprehension hold a similar relationship. This suggests a natural analogy between the program-to-program context and the sentence-to-natural context in <ref type="figure">Figure 1</ref>.</p><p>Program Executor is a black-box software that can execute a given program within the program context. An example could be the Python interpreter that executes each line of code, with its specific input data structures as program context. For POET, program executors play the role of teachers to educate students (i.e., LMs) on reasoning knowledge they contain. POET expects program executors to deterministically execute an input program with respect to a specific program context.</p><p>Execution Result is obtained from the program executor, given a program and program context as input. It is much analogous to the answer part in NL downstream tasks. The execution result is the primary observable data reflecting the intermediate reasoning process and serves as the supervision provided by the program executor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">POET with Singleton Executors</head><p>We first instantiate POET with two singleton (i.e., a single type of reasoning capability) executors and then move on to POET with integrated executors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning from Math Calculators</head><p>The POET-Math (Left in <ref type="figure" target="#fig_0">Figure 3</ref>) aims at injecting numerical reasoning skills into LMs via learning from math calculators. Specifically, POET-Math is designed to boost the basic arithmetic skills (i.e., addition and subtraction) of LMs on downstream tasks. This arithmetic skill aligns with requirements to answer questions centered on addition / subtraction between two numbers, such as "What is the difference in casualty numbers between Bavarian and Austrian?".</p><p>Pre-training Task Given several floating-point variables as the program context and a math expression only involving addition/ subtraction as the program, the pre-training task of POET-Math is to calculate the math expression. Taking the leftmost example from <ref type="figure" target="#fig_0">Figure 3</ref>, receiving the concatenation of the program and the program context as the input, POET-Math is trained to output the number 180.7. Considering the output can be an arbitrary number, the encoder-decoder model (Lewis et al., 2020) is more suitable for this pre-training task.</p><p>Pre-training Corpus Each example in the corpus contains a math expression containing up to 2 operators and 3 variables, and a program context that contains at most 30 floating-point variables 2 . The mathematical addition and subtraction operators are denoted by + and -, respectively. The values of variables vary from 0.0 to 1000.0. By random generation, we synthesize 4 million examples as the pre-training corpus for POET-Math.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning from Logic Solvers</head><p>The POET-Logic (Mid in <ref type="figure" target="#fig_0">Figure 3</ref>) aims at injecting logical reasoning (e.g., necessary conditional reasoning) skills into LMs via learning from logic solvers. For example, taking the facts "Only if the government reinforces basic education can we improve our nation's education to a new stage.</p><p>In order to stand out among other nations, we need to have a strong educational enterprise." as premises, POET-Logic is intended to help LMs identify whether the conclusion "In order to stand out among nations, we should reinforce basic education" is necessarily implied.</p><p>Pre-training Task Given a few first-order logic premise statements as the program context and one conclusion statement as the program, the pretraining task of POET-Logic is to identify if the program is necessarily implied from the program context. The execution result, i.e., the implication relationship between the program and the program context, is either True or False. Since the output is binary, an encoder-only model <ref type="bibr">(Liu et al., 2019)</ref> is sufficient to perform this pre-training task.</p><p>Pre-training Corpus Each example in the corpus contains several premise statements and a conclusion statement. Initially, the statement collection for each example is empty. To produce it, we first allocate 5 Boolean variables (e.g., p and q in <ref type="figure">Figure</ref> 3) and randomly sample at most 8 pairs from their pairwise combinations. For each sampled pair (p, q), we randomly select a statement from the set {p ? q, p ? ? q, ? p ? ? q, ? p ? q} and add it to the collection. Once the statement collection is prepared, we randomly select a statement as the conclusion statement (i.e., program) and the rest as the premise statements (i.e., program context). Last, we employ Z3 (De Moura and Bj?rner, 2008), the well-known satisfiability modulo theory solver, as our program executor to obtain the implied result. Finally, we synthesize 1 million examples as the pre-training corpus for POET-Logic, and nearly 16% examples 3 correspond to True.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Preliminary Observation</head><p>We perform experiments on DROP and LogiQA to verify if our method improves the reasoning capability required by the dataset. As observed in <ref type="figure" target="#fig_1">Figure 4</ref>, POET-Math boosts the numerical rea- </p><formula xml:id="formula_0">Type Example SQL Program Arithmetic SELECT [COL]1 -[COL]2 Superlative SELECT MAX([COL]1) Comparative SELECT [COL]1 WHERE [COL]2 &gt; [VAL]2 Aggregation SELECT COUNT([COL]1) Union SELECT [COL]1 WHERE [COL]2 = [VAL]2 OR [COL]3 = [VAL]3 Nested SELECT [COL]1 WHERE [COL]2 IN ( SELECT [COL]2 WHERE [COL]3 = [VAL]3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">POET with Integrated Executors</head><p>POET-Math and POET-Logic each focus on one specific reasoning skill, making the pre-training task heavily dependent on the downstream task. Different from them, POET-SQL is proposed to allow LMs to master different reasoning skills simultaneously. In our implementation, POET-SQL is pre-trained with an integrated SQL executor, since we believe that SQL queries are complex enough to encompass a wide variety of computational procedures <ref type="table" target="#tab_4">(Table 2)</ref>.</p><p>Pre-training Task Given a SQL query as the program and a database as the program context, the pre-training task of POET-SQL is to mimic the query result generation. As shown on the right side of <ref type="figure">Figure 5</ref>, given the concatenation of the program and the program context, the model is pre-trained to output the query result. Since the encoder-decoder LMs can generate arbitrary tokens, they are well suited for the task. On the other hand, encoderonly LMs have insufficient expressiveness to produce out-of-context query results. To allow them to benefit from the SQL execution, we tailor the task into a query result selection task for encoderonly LMs, which only utilizes query results that can be found in the database. Specifically, the task requires encoder-only LMs to perform an IO sequence tagging process to find the query results in the database, as shown on the left side of <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random SQL Query</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S E L E C T W H E R E = ORDER BY ASC LIMIT 1</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Database</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Output</head><p>Model Output <ref type="figure">Figure 5</ref>: The illustration of POET-SQL pre-training tasks: query result selection for encoder-only and query result generation for encoder-decoder LMs.</p><p>Note that the tag I is for tokens in the query results (e.g., Athens), while O is for other tokens.</p><p>Pre-training Corpus Each example in the corpus contains a SQL query, a database, and a query result. Notably, following Liu et al. <ref type="formula" target="#formula_1">(2022)</ref>, each database is flattened into a sequence when it is fed into LMs. Meanwhile, to avoid databases being too large to fit into memory, we randomly drop the rows of large databases until their flattened sequences contain less than 450 tokens. For the query result generation task, we follow the same corpus construction strategy as described in <ref type="bibr">Liu et al. (2022</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments and Analysis</head><p>To verify the effectiveness of POET-SQL on boosting the reasoning capabilities of LMs, we first apply our method on several backbone models, including encoder-only models and encoder-decoder models. Then we conduct experiments on five typical reasoning benchmark datasets and compare POET-SQL with previous methods. Last, we perform a detailed model analysis to provide more insights. 87.4 (+2.1) 68.7 (+1.1) 81.6 (+0.5) 59.1 (+3.9) 65.9 (+3.2) -67.5 (+3.3)  we treat all datasets as generative tasks and finetune models to generate answers. As for POET-SQL RoBERTa , the fine-tuning strategies on different datasets are slightly different, and more implementation details can be found in Appendix ? C. Notably, on all datasets, our models are evaluated with official evaluation metrics EM and F1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Results</head><p>Comparing to Vanilla LMs <ref type="table" target="#tab_7">Table 3</ref> presents an apple-to-apple performance comparison between POET-SQL models and their associated vanilla LMs. Across all instances, we observe significant performance increment on downstream NL reasoning tasks. Specifically, POET-SQL equips popular encoder-only and encoder-decoder models with an integrated package of reasoning skills, effectively improving their performance on five benchmark datasets. As a highlighted example, POET-SQL BART obtains 11.5% (DROP) and 21.1% (SVAMP) improvements on EM, compared with the vanilla BART. Since POET pre-training is carried purely on program context, whereas all downstream tasks are on natural context, our hypothesis that reasoning capability is transferable from program executors to NL scenarios gets verified. margin, demonstrating the effectiveness of our proposed program execution pre-training. For example, compared with PReasM initialized from T5-Large, POET-SQL BART initialized from BART-Large exceeds it by 8.3%. Furthermore, POET that learns from a mix of program executors (i.e., POET-Math+SQL BART ) achieves a slightly better performance than the single prgoram executor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing to Previous Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Pre-training Analysis</head><p>We show part of the analysis results below due to the limited space, and more analysis can be found in Appendix ? A and ? D.</p><p>Necessity of Program Execution POET hypothesizes that the acquisition of reasoning ability by models happens at the stage of mimicking program execution, rather than program language modeling. To verify it, we ablate the program executor in POET-SQL BART and carry out a SQL language modeling pre-training instead. Practically, we mask each SQL query in the pre-training corpus of POET-SQL using the strategy adopted in BART (Lewis et al., 2020), and pre-train BART to output the complete SQL query given the masked SQL query and the database. The trivial performance variance corroborates the necessity of program execution. Task Performance (%)</p><p>RoBERTa-Large POET-SQLRoBERTa <ref type="figure">Figure 7</ref>: The performance comparison between RoBERTa-Large and POET-SQL RoBERTa on representative NLU tasks. On SQuAD and QuoRef, we report F 1 , whereas on MNLI we report accuracy.</p><p>yields better performance on the pre-training dev set, 10% (500k) data can already converge approximately to the same asymptote as the full data pretraining, showing the high data efficiency of POET. The highly plateaued curve also serves as sound evidence that execution pre-training is a data-efficient pre-training approach that converges quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Open Questions</head><p>In this section, we carry out comprehensive studies on POET, summarize interesting open questions, and provide insights for future work.</p><p>Does POET improve reasoning skills at the sacrifice of NL understanding abilities? No.</p><p>During pre-training, our models are exposed to artificial programs that are dramatically different from NL sentences, raising the concern that models may catastrophically forget their original NL understanding ability. We explore this by comparing POET-SQL RoBERTa and the vanilla RoBERTa model on tasks focusing on NL understanding, including SQuAD, MNLI and QuoRef. As can be observed in <ref type="figure">Figure 7</ref>, POET-SQL RoBERTa performs almost equally well as RoBERTa on two datasets (i.e., SQuAD and QuoRef), which suggests that POET barely sacrifices the intrinsic NL understanding ability of LMs. And the performance drop on MNLI (1.2%) is also noteworthy, which may be alleviated by joint pre-training on language modeling and our proposed program execution. More experimental details can be found in Appendix ? E.</p><p>Will POET be affected by naturalness of program context or program?</p><p>No.</p><p>An intuitive hypothesis is that the effectiveness of POET should be positively associated with the naturalness of program and program context, due to   closer learning objectives. To test this hypothesis, we design two groups of experiments: (i) Tuning the naturalness of program -we follow <ref type="bibr">Liu et al. (2022)</ref> to translate SQL queries into NL sentences to make a more natural program, and replace SQL reserved keywords with low-frequency tokens to make a more unnatural program. (ii) Tuning the naturalness of program context -we follow <ref type="bibr" target="#b9">Chen et al. (2019a)</ref> to convert each database in POET-SQL pretraining into a set of NL sentences. Surprisingly, results in <ref type="table" target="#tab_12">Table 5</ref> provide counter-evidence to the intuitive hypothesis, since tuning the naturalness of program or program context do not significantly impact POET effectiveness. For example, unnatural program only leads to a slight decrease in DROP EM from 77.7% to 76.9%. It also indictaes that the model learns certain abstract reasoning capabilities rather than lexical associations.</p><p>Does pre-training on NL reasoning benefit model learning on program execution? Yes.</p><p>If reasoning ability can be transferred from program execution to NL reasoning tasks in POET, then the reversed process of POET may also work, i.e., models pre-trained with NL reasoning would have better learnability on program execution. To test this speculation, we compare the behavioral difference of vanilla BART and BART pre-trained on DROP in terms of learning SQL execution in <ref type="figure">Figure</ref>   to characterize the behavior of LMs on the SQL execution task: execution accuracy and perplexity, and the execution accuracy always goes higher when the perplexity goes lower. Here the perplexity is presented because it is smoother compared to the execution accuracy, which is either 100% or 0%. Parallel with our expectation, pre-training on DROP leads to observably lower perplexity for SQL execution learning on both the train and dev sets. The bidirectional enhancement suggests some relative independence between reasoning mechanisms and their symbolic representations.</p><p>Can POET boost reasoning abilities of giant pre-trained language models? Yes.</p><p>Recent work suggest that giant LMs excel at reasoning <ref type="bibr">(Brown et al., 2020)</ref>, so we are curious if POET is effective for them. Following the same procedure as in ? 6, we apply POET-SQL to T5-11B, one of the largest publicly available LMs. As shown in <ref type="table" target="#tab_14">Table 6</ref>, albeit not as shining as in cases of smaller LMs, POET still succeeds in boosting numerical reasoning abilities of giant LMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion &amp; Future Work</head><p>We introduce POET, a new pre-training paradigm for boosting reasoning capability of language models via imitating program executors. Experimental results on six datasets demonstrate that POET can significantly boost existing language models on several reasoning skills, including numerical, logical and multi-hop reasoning. Our best language model under POET can reach highly competitive performance with previous specialized models. In the future, we hope our work could inspire more transference of reasoning knowledge from program executors to models. And we will also investigate the causes of the reasoning transfer with more insightful experiments, since we still do not know how the reasoning transfer occurs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The first limitation of our approach is that it has a relatively strong coupling between the reasoning skills learned in the pre-training task and the reasoning skills required for the downstream task. In other words, POET expects reasoning abilities of the program executor to overlap with the downstream reasoning requirements to make the execution learning transferable. Such an expectation also applied fo POET-SQL, although it allows LM to master different reasoning skills at the same time. For example, when ablating all programs involving math operations from the pre-training corpus of POET-SQL, it shows poor performance on DROP. The second limitation is that POET still employs instantiated program templates rather than probabilistic context-free grammars to synthesize programs. The latter usually offers a more diverse range of programs that may contribute to the generalization of the pre-trained language models, but are often more complex. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Program Context Analysis</head><p>POET emphasizes the importance of program context for reasoning transferability, owing to the analogy between the program to program context and the sentence to natural context drawn in <ref type="figure">Figure 1</ref>.</p><p>To investigate it, we explore the effect of different program context design choices on reasoning transferability by conducting experiments on welldesigned POET-Math variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 The Necessairty of Program Context</head><p>To verify the necessairty of program context, we experiment with POET-Math without program context, i.e. a variable-free POET-Math variant whose program context is empty. Taking the example  of POET-Math in <ref type="figure" target="#fig_0">Figure 3</ref>, the program is transformed into 152.0 + 99.0 -70.3. The experimental results are shown in <ref type="table" target="#tab_17">Table 7</ref>. One can see that there is a dramatic performance drop in the variant compared to POET-Math, verifying the importance of program context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The Variables Design in Program Context</head><p>In the pre-training task of POET-Math, the program context is several floating-point variables. These variables include necessary variables (i.e., variables required by the program) and irrelevant variables. The irrelevant variables exist to make the program context closer to the natural context which generally contains irrelevant sentences. For example, given the program a + b and the program context a = 1; b = 2; c = 3; d = 4;, variables c and d are what we refer to as irrelevant variables. This is motivated by the fact that passages are usually full of irrelevant information regarding a specific question in NL downstream tasks. In this section, we explore impacts on pretraining effectiveness brought by numbers of irrelevant variables. Empirically, we experiment on pre-training with 0, 10, 30 irrelevant variables. The total length of 30 irrelevant variables approaches the maximum input length of pre-trained LMs, and thus we do not try more settings. The experimental results are shown in <ref type="table" target="#tab_17">Table 7</ref>. As observed, (i) models can still learn numerical reasoning during pre-training where the program context is free from irrelevant variables, though less effective. (ii) the setting of 30 irrelevant variables brings BART-Large more performance improvement than the setting of 10 irrelevant variables. Considering there are plenty of lengthy passages in the DROP dataset, we therefore hypothesize that the noise level brought by irrelevant variables in the program context during pre-training should be made closer with the counterpart in the natural context during fine-tuning.   HotpotQA An extractive reading comprehension dataset that requires models to perform multi-hop reasoning over different passages <ref type="bibr">(Yang et al., 2018)</ref>. It contains two settings (i) Distractor: reasoning over 2 gold paragraphs along with 8 similar distractor paragraphs and (ii) Full wiki: reasoning over customized retrieval results from full Wikipedia passages. We experiment with its distractor setting since retrieval strategy is beyond our focus in this work.</p><p>TAT-QA A question answering benchmark to measure reasoning ability over hybrid context, i.e., passages and tables <ref type="bibr">(Zhu et al., 2021)</ref>. It is curated by combing paragraphs and tables from real-world financial reports. According to the source(s) the answers are derived from, the dataset can be divided into three subsets: EQUATE The first benchmark dataset to explore quantitative reasoning under the task of natural language inference <ref type="bibr">(Ravichander et al., 2019)</ref>. As a test-only dataset, it requires fine-tuned models on MNLI to perform zero-shot natural language inference tasks over quantitative statements described in (premise, hypothesis) pairs to reach final entailment decisions.</p><p>LogiQA A multi-choice reading comprehension dataset that evaluates the logical reasoning ability, whose questions are designed by domain experts <ref type="bibr">(Liu et al., 2020)</ref>. It contains four types of logical reasoning, including categorical reasoning, disjunctive reasoning, conjunctive reasoning and conditional reasoning.</p><p>SVAMP A challenging math word problem dataset <ref type="bibr">(Patel et al., 2021)</ref>. It is designed specifically to hack models who leverage spurious patterns to perform arithmetic operations without true understanding of context. We only keep addition and subtraction problems in accordance with our pre-training coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Baseline Setup</head><p>We summarize the baseline methods in short below, and refer readers to their papers for more details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POET-Math</head><p>The pre-training procedure lasts for 10, 000 steps with a batch size of 512. After the warm up in the first 2000 steps, the learning rate arrives the peak at 3?10 ?5 during pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>POET-Logic</head><p>The pre-training procedure lasts for 5, 000 steps with a batch size of 512. After the warm up in the first 1000 steps, the learning rate arrives the peak at 3?10 ?5 during pre-training.</p><p>POET-SQL For POET-SQL BART and POET-SQL RoBERTa , the pre-training procedure lasts for 50, 000 steps with a batch size of 512. After the warm up in the first 5000 steps, the learning rate arrives the peak at 3?10 ?5 during pre-training. To save memory, each example in the pre-training corpus could at most contains 512 tokens. For POET-SQL T5 , the pre-training procedure lasts for 20, 000 steps with a batch size of 512. After the warm up in the first 2000 steps, the learning rate arrives the peak at 1?10 ?5 during pre-training. The maximum input length in each example is truncated to 384 tokens to increase the batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Fintuning Details</head><p>We implement our models based on transform-    <ref type="bibr">2020)</ref>. Therefore, we employ these design strategies on DROP and SVAMP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Fine-tuning Hyperpameters</head><p>By default, we apply AdamW as fine-tuning optimizer with default scheduling parameters on all datasets. To ensure statistical significance, all finetuning procedures are run with three random seeds, except for T5-11B and POET-SQL T5 due to the limit of computation budgets.</p><p>DROP POET-SQL RoBERTa and RoBERTa-Large are trained with the subset of questions marked as "span" from the DROP dataset.t Since a gold answer may occur multiple times in the passage, we optimize over the sum of negative log probability for all possibly-correct IO sequences where each one of gold answers is included at least once,  as done in <ref type="bibr">Segal et al. (2020)</ref>. The fine-tuning procedure runs up to 25, 000 steps with a batch size of 64, with the learning rate of 7.5?10 ?6 . As for BART-Large (and POET-SQL BART , POET-Math, the same below) and T5-11B (and POET-SQL T5 , the same below), they are trained with the whole DROP dataset. For BART-Large, the fine-tuning procedure runs up to 20, 000 steps with a batch size as 128 and a learning rate as 3?10 ?5 . For T5-11B, due to the computational budget, the fine-tuning procedure only lasts for 10, 000 steps with a batch size of 32, and the learning rate is 1?10 ?5 .</p><p>TAT-QA In the experiment of TAT-QA, we employ the official implementation and the default hyperparameters provided in TAGOP 5 . The finetuning procedure runs up to 50 epochs with a batch size of 48. For modules introduced in TAGOP, the learning rate is set as 5?10 ?4 , while for RoBERTa-Large (and POET-SQL RoBERTa ), the learning rate is set as 1.5?10 ?5 .</p><p>HotpotQA The fine-tuning procedure runs up to 30, 000 steps with a batch size of 64. The learning rate is 1?10 ?5 . Overlong inputs are truncated to 512 tokens for both RoBERTa-Large (and POET-SQL RoBERTa ), T5-11B (and POET-SQL T5 ) and BART-Large (and POET-SQL BART ).</p><p>EQUATE The fine-tuning procedure runs up to 20, 000 steps on MNLI with a batch size of 128 for both RoBERTa-Large (and POET-SQL RoBERTa ) and BART-Large (and POET-SQL BART ), with learning rate is 1?10 ?5 . After fine-tuning, models are directly evaluated on EQUATE.</p><p>LogiQA In the experiment of LogiQA, we employ the open-source implementation and the default hyperparameters provided in ReClor 6 (Yu et al., 2020) to fine-tune RoBERTa-Large (and POET-SQL RoBERTa ). The fine-tuning procedure 5 https://github.com/NExTplusplus/TAT-QA 6 https://github.com/yuweihao/reclor runs up to 10 epochs with a batch size of 24. The learning rate is set as 1?10 ?5 .</p><p>D Fine-grained Analysis DROP In <ref type="table" target="#tab_24">Table 9</ref> we report model F 1 scores by question type on DROP. Comparing three POET pre-trained models with their vanilla versions, we observe that: (i) POET-SQL BART outperforms the vanilla BART-large with a wide margin in all types of questions, i.e. number (15.3%), date (9.8%), span (around 5%). (ii) POET-SQL RoBERTa only deals with span selection questions, and obtain 1.9%, 3.2% gain on span, spans questions, respectively. (iii) For the giant POET-SQL T5 , we also observe 2% improvement on number questions, 2.2% on span and 0.8% on spans questions. These model-agnostic performance boost on DROP reveals the extra numerical reasoning knowledge models learned from SQL program executors.</p><p>EQUATE <ref type="table" target="#tab_1">Table 10</ref> presents performance breakdown by subsets of EQUATE <ref type="bibr">(Ravichander et al., 2019)</ref>, where we compare POET-SQL BART and POET-SQL RoBERTa with their vanilla versions and previous baselines. For both models, we observe around 10% acc improvement on the NR ST subset, where numerical comparison and quantifiers are especially emphasized. Stable performance improvement was also observed in both pre-trained models on the RTE-Q subset, where arithmetics and ranges are primary focus. Interestingly, POET-SQL RoBERTa alone demonstrate improvement on RedditNLI (emphasizes approximation and verbal quantitative reasoning) subset. Performance on other subsets are approximately comparable between POET pre-trained models and vanilla models, suggesting that POET does not harm intrinsic abilities of language models.</p><p>TAT-QA  only performed on table-like texts (i.e., the flatten sequence of databases), it is highly non-trivial for our model to generalize to such a hybrid scenario containing both tables and passages, again illustrating the transferability of reasoning capabilities.  <ref type="table" target="#tab_1">Table 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E NL Understanding Performance</head><p>Implementation Details (i) On SQuAD, we cast the span selection task as a sequence tagging problem following <ref type="bibr">Segal et al. (2020)</ref>. (ii) On MNLImatched, we train both models to perform sequence classification on concatenated premise-hypothesis pairs. (iii) On Quoref, we cast the span(s) selection task as an IO sequence tagging problem following Segal et al. <ref type="bibr">(2020)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The illustration of POET-Math and POET-Logic. During pre-training, the concatenation of program and program context are fed into language model and the model is expected to output result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Fine-tuning EM performance [%] of different models on DROP (blue) and LogiQA (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>The train and dev perplexity of vanilla BART and BART pre-trained on DROP (BART+DROP) on the pre-training corpus of POET-SQL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Dataset Details We fine-tune POET-SQL RoBERTa on (i) SQuAD v1.0: (Rajpurkar et al., 2016): one of the most classical single-span selection RC benchmarks measuring understanding over natural language context; (ii) MNLI (Williams et al., 2018): a large-scale NLI dataset measuring cross-domain and cross-genre generalization of NLU. Notably, our model is evaluated on the matched setting for the purpose of simplicity. (iii) QuoRef (Dasigi et al., 2019): A Wikipedia-based multi-span selection RC benchmark with a special emphasis on coreference resolution. All dataset Statistics are shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. This drives us to arXiv:2201.11473v2 [cs.CL] 22 Oct 2022 What is the difference in casualty numbers between Bavarian and Austrian? Passage: [DOC] The popular uprising included large areas of . . . One employee supervises another who gets more salary than himself. Fact: [DOC] David, Jack and Mark are colleagues in a company. David supervises Jack, and Jack supervises Mark. David gets more . . . At which university does the biographer of John Clare teach English Literature? Passage: [DOC] John Clare : John Clare was an English poet . . . [DOC] CMS College Kottayam : The CMS College is one . . . What was the percentage change in gaming between 2018 and 2019? Context: [TAB] Server products and cloud services | 32, 622 | 26, 129 . . . [DOC] Our commercial cloud revenue, which includes Office . . . Teva earns $7 billion a year. Premise: After the deal closes, Teva will generate sales of about $7 billion a year, the company said.</figDesc><table><row><cell>Type</cell><cell>Example</cell><cell>Dataset</cell><cell>Task</cell></row><row><cell>Numerical</cell><cell cols="2">Question: DROP (Dua</cell><cell>Reading Comprehension</cell></row><row><cell></cell><cell></cell><cell>et al., 2019)</cell><cell>(RC)</cell></row><row><cell>Logical</cell><cell cols="2">Conclusion: LogiQA (Liu</cell><cell>Reading Comprehension</cell></row><row><cell></cell><cell></cell><cell>et al., 2020)</cell><cell>(RC)</cell></row><row><cell>Multi-hop</cell><cell cols="2">Question: HotpotQA (Yang et al.,</cell><cell>Reading Comprehension (RC)</cell></row><row><cell></cell><cell></cell><cell>2018)</cell><cell></cell></row><row><cell>Hybrid</cell><cell cols="2">Question: TAT-QA (Zhu</cell><cell>Question Answering</cell></row><row><cell></cell><cell></cell><cell>et al., 2021)</cell><cell>(QA)</cell></row><row><cell cols="3">Quantitative Hypothesis: EQUATE</cell><cell>Natural Language Infer-</cell></row><row><cell></cell><cell></cell><cell>(Ravichander</cell><cell>ence (NLI)</cell></row><row><cell></cell><cell></cell><cell>et al., 2019)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The demonstration of five representative reasoning types. Listed are the types, the example questions, the representative dataset, and their corresponding tasks.[DOC] and [TAB] indicates the start of a passage and a semi-structured table respectively. Here we regard Question , Conclusion and Hypothesis as sentence, and Passage , Fact , Context and Premise as natural context inFigure 1.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The six typical SQL programs that require reasoning. Listed are the type and the example SQL programs. [COL] and [VAL] represent the table column and the table cell value, respectively.</figDesc><table /><note>soning ability of BART, bringing in 9.0% EM gain on DROP. Meanwhile, POET-Logic improves the logical reasoning skills of RoBERTa, resulting in a 2.2% EM improvement on LogiQA. These sig- nificant improvements support our main claim that reasoning knowledge of program executors can be transferred to NL scenarios via pre-training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>(+11.5) 80.6 (+11.4) 66.5 (+0.9) 79.7 (+0.8) 41.5 (+2.7) 49.6 (+2.9) 33.5 (+21.1) 66.5 (+3.9) POET-SQLRoBERTa 79.8 (+1.7)</figDesc><table><row><cell>Models</cell><cell></cell><cell>DROP ?</cell><cell></cell><cell>HotpotQA ?</cell><cell></cell><cell>TAT-QA ?</cell><cell>SVAMP</cell><cell>EQUATE</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>EM</cell></row><row><cell>BART-Large</cell><cell>66.2</cell><cell>69.2</cell><cell>65.6</cell><cell>78.9</cell><cell>38.8</cell><cell>46.7</cell><cell>12.4</cell><cell>62.6</cell></row><row><cell cols="2">POET-SQLBART 77.7 RoBERTa-Large 78.1</cell><cell>85.3</cell><cell>67.6</cell><cell>81.1</cell><cell>55.2</cell><cell>62.7</cell><cell>-</cell><cell>64.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The main experimental results of different backbone models on test sets and dev sets (?) of datasets with or without our POET-SQL. The results of POET are significantly better than the vanilla LMs (p &lt; 0.05). Note the performance of RoBERTa and POET-SQL RoBERTa are reported on the subset of DROP where the answer is span(s).</figDesc><table><row><cell>Dataset</cell><cell>Models</cell><cell>EM</cell><cell>F1</cell><cell>Dataset</cell><cell>Models</cell><cell>EM</cell><cell>F1</cell></row><row><cell></cell><cell cols="2">Specialized Models</cell><cell></cell><cell></cell><cell>Specialized Models</cell><cell></cell><cell></cell></row><row><cell></cell><cell>NumNet (Ran et al., 2019)</cell><cell>64.9</cell><cell>68.3</cell><cell></cell><cell>DFGN (Qiu et al., 2019)</cell><cell>55.7</cell><cell>69.3</cell></row><row><cell></cell><cell>MTMSN (Hu et al., 2019)</cell><cell>76.7</cell><cell>80.5</cell><cell></cell><cell>SAE (Tu et al., 2020)</cell><cell>67.7</cell><cell>80.8</cell></row><row><cell></cell><cell>NeRd (Chen et al., 2020c)</cell><cell>78.6</cell><cell>81.9</cell><cell></cell><cell>C2F Reader (Shao et al., 2020)</cell><cell>68.0</cell><cell>81.2</cell></row><row><cell></cell><cell>NumNet+ (Ran et al., 2019)</cell><cell>81.1</cell><cell>84.4</cell><cell></cell><cell>HGN (Fang et al., 2020)</cell><cell cols="2">69.2 82.2</cell></row><row><cell>DROP ?</cell><cell cols="3">QDGAT (Chen et al., 2020a) 84.1 87.1 Language Models</cell><cell>HotpotQA ?</cell><cell>Language Models BERT (Devlin et al., 2019)</cell><cell>59.1</cell><cell>73.4</cell></row><row><cell></cell><cell>T5 (Yoran et al., 2022)</cell><cell>61.8</cell><cell>64.6</cell><cell></cell><cell>ReasonBERT (Deng et al., 2021)</cell><cell>64.8</cell><cell>79.2</cell></row><row><cell></cell><cell cols="2">GenBERT (Geva et al., 2020) 68.8</cell><cell>72.3</cell><cell></cell><cell>POET-SQLBART</cell><cell>66.5</cell><cell>79.7</cell></row><row><cell></cell><cell>PReasM (Yoran et al., 2022)</cell><cell>69.4</cell><cell>72.3</cell><cell></cell><cell>SpanBERT (Joshi et al., 2020)</cell><cell>67.4</cell><cell>81.2</cell></row><row><cell></cell><cell>POET-SQLBART</cell><cell>77.7</cell><cell>80.6</cell><cell></cell><cell>POET-SQLRoBERTa</cell><cell cols="2">68.7 81.6</cell></row><row><cell></cell><cell>POET-Math+SQLBART</cell><cell cols="2">78.0 80.9</cell><cell></cell><cell>BERT (Devlin et al., 2019)</cell><cell>51.8</cell><cell>-</cell></row><row><cell></cell><cell>TAPAS (Herzig et al., 2020)</cell><cell>18.9</cell><cell>26.5</cell><cell></cell><cell>GPT (Radford et al., 2019)</cell><cell>55.8</cell><cell>-</cell></row><row><cell>TAT-QA ?</cell><cell>NumNet+ (Ran et al., 2019) TAGOP (Zhu et al., 2021)</cell><cell>38.1 55.2</cell><cell>48.3 62.7</cell><cell>EQUATE</cell><cell cols="2">QREAS (Ravichander et al., 2019) 60.7 POET-SQLBART 66.5</cell><cell>--</cell></row><row><cell></cell><cell>TAGOP + POET-SQLRoBERTa</cell><cell cols="2">59.1 65.9</cell><cell></cell><cell>POET-SQLRoBERTa</cell><cell>67.5</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>The comparison of our models with baselines on test sets and dev sets (?) of different datasets. LMs used by all baselines are in Large size, except for ReasonBERT. Bold numbers indicate the best results.Dataset SetupWe perform experiments on different datasets including DROP, HotpotQA, TAT-QA, and EQUATE.Table 1shows examples of these datasets and their corresponding reasoning types. Furthermore, SVAMP (Patel et al., 2021), the challenging diagnostic dataset for probing numerical reasoning, is employed in our experiments to test the generalization capability of our finetuned models on DROP. We evalute models on their addition and subtraction subsets. More details about datasets can be found in Appendix ? B.Backbone Model RoBERTa (Liu et al., 2019) is elected as the backbone in encoder-only LMs, while BART (Lewis et al., 2020) is chosen as the backbone in encoder-decoder LMs. Afterward, We mark the RoBERTa model and the BART model trained under POET as POET-SQL RoBERTa and POET-SQL BART , respectively. For POET-SQL BART ,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>lists</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Effect of the Pre-training illustrates the pre-training and downstream performance with different pre-training steps and scales. It can be seen that both pre-training and downstream performance gradually increase towards the asymptote with increasing pre-training steps, while extra pre-training data steadily accelerate the convergence rate. Although a larger scale</figDesc><table><row><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>90</cell><cell>93.3</cell><cell>93.2</cell><cell>90.8</cell><cell>89.6</cell><cell>84.9</cell><cell>84.7</cell></row><row><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">SQuAD</cell><cell cols="2">MNLI</cell><cell cols="2">QuoRef</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Tuning Program Context ? w. Natural program context 76.5 79.0</figDesc><table><row><cell>Settings</cell><cell>EM</cell><cell>F1</cell></row><row><cell>POET-SQLBART</cell><cell cols="2">77.7 80.6</cell></row><row><cell>Tuning Program</cell><cell></cell><cell></cell></row><row><cell>? w. Nnatural program</cell><cell cols="2">77.2 79.9</cell></row><row><cell>? w. Unnatural program</cell><cell cols="2">76.9 79.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>The EM and F 1 of POET-SQL BART on the DROP dev set with respect to different naturalness of program and program context.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>8. There are two indicators that can be used POET-SQLT5 85.2 (+1.7) 87.6 (+1.7) 57.4 (+4.5)</figDesc><table><row><cell>Models</cell><cell></cell><cell>DROP ?</cell><cell>SVAMP</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell></row><row><cell>T5-11B</cell><cell>83.5</cell><cell>85.9</cell><cell>52.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 6 :</head><label>6</label><figDesc>The experimental results of T5-11B and POET-SQL T5 on test sets and dev sets (?) of different datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>HybridQA: A dataset of multi-hop question answering over tabular and textual data. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1026-1036, Online. Association for Computational Linguistics. Human Language Technologies, pages 5848-5855, Online. Association for Computational Linguistics. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT: Improving pre-training by representing and predicting spans. Transactions of the Association for Computational Linguistics, 8:64-77. Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis &amp; insights from training gopher. CoRR, abs/2112.11446. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392, Austin, Texas. Association for Computational Linguistics. Prasanna Parthasarathi, Joelle Pineau, and Adina Williams. 2021. UnNatural Language Inference. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), ArXiv, abs/1807.03100. Shuohang Wang, Mo Yu, Jing Jiang, and Shiyu Chang. 2018c. A co-matching model for multi-choice reading comprehension. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 746-751, Melbourne, Australia. Association for Computational Linguistics. Zihang Jiang, Yanfei Dong, and Jiashi Feng. 2020. Reclor: A reading comprehension dataset requiring logical reasoning. In International Conference on Learning Representations (ICLR).</figDesc><table><row><cell>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2020c. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading compre-hension. In International Conference on Learning Representations. Xinyun Chen, Chang Liu, and Dawn Song. 2019b. Execution-guided neural program synthesis. In International Conference on Learning Representa-tions. Pradeep Dasigi, Nelson F. Liu, Ana Marasovi?, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions re-quiring coreferential reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5925-5932, Hong Kong, China. Association for Computational Linguistics. Xiang Deng, Yu Su, Alyssa Lees, You Wu, Cong Yu, and Huan Sun. 2021. ReasonBERT: Pre-trained to reason with distant supervision. In Proceedings of the 2021 Conference on Empirical Methods in Natu-ral Language Processing, pages 6112-6127, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under-standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota. Associ-ation for Computational Linguistics. Ming Ding, Chang Zhou, Qibin Chen, Hongxia Yang, and Jie Tang. 2019. Cognitive graph for multi-hop reading comprehension at scale. In Proceedings of the 57th Annual Meeting of the Association for Com-putational Linguistics, pages 2694-2703, Florence, Italy. Association for Computational Linguistics. Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requir-ing discrete reasoning over paragraphs. In Proceed-ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1 Tushar Khot, Daniel Khashabi, Kyle Richardson, Pe-ter Clark, and Ashish Sabharwal. 2021. Text mod-ular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Hu-man Language Technologies, pages 1264-1279, On-line. Association for Computational Linguistics. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-taka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916. Mike Lewis, Yinhan Liu, Naman Goyal, Mar-jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th An-nual Meeting of the Association for Computational Linguistics, pages 7871-7880, Online. Association for Computational Linguistics. Moxin Li, Fuli Feng, Hanwang Zhang, Xiangnan He, Fengbin Zhu, and Tat-Seng Chua. 2022a. Learn-ing to imagine: Integrating counterfactual thinking in neural discrete reasoning. In Proceedings of the 60th Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages 57-69, Dublin, Ireland. Association for Computa-tional Linguistics. Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. 2022b. On the advance of making language models better reason-ers. arXiv preprint arXiv:2206.02336. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: A challenge dataset for machine reading comprehen-sion with logical reasoning. In Proceedings of the Twenty-Ninth International Joint Conference on Ar-tificial Intelligence, IJCAI-20, pages 3622-3628. In-ternational Joint Conferences on Artificial Intelli-gence Organization. Main track. Qian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi Lin, Weizhu Chen, and Jian-Guang Lou. 2022. TAPEX: Table pre-training via learning a neural SQL executor. In International Conference on Learning Representations. Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu. 2019. NumNet: Machine reading comprehen-sion with numerical reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Nat-ural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2474-2484, Hong Kong, China. Association for Computational Linguistics. Abhilasha Ravichander, Aakanksha Naik, Carolyn Rose, and Eduard Hovy. 2019. EQUATE: A bench-mark evaluation framework for quantitative reason-ing in natural language inference. In Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pages 349-361, Hong Kong, China. Association for Computational Lin-guistics. Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure Leskovec, and Denny Zhou. 2021. Lego: La-tent execution-guided reasoning for multi-hop ques-tion answering on knowledge graphs. In ICML. Michael Scriven. 1976. Reasoning. New York: McGraw-Hill. Elad Segal, Avia Efrat, Mor Shoham, Amir Globerson, and Jonathan Berant. 2020. A simple and effec-tive model for answering multi-span questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3074-3080, Online. Association for Computa-tional Linguistics. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units. CoRR, abs/1508.07909. Nan Shao, Yiming Cui, Ting Liu, Shijin Wang, and Guoping Hu. 2020. Is Graph Structure Necessary for Multi-hop Question Answering? In Proceed-ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7187-7192, Online. Association for Computational Linguistics. Tianze Shi, Chen Zhao, Jordan Boyd-Graber, Hal Daum? III, and Lillian Lee. 2020. On the poten-tial of lexico-logical alignments for semantic pars-ing to SQL queries. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1849-1864, Online. Association for Computational Linguistics. Wei, Zhihao Fan, Daxin Jiang, Ming Zhou, and Nan Duan. 2022. Logic-driven context extension and data augmentation for logical reasoning of text. In Findings of the Association for Computational Lin-guistics: ACL 2022, pages 1619-1629, Dublin, Ire-land. Association for Computational Linguistics. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903. Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sen-tence understanding through inference. In Proceed-ings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112-1122, New Orleans, Louisiana. Association for Computational Linguis-tics. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans-formers: State-of-the-art natural language process-ing. In Proceedings of the 2020 Conference on Em-pirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Asso-ciation for Computational Linguistics. ner, Yoav Goldberg, Daniel Deutch, and Jonathan Berant. 2020. Break it down: A question under-standing benchmark. Transactions of the Associa-tion for Computational Linguistics, 8:183-198. Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christo-pher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer-ing. In Proceedings of the 2018 Conference on Em-pirical Methods in Natural Language Processing, pages 2369-2380, Brussels, Belgium. Association for Computational Linguistics. Ori Yoran, Alon Talmor, and Jonathan Berant. 2022. Turning tables: Generating examples from semi-structured tables for endowing language models with reasoning skills. In Proceedings of the 60th Annual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages decoding. Siyuan Wang, Wanjun Zhong, Duyu Tang, Zhongyu (Long and Short Papers), pages 2368-2378, Min-neapolis, Minnesota. Association for Computational Linguistics. Kevin Ellis, Maxwell I. Nye, Yewen Pu, Felix Sosa, Josh Tenenbaum, and Armando Solar-Lezama. 2019. Write, execute, assess: Program synthesis with a REPL. In Advances in Neural Information Process-ing Systems 32: Annual Conference on Neural Infor-mation Processing Systems 2019, NeurIPS 2019, De-cember 8-14, 2019, Vancouver, BC, Canada, pages 9165-9174. Yuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuo-hang Wang, and Jingjing Liu. 2020. Hierarchical graph network for multi-hop question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8823-8838, Online. Association for Computa-tional Linguistics. Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguis-tics, pages 946-958, Online. Association for Com-putational Linguistics. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations. Chadi Helwe, Chlo? Clavel, and Fabian M. Suchanek. 2021. Reasoning with transformer-based models: Deep learning, but shallow reasoning. In 3rd Confer-ence on Automated Knowledge Base Construction. Jonathan Herzig, Pawel Krzysztof Nowak, Thomas M?ller, Francesco Piccinno, and Julian Eisenschlos. 2020. TaPas: Weakly supervised table parsing via pre-training. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin-guistics, pages 4320-4333, Online. Association for Computational Linguistics. Minghao Hu, Yuxing Peng, Zhen Huang, and Dong-sheng Li. 2019. A multi-type multi-span network for reading comprehension that requires discrete rea-soning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-ral Language Processing (EMNLP-IJCNLP), pages 1596-1606, Hong Kong, China. Association for Computational Linguistics. 6016-6031, Dublin, Ireland. Association for Com-putational Linguistics. Koustuv Sinha, pages 7329-7346, Online. Association for Computa-tional Linguistics. Shao-Hua Sun, Hyeonwoo Noh, Sriram Somasun-daram, and Joseph Lim. 2018. Neural program syn-thesis from diverse demonstration videos. In Inter-national Conference on Machine Learning, pages 4790-4799. PMLR. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: A ques-tion answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149-4158, Minneapolis, Minnesota. Associ-ation for Computational Linguistics. Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin El-lis, William T. Freeman, Joshua B. Tenenbaum, and Jiajun Wu. 2019. Learning to infer and execute 3d shape programs. Ming Tu, Kevin Huang, Guangtao Wang, Jing Huang, Xiaodong He, and Bowen Zhou. 2020. Select, an-swer and explain: Interpretable multi-hop reading comprehension over multiple documents. In The Thirty-Fourth AAAI Conference on Artificial Intelli-gence, AAAI 2020, The Thirty-Second Innovative Ap-plications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 9073-9080. AAAI Press. Eric Wallace, Yizhong Wang, Sujian Li, Sameer Singh, and Matt Gardner. 2019. Do NLP models know numbers? probing numeracy in embeddings. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Lan-guage Processing (EMNLP-IJCNLP), pages 5307-5315, Hong Kong, China. Association for Computa-tional Linguistics. Alex Wang, Amanpreet Singh, Julian Michael, Fe-lix Hill, Omer Levy, and Samuel Bowman. 2018a. GLUE: A multi-task benchmark and analysis plat-form for natural language understanding. In Pro-ceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Net-works for NLP, pages 353-355, Brussels, Belgium. Association for Computational Linguistics. Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi Xin Mao, Oleksandr Polozov, and Rishabh Singh. 2018b. Robust text-to-sql generation with execution-guided Weihao Yu, Rowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. SWAG: A large-scale adversar-ial dataset for grounded commonsense inference. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93-104, Brussels, Belgium. Association for Computa-tional Linguistics. Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. 2021. Neural, symbolic and neural-symbolic reasoning on knowledge graphs. AI Open, 2:14-35. Victor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2SQL: Generating structured queries from natural language using reinforcement learning. arXiv, abs/1709.00103. Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A question answer-ing benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th An-nual Meeting of the Association for Computational Linguistics and the 11th International Joint Confer-ence on Natural Language Processing (Volume 1: Long Papers), pages 3277-3287, Online. Associa-tion for Computational Linguistics. Amit Zohar and Lior Wolf. 2018. Automatic program synthesis of long programs with a learned garbage collector. In Advances in Neural Information Pro-cessing Systems 31: Annual Conference on Neu-ral Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr?al, Canada, Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-pages 2098-2107.</cell></row></table><note>Leonardo De Moura and Nikolaj Bj?rner. 2008. Z3: An efficient smt solver. In International conference on Tools and Algorithms for the Construction and Analysis of Systems, pages 337-340. Springer.Nitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2019. Neural module networks for reasoning over text. CoRR, abs/1912.04971.Yinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. 2021. DAGN: Discourse-aware graph network for logical reasoning. In Proceedings of the 2021 Conference of the North American Chap- ter of the Association for Computational Linguistics:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>POET-Math with 10 irrelevant variables 74.6 77.5 POET-Math with 30 irrelevant variables 75.2 78.1</figDesc><table><row><cell>Models</cell><cell>EM</cell><cell>F1</cell></row><row><cell>BART-Large</cell><cell cols="2">66.2 69.2</cell></row><row><cell>POET-Math without program context</cell><cell cols="2">67.4 70.5</cell></row><row><cell>POET-Math with 0 irrelevant variable</cell><cell cols="2">71.5 74.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 7 :</head><label>7</label><figDesc>The DROP performance with different numbers of irrelevant variables in POET-Math pre-training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 8 :</head><label>8</label><figDesc>The statistics of our experimental datasets.</figDesc><table><row><cell>B Experimental Setup</cell></row><row><cell>B.1 Dataset Setup</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>presents some statistics about our experi-</cell></row><row><cell>mental datasets. Below we introduce each dataset</cell></row><row><cell>in detail.</cell></row><row><cell>DROP A reading comprehension benchmark to</cell></row><row><cell>measure numerical reasoning ability over a given</cell></row><row><cell>passage (Dua et al., 2019). It contains three sub-</cell></row><row><cell>sets of questions: span, number, and date, each</cell></row><row><cell>of which involves a lot of numerical operations.</cell></row><row><cell>Unlike traditional reading comprehension datasets</cell></row><row><cell>such as SQuAD (Rajpurkar et al., 2016) where</cell></row><row><cell>answers are always a single span from context, sev-</cell></row><row><cell>eral answers in the span subset of DROP contains</cell></row><row><cell>multiple spans. The number and date answers are</cell></row><row><cell>mostly out of context and need generative-level</cell></row><row><cell>expressiveness.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table ,</head><label>,</label><figDesc>Text and Table-Text(both).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>POET-SQL RoBERTa on Different DatasetsOn DROP, we cast the span selection task as a sequence tagging problem following Segal et al.(2020). On TAT-QA, we in-place substitute the RoBERTa-Large encoder in TAGOP(Zhu et al.,  2021)  with our POET-SQL RoBERTa to verify its effectiveness, and keep the rest of the components unchanged. On HotpotQA, we train two classifiers independently to predict the start and end positions of the answer span, as done inDevlin  et al. (2019).</figDesc><table><row><cell>i) On DROP, we include two families of mod-els for comparison: specialized models such as NumNet(+) (Ran et al., 2019), MTMSN (Hu et al., 2019), NeRd (Chen et al., 2020c), QDGAT (Chen et al., 2020a) and language models such as Gen-BERT (Geva et al., 2020) and PReaM (Yoran et al., 2022). (ii) Similarly, on HotpotQA (Distractor), specialized model baselines include DFGN (Qiu et al., 2019), SAE (Tu et al., 2020), C2F Reader (Shao et al., 2020) and the SOTA model HGN (Fang et al., 2020). The language model baselines consist of BERT (Devlin et al., 2019), SpanBERT (Joshi et al., 2020) and Rea-sonBERT (Deng et al., 2021). (iii) On TAT-QA, we adopt the official baselines, including TAPAS (Herzig et al., 2020), NumNet+ V2 and the SOTA model TAGOP (Zhu et al., 2021). (iv) On EQUATE, we compare our methods with BERT (Devlin et al., 2019), GPT (Radford et al., 2019) and Q-REAS (Ravichander et al., 2019). (v) On LogiQA, we compare our methods with Co-Matching Network (Wang et al., 2018c) and the SOTA model DAGN (Huang et al., 2021). C Implementation Details C.1 On EQUATE, we train a classi-fier to perform sequence classification on concate-nated premise-hypothesis pairs. Notably, we fol-low the official setup to train LMs on the MNLI dataset (Williams et al., 2018) and evaluate their zero-shot performance on EQUATE. On SVAMP, the encoder-only model is not suitable since the answers are out-of-context. C.2 Pre-training Details By default, we apply AdamW as pre-training opti-mizer with default scheduling parameters in fairseq. The coefficient of weight decay is set as 0.05 to al-leviate over-fitting of pre-trained models. Addition-ally, we employ fp16 to accelerate the pre-training.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 9 :</head><label>9</label><figDesc>Breakdown of model F 1 score by answer types on the dev set of DROP. Some works only report overall span type performance (marked by *), and single-span is non-separable from multi-span performance. Bold and underlined numbers indicate the best and second-best results, respectively.</figDesc><table><row><cell>Models</cell><cell cols="6">RTE-Q NewsNLI RedditNLI NR ST AWPNLI Average</cell></row><row><cell></cell><cell></cell><cell cols="2">Previous Systems</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MAJ</cell><cell>57.8</cell><cell>50.7</cell><cell>58.4</cell><cell>33.3</cell><cell>50.0</cell><cell>50.4</cell></row><row><cell>BERT</cell><cell>57.2</cell><cell>72.8</cell><cell>49.6</cell><cell>36.9</cell><cell>42.2</cell><cell>51.8</cell></row><row><cell>GPT</cell><cell>68.1</cell><cell>72.2</cell><cell>52.4</cell><cell>36.4</cell><cell>50.0</cell><cell>55.8</cell></row><row><cell>Q-REAS</cell><cell>56.6</cell><cell>61.1</cell><cell>50.8</cell><cell>63.3</cell><cell>71.5</cell><cell>60.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Original LMs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>BART-Large</cell><cell>68.1</cell><cell>76.2</cell><cell>65.0</cell><cell>53.7</cell><cell>49.7</cell><cell>62.6</cell></row><row><cell>RoBERTa-Large</cell><cell>69.3</cell><cell>75.5</cell><cell>65.6</cell><cell>60.1</cell><cell>50.7</cell><cell>64.2</cell></row><row><cell></cell><cell></cell><cell cols="2">POET Models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>POET-SQLBART</cell><cell>72.3</cell><cell>75.2</cell><cell>64.8</cell><cell>70.7</cell><cell>49.5</cell><cell>66.5</cell></row><row><cell>POET-SQLRoBERTa</cell><cell>75.3</cell><cell>75.5</cell><cell>68.1</cell><cell>69.2</cell><cell>50.5</cell><cell>67.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 10 :</head><label>10</label><figDesc>The EM performance of different models on all subsets of the EQUATE benchmark. Bold and underlined numbers indicate the best and second-best results, respectively.</figDesc><table><row><cell>Passage Retrieval in HotpotQA Since the total</cell></row><row><cell>length of the original passages in HotpotQA is too</cell></row><row><cell>long to fit into memory, we train a classifier to filter</cell></row><row><cell>out top-3 passages, as done in previous work (Deng</cell></row><row><cell>et al., 2021). Specifically, a RoBERTa-Large model</cell></row><row><cell>is fine-tuned to discriminate if an input passage is</cell></row><row><cell>required to answer the question. The Hits@3 score</cell></row><row><cell>of the classifier on HotpotQA is 97.2%.</cell></row><row><cell>Numerical Design in DROP and SVAMP As</cell></row><row><cell>noticed by previous works, sub-word tokenization</cell></row><row><cell>methods such as byte pair encoding (Sennrich et al.,</cell></row><row><cell>2015) potentially undermines the arithmetic abil-</cell></row><row><cell>ity of models. Instead, the character-level number</cell></row><row><cell>representation is argued to be a more effective al-</cell></row><row><cell>leviation (Wallace et al., 2019). Additionally, the</cell></row><row><cell>reverse decoding of numbers is proposed as a bet-</cell></row><row><cell>ter way of modelling arithmetic carry (Geva et al.,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table Text Table</head><label>Text</label><figDesc>/ 50.1 43.8 / 50.0 55.6 / 55.6 51.5 / 51.5 Counting 66.7 / 66.7 -/ -90.0 / 90.0 81.3 / 81.3 Spans 67.4 / 80.6 54.2 / 80.8 79.2 / 84.8 71.4 / 82.6 Span 68.4 / 68.4 51.2 / 76.0 76.2 / 77.8 61.9 / 74.6 Total 56.5 / 58.0 51.1 / 75.0 69.0 / 70.7 59.1 / 65.9</figDesc><table><row><cell></cell><cell></cell><cell>-Text</cell><cell>Total</cell></row><row><cell>EM / F1</cell><cell>EM / F1</cell><cell>EM / F1</cell><cell>EM / F1</cell></row><row><cell>Arithmetic 50.1</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 11 :</head><label>11</label><figDesc>The EM performance of TAGOP (POET-SQL RoBERTa ) with respect to answer types and sources on the dev set of TAT-QA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>Table 11shows the detailed experimental results of TAGOP (POET-SQL RoBERTa ). Considering that the pre-training of POET-SQL RoBERTa is</figDesc><table><row><cell></cell><cell>Train</cell><cell></cell><cell>Dev</cell><cell></cell></row><row><cell>Dataset</cell><cell># Questions</cell><cell># Docs</cell><cell cols="2"># Questions # Docs</cell></row><row><cell>SQuAD</cell><cell>77, 409</cell><cell>5, 565</cell><cell>9, 536</cell><cell>582</cell></row><row><cell>MNLI</cell><cell>392, 702</cell><cell>392, 702</cell><cell>9, 815</cell><cell>9, 815</cell></row><row><cell>QuoRef</cell><cell>19, 399</cell><cell>3, 771</cell><cell>2, 418</cell><cell>454</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 12 :</head><label>12</label><figDesc>POET on NL understanding experiment dataset statistics.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The code is available at https://github.com/ microsoft/ContextualSP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">More discussion can be found in Appendix ? A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">To clarify, 16% is not a specific-purpose design but a statistical result.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">http://github.com/microsoft/DeepSpeed</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Frank F. Xu, Zhengbao Jiang, Bill Yuchen Lin, Shuyan Zhou, Zhiruo Wang, Libo Qin, Jize Jiang, and Jonathan Livengood for fruitful discussion. We also thank all the anonymous reviewers for their constructive feedback and insightful comments. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Giving BERT a calculator: Finding operations and arguments with reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5947" to="5952" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Logicguided data augmentation and regularization for consistent question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.499</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5642" to="5650" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural-symbolic learning and reasoning: A survey and interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artur</forename><forename type="middle">S</forename><surname>Besold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Uwe</forename><surname>Hitzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu?s</forename><forename type="middle">C</forename><surname>K?hnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priscila</forename><surname>Lowd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Machado Vieira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>De Penning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pinkas</surname></persName>
		</author>
		<idno>abs/1711.03902</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Hoifung Poon, and Gerson Zaverucha</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abductive commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannah</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Downey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Zero-shot transfer learning with synthesized data for multi-domain dialogue state tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Campagna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Foryciarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrad</forename><surname>Moradshahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Lam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Question directed graph attention network for numerical reasoning over text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zou</forename><surname>Xiaochuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.549</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6759" to="6768" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ReTraCk: A flexible and efficient framework for knowledge base question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Guang</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-demo.39</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="325" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tabfact: A large-scale dataset for table-based fact verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
