<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpretable Visualizations with Differentiating Embedding Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Robinson</surname></persName>
							<email>isaac.robinson@yale.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Yale University New Haven</orgName>
								<address>
									<postCode>06510</postCode>
									<region>CT</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Interpretable Visualizations with Differentiating Embedding Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a visualization algorithm based on a novel unsupervised Siamese neural network training regime and loss function, called Differentiating Embedding Networks (DEN). The Siamese neural network finds differentiating or similar features between specific pairs of samples in a dataset, and uses these features to embed the dataset in a lower dimensional space where it can be visualized. Unlike existing visualization algorithms such as UMAP or t-SNE, DEN is parametric, meaning it can be interpreted by techniques such as SHAP. To interpret DEN, we create an end-to-end parametric clustering algorithm on top of the visualization, and then leverage SHAP scores to determine which features in the sample space are important for understanding the structures shown in the visualization based on the clusters found. We compare DEN visualizations with existing techniques on a variety of datasets, including image and scRNA-seq data. We then show that our clustering algorithm performs similarly to the state of the art despite not having prior knowledge of the number of clusters, and sets a new state of the art on FashionMNIST. Finally, we demonstrate finding differentiating features of a dataset. Code available at this https url.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Exploratory data analysis is a primary application of unsupervised machine learning. Exploratory techniques such as clustering and visualization are indispensable when the underlying structure of data is unknown. With ever increasing volumes of complex data generated by, for example, biomedical research or edge devices, there is an increasing need for more sensitive data exploration tools.</p><p>Visualization techniques such as t-SNE <ref type="bibr" target="#b13">[14]</ref> and UMAP <ref type="bibr" target="#b16">[17]</ref> have gained wide usage as standard methods for exploratory data analysis. They attempt to approximate the manifold upon which data lies in two or three dimensions for visualization so that the structure of the data can be easily understood by researchers. Insights offered by such techniques are relied upon enough that they are used to help established the standard 'ground truth' in scRNA-seq datasets <ref type="bibr" target="#b24">[25]</ref> against which other approaches are judged. Furthermore, they have easily understood hyperparameters that allow people who are not experts in machine learning to successfully make use of them for whatever application they have in mind <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>Clustering, another indispensable tool of exploratory data analysis, attempts to elucidate the deeper structure of the data by splitting it into discrete subsets linked by some characteristic. Visualization techniques are often used to gain insight into the clustered underlying structure of data, and indeed t-SNE can be shown to preserve clusters in its embedding <ref type="bibr" target="#b10">[11]</ref>. However, they do not explicitly provide cluster labels for the data <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>, so to successfully employ t-SNE or UMAP to generate clusters, a separate clustering algorithm has to be run on top of the t-SNE or UMAP visualization. This decouples the two exploratory data analysis techniques so that the embedding generated for visualization may not optimally reveal structures relevant for different clustering techniques. Nevertheless, clustering approaches such as Louvain tend to agree enough about the structure of the underlying data that both t-SNE and Louvain <ref type="bibr" target="#b1">[2]</ref> can be used to represent the same standard 'ground truth' <ref type="bibr" target="#b24">[25]</ref>.</p><p>While these approaches have offered invaluable insights into the structure of complex datasets, it remains a non-trivial task to understand which features of the data are important for understanding the revealed structures. While powerful model interpretation techniques exist <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b12">13]</ref>, for the most part they depend on parametric representations of the models. t-SNE and UMAP work by moving representative points in a lower dimensional embedding according to a loss function <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>, ultimately decoupling the visualization from interpretable features in the sample space, as they do not create a parametric relationship between their embedding and the samples they represent. Similarly, most clustering algorithms are non-parametric and do not create explicit, interpretable models for the data they attempt to represent.</p><p>While there have been advances in parametric clustering models, mostly built around deep neural networks, most require prior knowledge of the number of clusters <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19]</ref>, which severely limits their applicability to exploratory data analysis, or they rely on an intermediary non-parametric dimensionality reduction technique such as t-SNE <ref type="bibr" target="#b20">[21]</ref>, which eliminates their parametric modeling interpretability.</p><p>In this work we introduce Differentiating Embedding Networks (DEN), an interpretable visualization and clustering algorithm that does not require prior knowledge of the number of clusters. Based on the success of SpectralNet for clustering, which uses a Siamese neural network to learn a kernel <ref type="bibr" target="#b23">[24]</ref>, and t-SNE, which attempts to move points in a lower dimensional embedding such that the probability distribution between the lower dimensional points matches that of the data in sample-space <ref type="bibr" target="#b13">[14]</ref>, we create a Siamese neural network that learns a metric in the sample space and attempts to embed the data in a lower dimensional space according to a probability distribution to fit the learned metric. We learn the Siamese neural network metric as we embed the data, creating a parametric visualization tool. We then cluster the data and build a parametric model to take points from the embedding and predict which cluster they belong to, which, when combined with the Siamese network, creates an end-to-end parametric clustering algorithm. Finally, we use SHAP scores to determine which features in sample space are important <ref type="bibr" target="#b12">[13]</ref> for understanding the structures presented in the visualization as captured by the clustering algorithm. Our experiments show that DEN creates visualizations very similar to t-SNE and UMAP, but with tighter clusters. An example visualization compared to UMAP and t-SNE applied to the FasionMNIST dataset <ref type="bibr" target="#b27">[28]</ref> can be seen in <ref type="figure" target="#fig_0">Figure 1</ref>. We also show that our clustering results based off the DEN embedding perform similarly to the state of the art despite not having prior knowledge of the number of clusters. Finally, we demonstrate automatic determination of important and differentiating features for a dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>In this section we describe the implementation and training regime for Siamese kernel networks. We describe the creation of an initial unweighted graph connecting various samples from the dataset motivated by the t-SNE adjacency matrix commonly used to accelerate t-SNE <ref type="bibr" target="#b11">[12]</ref>. We then describe the Siamese neural network and our novel loss function based on the F -distribution. Finally, we describe our method for interpreting the DEN visualization via clustering and SHAP scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Building the Graph</head><p>Like t-SNE, UMAP, and related techniques, we assume samples that are very close to each other according to some relevant metric in the sample space should be close to each other in the embedding space <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>. Similarly, we assume samples that are very far from each other in the sample space likely should be represented as far apart in the embedding space. Beyond that, we don't assume a metric such as the Euclidean distance in the sample space is well suited to describe a manifold upon which the data lies. To create a graph that leverages these assumptions and generate a training dataset for the Siamese kernel network, we label pairs of points as positive or negative, depending on if their distances in the sample space indicate that they should be close to or far from each other in the embedding space.</p><p>To determine pairs of positive points, we generate a k-nearest neighbors (KNN) graph based on some metric. We then create a shared nearest neighbor graph <ref type="bibr" target="#b3">[4]</ref> among each point's top k-nearest neighbors. In order to make sure each point is well represented, if a point does not have any shared nearest neighbors, we add its closest neighbors to the graph until it has at least j neighbors, j &lt; k. Therefore, each sample has at least j positive samples associated with it. Both j and k are hyperparameters. Empirically, j = 1 and k = 10 work well.</p><p>To create negative pairs, for each sample we randomly create k negative pairs where the probability of being paired with a particular other sample is proportional to the distance between the two samples according to the same metric. This approach is similar to the D 2 weighting used in <ref type="bibr" target="#b0">[1]</ref>. We don't prune this graph at all. Note that, while t-SNE can be interpreted as the balancing of a repulsive and attractive force <ref type="bibr" target="#b13">[14]</ref>, where the repulsive force is universally applied to all pairs of samples, this graph deliberately selects only particular pairs to have a repulsive force between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Siamese Neural Network</head><p>We use different neural network architectures to embed different types of data. In this way, we can ensure we are taking sufficient advantage of the neural network's ability to determine interesting features about the data, highlighting its distinction from existing manifold embedding techniques. To embed images, we use a standard convolutional neural network. To embed text and other token-based data, we use an embedding layer, take the average of the token vectors, and pass this vector through a hidden layer, much like fastText <ref type="bibr" target="#b6">[7]</ref>. For other kinds of data, including scRNA-seq, we use a standard fully connected deep neural network.</p><p>In creating our embedding space, we would ideally want the Euclidean distances between pairs of points to reflect how likely they are to be related based on the features learned by the Siamese neural network. We can interpret the last layer of a deep neural network with no following activation function as linear regression on the deep features learned by that network. Since the Euclidean distance between two points x and y in an n-dimensional embedding space is</p><formula xml:id="formula_0">||x ? y|| 2 = n i=1 (x i ? y i ) 2 ,</formula><p>we can interpret the square of the Euclidean distance as the sum of the squared residuals between x and y. Since we're interpreting the final layer of the neural network as linear regression, we can argue that, if the learned features should ideally map x and y to the same part of the embedding, indicating a feature-based or semantic similarity between x and y, then the residuals of the linear regression model should be approximately distributed according to an F -distribution <ref type="bibr" target="#b19">[20]</ref>.</p><p>To translate this interpretation into a loss function, we run a regression F -test and evaluate the cumulative density function (CDF) of the F -distribution with d 1 = 1 and d 2 = n for the residual between x and y. This parameter setting can be interpreted as the goodness of fit of each comparison between two n-dimensional embeddings being measured by the fit of n one-dimensional regressions, which allows the network to cleanly take advantage of an increase in the dimension of the embedding space. From this, we get that the probability that x and y should be close to each other given their Euclidean distance ||x ? y|| 2 = d is</p><formula xml:id="formula_1">P (x, y) = I d 2 d 2 +n 1 2 , n 2</formula><p>where I x (a, b) refers to the regularized incomplete beta function, defined as</p><formula xml:id="formula_2">I x (a, b) = B(x; a, b) B(a, b)</formula><p>where B(x; a, b) is the incomplete beta function and B(a, b) is the beta function.</p><p>In order to quickly differentiate the incomplete beta function B(x; a, b) as required to train a neural network with gradient descent, we let</p><formula xml:id="formula_3">B(x; a, b) = 2 F 1 (a, 1 ? b, a + 1, x)</formula><p>x a a and we then use a Laplace approximation of the hypergeometric function 2 F 1 to make our loss function differentiable and closed-form <ref type="bibr" target="#b2">[3]</ref>.</p><p>Once we have P (x, y), if x and y form a positive pair, we minimize P , and if they form a negative pair, we minimize 1 ? P . Note that since P is monotonic with the F -statistic, which is equal to the square of the Euclidean distance ||x ? y|| 2 , that this minimization is equivalent to minimizing the distance between positive pairs and maximizing the distance between negative pairs. This yields the desired property of the embedding space. It is worth noting that the t-distribution has been successful in t-SNE <ref type="bibr" target="#b13">[14]</ref>, and the square of t-distributed random variable is distributed according to an F -distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Interpretation</head><p>We describe our method for interpreting DEN embeddings and visualizations. First, we cluster the embedding with spectral clustering <ref type="bibr" target="#b25">[26]</ref>. Then, we build a parametric model to predict cluster labels from the embedding. Finally, we turn the Siamese network and this clustering model into an end-to-end parametric model to assign cluster labels, and use SHAP scores to interpret which data features are important for placing a sample into a specific cluster <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Clustering</head><p>In order to facilitate exploratory data analysis, we want to create cluster labels on the embedding without prior knowledge of the number of clusters. Currently, most of the top performing clustering methods on a number of datasets require prior knowledge of the number of clusters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b18">19]</ref>, with a notable exception being DDC <ref type="bibr" target="#b20">[21]</ref>. To address this issue, we use spectral clustering to cluster samples in the embedding space, which does not necessarily require prior knowledge of the number of clusters <ref type="bibr" target="#b25">[26]</ref>.</p><p>For our spectral clustering implementation, we first construct an affinity matrix using a Gaussian kernel in the embedding space from a subset of the total points. To speed up the spectral clustering algorithm, we only perform clustering on a subset of the data. The feasibility of performing spectral clustering only on a subset of the data is supported by the success of the Nystr?m method for kernel approximation <ref type="bibr" target="#b26">[27]</ref>. From the graph construction, we already have designated points that should be near each other in the embedding space, namely the positive pairs. Our affinity matrix is supposed to represent relatedness of points, so we can calculate the expected distance between positive pairs in the embedding space and use this as the bandwidth of our Gaussian kernel. This way, our affinity matrix captures the same definition of 'relatedness' we used to build our training samples. We calculate this expected distance by approximating the mean of the distance between all positive pairs in the embedding space. Calling this distance d avg , our kernel can then be expressed as</p><formula xml:id="formula_4">k(x, y) = exp ? ||x ? y|| 2 2 ?d 2</formula><p>avg where x and y are two points and ? is a scaling factor. In practice, we let ? = 1, except when the clusters in the data are particularly small, as in the case of embedding the AG_NEWS dataset in <ref type="figure">Figure 2</ref>.</p><p>Calling this affinity matrix A, we compute the row sums and create a diagonal matrix with them called D. Using these matrices, we calculate the unnormalized graph Laplacian L = D ? A <ref type="bibr" target="#b25">[26]</ref>. To perform our spectral clustering, we compute the eigenvalues and eigenvectors of L, get an estimate for the number of clusters k by counting how many eigenvalues are below a certain threshold, and then run the k-means algorithm on the first k eigenvectors. We don't expect our graph components to be completely disconnected, so our threshold must be greater than 0 <ref type="bibr" target="#b25">[26]</ref>. In practice, 10 ?2 seems to work well. This gives an overestimate of the number of clusters present since we impose such a weak threshold to count the number of clusters.</p><p>To deal with this overestimate and extend our cluster labels to samples outside of our subset used in the construction of the affinity matrix, we apply a KNN filter to the data. We train a KNN classifier to classify samples from the embedding space based on their cluster labels, with a relatively large number of neighbors. We then classify all points in the embedding with this classifier. The large number of neighbors works as a low-pass filter to remove high-frequency label signals, effectively denoising the labels domain. Labels that shift quickly among nearest neighbors are less likely to survive a KNN classifier, and are also more likely to be noise. This is similar to denoising via a low-passed graph Fourier transform <ref type="bibr" target="#b22">[23]</ref>. To the best of our knowledge, this is the first time a KNN classifier has been used to clean up spectral clustering labels in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Interpretation via Parametric Clustering</head><p>Like the approaches used in N2D <ref type="bibr" target="#b14">[15]</ref> and DDC <ref type="bibr" target="#b20">[21]</ref>, this spectral approach to clustering the DEN embedding is non-parametric and does not immediately support out-of-sample extension. Our goal is to create an interpretable visualization method, where the entire pipeline is parametric and applicable to new data, so we have to move beyond these non-parametric clustering approaches.</p><p>To address this problem, we create a second neural network to predict the cluster labels discovered by our spectral clustering approach from the embedding. Because we don't want to make any assumptions about the distribution of data in the embedding space, we use a self-normalizing neural network with the SeLU activation function <ref type="bibr" target="#b7">[8]</ref>. We train this network for 50 epochs. We then combine this clustering network with the Siamese network that generates the embedding in an end-to-end way, and fine-tune the whole network by training it for an additional 50 epochs at a lower learning rate. In this way, we create an end-to-end network that can take samples and predict which cluster they should belong to. To the best of our knowledge, this is the first fully parametric clustering approach that does not require prior knowledge of the number of clusters. As we now have a fully parametric clustering model, we can use SHAP scores to explain which features of a given sample are important for its being placed in a particular cluster <ref type="bibr" target="#b12">[13]</ref>, thus revealing which features are important for structures shown in the DEN visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>Differentiating Embedding Networks place no limit on the dimension of the embedding space. Our experiments have shown no significant difference in clustering performance with different numbers of dimensions in the embedding space, but in theory, a higher dimensional representation could encapsulate more information about the structure of the data than a lower dimensional one. Having said that, for the sake of direct comparison with t-SNE and UMAP, which are frequently used to visualize data in two dimensions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, we present all of our results based on a twodimensional embedding. First we present results of data visualizations, then of clustering, and finally of automatically-determined differentiating features of clustering results. Since algorithms for visualization cannot be directly compared with clustering algorithms, we compare against different algorithms in each section. <ref type="figure">Figure 2</ref> shows visualizations produced by DEN, UMAP, and t-SNE, on three different datasets. Our approach tends to produce tighter clusters than either t-SNE or UMAP, with paths of samples bridging the gaps between the different clusters. When visualized as in <ref type="figure" target="#fig_1">Figure 3</ref>, it becomes clear that these paths represent intermediate forms between the clusters based on the deep features extracted by the neural network. Since UMAP and t-SNE don't extract features, and instead attempt to represent the data manifold <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, such paths won't show up in their visualizations. It should also be noted that our approach is scale-sensitive, in that running on a subset of the data produces a different representation for that data than would running it in the context of more data, because the selection of negative pairs is likely different. This scale sensativity is not present in t-SNE, and is much less <ref type="figure">Figure 2</ref>: Comparison of visualization techniques on different datasets. Note DEN has been zoomed to the region of interest, leaving out a few outliers. MNIST is from <ref type="bibr" target="#b9">[10]</ref>, AG_NEWS is from <ref type="bibr" target="#b4">[5]</ref>, and the scRNA-seq dataset is from <ref type="bibr" target="#b24">[25]</ref>. We generated all figures ourselves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Visualization</head><p>pronounced in UMAP <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>. The difference in the presentations of the clusters containing digits 4, 7, and 9 in MNIST between Figures 2 and 3 show this scale sensativity; in <ref type="figure" target="#fig_1">Figure 3</ref>, the clusters are much less concentrated than their counterparts in <ref type="figure">Figure 2</ref>, where 4, 7, and 9 are found along the bottom of the MNIST visualization.</p><p>It should be noted that, in order to generate the MNIST and AG_NEWS visualizations, we use feature extraction techniques that are unique to neural networks, notably convolutional neural networks and the architecture from fastText <ref type="bibr" target="#b6">[7]</ref>, respectively. These feature extractors allow us to build very different embeddings from those captured by UMAP and t-SNE, which operate in pixel space for MNIST and on TF-IDF vectors for AG_NEWS. This highlights the potential of DEN for processing more complex data than is feasible with approaches that can't leverage the flexibility of neural networks.</p><p>Each of the clusters in the AG_NEWS visualization refers to a separate news subject, including differentiations between American and international sports news, between news pertaining to the Israeli/Palestinian conflict and civil unrest in a country, and between news pertaining to takeovers of tech companies and the interaction of tech companies and governments. These clusters are more specific than the labels provided with the dataset, which divide the data into World, Sports, Business, and Tech/Science.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Clustering</head><p>We compare the performance of our clustering algorithm against the current state of the art. Note that DEN, DDC-DA <ref type="bibr" target="#b20">[21]</ref>, and UMAP+HDBSCAN are the only methods in the table that don't require prior knowledge of the number of clusters. When possible, we report both the accuracy (ACC) and normalized mutual information (NMI) of the algorithm, but if the number of clusters found is incorrect then we cannot report accuracy. Our algorithm comes in second of those in the table on USPS <ref type="bibr" target="#b5">[6]</ref> and MNIST <ref type="bibr" target="#b9">[10]</ref>, and sets a new state of the art in terms of NMI on FashionMNIST, which is considered a substantially more difficult dataset <ref type="bibr" target="#b27">[28]</ref>. These clustering results were produced based off a two-dimensional embedding that can also be used to visualize the data. With the exception of UMAP, no other algorithm in the table attempts to produce a visualization output. It should be noted that both N2D and DDC-DA do produce intermediate embeddings via UMAP or t-SNE that can be visualized, but the authors don't propose using their methods as generalized visualization approaches. DynAE is from <ref type="bibr" target="#b17">[18]</ref>, N2D is from <ref type="bibr" target="#b14">[15]</ref>, DDC-DA is from <ref type="bibr" target="#b20">[21]</ref>, JULE is from <ref type="bibr" target="#b28">[29]</ref>, ClusterGAN is from <ref type="bibr" target="#b18">[19]</ref>, SpectralNet is from <ref type="bibr" target="#b23">[24]</ref>, GDL is from <ref type="bibr" target="#b29">[30]</ref>, and HDBSCAN is from <ref type="bibr" target="#b15">[16]</ref>. Even though it isn't published as a standalone method, we put UMAP+HDBSCAN in our table because it is often discussed as a strong clustering method, and since UMAP is typically used as a visualization method, we felt it deserved direct comparison in terms of clustering performance. UMAP with HDBSCAN did not find the correct number of clusters on any dataset, so we did not calculate its accuracy, and in terms of its normalized mutual information score it did not outperform DEN.</p><p>All algorithms in the table except for GDL and UMAP are deep learning based. However, the only one that employs a similar neural network to ours is SpectralNet <ref type="bibr" target="#b23">[24]</ref>, which trains a Siamese neural network <ref type="bibr" target="#b8">[9]</ref> to work as its kernel for its deep spectral clustering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Explanations</head><p>The fact that DEN determines on its own the optimal number of clusters and builds an end-to-end parametric model that can predict cluster labels means we can leverage SHAP to explain what characteristics of a sample contribute to its being placed in a particular cluster <ref type="bibr" target="#b12">[13]</ref>. This amounts to determining the defining characteristics of each sample that differentiate it from the rest of the dataset. <ref type="figure" target="#fig_2">Figure 4</ref> shows such cluster label explanations applied to randomly selected examples of each MNIST digit. Red marks positive evidence for belonging to the cluster and blue marks negative evidence. Note that these descriptions are sample-specific. For example, the 1 is tilted to the side and might therefore be considered a 4. But the red coloring indicates that the missing arm from the 4 contributes strongly to its being clustered with the 1s instead. For the 5, we see three red blotches on the body corresponding to typical structures found in 5s. Furthermore, we see red blotches in curves where, if there had been curves present, it would have been classified as an 8, so the absence of those curves is significant for its classification as a 5. It is not visible in this display, but the dark blue on the upper left side of this particular 5 image that was chosen for analysis hides a jut that is atypical of 5s and therefore contributes evidence that perhaps this sample should not be a 5. Each of the other digits also has distinctive characteristics captured by DEN. While displaying defining characteristics of images clearly demonstrates the underlying idea, the method is not limited to images. The same approach can be applied to determine which proteins or particular RNA expressions contribute to biological data being grouped in a particular way. Because t-SNE, UMAP, and other related methods are nonparametric <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, the same approach cannot be applied to explain their cluster predictions <ref type="bibr" target="#b12">[13]</ref>. Conversely, the existing state of the art parametric clustering algorithms require prior knowledge of the number of clusters, and so can't be used for purely exploratory data analysis. In light of these facts, DEN occupies a potentially useful, unfilled niche.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Our novel approach to visualization, Differentiating Embedding Networks, shows promise for exploratory data visualization and clustering. It shows similar structure to that captured by UMAP and t-SNE, but with tighter clusters and the possibility of visualizing feature-based intermediate forms between clusters. For clustering, it performs similar to the current state of the art despite lacking prior knowledge of the number of clusters. We also set a new state of the art on FashionMNIST. Finally, DEN performs its clustering in an end-to-end parametric model, meaning we can explain why samples belong to particular clusters via SHAP scores. This allows us to visualize, cluster, and then identify defining characteristics of the data.</p><p>Future work includes creating a hierarchical organization approach taking advantage of DEN's inherent scale sensitivity, and creating a variant on DEN that can leverage labels to create an optionally semi-supervised algorithm, just as UMAP is able to leverage partially labeled data <ref type="bibr" target="#b16">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>DEN embedding of FashionMNIST compared to UMAP and t-SNE embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>A DEN embedding of the digits 4, 7, and 9 from the MNIST dataset, with the images shown in their embedding locations. Note that we can see paths between clusters showing intermediate forms of digits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Explanations of what factors contribute to MNIST samples being placed into their appropriate clusters discovered by DEN, as determined by SHAP's DeepExplainer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Clustering Performance (ACC/NMI)</cell><cell></cell></row><row><cell></cell><cell>USPS</cell><cell></cell><cell>MNIST</cell><cell>FashionMNIST</cell><cell></cell></row><row><cell>DynAE</cell><cell cols="3">0.981 0.948 0.989</cell><cell>0.962 0.591</cell><cell>0.642</cell></row><row><cell>DDC-DA</cell><cell cols="3">0.977 0.939 0.969</cell><cell>0.941 0.609</cell><cell>0.661</cell></row><row><cell>N2D</cell><cell cols="3">0.958 0.901 0.979</cell><cell>0.942 0.672</cell><cell>0.684</cell></row><row><cell>JULE</cell><cell cols="3">0.950 0.913 0.964</cell><cell>0.913 -</cell><cell>-</cell></row><row><cell>ClusterGAN</cell><cell cols="3">0.970 0.931 0.964</cell><cell>0.921 -</cell><cell>-</cell></row><row><cell>SpectralNet</cell><cell>-</cell><cell>-</cell><cell>0.971</cell><cell>0.924 -</cell><cell>-</cell></row><row><cell>GDL</cell><cell>-</cell><cell>-</cell><cell>0.964</cell><cell>0.910 0.627</cell><cell>0.660</cell></row><row><cell cols="2">UMAP+HDBSCAN -</cell><cell cols="2">0.877 -</cell><cell>0.884 -</cell><cell>0.594</cell></row><row><cell>Ours</cell><cell cols="3">0.979 0.944 0.984</cell><cell>0.956 0.635</cell><cell>0.710</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We'd like to thank Ariel Jaffe for his mentorship, encouragement, and advice; Yuval Kluger for his support and his offering of access to computing resources; and Stefan Steinerberger for essential inspiration and encouragement. Thank you.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vassilvitskii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast unfolding of communities in large networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">D</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lambiotte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lefebvre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical mechanics: theory and experiment</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">10008</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplace approximations for hypergeometric functions with matrix argument</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1155" to="1177" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new shared nearest neighbor clustering algorithm and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ertoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steinbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on clustering high dimensional data and its applications at 2nd SIAM international conference on data mining</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="105" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ag&apos;s corpus of news articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulli</surname></persName>
		</author>
		<ptr target="http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="554" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01759</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Clustering with t-sne, provably</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Mathematics of Data Science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="332" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast interpolationbased t-sne for improved visualization of single-cell rna-seq data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rachh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Hoskins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Steinerberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="245" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-I</forename><surname>Lee ; I. Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">N2d:(not too) deep clustering via clustering the local manifold of an autoencoded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcconville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Santos-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Piechocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Craddock</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05968</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">hdbscan: Hierarchical density based clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Astels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">205</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Deep clustering with a dynamic autoencoder: From reconstruction towards centroids construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrabah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ksantini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lachiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.07752</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clustergan: Latent space clustering in generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Asnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4610" to="4617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tests for specification errors in classical linear least-squares regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Ramsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="350" to="371" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep density-based image clustering. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">105841</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Model-agnostic interpretability of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs: Frequency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3042" to="3054" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Spectralnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01587</idno>
		<title level="m">Spectral clustering using deep neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Comprehensive classification of retinal bipolar neurons by single-cell transcriptomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lapan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Macosko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Adiconis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nemesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1308" to="1323" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A tutorial on spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="395" to="416" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Using the nystr?m method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph degree linkage: Agglomerative clustering on a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
