<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghao</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiangyu@megvii.com</email>
							<affiliation key="aff2">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Aberystwyth University</orgName>
								<address>
									<postCode>SY23 3FL</postCode>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
							<email>jungonghan77@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
							<email>dinggg@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="department" key="dep2">School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Compared to convolutional layers, fully-connected (FC) layers are better at modeling the long-range dependencies but worse at capturing the local patterns, hence usually less favored for image recognition. In this paper, we propose a methodology, Locality Injection, to incorporate local priors into an FC layer via merging the trained parameters of a parallel conv kernel into the FC kernel. Locality Injection can be viewed as a novel Structural Re-parameterization method since it equivalently converts the structures via transforming the parameters. Based on that, we propose a multi-layer-perceptron (MLP) block named RepMLP Block, which uses three FC layers to extract features, and a novel architecture named RepMLPNet. The hierarchical design distinguishes RepMLPNet from the other concurrently proposed vision MLPs. As it produces feature maps of different levels, it qualifies as a backbone model for downstream tasks like semantic segmentation. Our results reveal that 1) Locality Injection is a general methodology for MLP models; 2) RepMLPNet has favorable accuracy-efficiency trade-off compared to the other MLPs; 3) RepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic segmentation. The code and models are available at https://github.com/DingXiaoH/RepMLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The locality of images (i.e., a pixel is more related to its neighbors than the distant pixels) makes Convolutional Besides, we also desire the ability to build up long-range dependencies, which is referred to as the global capacity in this paper. Traditional CNNs model the long-range dependencies by deep stacks of conv layers <ref type="bibr" target="#b41">[40]</ref>. However, repeating local operations may cause optimization difficulties <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b41">40]</ref>. Some prior works enhance the global capacity with self-attention-based modules <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b40">39,</ref><ref type="bibr" target="#b41">40]</ref>, which have no local prior. For example, due to the lack of local prior, ViT <ref type="bibr" target="#b19">[18]</ref> requires an enormous amount of training data (3 ? 10 8 images in JFT-300M) to converge. On the other hand, a fully-connected (FC) layer can also directly model the dependencies between any two input points, which is as simple as flattening the feature map as a vector, linearly mapping it into another vector, and reshaping the resultant vector back into a feature map. However, this process has no locality either. Without such an important inductive bias, the concurrently proposed MLPs <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b44">43]</ref> usually demand a huge amount of training data (e.g., JFT-300M), more training epochs (300 or 400 ImageNet <ref type="bibr" target="#b8">[8]</ref> epochs) or special training techniques (e.g., a DeiT-style distillation method <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b39">38]</ref>) to learn the inductive bias from scratch. Otherwise, they may not reach a comparable level of performance with traditional CNNs.</p><p>We desire an MLP model that is 1) friendly to small-data, 2) trainable with ordinary training methods, and 3) effective in visual recognition. To this end, we make contributions in three aspects: methodology, component and architecture.</p><p>Methodology We propose a novel methodology, Locality Injection, to provide an FC layer with what it demands for effective visual understanding: the locality. Specifically, we place one or more conv layers parallel to the FC and add up their outputs. Though the FC simply views  <ref type="figure">Figure 1</ref>. RepMLP Block, where n, c, h, w are the batch size, number of input channels, height and width of the feature map, s is the number of share-sets, p is the padding. This example assumes n = 1, c = 4, s = 2. 1) The Global Perceptron aggregates the information across all the spatial locations and establishes the relations among channels. 2) In parallel, the input feature map is split into s share-sets and fed into the Channel Perceptron, which simply reshapes the features into vectors, linearly maps it to the output vectors, and reshapes them back.</p><p>3) The Local Perceptron takes the same inputs as the Channel Perceptron but convolve with small kernels to capture the local patterns. This example uses 1?1 and 3?3 so that the padding should be p = 0 and 1, respectively, to maintain the feature map size. Through batch normalization (BN) <ref type="bibr" target="#b23">[22]</ref>, the outputs of Local Perceptron and Channel Perceptron and added up. Finally, we combine the global and channel-wise information by merging the outcomes of the Global Perceptron. After training, the conv layers are absorbed into the FC3 kernel via Locality Injection, so that the training-time block is equivalently converted into a three-FC block and used for inference.</p><p>the feature maps as vectors, completely ignoring the locality, the conv layers can capture the local patterns. However, though such conv layers bring only negligible parameters and compute, the inference speed may be considerably slowed down because of the reduction of degree of parallelism on high-power computing devices like GPUs <ref type="bibr" target="#b31">[30]</ref>. Therefore, we propose to equivalently merge the conv layers into the FC kernels after training to speed up the inference. By doing so, we obtain an FC layer that is structurally identical to a normally trained FC layer but is parameterized by a special matrix with locality. Since Locality Injection converts the training-time structure (FC + conv) to the inference-time (a single FC) via an equivalent transformation on the parameters, it can be viewed as a novel Structural Re-parameterization <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b14">[13]</ref><ref type="bibr" target="#b15">[14]</ref><ref type="bibr" target="#b16">[15]</ref><ref type="bibr" target="#b17">[16]</ref> technique. In other words, we equivalently incorporate the inductive bias into a trained FC layer, instead of letting it learn from scratch.</p><p>The key to such an equivalent transformation is converting an arbitrary conv kernel to an FC kernel (i.e., a Toeplitz matrix). In this paper, we propose a simple, platform-agnostic and differentiable approach (Sec. 3). Component Based on Locality Injection, we propose RepMLP Block as an MLP building block. <ref type="figure">Fig. 1</ref> shows a training-time RepMLP Block with FC, conv and batch normalization <ref type="bibr" target="#b23">[22]</ref> (BN) layers can be equivalently converted into an inference-time block with only three FC layers. Architecture The hierarchical design has been shown to benefit visual understanding <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b35">34]</ref>. Therefore, we propose a hierarchical MLP architecture composed of RepMLP Blocks. In other words, as the feature map size reduces, the number of channels increases. We reckon that the major obstacle for adopting hierarchical design in a ResMLP- <ref type="bibr" target="#b38">[37]</ref> or MLP-Mixer-style <ref type="bibr" target="#b37">[36]</ref> model is that the number of parameters is coupled with the feature map size 1 , so that the number of parameters of the lower-level FC layers would be several orders of magnitude greater than the higher-level layers. For example, assume the lowest-level feature maps are of 56?56, an FC layer requires 56 4 = 9.8M parameters to map a channel to another, without any cross-channel correlations (i.e., like depth-wise convolution). We may let all the channels share the same set of parameters, so that the layer will have a total of 9.8M parameters. However, let the highest-level feature maps be of 7?7, the parameter count is only 7 4 = 2.4K but the number of channels is large. Predictably, sharing so few parameters among so many channels restricts the representational capacity hence results in inferior performance. We solve this problem by a set-sharing linear mapping (Sec. 4.1) so that we can independently control the parameter count of each layer by letting the channels share a configurable number of param-eter sets s. With a smaller s for the lower-level layers and a larger s for the higher-level ones, we can balance the model size and the representational capacity. Another drawback of the concurrently proposed MLPs is the difficulty of transferring to the downstream tasks like semantic segmentation. For example, MLP-Mixer <ref type="bibr" target="#b37">[36]</ref> demonstrates satisfactory performance on ImageNet but does not qualify as the backbone of a segmentation framework like UperNet <ref type="bibr" target="#b42">[41]</ref> as it aggressively embeds (i.e., downsamples) the images by 16? and repeatedly transforms the embeddings, so that it cannot produce multi-scale feature maps with different levels of semantic information. In contrast, our hierarchical design produces semantic information on different levels, so that it can be readily used as the backbone of the common downstream frameworks.</p><p>In summary, with Locality Injection, RepMLP Block and a hierarchical architecture, RepMLPNet achieves favorable accuracy-efficiency trade-off with only 100 training epochs on ImageNet, compared to the other MLP models trained in 300 or 400 epochs. We also show the universality of Locality Injection as it improves the performance of not only RepMLPNet but also ResMLP <ref type="bibr" target="#b38">[37]</ref>. Moreover, RepMLP-Net shows satisfactory performance as the first attempt to transfer an MLP-style backbone to semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">From Vision Transformer to MLP</head><p>Vision Transformers <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b39">38]</ref> heavily adopt selfattention modules to capture the spatial patterns. A primary motivation for using Transformers on vision tasks is to reduce the inductive bias designed by human and let the model automatically learn a better bias from big data. Concurrent with our work, MLP-Mixer <ref type="bibr" target="#b37">[36]</ref>, ResMLP <ref type="bibr" target="#b38">[37]</ref> and gMLP <ref type="bibr" target="#b27">[26]</ref> are proposed. For example, MLP-Mixer alternatively mix the information across channels (channelmixing, implemented by 1?1 conv) and within channels (token-mixing). Specifically, the token-mixing component projects the feature maps along the spatial dimension (i.e., transpose the feature map tensor), feed them into a 1?1 conv, and transpose the outcomes back. Therefore, the token-mixing can be viewed as an FC layer that flattens every channel as a vector, linearly maps it into another vector, and reshapes it back into a channel, completely ignoring the positional information; and all the channels share the same kernel matrix. In this way, MLP-Mixer realizes the communications among spatial locations. ResMLP <ref type="bibr" target="#b38">[37]</ref> and gMLP <ref type="bibr" target="#b27">[26]</ref> present different architectures which mix the spatial information with a similar mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Structural Re-parameterization</head><p>The core of Locality Injection is to equivalently merge a trained conv kernel into a trained FC kernel to inject the locality, so it can be categorized into Structural Reparameterization, which is a methodology of converting structures via transforming the parameters. A representative of Structural Re-parameterization is RepVGG <ref type="bibr" target="#b16">[15]</ref>, which is a VGG-like architecture that uses only 3?3 conv and ReLU for inference. Such an inference-time architecture is equivalently converted from a training-time architecture with identity and 1?1 branches. Asymmetric Convolution Block (ACB) <ref type="bibr" target="#b12">[11]</ref> and Diverse Branch Block (DBB) <ref type="bibr" target="#b15">[14]</ref> are two replacements for regular conv layers. Via constructing extra training-time paths (e.g., 1?3, 3?1, or 1?1-3?3), they can improve a regular CNN without extra inference costs. ResRep <ref type="bibr" target="#b14">[13]</ref> uses Structural Re-parameterization for channel pruning <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b29">28]</ref> and achieves state-ofthe-art results, which reduces the filters in a conv layer via constructing and pruning a following 1?1 layer. Re-pLKNet <ref type="bibr" target="#b17">[16]</ref> heavily uses very large (e.g., 31?31) convolutional kernels, where Structural Re-parameterization with small kernels helps to make up the optimization issue.</p><p>Locality Injection is a remarkable attempt to generalize Re-parameterization beyond convolution. By merging a conv into an FC kernel, we bridge conv and FC with a simple, platform-agnostic and differentiable method (Sec. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Locality Injection via Re-parameterization</head><p>This section derives the explicit formula (Eq. 12) to convert any given conv kernel into an FC kernel (a Toeplitz matrix), which is the key to further merging the conv into the parallel FC. The derivation can be safely skipped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>In this paper, a feature map is denoted by a tensor M ? R n?c?h?w , where n, c, h, w are the batch size, number of channels, height and width, respectively. We use F and W for the kernel of conv and FC, respectively. For the ease of re-implementation, we formulate the computation in a PyTorch-like <ref type="bibr" target="#b33">[32]</ref> pseudo-code style. For example, the data flow through a k ? k conv is formulated as</p><formula xml:id="formula_0">M (out) = CONV(M (in) , F, p) ,<label>(1)</label></formula><p>where M (out) ? R n?o?h ?w is the output, o is the number of output channels, p is the number of pixels to pad, F ? R o?c?k?k is the conv kernel (we temporarily assume the conv is dense, i.e., the number of groups is 1). From now on, we assume h = h, w = w for the simplicity.</p><p>For an FC, let p and q be the input and output dimensions, V (in) ? R n?p and V (out) ? R n?q be the input and output, respectively, the kernel is W ? R q?p and the matrix multiplication (MMUL) is formulated as</p><formula xml:id="formula_1">V (out) = MMUL(V (in) , W) = V (in) ? W .<label>(2)</label></formula><p>We now focus on an FC that takes M (in) as input and outputs M (out) and assume the output shape is the same as the input. We use RS (short for "reshape") as the function that only changes the shape specification of tensors but not the order of data in memory, which is cost-free. The input is first flattened into n vectors of length chw, which is V (in) = RS(M (in) , (n, chw)), multiplied by the kernel W(ohw, chw), then the output V (out) (n, ohw) is reshaped back into M (out) (n, o, h, w). For the better readability, we omit the RS if there is no ambiguity,</p><formula xml:id="formula_2">M (out) = MMUL(M (in) , W) .<label>(3)</label></formula><p>Obviously, such an FC cannot take advantage of the locality of images as it computes each output point according to every input point, unaware of the positional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Locality Injection</head><p>Assume there is a conv layer parallel to the FC (like the Channel Perceptron and Local Perceptron shown in <ref type="figure">Fig. 1</ref>), which takes M (in) as input, we describe how to equivalently merge it into the FC. In the following, we assume the FC kernel is W(ohw, chw), conv kernel is F(o, c, k, k), padding is p. Formally, we desire to construct W so that</p><formula xml:id="formula_3">MMUL(M (in) , W ) = MMUL(M (in) , W) + CONV(M (in) , F, p) .<label>(4)</label></formula><p>We note that for any kernel W (2) of the same shape as W <ref type="bibr" target="#b1">(1)</ref> , the additivity of MMUL ensures that</p><formula xml:id="formula_4">MMUL(M (in) , W (1) ) + MMUL(M (in) , W (2) ) = MMUL(M (in) , W (1) + W (2) ) .<label>(5)</label></formula><p>Therefore, we can merge F into W if we can construct W (F,p) of the same shape as W which satisfies</p><formula xml:id="formula_5">MMUL(M (in) , W (F,p) ) = CONV(M (in) , F, p) . (6)</formula><p>Obviously, for any M (in) , F, p, the corresponding W (F,p) must exist, since a conv can be viewed as a sparse FC that shares parameters among spatial positions (i.e., a Toeplitz matrix), but it is nontrivial to construct it with a given F.</p><p>Then we seek for the explicit formula of W (F,p) . With the formulation used before (Eq. 2), we have</p><formula xml:id="formula_6">V (out) = V (in) ? W (F,p) .<label>(7)</label></formula><p>We insert an identity matrix I (chw, chw) and use the associative law</p><formula xml:id="formula_7">V (out) = V (in) ? (I ? W (F,p) ) .<label>(8)</label></formula><p>With explicit RS, we rewrite Eq. 8 as</p><formula xml:id="formula_8">V (out) = V (in) ? RS(I ? W (F,p) , (chw, ohw)) . (9)</formula><p>We note that W (F,p) is constructed with an existing conv kernel F, so that I ? W (F,p) is exactly a convolution with F on a feature map M (I) which is reshaped from I. That is</p><formula xml:id="formula_9">I ? W (F,p) = CONV(M (I) , F, p) ,<label>(10)</label></formula><p>where M (I) is reshaped from a constructed identity matrix</p><formula xml:id="formula_10">M (I) = RS(I, (chw, c, h, w)) .<label>(11)</label></formula><p>Comparing Eq. 7 with Eq. 9, Eq. 10, we have</p><formula xml:id="formula_11">W (F,p) = RS(CONV(M (I) , F, p), (chw, ohw)) ,<label>(12)</label></formula><p>which is exactly the formula to construct W (F,p) with F, p.</p><p>In brief, the equivalent FC kernel of a conv kernel is the result of convolution on an identity matrix with proper reshaping. Better still, the conversion is efficient and differentiable, so one may derive the FC kernel during training and use it in the objective function (e.g., for penalty-based pruning <ref type="bibr" target="#b18">[17,</ref><ref type="bibr" target="#b20">19]</ref>). The formulas for the group-wise case are derived similarly and the code is released on GitHub.</p><p>Why is this solution nontrivial? Given F, p, there is no existing generic method to construct the corresponding Toeplitz matrix W (F,p) as the existing conv implementations do not require such a step. As discussed above, though W (F,p) must exist, it is nontrivial to construct it in a platform-agnostic way. Modern platforms use different algorithms of conv, e.g., based on iGEMM <ref type="bibr" target="#b17">[16]</ref> (for very large kernel), Winograd <ref type="bibr" target="#b25">[24]</ref> (for 3?3), im2col <ref type="bibr" target="#b2">[2]</ref> (converting the feature map, rather than the kernel, to a matrix), FFT <ref type="bibr" target="#b32">[31]</ref>, MEC <ref type="bibr" target="#b4">[4]</ref> and sliding-window). Moreover, the memory allocation of data and implementations of padding may be different. Therefore, given F, p and the FC kernel W (F,p) , the equivalency (Eq. 6) may hold on a platform but break on another platform (e.g., the simplest case is the two platforms arrange the kernels in memory differently). Our method (Eq. 12) is platform-agnostic because its derivation does not rely on the concrete form of CONV. On any platform, the W (F,p) constructed with its specific CONV implementation must ensure Eq. 6 with the same CONV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RepMLPNet</head><p>RepMLPNet is a hierarchical MLP-style architecture composed of RepMLP Blocks. We first introduce RepMLP Block ( <ref type="figure">Fig. 1)</ref> in Sec. 4.1 and then describe the overall architecture ( <ref type="figure" target="#fig_1">Fig. 2)</ref> in Sec. 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Components of RepMLP Block</head><p>A training-time RepMLP Block is composed of three parts termed as Global Perceptron, Channel Perceptron and Local Perceptron <ref type="figure">(Fig. 1)</ref>, which are designed to model the information on different levels. Global Perceptron models the coarse global dependencies across spatial locations  <ref type="figure" target="#fig_1">Figure 2</ref>. Architecture of RepMLPNet. Apart from RepMLP Blocks, we also use a FFN-style block (1?1-GELU [21]-1?1) to increase the depth, which has been widely used in Vision Transformers <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b28">27]</ref>, MLPs <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b38">37]</ref> and CNN (RepLKNet <ref type="bibr" target="#b17">[16]</ref>). among all the channels; Channel Perceptron is designed for modeling the long-range spatial dependencies within each channel; Local Perceptron captures the local patterns. The outputs of the three components are combined to obtain a comprehensive transformation of the input features.</p><p>Global Perceptron average-pools the inputs (n, c, h, w) into vectors (n, c, 1, 1) and feed them into two FC layers to obtain a vector that encodes the global information.</p><p>Channel Perceptron contains an FC layer that directly performs the feature transformation, where the key is the setsharing mechanism. We follow the formulation in Sec. 3 and assume o = c for the convenience. We note that a normal FC layer has (chw) 2 parameters. Taking a 64?64 feature map with 128 channels for example, the parameter count of a normal dense FC will be 2.1B, which is unacceptable. A natural solution is to make the FC "depth-wise" just like depth-wise conv, which will not be able to model cross-channel dependencies but has only 1/c parameters and FLOPs. However, even a parameter count of c(hw) 2 is too large. Our solution is to make multiple channels share a set of spatial-mapping parameters, so that the parameters are reduced to s(hw) 2 , where s is the number of share-sets of parameters. In other words, every c s channels share the same set of parameters, and there are s such share-sets in total. Specifically, we first evenly split the c channels into c/s groups, which means (n, c, h, w) ? ( nc s , s, h, w), and then flatten them into nc s vectors each of length shw, feed the vectors into a "depth-wise" FC, and reshape the output back. Compared to "depth-wise" FC, set-sharing FC not only breaks the correlation between channels ((chw) 2 parameters ? c(hw) 2 )), but reduces the parameters even further (c(hw) 2 ) ? s(hw) 2 ); but it does not reduce the computations compared to a "depth-wise" FC. It should be noted that the spatial mappings in ResMLP and MLP-Mixer are implemented in a different way (transpose, 1?1 conv and transpose back) but are mathematically equivalent to a special case of set-sharing FC with s = 1, which means all the channels share the same (hw) 2 parameters. We will show increasing s can improve the performance with more parameters but no extra FLOPs, which is useful in scenarios where the model size is not a major concern (Table 4).</p><p>In practice, though set-sharing FC is not directly supported by some computing frameworks like PyTorch, it can be implemented by a group-wise 1?1 conv. Formally, let V (in) ( nc s , shw) be the vectors split in share-sets, the implementation is composed of three steps: 1) reshaping V (in) as a "feature map" with spatial size of 1?1, which is ( nc s , shw, 1, 1); 2) performing 1?1 conv with s groups (so that the parameters are (shw) 2 /s = s(hw) 2 ); 3) reshaping the output into ( nc s , s, h, w), then (n, c, h, w). Local Perceptron takes the same inputs as Channel Perceptron. Each conv (with a following BN <ref type="bibr" target="#b23">[22]</ref> as a common practice) is depth-wise and has the same number of sharesets s on the s-channel inputs, so the kernel is (s, 1, k, k).</p><p>Merging the Local Perceptron into Channel Perceptron via Locality Injection requires fusing the BN into the preceding conv layers or FC3. Note the conv layers are depth-wise and the number of channels is s. Let F ? R s?1?k?k be the conv kernel, ?, ?, ?, ? ? R s be the accumulated mean, standard deviation and learned scaling factor and bias of the following BN, we construct the kernel F and bias b as</p><formula xml:id="formula_12">F i,:,:,: = ? i ? i F i,:,:,: , b i = ? ? i ? i ? i + ? i .<label>(13)</label></formula><p>Then it is easy to verify the equivalence:</p><formula xml:id="formula_13">? i ? i (CONV(M, F, p) :,i,:,: ? ? i ) + ? i = CONV(M, F , p) :,i,:,: + b i , ?1 ? i ? s ,<label>(14)</label></formula><p>where the left side is the original computation flow of a conv-BN, and the right is the constructed conv with bias. The FC3 and BN in Channel Perceptron are fused in a similar way into? ? R shw?hw ,b ? R shw . Then we convert every conv via Eq. 12 and add the resultant matrix onto?. The biases of conv are simply replicated by hw times (because all the points on the same channel share a bias value) and added ontob. Finally, we obtain a single FC kernel and a single bias vector, which will be used to parameterize the inference-time FC3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hierarchical Architectural Design</head><p>Some recent vision MLP models <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b38">37]</ref> show a similar design pattern: downsampling the inputs aggressively (e.g., by 16?) at the very beginning, and stacking multiple blocks to process the small-sized features. In contrast, we adopt a hierarchical design, which has proven effective by previous studies on CNNs <ref type="bibr" target="#b21">[20,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b43">42]</ref> and Transformers <ref type="bibr" target="#b28">[27]</ref>.</p><p>Specifically, we arrange RepMLP Blocks in four stages, and the blocks in a stage share the same structural hyperparameters. The input images are downsampled by 4? with an embedding layer, which is implemented by a conv layer with a kernel size of 4?4 and stride of 4. From a stage to the next, we use an embedding layer to halve the width and height of feature maps and double the channels. Therefore, a RepMLPNet can be instantiated with the following hyper-parameters: the number of RepMLP Blocks in each stage <ref type="figure" target="#fig_1">[B 1 , B 2 , B 3 , B 4</ref> ], the number of channels of the first stage C (the four stages will have C, 2C, 4C, 8C channels, respectively), the input resolution H?W (so that h 1 = H/4, w 1 = W/4, ..., h 4 = H/32, w 4 = W/32), and the number of share-sets of each stage ([S 1 , S 2 , S 3 , S 4 ]).</p><p>Considering the number of parameters in FC3 is s(hw) 2 , we use a smaller s at an earlier stage.</p><p>An advantage of our hierarchical architecture is that the feature maps produced by any stage can be readily used by a downstream framework. For example, UperNet <ref type="bibr" target="#b42">[41]</ref> requires four levels of feature maps with different sizes, so it cannot use MLP-Mixer or ResMLP as backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">ImageNet Classification</head><p>We first instantiate a series of RepMLPNets with different architectural hyper-parameters, as shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>We evaluate RepMLPNets on ImageNet <ref type="bibr" target="#b8">[8]</ref>. All the RepMLPNets are trained with identical settings: a global batch size of 256 distributed on 8 GPUs, AdamW <ref type="bibr" target="#b30">[29]</ref> optimizer with initial learning rate of 0.002, weight decay of 0.1 and momentum of 0.9. We train for only 100 epochs in total with cosine learning rate annealing, including a 10epoch warm-up at the beginning. We use label smoothing of 0.1, mixup <ref type="bibr">[45]</ref> with ? = 0.4, CutMix <ref type="bibr" target="#b45">[44]</ref> with ? = 1.0, and RandAugment <ref type="bibr" target="#b7">[7]</ref>. As a series of strong baselines, we present ResNet-101 <ref type="bibr" target="#b21">[20]</ref>, ResNeXt-101 <ref type="bibr" target="#b43">[42]</ref> and RegNetX-6.4GF <ref type="bibr" target="#b34">[33]</ref> trained with a strong scheme with RandAugment, mixup and label smoothing. We would like the comparison to be slightly biased towards the traditional CNNs, so we train them for 120 epochs. All the models are evaluated with single crop and the throughput (samples/second) is tested on 2080Ti GPU with a batch size of 128. The BN layers in CNNs are also fused for the fair comparison.</p><p>It should be noted that most of the results reported by <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b37">36,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b44">43]</ref> are produced with a long training schedule of 300 or 400 epochs, or an advanced distillation method (the DeiT-style training <ref type="bibr" target="#b39">[38]</ref>). Therefore, except for the results cited from the corresponding papers, we train an MLP-Mixer and a ResMLP-S12 with simple training settings for a fair comparison (labeled as "our impl" in <ref type="table">Table 2</ref>). Specifically, the MLP-Mixer is trained with the settings identical to RepMLPNets; the ResMLP-S12 is trained with the official code and the same hyper-parameters as its reported 120-epoch result <ref type="bibr" target="#b38">[37]</ref> except that we use a smaller batch size due to limited resources (but our reproduced accuracy is 2.7% higher than its reported result <ref type="bibr" target="#b38">[37]</ref>).</p><p>Compared to the other CNNs and MLPs, we make the following observations. 1) With fair settings, RepMLPNet shows superiority over the other MLPs, e.g., RepMLPNet-T256 outperforms MLP-Mixer (our impl, 256?256 inputs) by 0.5% in the accuracy while the FLOPs of the former is only 1/4 of the latter. MLPs are faster than CNNs, e.g., RepMLPNet-D256 has higher FLOPs than ResNeXt-101 but runs 1.6? as fast as the latter. This discovery suggests that MLPs are promising as high-throughput inference models.</p><p>To further demonstrate that FLOPs may not reflect the throughput <ref type="bibr" target="#b16">[15]</ref>, we train EfficientNet-B1/B2 <ref type="bibr" target="#b36">[35]</ref> with the aforementioned 120-epoch strong scheme and report the results in <ref type="table" target="#tab_2">Table 3</ref>. Interestingly, though EfficientNets have very low FLOPs, the actual performance on GPU is inferior: RepMLPNet-T224 has 4? FLOPs as EfficientNet-B1 but runs 1.9? as fast as the latter; with comparable throughput, the accuracy of RepMLPNet-B256 is 3.6% higher than EfficientNet-B2. We reckon the high throughput of RepMLPNet on GPU can be attributed to not only the efficiency of matrix multiplication but also the simplicity of architecture hence high degree of parallelism <ref type="bibr" target="#b31">[30]</ref>.</p><p>We then study the effects of two key designs in RepMLP Block: the Global Perceptron and the set-sharing linear mapping of FC3. We increase the number of share-sets S or ablate the Global Perceptron and observe the performance as well as the model size. <ref type="figure">Fig. 4</ref> shows that Global Perceptron only adds negligible parameters and FLOPs (0.5M) but improves the accuracy by around 1%. This is expected as the Local Perceptron and Channel Perceptron do not communicate information across channels, which is compensated by Global Perceptron. By increasing S, fewer channels will be sharing the same set of mapping parameters, resulting in a significant performance gain without any extra FLOPs. Therefore, for the application scenarios where the speed-accuracy trade-off is the primary concern while the model size is not (e.g., in high-power computing centers), we may increase S for higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Locality Injection Matters</head><p>We conduct ablation studies on CIFAR-100 <ref type="bibr" target="#b24">[23]</ref> and ImageNet to evaluate Locality Injection. Specifically, we build a small RepMLPNet with two stages, B = <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b6">6]</ref>, S = <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b33">32]</ref>, C = 128. As another benchmark model, we scale down ResMLP-12 by reducing the channel dimensions. Besides, we change the downsampling ratio at the very beginning of all the models to 2?, so that the embedding dimension becomes 16?16, which is close to the case of ResMLP designed for ImageNet (14?14).</p><p>For the ResMLP, we add 1?1 and 3?3 branches parallel to the spatial aggregation layer. Note that the spatial aggregation in ResMLP and MLP-Mixer is equivalent to our setsharing FC with only one share-set (i.e., all the channels use the same set of (hw) 2 parameters). In this case, all the conv layers should have s = 1 accordingly, which means a depthwise conv with one input channel and one output channel. Consequently, adding a 1?1 conv introduces only five parameters (one for the 1?1?1?1 kernel and four for the single-channel BN layer including ?, ?, ?, ?), so the whole model has only 5 ? 12 = 60 extra parameters. Similarly, adding a 3?3 layer brings only (3 ? 3 + 4) ? 12 = 156 parameters. Though the extra parameters and FLOPs are negligible, the speed is observably slowed down (e.g., with 1?1 and 3?3 conv, the training-time ResMLP-12 has only 0.4% higher FLOPs but runs 37% slower) due to the reduction of degree of parallelism <ref type="bibr" target="#b31">[30]</ref>, which highlights the significance of merging the conv layers into the FC. For the RepMLPNet, the extra parameters brought by adding a 3?3 conv is (3 ? 3 + 4)s due to the set-sharing mechanism.</p><p>All the ResMLPs and RepMLPNets on CIFAR-100 are trained with the same learning rate schedule and weight decay as described before, a batch size of 128 on a single GPU, and the standard data augmentation: padding to 40?40, randomly cropping to 32?32 and left-right random flipping. Predictably, the performance is not satisfactory since CIFAR-100 is too small for the FC layer to learn the inductive bias from the data. This discovery is consistent with the concurrent works <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b38">37]</ref> which highlight that MLPs show inferior performance on small datasets. Adding the conv branches only during training significantly boost the accuracy even though they are eventually eliminated. Impressively, though the ResMLP has only 216 extra training-time parameters, the accuracy raises by 9.51%, and we observe a similar phenomenon on RepMLPNet. We then experiment with RepMLP-T224/D256 on ImageNet.</p><p>With the Local Perceptron removed, the accuracy decreases by 2.15% and 2.30%, respectively. The gap is narrower compared to the results on CIFAR, which is expected because ImageNet is significantly larger, allowing the model to learn some inductive bias from data <ref type="bibr" target="#b37">[36]</ref>). In summary, as Locality Injection works on different models and datasets, we conclude that it is a universal tool for vision MLPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Semantic Segmentation</head><p>The hierarchical design of RepMLPNet qualifies it as a backbone for the off-the-shelf downstream frameworks that require feature maps of different levels, e.g., UperNet <ref type="bibr" target="#b42">[41]</ref>. However, transferring an ImageNet-pretrained MLP to the downstream task is challenging. Taking Cityscapes <ref type="bibr" target="#b6">[6]</ref> semantic segmentation as the example, we reckon there are two primary obstacles for using an MLP backbone. 1) The backbone is usually trained with low resolution (e.g., 256?256 on ImageNet) then transferred to the highresolution task (e.g., 1024?2048 of Cityscapes). However, the parameter count of MLP is coupled with the input resolution (by our specific definition of "MLP"), making the transfer difficult. 2) The resolution for training (512?1024 on Cityscapes) and testing (1024?2048) may not be the same, so the backbone has to adapt to a variable resolution.</p><p>In brief, our solution is to split the inputs into nonoverlapping patches, feed the patches into the backbone, restore the output patches to form the feature maps, which are then fed into the downstream frameworks. For example, the first RepMLP Block of RepMLPNet-D256 works with 64?64 inputs because the FC kernel is (64 2 , 64 2 ). On Cityscapes, we can split the feature map into several 64?64 patches and feed the patches into the RepMLP Block. How- ever, doing so breaks the correlations among patches hence hinders a global understanding of the semantic information. Accordingly, we propose to replace the embedding (2?2 conv) layers by regular conv (3?3 conv) layers to communicate information across the patch borders. Interestingly, one may worry such a strategy would yield poor results at the edges of patches, but we observe that the predictions at the edges are as good as the other parts (see the Appendix).</p><p>As the first attempt to use an MLP as the backbone for Cityscapes semantic segmentation, RepMLPNet delivers promising results <ref type="table" target="#tab_4">(Table 6</ref>). By further replacing the 3?3 downsampling layers by 5?5, the mIoU improves with negligible extra FLOPs, which is expected as a larger convolution enables better communications among patches. Specifically, we use the implementation of UperNet <ref type="bibr" target="#b42">[41]</ref> in MM-Segmentation [5] and the 80K-iteration training schedule. We present the details and analysis in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Limitations and Conclusions</head><p>This paper proposes a re-parameterization method to inject locality into FC layers, a novel MLP-style block, and a hierarchical MLP architecture. RepMLPNet is favorable compared to several concurrently proposed MLPs in terms of the accuracy-efficiency trade-off and the training costs.</p><p>However, as an MLP, RepMLPNet has several noticeable common weaknesses. 1) Similar to the Vision Transformers, MLPs are easy to overfit, requiring strong data augmentation and regularization techniques. 2) On the low-power devices like mobile phones, the model size of MLPs may be an obstacle. 3) Though the results of our first attempt to use MLP backbone for semantic segmentation are promising, we observe no superiority over the traditional CNNs.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Visualizing Locality Injection</head><p>We demonstrate the locality is injected into the FC kernel by showing the kernel weights.</p><p>Specifically, we visualize the weights of FC3 kernel sampled from the 10th RepMLP Block of the 3rd stage of RepMLPNet-D256 trained on ImageNet. We reshape the kernel intoW(s, h, w, 1, h, w), which is <ref type="bibr" target="#b17">(16,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b1">1,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b17">16)</ref>, then sample the weights related to the first input channel and the (7,7) point (marked by a purple square) on the first output channel, which isW 1,7,7,1,:,: (indices starting from 1). Then we take the absolute value, rescale by the minimum value of the whole matrix, and take the logarithm for the better readability.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, a point with darker color indicates the FC considers the corresponding position on the input channel more related to the output point at <ref type="bibr" target="#b7">(7,</ref><ref type="bibr" target="#b7">7)</ref>. Obviously, the original kernel has no locality as the marked point and the neighbors have no larger values than the others.</p><p>Then we merge the parallel conv layers into the kernel via Locality Injection. The resultant kernel has larger values around the marked point, suggesting that the model focuses more on the neighbors, which is expected. Besides, the kernel's capacity to model the long-range dependencies is not lost as some points (marked by red dashed rectangles) outside the largest conv kernel (3?3 in this case, marked by a blue square) still have larger values than some points inside. <ref type="figure">Figure 4</ref>. An example of using RepMLPNet as the backbone of UperNet. After 4? downsampling on the 512?1024 inputs, the feature map size becomes 128?256. Then we split the feature maps into 2?4 non-overlapping patches, each of 64?64, because the first RepMLP Block maps inputs of 64?64 into 64?64 (i.e., the FC kernel is (64 2 ,64 2 )). Similarly, the outputs of the four stages are reshaped back and fed into the UperNet. a 3?3 stride-2 depth-wise conv for downsampling. <ref type="figure">Fig. 5</ref> shows the difference between a 2? embedding and a 3?3 conv.</p><p>Interestingly, as the input is split into non-overlapping patches, one may expect that the predictions would be less accurate at the edges of patches, but we observe no such phenomenon. <ref type="figure">Fig. 6</ref> shows that the predictions at the edges are as good as the internal pixels within patches. This discovery suggests that the dependencies across patches have been well established and that the representational capacity of MLP is strong enough for such a dense prediction task.</p><p>We hope our results spark further research on the application of MLP on downstream tasks.  <ref type="figure">Figure 5</ref>. The difference between 2? embedding and 3?3 conv is that the latter realizes communications across patch edges. In this figure, a square denotes a pixel on a feature map and the thick lines denote the edges of patches. We take the upper left corner of a patch as an example <ref type="figure">Figure 6</ref>. The predictions at the edges of patches are no observably worse. We show two images from the Cityscapes validation set as examples. As the test resolution is 1024?2048, the input to the first RepMLP Block is 256?512 (after the beginning 4? downsampling), so that the input is split into 32 non-overlapping patches and then fed into RepMLP Blocks. We use red dashed lines to denote the edges of patches and it is observed that the predictions at the edges are almost as good as the other parts.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 )</head><label>2</label><figDesc>With simple training methods, ResMLP and MLP-Mixer significantly degrade, e.g., the accuracy of ResMLP-S12 drops by 8.9% (76.6% ? 67.7%) without the 300-epoch DeiT-style training. 3) RepMLPNet with 100-epoch training delivers a favorable accuracy-efficiency trade-off: RepMLPNet-B256 matches the accuracy of ResMLP-B24 without DeiT-style distillation, consumes 1/4 training epochs, has only 40% FLOPs and fewer parameters. 4) With the comparable FLOPs,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc>A) Original FC kernel (B) FC kernel with locality injected</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>FC kernel before and after Locality Injection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>RepMLP Block ?B 1 RepMLP Block ?B 2 RepMLP Block ?B 3 RepMLP Block ?B 4 RepMLP Block</head><label></label><figDesc></figDesc><table><row><cell>input</cell><cell>( , 3, 256, 256)</cell><cell></cell></row><row><cell>Embed 4?</cell><cell>a stage</cell><cell>BN</cell></row><row><cell></cell><cell>( , , 64, 64)</cell><cell></cell></row><row><cell>Stage1</cell><cell></cell><cell></cell></row><row><cell>Embed 2?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>( , 2 , 32, 32)</cell><cell>conv 1x1 BN</cell></row><row><cell></cell><cell></cell><cell>GELU</cell></row><row><cell>Stage2</cell><cell></cell><cell>conv 1x1</cell></row><row><cell>Embed 2?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>( , 4 , 16, 16)</cell><cell>BN</cell></row><row><cell></cell><cell></cell><cell>RepMLP</cell></row><row><cell>Stage3</cell><cell></cell><cell>Block</cell></row><row><cell>Embed 2?</cell><cell></cell><cell></cell></row><row><cell></cell><cell>( , 8 , 8, 8)</cell><cell>BN</cell></row><row><cell></cell><cell></cell><cell>conv 1x1</cell></row><row><cell>Stage4</cell><cell></cell><cell>GELU</cell></row><row><cell></cell><cell></cell><cell>conv 1x1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Architectural hyper-parameters of RepMLPNet models (T for tiny, B for base, D for deep, L for large). Of note is that RepMLP-D256 is deeper than RepMLP-B256 but narrower so that they have comparable FLOPs and number of parameters.</figDesc><table><row><cell>Name</cell><cell>Input</cell><cell>B</cell><cell>C</cell><cell>S</cell></row><row><cell cols="5">RepMLP-T224 224?224 [2, 2, 6, 2] 64 [1, 4, 16, 128]</cell></row><row><cell cols="5">RepMLP-B224 224?224 [2, 2, 12, 2] 96 [1, 4, 32, 128]</cell></row><row><cell cols="5">RepMLP-T256 256?256 [2, 2, 6, 2] 64 [1, 4, 16, 128]</cell></row><row><cell cols="5">RepMLP-B256 256?256 [2, 2, 12, 2] 96 [1, 4, 32, 128]</cell></row><row><cell cols="5">RepMLP-D256 256?256 [2, 2, 18, 2] 80 [1, 4, 16, 128]</cell></row><row><cell cols="5">RepMLP-L256 256?256 [2, 2, 18, 2] 96 [1, 4, 32, 256]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparisons with EfficientNets. The throughput (samples/second) is tested on 2080Ti GPU with a batch size of 128.</figDesc><table><row><cell>Model</cell><cell cols="4">Input Top-1 acc FLOPs (B) Throughput</cell></row><row><cell cols="2">RepMLPNet-T224 224</cell><cell>76.4</cell><cell>2.8</cell><cell>1709</cell></row><row><cell>EfficientNet-B1</cell><cell>240</cell><cell>76.3</cell><cell>0.7</cell><cell>912</cell></row><row><cell cols="2">RepMLPNet-B256 256</cell><cell>81.0</cell><cell>9.6</cell><cell>708</cell></row><row><cell>EfficientNet-B2</cell><cell>260</cell><cell>77.4</cell><cell>1.0</cell><cell>707</cell></row><row><cell cols="4">Table 4. Ablation studies on RepMLP-T224.</cell><cell></cell></row><row><cell>S</cell><cell cols="4">Global Perceptron Top-1 acc FLOPs Params</cell></row><row><cell>[1, 4, 16, 128]</cell><cell></cell><cell>75.78</cell><cell cols="2">2.7B 37.8M</cell></row><row><cell>[1, 4, 16, 128]</cell><cell></cell><cell>76.48</cell><cell cols="2">+0.5M 38.3M</cell></row><row><cell>[2, 8, 32, 256]</cell><cell></cell><cell>75.94</cell><cell cols="2">2.7B 66.7M</cell></row><row><cell>[2, 8, 32, 256]</cell><cell></cell><cell>77.19</cell><cell cols="2">+0.5M 67.2M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Studies on the effect of Locality Injection. The throughput is tested on the same 2080Ti GPU with a batch size of 128 and measured in samples/second. Note the throughput is observably reduced by adding the conv layers with negligible FLOPs.Dataset Model 1 ? 1 conv 3 ? 3 conv Top-1 acc FLOPs Params Throughput</figDesc><table><row><cell></cell><cell>ResMLP-12</cell><cell>55.58</cell><cell>1812M</cell><cell>7.1M</cell><cell>2561</cell></row><row><cell>CIFAR-100</cell><cell>ResMLP-12 ResMLP-12</cell><cell>57.96 63.76</cell><cell cols="2">+786K +7077K +156 +60</cell><cell>2093 1858</cell></row><row><cell></cell><cell>ResMLP-12</cell><cell>65.09</cell><cell cols="2">+7864K +216</cell><cell>1619</cell></row><row><cell></cell><cell>RepMLPNet</cell><cell>59.07</cell><cell>468M</cell><cell>8.3M</cell><cell>6273</cell></row><row><cell>CIFAR-100</cell><cell>RepMLPNet RepMLPNet</cell><cell>60.73 65.36</cell><cell cols="2">+294K +2654K +2640 +720</cell><cell>5872 5721</cell></row><row><cell></cell><cell>RepMLPNet</cell><cell>67.43</cell><cell cols="2">+2949K +3360</cell><cell>5328</cell></row><row><cell></cell><cell>RepMLPNet-T224</cell><cell>74.33</cell><cell>2.79B</cell><cell>38.3M</cell><cell>1709</cell></row><row><cell>ImageNet</cell><cell>RepMLPNet-T224 RepMLPNet-D256</cell><cell>76.48 78.58</cell><cell cols="2">+10M 8.61B 86.94M +20K</cell><cell>1354 715</cell></row><row><cell></cell><cell>RepMLPNet-D256</cell><cell>80.88</cell><cell>+26M</cell><cell>+60k</cell><cell>570</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Results on Cityscapes val set. By replacing the 3?3 downsampling layers with 5?5, the mIoU further increases.</figDesc><table><row><cell>Backbone</cell><cell>FLOPs mIoU</cell></row><row><cell>ResNet-101</cell><cell>2049.82G 79.03</cell></row><row><cell>RepMLPNet-D256</cell><cell>1960.01G 76.27</cell></row><row><cell cols="2">RepMLPNet-D256 (conv5) 1960.16G 77.12</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper, an MLP refers to a model that mostly uses FC layers to linearly map features from a vector to another, so that the number of parameters must be proportional to the input size and output size. By our definition, another model, CycleMLP<ref type="bibr" target="#b3">[3]</ref>, is not referred to as an MLP.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Details of Semantic Segmentation</head><p>We solve the problem of using MLP as the backbone for semantic segmentation (e.g., UperNet) by 1) the hierarchical design, 2) splitting feature maps into non-overlapping patches and 3) communications between patches. <ref type="figure">Fig. 4</ref> shows an example of RepMLPNet + UperNet. 1) UperNet requires feauture maps of 4 different levels, which fits our hierarchical architecture. 2) Since RepMLP Block works with a fixed input feature map size, we split the feature maps into non-overlapping patches each with the required size.</p><p>3) The original 2? embedding cannot realize inter-patch communication, so we replace it with 3?3 conv. To reduce the computational cost of 3?3 conv, we decompose it into a 1?1 conv for expanding the channels and </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ImageNet Results. The throughput (samples/second) is tested on the same 2080Ti GPU with a batch size of 128</title>
	</analytic>
	<monogr>
		<title level="m">Model Input resolution Train epochs Top-1 acc FLOPs (B) Params (M) Throughput</title>
		<imprint/>
	</monogr>
	<note>Table 2</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Gcnet: Non-local networks meet squeeze-excitation networks and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High performance convolutional neural networks for document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Chellapilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sidd</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Workshop on Frontiers in Handwriting Recognition. Suvisoft</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cyclemlp: A mlp-like architecture for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoufa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongjian</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.10224</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mec: memory-efficient convolution for deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Brand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="815" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="702" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Centripetal sgd for pruning very deep convolutional networks with complicated structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4943" to="4953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Approximated oracle filter pruning for destructive cnn width optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1607" to="1616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1911" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Manipulating identical filter redundancy for efficient pruning on deep and complicated cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14444</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Resrep: Lossless cnn pruning via decoupling remembering and forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diverse branch block: Building a convolution as an inception-like unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03697</idno>
		<title level="m">Repvgg: Making vgg-style convnets great again</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.06717</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Global sparse momentum sgd for pruning very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gaussian error linear units (gelus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast algorithms for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4013" to="4021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanan</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><forename type="middle">Peter</forename><surname>Graf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08710</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Pruning filters for efficient convnets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08050</idno>
		<title level="m">Pay attention to mlps</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjie</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
	<note>Yoshua Bengio and Yann LeCun</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.5851</idno>
		<title level="m">Fast training of convolutional networks through ffts</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Mlpmixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07477</idno>
		<title level="m">S2-mlp: Spatial-shift mlp architecture for vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
