<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Huynh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Kornblith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><forename type="middle">Khademi</forename><surname>Google</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">TTI-Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised representation learning has made significant leaps fueled by progress in contrastive learning, which seeks to learn transformations that embed positive input pairs nearby, while pushing negative pairs far apart. While positive pairs can be generated reliably (e.g., as different views of the same image), it is difficult to accurately establish negative pairs, defined as samples from different images regardless of their semantic content or visual features. A fundamental problem in contrastive learning is mitigating the effects of false negatives. Contrasting false negatives induces two critical issues in representation learning: discarding semantic information and slow convergence. In this paper, we propose novel approaches to identify false negatives, as well as two strategies to mitigate their effect, i.e. false negative elimination and attraction, while systematically performing rigorous evaluations to study this problem in detail. Our method exhibits consistent improvements over existing contrastive learning-based methods. Without labels, we identify false negatives with ?40% accuracy among 1000 semantic classes on ImageNet, and achieve 5.8% absolute improvement in top-1 accuracy over the previous state-of-the-art when finetuning with 1% labels. Our code is available at https://github.com/google-research/fnc * Work done during an internship at Google.</p><p>Correspondence to Tri Huynh &lt;trihuynh@google.com&gt;, Maryam Khademi &lt;maryamkhademi@google.com&gt;.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Representation learning has become the backbone of most modern AI agents. High quality pretrained representations are essential to improving performance on downstream tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b28">29]</ref>. While conventional approaches rely on labeled data, there has been a recent surge in self-supervised representation learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b31">32]</ref>. In fact, self-supervised representation learning has been closing the gap with and, in some cases, even surpassing its supervised counterpart <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>. Notably, most state-of-the-art methods are converging around and fueled by the central concept of contrastive learn- ing <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In contrastive learning, the embedding space is governed by two opposing forces, the attraction of positive pairs and repellence of negative pairs, effectively actualized through the contrastive loss. Without labels, recent breakthroughs rely on the instance discrimination task in which positive pairs are defined as different views of the same image, while negative pairs are formed by sampling views from different images, regardless of their semantic information <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b34">35]</ref>. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this process. Positive pairs generated from different views of the same image are generally reliable since they likely contain similar semantic content or correlated features <ref type="bibr" target="#b18">[19]</ref>. However, the creation of valid negative pairs is far more difficult. The common approach of defining negative pairs as samples from different images ignores their semantic content. For example, two images of a dog are considered a negative pair, as seen in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Contrasting undesirable negative pairs encourages the model to discard their common features through the embedding, which are indeed the common semantic content, e.g., dog features in the previous example. We define those undesirable negatives as false negatives, i.e., negative pairs from the same semantic category. Besides disregarding semantic information, false negatives also hinder the convergence of contrastive learning-based objectives due to the appearance of contradicting objectives. For instance, in <ref type="figure" target="#fig_0">Figure 1</ref>, the dog's head on the left is attracted to its fur (positive pair), but repelled from similar fur of another dog image on the right (negative pair), creating contradicting objectives. While recent efforts focus on improved architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b23">24]</ref> and data augmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b43">44]</ref>, relatively little work considers the effects of negative samples, especially that of false negatives. Most existing methods focus on mining hard negatives <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27]</ref>, or most recently, reweighing positive and negative terms <ref type="bibr" target="#b11">[12]</ref>. However, there has been little effort attempting to identify false negatives.</p><p>False negatives remain a fundamental problem in contrastive self-supervised learning. Without labels, this problem is very hard to adequately resolve, as it boils down to a chicken-and-egg problem, where we want to learn good semantic representations, but may need certain semantic information to start with. Nevertheless, in this paper, we attempt to study this problem in detail and propose novel ideas to overcome its limitations, as overviewed in <ref type="figure" target="#fig_1">Figure  2</ref>. Particularly, the contributions of the paper are as follows:</p><p>? We propose simple yet effective strategies to find potential false negatives in contrastive learning. Without labels, the method effectively finds false negatives with ?40% accuracy among 1000 semantic categories on ImageNet <ref type="bibr" target="#b40">[41]</ref>. ? We propose and study the effect of two different strategies to improve the contrastive loss based on the estimated false negatives, i.e., false negative elimination and attraction. The experiments reveal the significant impact false negatives have on contrastive learning. ? Our method consistently improves over existing approaches across a wide range of settings, e.g., with or without momentum contrast <ref type="bibr" target="#b23">[24]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early work employs proxy tasks to guide the learned embeddings, e.g. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b45">[46]</ref>. While effective, the proxy tasks are heuristic and lack generality.</p><p>Other works use clustering-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b49">50]</ref>. Caron et al. <ref type="bibr" target="#b5">[6]</ref> iteratively improve the representations by clustering samples and using these clusters as pseudolabels. They then train the network to classify samples based on these pseudo-labels. While our elimination-based strategy is distinct from clustering, our attraction-based approach also attempts to group visually connected samples, yet, the formulation and context differ.</p><p>We take a contrastive learning-based approach to selfsupervised representation learning. Earlier work in this area includes CPC <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b24">25]</ref>, Deep InfoMax <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b1">2]</ref>, and CMC <ref type="bibr" target="#b42">[43]</ref>. Recognizing the importance of a large negative pool, PIRL <ref type="bibr" target="#b34">[35]</ref> maintains a memory bank of all image representations, which limits scalability. MoCo <ref type="bibr" target="#b23">[24]</ref> addresses this using a momentum encoder. SimCLR <ref type="bibr" target="#b8">[9]</ref> eschews a momentum encoder in favor of a large batch size, and proposes updates to the projection head and data augmentation.</p><p>Realizing the important role of negative samples in contrastive learning, a few recent methods investigate ways to improve negative sampling. However, most works focus on the effect of hard negatives (i.e., the true negatives that are close to the anchor, which are distinct from false negatives) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b26">27]</ref>. Arguing that hard negatives could be noisy, Ring <ref type="bibr" target="#b47">[48]</ref> proposes to sample negatives within percentile ranges. Besides hard negatives, other methods extend to also consider hard positives. While <ref type="bibr" target="#b46">[47]</ref> performs hard selection of samples, <ref type="bibr" target="#b53">[54]</ref> utilizes soft selection by conditional probability. Chuang et al. <ref type="bibr" target="#b11">[12]</ref> reweigh positive and negative terms to reduce the effects of undesirable negatives. While formulated differently, their approach is similar to multi-crop <ref type="bibr" target="#b6">[7]</ref>, in that it increases the contribution of positive terms by subtracting them in the denominator of the contrastive loss, whereas multi-crop does so by adding positive terms in the numerator. However, both methods fail to identify false negatives and lack semantic feature diversity.</p><p>Inspired by contrastive learning, recent methods predict one positive view from another. However, these methods are not categorized as contrastive learning. They have different formulations at their core and do not contrast against negative samples, a defining element of contrastive learning. SwAV <ref type="bibr" target="#b6">[7]</ref> is an online clustering-based method that employs swapping prediction of different views from an image, while BYOL [23] employs a momentum encoder as a prediction target for another view from the main encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive Learning</head><p>Contrastive learning seeks to learn a transformation that brings positive pairs "nearby" in an embedding space while pushing negative pairs apart. This is done by minimizing a contrastive loss that, for each anchor image i, measures the (negative) similarity between its embedding z i and that of its positive match z j relative to the similarity between the anchor embedding of k ? {1, . . . , M } negative matches:</p><formula xml:id="formula_0">l i = ? log exp(sim(z i , z j )/? ) M k=1 1 [k =i] exp(sim(z i , z k )/? ) ,<label>(1)</label></formula><p>where sim(u, v) is a similarity function, e.g., the L 2 normalized cosine similarity sim(u, v) = u T v/ u v , and ? is a temperature parameter. Without known correspondence, self-supervised methods commonly define positive pairs as different augmentations of the same image and negative pairs as samples from different images. Consider a batch of N images, each augmented to form N positive pairs for a total of 2N images. Methods like SimCLR use samples from the same batch as negative pairs (i.e., M = 2N examples for each anchor i), while MoCo uses samples from a momentum encoder to avoid the use of a large batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">False Negative Cancellation</head><p>Negative pairs are samples from different images that may or may not possess similar semantic content or visual features. Consequently, it is possible that some samples k have the same semantic content as the anchor i, and are thus false negatives. As discussed earlier, false negatives give rise to two critical problems in contrastive learning: they discard semantic information and slow convergence.</p><p>Supposing we can find the false negatives, we propose two strategies that use them to improve contrastive learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">False Negative Elimination</head><p>The simplest strategy for mitigating the effects of false negatives is to not contrast against them. This amounts to the following modification to the contrastive objective <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_1">l elim i = ? log exp(sim(z i , z j )/? ) 2N k=1 1 [k =i,k / ?Fi] exp(sim(z i , z k )/? ) ,<label>(2)</label></formula><p>where F i is the set of the detected false negatives with respect to an anchor i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">False Negative Attraction</head><p>While eliminating false negatives alleviates the undesireable effects of contrasting against them, it ignores information available in what are actually true positives. Minimizing the original contrastive loss (Eqn. 1) only seeks to attract an anchor to different views of the same image. Including true positives drawn from different images would increase the diversity of the training data and, in turn, has the potential to improve the quality of the learned embeddings. Indeed, Khosla et al. <ref type="bibr" target="#b27">[28]</ref> show that supervised contrastive learning (i.e., where an anchor is attracted to samples having the same semantic label) can be more effective than the traditional supervised cross-entropy loss. Thus, we propose to treat the false negatives that have been identified as true positives and attract the anchor to this set. This yields the following expression for the contrastive loss:</p><formula xml:id="formula_2">l att i = ? 1 1 + |F i | log exp(sim(z i , z j )/? ) 2N k=1 1 [k =i] exp(sim(z i , z k )/? ) + f ?Fi log exp(sim(z i , z f )/? ) 2N k=1 1 [k =i] exp(sim(z i , z k )/? )<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Finding False Negatives</head><p>Unfortunately, the process of identifying false negatives is fundamentally difficult, amounting to a chicken-and-egg problem-without labels, the learned semantic information can be used to establish valid and invalid correspondences, yet the correctness of these embeddings depends on the ability to identify correspondences. Our approach to identify false negatives based on the following observations:</p><p>? False negatives are samples from different images with the same semantic content, therefore they should hold certain similarity (e.g., dog features). ? A false negative may not be as similar to the anchor as it is to other augmentations of the same image, as each augmentation only holds a specific view of the object <ref type="figure" target="#fig_2">(Figure 3</ref>).</p><p>The above observations mean that we may be able to approximate a false negative with more augmented views of the anchor. As an example, consider <ref type="figure" target="#fig_2">Figure 3</ref> where we treat the picture of the dog's head on the left ("main views," in red) as the anchor image. The support views on the left are other augmented views generated from the same image and serve as positive matches. The picture of the dog's head on the far right ("main view," in blue) is not an augmented version of the anchor. Consequently, while it is similar to the anchor image, it would thus be treated as a negative match by contemporary self-supervised methods (i.e., it is a false negative). However, we see that this image is more similar to the augmented view of the anchor ("support view," in blue) than it is to the anchor with respect to the orientation of the dog's face. Similarly, the head and fur of the dog are expected to be a positive pair. While the false negative fur on the far right (orange) could look different than the head anchor, it should be more similar to the fur in the support views (orange).</p><p>Motivated by these observations, we propose a strategy for identifying candidate false negatives that follows as: </p><formula xml:id="formula_3">F i = best(score i ),</formula><p>where score i = {score m,i |m} is the set of scores for each negative sample with respect to anchor i.</p><p>For each element in the above procedure, there are several considerations one can make, including the choice of the similarity function, the strategy for aggregating scores, and  the manner in which the most similar samples are defined. In this work, we investigate the following options:</p><p>Similarity Function We use the cosine similarity function, since it is used in the contrastive loss during pretraining.</p><p>Aggregation Strategy We consider both mean aggregation, score m,i = 1 |S| |S| s=1 sim(z m , z s i ), and max aggregation, score m,i = max s?{1,...,|S|} sim(z m , z s i ), and discuss their effects in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Screening Strategy</head><p>We consider two choices for the most similar samples, one that considers the top-k matches, best(score i ) = {z m |score m,i ? top(score i , k)}, and one that considers those above a threshold t, best(score i ) = {z m |score m,i &gt; t}. A top-k strategy may be preferred given information about the approximate number of false negatives, while thresholding may be better suited when a dynamic adaptation is expected. We also consider a strategy that combines top-k and and thresholding, best(score i ) = {z m |score m,i ? top(score i , k) &amp; score m,i &gt; t}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Studies</head><p>We use the same configuration as SimCLR v2 for pretraining and evaluation. The base encoder is ResNet-50 with a 3-layer MLP projection head. Data augmentation includes random crops, color distortion, and Gaussian blur. For each experiment, we pretrain for 100 epochs on the Im-ageNet ILSVRC-2012 training set, then freeze the encoder and train a linear classifier on top, which is then evaluated on the ImageNet evaluation set. We pretrain on 128 Cloud TPUs with a batch size of 4096. We use the LARS optimizer <ref type="bibr" target="#b20">[21]</ref> with a learning rate of 6.4 and a cosine decay schedule, and a weight decay of 1 ? 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">False Negative Cancellation Strategies</head><p>We evaluate the effects of various approaches to false negative mitigation, including the choice of cancellation strat-egy, aggregation score, and screening strategies, and draw the following conclusions.</p><p>False negative elimination consistently improves contrastive learning across crop sizes, and the gap is higher for bigger crops. <ref type="figure" target="#fig_4">Figure 4</ref> (left) demonstrates that the inclusion of false negative elimination yields top-1 accuracy that is strictly better than that of the SimCLR baseline accross the full range of crop ratios. We postulate that the bigger gap for larger crop sizes is due to the increased chance of having common semantic content in big crops, which leads to a higher ratio of false negatives. It is also worth noting that in <ref type="figure" target="#fig_4">Figure 4</ref> (left), we only eliminate a negligible number of two potential false negatives among 8190 (batch size 4096) negative samples for each anchor, but it could affect top-1 accuracy by as much as 1%. This supports the significant effect of false negatives in contrastive learning.</p><p>Having a support set helps in finding false negatives regardless of the cancellation strategy, with greater benefits with the attraction strategy. <ref type="figure" target="#fig_5">Figure 5</ref> contrasts the top-1 accuracy when we compare negative samples to a support set (Section 3.2.3) to the case in which we only compare negative samples to the anchor itself. The use of a support set results in larger performance gains when using false negative attraction (?2%) compared to the false negative elimination strategy (?0.2%). Further, while the elimination strategy improves performance relative to the Sim-CLR baseline whether or not a support set is used, attracting false negatives found without support set actually hurts performance ( <ref type="figure" target="#fig_5">Figure 5, right)</ref>. This likely results from the fact that embeddings learned with attraction strategy are more sensitive to invalid false negatives (discussed next), justifying the use of a support set to reliably find false negatives.</p><p>The attraction strategy is much more sensitive to the quality of the found false negatives compared to the elimination strategy. This property is consistently confirmed through <ref type="figure" target="#fig_5">Figure 5</ref> (right) as previously discussed, and <ref type="figure" target="#fig_4">Figure 4</ref> (right), where the attraction strategy only works with more reliable false negatives, those having very high similarity scores, while the elimination method is not very sensitive to the thresholds.    Max aggregation significantly and consistently outperforms mean aggregation for the attraction strategy, while the gains are less pronounced with false negative elimination. <ref type="figure" target="#fig_6">Figure 6</ref> demonstrates that max aggregation outperforms mean aggregation for all support sizes and topk values in the attraction strategy, with a gap in some cases greater than 1%. This may be due to the fact that false negatives are similar to a strict subset of the support set, in which case considering all elements as in mean aggregation corrupts the similarity score. The difference is more pronounced for the attraction strategy, which is more sensitive to invalid false negatives.  Filtering by top-k tends to perform better than by a threshold, while a combination of both provides the best balance. As seen in <ref type="figure" target="#fig_7">Figure 7</ref>, the best choice of top-k is better than the best threshold. A strategy that combines the two approaches achieves greater accuracy, with the exception of false negative attraction at top-4, for which there is a negligible degradation in performance.   when the detected false negatives are valid. As shown in <ref type="figure" target="#fig_8">Figure 8</ref> (left) and <ref type="table" target="#tab_1">Table 1</ref>, false negative elimination improves upon SimCLR by 1.02%, while false negative attraction results in a 1.75% improvement. These results use max aggregation, a support size of eight, top-4 for false negative attraction, and top-8 filtering for elimination. As false negative attraction works better, we employ the attraction strategy in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False negative attraction is superior to elimination</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">With Multi-crop</head><p>Caron et al. <ref type="bibr" target="#b6">[7]</ref> propose a multi-crop data augmentation strategy that increases the number of positive views attracted to each anchor image, improving the quality of the learned embeddings. Multi-crop is closely related and complementary to false negative attraction in multiple facets, from principle to computational efficiency. While multicrop attracts more positive samples, false negative attraction tries to attract samples that would otherwise be erroneously treated as negatives. The positive samples should be more reliable than the false negatives we attempt to find; however, multi-crop lacks semantic feature diversity, as it never attracts samples from different images. Because of these characteristics, multi-crop and false negative attraction offer complementary advantages. Furthermore, they can share computational overhead by using a common support set. Thus far, we have only used the support set to find false negatives, but they can also be used as additional positive views for multi-crop. In doing so, we may be able to double the performance without noticeable overhead if the respective improvements are complementary. <ref type="figure" target="#fig_9">Figure 9</ref> and <ref type="table">Table 2</ref> demonstrate the advantages of using false negative cancellation together with multi-crop data augmentation. Adding multi-crop improves the performance of the SimCLR baseline by 2.09%, while incurring an additional 4.77 hours of computational overhead. Further adding false negative attraction on top of multi-crop yields a 1.92% absolute improvement in accuracy (i.e., similar to the 2.09% gain provided by multi-crop), while incurring only 0.1 hours of computation time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">With Momentum Encoders</head><p>Thus far, we have identified false negatives in the current batch from a single encoder. However, other methods may store negative samples in a memory bank, or encoded by a momentum encoder. Here, we investigate the behaviors of the proposed method in these settings. Momentum contrast <ref type="bibr" target="#b23">[24]</ref> employs two encoders, the main encoder and momentum encoder, and a memory bank where negative samples are stored alongside the samples from the current batch. This offers more options for finding false negatives, i.e., whether to use the support set from the main encoder or the momentum encoder, or whether to find negatives from samples in the current batch or all samples in memory. <ref type="figure" target="#fig_8">Figure 8 (right)</ref> indicates that it is better to generate the support set using samples from the momentum encoder compared to the main encoder. Further, finding false negatives in the memory bank yields greater top-1 accuracy than drawing false negatives from the current batch.  <ref type="table">Table 3</ref>. Top-1 accuracy improvement of false negative cancellation for different baselines. <ref type="figure" target="#fig_9">Figure 9</ref> and <ref type="table">Table 3</ref> show that our method works across different configurations, with or without the presence of either momentum contrast or multi-crop. Notably, the performance gain from false negative cancellation in the presence of momentum contrast is even higher at 2.27%, a substantial boost in this large-scale ImageNet-1K setting. For context, this is larger than recent improvements in the state-of-theart, such as BYOL over InfoMin (1.3%) <ref type="bibr" target="#b22">[23]</ref>, and MoCo over LocalAgg (1.8%) <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">With True Labels &amp; False Negatives Accuracy</head><p>We now consider the effectiveness of our false negative detection and cancellation strategy relative to the ideal setting in which we have access to ground-truth labels, which provides an upper-bound on performance. <ref type="figure" target="#fig_0">Figure 10</ref>    the top-1 accuracy when using false negative cancellation combined with multi-crop data augmentation as well as the accuracy that results from false negative cancellation using ground-truth labels. As expected, cancelling false negatives helps substantially (8.18%) when true labels are used. However, we close half the gap by just using multi-crop and the false negatives our method finds, increasing top-1 accuracy by 4.01% over the SimCLR baseline.</p><p>To better understand the extent to which we are able to identify false negatives, <ref type="figure" target="#fig_0">Figure 10</ref> (right) plots the accuracy of the false negative detections over 100 epochs of pretraining. We see that the false negative detection accuracy steadily increases, reaching approximately 40% accuracy by 100 epochs. Note that the false negatives accuracy is computed based on human-defined semantic labels, with 1000 categories in ImageNet. The chance of finding a false negative for an anchor at random is just 0.1%. <ref type="table" target="#tab_9">Table 4</ref> compares the computational cost of our false negative cancellation strategy compared to the SimCLR baseline. As expected, we can see that for the same number of epochs, the process of detecting and incorporating false negatives incurs additional computational time. However, to achieve the same accuracy, SimCLR requires more than three times the amount of computation (26.22 h and 1000 epochs) than our framework (7.50 h and 100 epochs). <ref type="figure" target="#fig_0">Figure 11</ref> shows that the embedding learned with false negative cancellation indeed demonstrates better class separation compared to baseline SimCLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Computational Efficiency &amp; Visualization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Art</head><p>We compare our improved model with false negative cancellation to other state-of-the-art methods on standard ImageNet evaluations and transfer to downstream tasks. Accuracy (%) <ref type="figure" target="#fig_0">Figure 10</ref>. A visualization of (left) top-1 accuracy with false negative cancellation using detected vs. ground-truth labels and (right) the accuracy of false negative detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SimCLR v2</head><p>False Negative Cancellation SimCLR v2 False Negative Cancellation <ref type="figure" target="#fig_0">Figure 11</ref>. t-SNE visualizations of SimCLR and FNC for 10 random classes (left) and 10 dog classes (right) from ImageNet.</p><p>Pretraining Settings We use similar configurations as SimCLR v2. Specifically, we use ResNet-50 as the base encoder, with a 3-layer MLP projection head. We use a 65k memory buffer for the momentum encoder, with a momentum of 0.999. Following Tian et al. <ref type="bibr" target="#b43">[44]</ref>, we use random crops, color distortion, Gaussian blur, and RandAugment <ref type="bibr" target="#b13">[14]</ref> for data augmentation. For false negative cancellation, we use the attraction strategy with max aggregation. The top-k is set to 10 and a threshold of 0.7 is used for filtering the scores. The support size is 8, which is shared with multi-crop. We pretrain for 1000 epochs on 128 Cloud TPUs with a batch size of 4096. We use the LARS optimizer with a learning rate of 6.4, a cosine schedule for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Learning Contrastive learning</head><p>MoCo v1 <ref type="bibr" target="#b23">[24]</ref> 60.6 -PIRL <ref type="bibr" target="#b34">[35]</ref> 63.6 -PCL <ref type="bibr" target="#b32">[33]</ref> 65.9 -SimCLR v1 <ref type="bibr" target="#b8">[9]</ref> 69.3 89.0 MoCo v2 <ref type="bibr" target="#b10">[11]</ref> 71.   <ref type="table">Table 7</ref>. Transfer learning on classification task using ImageNet-pretrained ResNet models across 12 data sets.</p><p>Method AP50 Supervised 81.3 MoCo v2 <ref type="bibr" target="#b10">[11]</ref> 82.5 SwAV <ref type="bibr" target="#b6">[7]</ref> 82.6 FNC (ours) 82.8 decaying the learning rate, and a weight decay of 1 ? 10 ?4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">ImageNet Evaluation</head><p>Our evaluation on ImageNet follows a protocol similar to that of SimCLR v2. Specifically, we conduct linear evaluation, in which a linear classifier is trained on top of the frozen pretrained features, and consider semi-supervised settings, in which we finetune the network with 1% and 10% labels available. We use a batch size of 1024 with a 0.16 learning rate, while weight decay is removed. We finetune for 90 epochs in linear evaluation, 60 epochs for 1%, and 30 epochs for 10% labels in semi-supervised settings. <ref type="table">Table 5</ref> presents the linear evaluation results. Our method achieves state-of-the-art performance for contrastive learning-based models with a top-1 accuracy of 74.4%, a 2.7% improvement over SimCLR v2 and a 1.4% boost from the previous best. Among all approaches, our method is second only to SwAV <ref type="bibr" target="#b6">[7]</ref>, a clustering-based method, and reaches competitive results with BYOL <ref type="bibr" target="#b22">[23]</ref>, a recent state-of-the-art method that only uses positive samples. <ref type="table" target="#tab_12">Table 6</ref> presents the results in the semi-supervised setting, in which our proposed model not only exceeds other contrastive learning-based methods, but also achieves the best performance among all models across all measures. The strong semi-supervised performance could be attributed to our improvement upon the baseline SimCLR v2, which is already quite good in this setting. Notably, in the 1% labels case, our method significantly improves over the previous best to achieve 63.7% top-1 accuracy (a 5.8% absolute improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Transferring Features</head><p>Image Classification Following SimCLR v1, we perform the same evaluations on 12 classification datasets: Food <ref type="bibr" target="#b4">[5]</ref>, CIFAR10 <ref type="bibr" target="#b30">[31]</ref>, CIFAR100 <ref type="bibr" target="#b30">[31]</ref>, Birdsnap <ref type="bibr" target="#b3">[4]</ref>, SUN397 <ref type="bibr" target="#b48">[49]</ref>, Cars <ref type="bibr" target="#b29">[30]</ref>, Aircraft <ref type="bibr" target="#b33">[34]</ref>, VOC2007 <ref type="bibr" target="#b16">[17]</ref>, DTD <ref type="bibr" target="#b12">[13]</ref>, Pets <ref type="bibr" target="#b37">[38]</ref>, Caltech-101 <ref type="bibr" target="#b17">[18]</ref>, and Flowers <ref type="bibr" target="#b35">[36]</ref>. We follow the same setup as in SimCLR v1. As Table 7 demonstrates, our approach achieves a significant improvement in performance among contrastive learningbased methods (i.e., SimCLR v1 and SimCLR v2) in both settings. In linear evaluation, our method outperforms Sim-CLR v1 on all 12 datasets and is better than SimCLR v2 on all but one dataset. In finetuning, the proposed model outperforms both SimCLR v1 and v2 on all but one dataset, and matches that of BYOL, with each being superior on about half of the datasets.</p><p>Object Detection To further evaluate the transferability of the learned embeddings, we finetune the model on PASCAL VOC object detection. We use similar settings as in MoCo <ref type="bibr" target="#b23">[24]</ref>, where we finetune on the VOC trainval07+12 set using Faster R-CNN with a R50-C4 backbone, and evaluate on VOC test2007. We train for 34K iterations with batch size 16. The learning rate is set to 0.02, which is reduced by a factor of 10 after 20K and 28K iterations. As <ref type="table" target="#tab_13">Table 8</ref> reveals, our proposed method outperforms both MoCo v2, SwAV, and the supervised baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we address a fundamental problem in contrastive self-supervised learning that has not been adequately studied, identifying false negatives, and propose strategies to utilize this ability to improve contrastive learning frameworks. In addition to bringing novel insights to this topic through in-depth experimental analysis, our proposed method significantly boosts existing models, and sets new performance standards for contrastive self-supervised learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>False negatives in contrastive learning. Without knowledge of labels, automatically selected negative pairs could actually belong to the same semantic category, creating false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed framework. Left: Original definition of the anchor, positive, and negative samples in contrastive learning. Middle: Identification of false negatives (blue). Right: false negative cancellation strategies, i.e. elimination and attraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Finding false negatives with the support views. Negative samples (main views, right) may not have as reliable similarity with the anchor itself (red) as they do with other augmented views of the same image (support views). For instance, the dog's face in the support view (left, blue) is closer to the negative sample (right, blue) in terms of the facial orientation than the anchor (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>A comparison of top-1 accuracy (left) between false negative elimination and SimCLR (lower bound of random crop ratio represents the lowest cropping ratio in random image augmentation); and (right) top-1 accuracy across filtering thresholds in false negative cancellation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>False negative cancellation with and without support set across top-k choices for different mitigation strategies. The dashed line denotes the performance of the SimCLR baseline. The results use mean aggregation in scoring potential false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>False negative cancellation with mean and max aggregation across support sizes and top-k for the false negative (left) elimination and (right) attraction strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>A comparison of top-k and threshold-based filtering for false negative (left) elimination and (right) attraction strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>A comparison of top-1 accuracy (left) for different false negative cancellation strategies relative to the SimCLR baseline and (right) the use of the primary and momentum encoders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>False negative cancellation with (left) multi-crop and (right) momentum encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>presents Model Epochs Time (h) Acc. (%)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Bas elin e Sim CLR FN Can cella tion FN Can cella tion + Mul ti-cr op FN Can cella tion with True Lab els</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1 .</head><label>1</label><figDesc>For each anchor i, generate a support set S i = {z s i } that contains other support views from the same image besides the two main views. 2. Compute similarity scores, score s m,i = sim(z m , z s i ), between a negative sample z m and each sample z s i in the support set. 3. Aggregate the computed scores for each negative sample, score m,i = aggregate s?S (score s m,i ). 4. Define a set of potential false negatives F i as the negative samples that are most similar to the support set based on the aggregated scores,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Top-1 accuracy improvement of false negative cancellation strategies over the SimCLR baseline.</figDesc><table><row><cell cols="4">SimCLR FN Elimination / Gain FN Attraction / Gain</cell></row><row><cell>66.41</cell><cell>67.43 / +1.02</cell><cell>68.16 / +1.75</cell></row><row><cell>Method</cell><cell></cell><cell>Accuracy (Diff.)</cell><cell>Time (Diff.)</cell></row><row><cell>SimCLR</cell><cell></cell><cell>66.41</cell><cell>2.63</cell></row><row><cell cols="2">SimCLR + Multi-crop</cell><cell>68.50 (+2.09)</cell><cell>7.40 (+4.77)</cell></row><row><cell cols="2">SimCLR + Multi-crop + FN Att.</cell><cell>70.42 (+1.92)</cell><cell>7.50 (+0.10)</cell></row><row><cell cols="4">Table 2. Complementary performance and computational effi-</cell></row><row><cell cols="3">ciency of multi-crop and false negative attraction.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 .</head><label>4</label><figDesc>Computational efficiency and accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 .</head><label>6</label><figDesc>ImageNet semi-supervised evaluation. Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers Avg</figDesc><table><row><cell>Linear eval</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR v1 [9]</cell><cell>68.4</cell><cell>90.6</cell><cell>71.6</cell><cell>37.4</cell><cell>58.8</cell><cell>50.3</cell><cell>50.3</cell><cell>80.5</cell><cell>74.5 83.6</cell><cell>90.3</cell><cell>91.2</cell><cell>70.6</cell></row><row><cell cols="2">SimCLR v2 [10] 73.9</cell><cell>92.4</cell><cell>76.0</cell><cell>44.7</cell><cell>61.0</cell><cell>54.9</cell><cell>51.1</cell><cell>81.2</cell><cell>76.5 85.0</cell><cell>91.2</cell><cell>93.5</cell><cell>73.4</cell></row><row><cell>BYOL [23]</cell><cell>75.3</cell><cell>91.3</cell><cell>78.4</cell><cell>57.2</cell><cell>62.2</cell><cell>67.8</cell><cell>60.6</cell><cell>82.5</cell><cell>75.5 90.4</cell><cell>94.2</cell><cell>96.1</cell><cell>77.6</cell></row><row><cell>FNC (ours)</cell><cell>74.4</cell><cell>93.0</cell><cell>76.8</cell><cell>54.0</cell><cell>63.2</cell><cell>68.8</cell><cell>61.3</cell><cell>83.0</cell><cell>76.3 89.0</cell><cell>93.5</cell><cell>94.9</cell><cell>77.3</cell></row><row><cell>Finetuned</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR v1 [9]</cell><cell>88.2</cell><cell>97.7</cell><cell>85.9</cell><cell>75.9</cell><cell>63.5</cell><cell>91.3</cell><cell>88.1</cell><cell>84.1</cell><cell>73.2 89.2</cell><cell>92.1</cell><cell>97.0</cell><cell>85.5</cell></row><row><cell cols="2">SimCLR v2 [10] 88.2</cell><cell>97.5</cell><cell>86.0</cell><cell>74.9</cell><cell>64.6</cell><cell>91.8</cell><cell>87.6</cell><cell>84.1</cell><cell>74.7 89.9</cell><cell>92.3</cell><cell>97.2</cell><cell>85.7</cell></row><row><cell>BYOL [23]</cell><cell>88.5</cell><cell>97.8</cell><cell>86.1</cell><cell>76.3</cell><cell>63.7</cell><cell>91.6</cell><cell>88.1</cell><cell>85.4</cell><cell>76.2 91.7</cell><cell>93.8</cell><cell>97.0</cell><cell>86.3</cell></row><row><cell>FNC (ours)</cell><cell>88.3</cell><cell>97.7</cell><cell>86.8</cell><cell>76.3</cell><cell>64.2</cell><cell>92.0</cell><cell>88.5</cell><cell>84.7</cell><cell>76.0 90.9</cell><cell>93.6</cell><cell>97.5</cell><cell>86.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 .</head><label>8</label><figDesc>Transfer learning on Pascal VOC object detection.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15535" to="15545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tikhoncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3846" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Food-101 Mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hinton. Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical data augmentation with no separate search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l. Conf. on Computer Vision (ICCV)</title>
		<meeting>Int&apos;l. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge. Int&apos;l</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR Workshop on Generative-Model Based Vision</title>
		<meeting>the CVPR Workshop on Generative-Model Based Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07780</idno>
		<title level="m">Shortcut learning in deep neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large batch training of convolutional networks with layer-wise adaptive rate scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<title level="m">Supervised contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Satellite image-based localization via learned embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S J</forename><surname>Dengand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l. Conf. on Computer Vision (ICCV)</title>
		<meeting>Int&apos;l. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Indian Conf. on Computer Vision, Graphics and Image Processing</title>
		<meeting>of the Indian Conf. on Computer Vision, Graphics and Image essing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04592</idno>
		<title level="m">Contrastive learning with hard negative samples</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<title level="m">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by invariance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fuchun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3510" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conditional negative sampling for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SUN Database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03746</idno>
		<title level="m">Contrastive attraction and contrastive repulsion for representation learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
