<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haokui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intellifusion</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenze</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intellifusion</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Intellifusion</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, vision transformers started to show impressive results which outperform large convolution based models significantly. However, in the area of small models for mobile or resource constrained devices, ConvNet still has its own advantages in both performance and model complexity. We propose ParC-Net, a pure ConvNet based backbone model that further strengthens these advantages by fusing the merits of vision transformers into ConvNets. Specifically, we propose position aware circular convolution (ParC),, a light-weight convolution op which boasts a global receptive field while producing location sensitive features as in local convolutions. We combine the ParCs and squeeze-exictation ops to form a meta-former like model block, which further has the attention mechanism like transformers. The aforementioned block can be used in plug-and-play manner to replace relevant blocks in ConvNets or transformers. Experiment results show that the proposed ParC-Net achieves better performance than popular light-weight ConvNets and vision transformer based models in common vision tasks and datasets, while having fewer parameters and faster inference speed. For classification on ImageNet-1k, ParC-Net achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11% parameters and 13% computational cost but gaining 0.2% higher accuracy and 23% faster inference speed (on ARM based Rockchip RK3288) compared with MobileViT, and uses only 0.5? parameters but gaining 2.7% accuracy compared with DeIT. On MS-COCO object detection and PAS-CAL VOC segmentation tasks, ParC-Net also shows better performance. Source code is available at https: //github.com/hkzhang91/ParC-Net</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, various vision transformers (ViTs) models have achieved remarkable results in many vision tasks, forming strong alternatives to convolutional neural networks (ConvNets) <ref type="bibr" target="#b4">[5]</ref> [33] <ref type="bibr" target="#b20">[21]</ref>.</p><p>However, we believe both ViTs and ConvNets are indispensable for the following reasons: 1) From application perspective, both ViTs and ConvNets have their advantages and disadvantages. ViT models generally have better performance but usually suffer from high computational cost and are difficult to train <ref type="bibr" target="#b32">[33]</ref>. Compared with ViTs, Con-vNets may show inferior performance, but they still have some unique advantages. For instance, ConvNets have better hardware support and are easy to train. In addition, as is summarized in <ref type="bibr" target="#b8">[9]</ref> and our experiments, ConvNets still dominate in the area of small models for mobile or edge devices. 2) From the information processing perspective, both ViTs and ConvNets have unique features. ViTs are good at extracting global information and use attention mechanism to extract information from different locations driven by input data <ref type="bibr" target="#b2">[3]</ref>  <ref type="bibr" target="#b24">[25]</ref>. ConvNets focus on modeling local relationships and have strong prior by inductive bias <ref type="bibr" target="#b3">[4]</ref>. The above analysis naturally raise a question: can we learn from ViTs to improve ConvNets for mobile or edge computing applications?.</p><p>In this paper, we aim to design new light-weight pure ConvNets that further enhance its strength in the area of mobile and edge computing friendly models. Pure convolution is more mobile friendly because convolutions are highly optimized by existing tool chains that are widely used to deploy model into these resource constrained devices. Even more, because of the huge popularity of ConvNets in the past few years, some existing neural network accelerators are designed mainly around convolution style operations, and the complex non-linear operations such as softmax and data bus bandwidth demanding large matrix multiplications are not efficiently supported. These hardware and software constraints make a pure convolutional light-weight model more preferable even if a ViT based model is equally competitive in other aspects.</p><p>To design such a ConvNet, we compare ConvNets with ViTs and summarize three main differences between them: 1) ViTs are good at extracting global features <ref type="bibr" target="#b2">[3]</ref> [25] <ref type="bibr" target="#b3">[4]</ref>; 2) ViTs adopt Meta-former block <ref type="bibr" target="#b39">[40]</ref>; 3) Information aggregations in ViTs are data driven (data dependent dynamic computation). Corresponding to these three points, we de-sign our ParC block. 1) We propose the position aware circular convolution (ParC) to extract global features; 2) Based on the proposed ParC, we build a pure ConvNet Metaformer block as the basic outer structure; 3) We add channel wise attention module to the feature forward network (FFN) part of meta-former, which makes our proposed ParC block adapt kernel weights according inputs. Finally, inspired by CoatNet <ref type="bibr" target="#b3">[4]</ref> and MobileViT <ref type="bibr" target="#b24">[25]</ref>, we use a bifurcate structure (section 3.2) as the outer frame to build a complete network ParC-Net.</p><p>Experiment results show that the proposed ParC-Net achieves solid performance on three popular vision tasks, including image classification, object detection and semantic segmentation. Taking experiment results of image classification as an example, ParC-Net achieves 78.6% top-1 accuracy with about 5.0 million parameters, saving 11% parameters and 13% computational cost but gaining 0.2% higher accuracy and 23% faster inference speed (on Rockchip RK3288) compared with MobileViT <ref type="bibr" target="#b24">[25]</ref>. For experiments of object detection and semantic segmentation, compared with other light-weight models, the proposed ParC-Net achieves higher mAP and mIOU, while having fewer parameters.</p><p>Our main contributions are summarized as follows:</p><p>? To overcome the restriction that traditional convolutions have limited perception fields, we propose position aware circular convolution (ParC), where baseinstance kernel and position embedding strategies are used to handle input size variations and inject location information to output feature maps respectively. We jointly use the proposed ParC and conventional convolution operations to extract local-global features, which brings higher accuracy.</p><p>? We propose ParC-Net, a pure ConvNet for mobile and edge computing applications. The proposed ParC-Net inherits advantages of ConvNets and ViTs. To our knowledge, this is the first attempt that combines strengths of ConvNets and ViTs to design a lightweight ConvNet.</p><p>? We apply the proposed ParC-Net on three vision tasks.</p><p>Compared with the baseline model, the proposed ParC-Net achieves better performance on all three tasks, while having fewer parameters, lower computational cost and higher inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Vision transformers</head><p>Vaswani et al. firstly proposed transformer <ref type="bibr" target="#b33">[34]</ref> for natural language processing (NLP) tasks. Compared with recurrent neural network (RNN) models, transformer has much higher computational efficiency and it is good at capturing relationship from any pair of elements in the input sequence. As a result, transformers replaced RNNs and dominate the NLP field.</p><p>In 2020, Dosovitskiy et al. introduced transformer into vision tasks and proposed vision transformer (ViT) <ref type="bibr" target="#b4">[5]</ref>, where each image is cropped into a sequence of patches to meet the input requirement of transformer and PE is adopted to ensure the model is sensitive to position information of the input patches. With pre-training on huge datasets such as JFT-300M <ref type="bibr" target="#b28">[29]</ref>, ViT achieves impressive performance on various vision tasks. However, the original ViT model has some restrictions, for instance, it is heavy-weight, having low computational efficiency and hard to train. Subsequent variants of ViTs are proposed to overcome these problems. From the point of improving training strategy, Touvron et al. <ref type="bibr" target="#b32">[33]</ref> proposed to use knowledge distillation to train ViT models, and achieved competitive accuracy with less pre-training data. To further improve the model architecture, some researchers attempted to optimize ViTs by learning from ConvNets. Among them, PVT <ref type="bibr" target="#b34">[35]</ref> and CVT <ref type="bibr" target="#b36">[37]</ref> insert convolutional operations into each stage of ViT model to reduce the number of tokens, and build hierarchical multi-stage structures. Swin transformer <ref type="bibr" target="#b20">[21]</ref> computes self attention within shifted local windows. PiT <ref type="bibr" target="#b10">[11]</ref> jointly use pooling layer and depth wise convolution layer to achieve channel multiplication and spatial reduction. CC-Net <ref type="bibr" target="#b14">[15]</ref> propose a simplified version of self attention mechanism criss-cross attention and inserted it into ConvNet to build ConvNet which has global receptive field. These papers clearly show that some techniques of ConvNets can be applied on vision transformers to design better vision transformer models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Hybrid structures combining ConvNet and vision transformers</head><p>Another popular line of research is combining elements of ViTs and ConvNets to design new backbones. Graham et al. mixed ConvNet and transformer in their LeVit model, which significantly outperforms previous ConvNet and ViT models with respect to the speed/accuracy tradeoff <ref type="bibr" target="#b7">[8]</ref>. BoTNet <ref type="bibr" target="#b27">[28]</ref> replaces the standard convolution with multi-head attention in the last several blocks of ResNet. ViT-C <ref type="bibr" target="#b37">[38]</ref> adds early convolutional stem to vanilla ViT. ConViT <ref type="bibr" target="#b5">[6]</ref> incorporates soft convolutional inductive biases via a gated positional self-attention. The CMT <ref type="bibr" target="#b8">[9]</ref> block consists of depth wise convolution based local perception unit and a light-weight transformer module. Coat-Net <ref type="bibr" target="#b3">[4]</ref> merges convolution and self-attention to design a new transformer module, which focuses on both local and global information. After comprehensive comparison, we find that these hybrid models simultaneously employed similar structure, that is using convolutional stem to extract lo- cal features in the beginning stages and transformer style models later to extract global or local-global features. We choose a similar structure when designing our pure convolutional model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Light-weight ConvNets and ViTs</head><p>Since 2017, light-weight ConvNets attract much attentions as more and more applications needs to run Con-vNet models on mobile devices. Now, there are a lot of light-weight ConvNets, such as ShuffleNets <ref type="bibr" target="#b23">[24]</ref> [24], Mo-bileNets <ref type="bibr" target="#b12">[13]</ref> [27] <ref type="bibr" target="#b11">[12]</ref>, MicroNet <ref type="bibr" target="#b17">[18]</ref>, GhostNet <ref type="bibr" target="#b9">[10]</ref>, Ef-ficientNet <ref type="bibr" target="#b31">[32]</ref>, TinyNet <ref type="bibr" target="#b1">[2]</ref> and MnasNet <ref type="bibr" target="#b30">[31]</ref>. Compared with standard ConvNets, light-weight ConvNets have fewer parameters, lower computational cost and faster inference speed. In addition, light-weight ConvNets can be applied on a wide range of devices. Despite these benefits, these lightweight models have inferior performance compared with heavy-weight models. Very recently, following the research line of combining strengths of ConvNet and ViT, some researcher attempted to build light-weight hybrid models for mobile vision tasks. Mobile-Former presents a parallel design of MobileNet and transformer, which leverages the advantages of MobileNet at extracting local features and transformer at capturing global information <ref type="bibr" target="#b2">[3]</ref>. Mehta and Rastegari proposed MobileViT, where the upper stages of MobileNetv2 <ref type="bibr" target="#b26">[27]</ref> are replaced with MobileViT block <ref type="bibr" target="#b24">[25]</ref>. In MobileViT block, local representations extracted by convolution and global representations are concatenated to gen-erate local-global representations.</p><p>In terms of purpose, our proposed ParC-Net is related to Mobile-Former and MobileViT. Different from these two models which still keep transformer blocks, our proposed ParC-Net is pure ConvNet, which makes our proposed ParC-Net more mobile friendly. Our experiments of deploying models on low power platform confirm this point. In terms of designing a pure ConvNet via learning from ViTs, our work is most closely related to a parallel work ConvNext <ref type="bibr" target="#b21">[22]</ref>. The two major differences are: 1) Ideas and architectures are different. The ConvNext modernizes a standard ResNet toward the design of a vision transformer by introducing a series (more than ten) of incremented but effective designs. Our proposed ParC-Net starts from three main differences between ConvNets and ViTs and fills the gaps from macro level. As the ideas are different, the corresponding structures are also different; 2) They are proposed for different purposes. Our ParC-Net is proposed for mobile devices. Compared with ConvNext, the proposed ParC-Net shows advantages when constraining models as light-weight models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The proposed method</head><p>In this section, we will introduce our ParC-Net in two parts, the details of the building block (ParC block) and the overall model structure (ParC-Net). </p><formula xml:id="formula_0">x p ParC-V x p x p stacking vertically * ParC-H pe V C?H?W C?B?1 C?H?1 F EV stacking horizontally * (a) C?H?W C?(2H-1)?W C?B?1 C?H?1 C?H?W C?1?B C?1?W C?H?W C?H?(2W-1) k F pe V x p x y C?H?W EH F C?1?W C?1?B F C?H?W (b) x x p x p y k</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">ParC block</head><p>The upper part of <ref type="figure" target="#fig_0">Figure 1</ref> shows three major differences between common ConvNets and ViTs. The bottom half of <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the architecture of our proposed ParC block. In the following, we will explain the motivation and the specific structure of each component of the proposed ParC block.</p><p>Extracting global features with ParC. In ConvNets, feature is calculated as y i = j?L(i) w i?j x j , where x i , y i are the input and output at position i respectively, and L(i) denotes a local neighborhood of i. In ViTs, selfattention modules extracts features based on formula</p><formula xml:id="formula_1">y i = j?G e (x T i x j ) k?G e (x T i x k ) x j ,</formula><p>where G means the global spatial space. Comparing these two formulas, we can see that self attention learns global features from the entire spatial locations but convolution gathers information from a local receptive field.</p><p>To overcome this issue, we propose the position aware circular convolution (ParC). As shown in <ref type="figure">Figure 2</ref>, our proposed ParC has two types, one is ParC of vertical direction (ParC-V) and the other one is ParC of horizontal direction (ParC-H). The receptive field of the ParC-V and ParC-H covers all pixel in the same column and the same row, respectively. Jointly using ParC-V and ParC-H can extract global features from all input pixels. For notational simplicity, we assume the input x has only one channel and the corresponding shape is 1 ? h ? w. The output of ParC-V at location (i, j) is computed with:</p><formula xml:id="formula_2">pe V = F ( pe V ) = [pe V 0 , pe V 1 , ? ? ? , pe V h?1 ] T pe V e = EV (pe V , w) k V = F ( k) = [k V 0 , k V 1 , ? ? ? , k V h?1 ] x p = x + pe V e y i,j = t?(0,h?1) k V t x p ((i+t)modh,j)<label>(1)</label></formula><p>where, pe V is instance position embedding (PE) and it is generated from a base embedding pe V via bilinear interpolation function F (). Here F () is used to adapt the size of position embedding to the size of input features. pe V e is expanded PE. k V is instance kernel. EV () is an expand function of vertical direction. After copying the input vector w times, EV () concatenates these copied vectors along horizontal direction to generate a h ? w-sized PE matrix. Similarly, the output of ParC-H at location (i, j) can be expressed as:</p><formula xml:id="formula_3">x p = x + pe H e y i,j = t?(0,w?1) k H t x p (i,(j+t)modw)<label>(2)</label></formula><p>where pe H e = EH(pe H , h) and EH() is an expand function. EH() expands input vector along the vertical direction. Implementing the ParC in modern deep learning libraries is straightforward. Taking the most complicated part y i,j = t?(0,w?1) k H t x p (i,(j+t)modw) as an example, it can be implemented with one line of code: y = F.conv2D(torch.cat(x p , x p , dim = 3), k H ). <ref type="figure">Figure 3</ref> illustrates the computational process in the case that the input is an one dimensional vector. From <ref type="figure">Figure 3</ref>, we can see that ParC-H perform convolutions along a circle generated by connecting the start and the end of the input. So, we ? The receptive field is increased to global spatial space. Note that, increasing the kernel size of tradition local convolution to full input size does not extract global features. In local convolution, zero padding is usually used to keep the size of convolutional feature the same with that of the input. Even if we increase the kernel size to global size, the global kernel only covers part pixels coming from input. Especially for extracting feature in edge portion, only about half of pixels that covered by global kernel are from input actual input, while others are simply zeros.</p><p>? The PE is used to keep the output feature sensitive to spatial location. Circular convolution can extract global features, but it disturbed the spatial structure of the original input. For classification, keeping spatial structure may not be a big issue. But, as is shown in ablation study, for location sensitive tasks such as segmentation and detection, keeping spatial structure does matter. Here, following the design in ViTs, we introduce PE to keep spatial structure. Experiment results in ablation study show that PE is useful in segmentation and detection tasks</p><p>? The kernel and PE are dynamically generated according to the input size. In ParC, the sizes of kernels and PE codes must be consistent with that of instance inputs. To handle the case that inputs have different spatial resolution, we generate instance kernels and PE codes via interpolation functions.</p><p>Designing ParC block with ParC. From ConvNets to ViTs, a considerable modification is meta-former block replaced residual block (the blue two-way arrow). A Metaformer block generally consists of a sequence of two components: a token mixer and a channel mixer. The token mixer is for exchanging information among tokens in different spatial locations. The channel mixer is for mixing information among different channels. Both two components use residual learning structure. Inspired by this, we insert ParC into Meta-former like block to build our ParC block. Specifically, we replace selfattention module with the proposed ParC to build an new spatial module to replace token mixer part. Here, we do this for two main reasons: 1) ParC can extract global features and interacts information among pixels from global space, which meets the requirement of token mixer module; 2) the computation complexity of self attention module is quadratic. Replacing this part with ParC can reduce computational cost significantly, which is helps achieving our goal of designing a light-weight ConvNet. Based on the proposed ParC, we build a pure ConvNet meta-former like block. Adding channel wise attention in channel mixer part.In ViTs, self attention module can adapt weights according input, which makes ViTs data driven models. By adopting attention mechanism, data driven models can focus on important features and suppress unnecessary ones, which brings better performance. Previous literature <ref type="bibr" target="#b13">[14]</ref>[36] <ref type="bibr" target="#b15">[16]</ref> already explained the importance of keep model data driven.</p><p>By replacing the self-attention with the proposed global circular convolution, we get a pure ConvNet which can extract global features. But the replaced model is no longer a data driven model. To compensate, we insert channel wise attention module into channel mixer part, as shown in <ref type="figure" target="#fig_0">Figure 1(c)</ref>. Following SENet <ref type="bibr" target="#b13">[14]</ref>, we first aggregate spatial information of input features x ? R c?h?w via global average pooling and get aggregated feature x a ? R c?1?1 ; Then we feed x a into a multi-layer perceptron to generate channel wise weight a ? R c?1?1 . The a is multiplied with x channel wise to generate the final output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ParC-Net</head><p>In section 3.1, we have presented the ParC block, which is a basic block and can be inserted into most of the current existing models. In this section, we select an outer frame for it and build the complete network ParC-Net.</p><p>Currently, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>, existing hybrid structures can be basically divided into three main structures, including serial structure <ref type="figure" target="#fig_2">(Figure 4(a)</ref>) <ref type="bibr" target="#b7">[8]</ref> <ref type="bibr" target="#b37">[38]</ref>, parallel structure <ref type="figure" target="#fig_2">(Figure 4(b)</ref>) <ref type="bibr" target="#b2">[3]</ref> and bifurcate structure <ref type="figure" target="#fig_2">(Figure 4(c)</ref>) <ref type="bibr" target="#b24">[25]</ref> <ref type="bibr" target="#b3">[4]</ref>. Among all three structures, the third one achieves best performance for now. At present, bifurcate model CoatNet <ref type="bibr" target="#b3">[4]</ref> achieves the highest classification accuracy on Imagenet-1k. Mobile device aimed model Mobile-ViT <ref type="bibr" target="#b24">[25]</ref> also adopts the third structure.</p><p>Inspired by this, we adopt bifurcate structure as our outer frame and build our final outer frame based on MobileViT. Specifically, taking the outer frame adopted in MobileViT  as baseline, we further make some improvements:</p><p>? MobileViT consists of two major types of modules. Shallow stages consist of MobileNetV2 blocks, which have local receptive field. Deep stages are made up of ViT blocks, which enjoy global receptive field. We keep all MobileNetV2 blocks and replacing ViT blocks with corresponding ParC blocks. This replacement converts the model from hybrid structure to pure Con-vNet.</p><p>? We appropriately increase the widths of ParC blocks. Even so, the replaced model still has fewer parameters and less computational cost.</p><p>? As show in <ref type="figure" target="#fig_2">Figure 4</ref>(c), the bifurcate structure contains some interaction modules, which are in charge of interacting information between local and global feature modules. In the original MobileViT, ViT blocks are the most heavy modules. After replacing ViT blocks with ParC blocks, the cost of these interaction modules becomes prominent. So, we introduce group convolution and point wise convolution into these modules, which decreases number of parameters without hurting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment results</head><p>In experiments, we show the overall advantages of the proposed ParC-Net on three typical vision tasks, and then conduct detailed study to show the value of our design choices, the model scaling characteristics, and its speed advantage on low power devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image classification</head><p>We conduct image classification experiments on ImageNet-1k, the most widely used benchmark dataset for this task. We train the proposed ParC-Net models on the training set of ImageNet-1K, and report top-1 accuracy on the validation set. Training setting. As we adopt MobileViT like structure as our outer framework, we train our model using a very similar training strategy as well. To be specific, we train each model for 300 epochs on 8 V100 or A100 GPUs with AdamW optimizer <ref type="bibr" target="#b22">[23]</ref>, where the maximum learning rate, minimum learning rate, weight decay and batchsize are set to 0.004, 0.0004, 0.025 and 1024 respectively. Optimizer momentum ? 1 and ? 2 of the AdamW optimizer are set to 0.9 and 0.999 respectively. We use the first 3000 iterations as warm up stage. We adjust learning rate following the cosine schedule. For data augmentation, we use random cropping, horizontal flipping and multi-scale sampler. We use label smoothing <ref type="bibr" target="#b29">[30]</ref> to regularize the networks and set smoothing factor to 0.1. We use Exponential Moving Average (EMA) <ref type="bibr" target="#b25">[26]</ref>. More details of the training settings and link to source code will be provided in supplementary materials.</p><p>Comparison results. The experiment results of image classification are listed in <ref type="figure">Figure 5</ref>. <ref type="figure">Figure 5 (a)</ref> shows that ParC-Net-S and MobileViT-S beat other model by a clear margin. <ref type="figure">Figure 5 (b)</ref> shows comparison with more models. The proposed ParC-Net-S achieves highest classification accuracy, and have fewer parameters than most models. Compared with the second best model MobileViT-S, our ParC-Net-S decreases the number of parameters by 11% and increases the top 1 accuracy by 0.2 percentage points.</p><p>Light-weight models. <ref type="table">Table ?</ref>? shows comparison results among light-weight models, which confirms our ideas and answers the question proposed in introduction.</p><p>Firstly, comparing results of light-weight ConvNets with that of ViTs, light-weight ConvNets show much better performance.</p><p>Secondly, comparing the popular ConvNets before ViT appears (pre-ConvNets), ViTs and hybrid structures, hybrid structures achieve the best performance. Therefore improving ConvNets by learning from the merits of ViT is feasible.</p><p>Finally, the proposed ParC-Net achieves the best performance among all comparison models. So indeed by learning from ViT design, performance of pure light-weight ConvNets can be improved significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object detection</head><p>We use MS-COCO <ref type="bibr" target="#b18">[19]</ref> datasets and its evaluation protocol for object detection experiments. Following <ref type="bibr" target="#b24">[25]</ref>[27], we take single shot object detection (SSD) <ref type="bibr" target="#b19">[20]</ref> as the detection framework and use separable convolution to replace the standard convolutions in the detection head.</p><p>Experiment setting. Taking models pretrained on ImageNet-1K as backbone, we finetune detection models on training set of MS-COCO with AdamW optimizer for 200 epochs. Batchsize and weight decay are set to 128 and 0.01. We use the first 500 iterations as warm up stage, where the learning rate is increased from 0.000001 to 0.0009. Both label smoothing and EMA are used during training.</p><p>Comparison results. <ref type="figure" target="#fig_4">Figure 6</ref> lists the corresponding results. Similar to results in image classifiction, MobileViT-S and ParC-Net-S achieve the the second best and the best in terms of mAP. Compared with the second best model, ParC-Net-S shows advantages in both model size and detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic segmentation</head><p>Experiment settings. DeepLabV3 is adopted as the semantic segmentation framework. We fine tune segmentation models on training set of PASCAL VOC <ref type="bibr" target="#b6">[7]</ref> and COCO dataset, then evaluate trained models on validation set of PASCAL VOC using mean intersection over union (mIOU) and report the final results for comparison. We fine tune   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation study</head><p>Using the MobileViT as a baseline model, we further conduct ablation analysis on three components proposed in our ParC-Net.</p><p>? Position aware circular convolution. The proposed ParC has two major characteristics: 1) Circular convolution brings global receptive field; 2) PE keeps spatial structure information. Experiment results confirm that both characteristics are important. 1) Results in rows 1-3 show that, using big kernel can also improve accuracy, but the benefit of it reaches a saturation point when kernel size reaches a certain level. This results are consistent with the statement claimed in <ref type="bibr" target="#b21">[22]</ref>. Using ParC can further improve accuracy, as shown in rows 2-3 and 6-7. 2) Introducing PE to ParC is necessary. As we explained in section 3.1, using circular convolution alone can indeed capture global features but it disturbs the original spatial structures. For classification task, PE has no impact (rows 6 and 7). However, for detection and segmentation tasks which are sensitive to spatial location, abandoning PE hurts performances (rows 9-10 and 12-13).</p><p>?  with the ResNeXt block <ref type="bibr" target="#b38">[39]</ref> to replace Meta-former architecture. By comparing row 4 and 7, we can see that using the proposed pure ConvNet meta-former architecture is useful.</p><p>? Channel wise attention. Results in rows 5 and 7 show that using channel wise attention can improves performance. Compared with ParC, channel wise attention brings less benefit.</p><p>In summary, all three components are useful. Connecting them as a whole achieves the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Inference speed on low power devices.</head><p>In this section, we conduct experiments to verify two points: 1) as we mentioned in introduction, the ParC-Net is proposed for edge computing devices. To verify whether the proposed ParC-Net meets our requirements, we deploy the proposed ParC-Net on a widely used low power chip Rockchip RK3288 and an in house low power neural network processor DP2000, compare it with baseline. We use ONNX <ref type="bibr" target="#b0">[1]</ref> and MNN <ref type="bibr" target="#b16">[17]</ref> to port these models to chips and time each model for 100 iterations to measure the average inference speed; 2) The proposed ParC block is an plugand-play block, it can be inserted into other models. We replaced convolutions in the last few blocks of typical CNNs with our proposed ParC (with PE and kernel generation etc.) Comparison results are listed in <ref type="table" target="#tab_4">Table 3</ref>.</p><p>As shown in rows 1-4 of <ref type="table" target="#tab_4">Table 3</ref>, compared with baseline, ParC-Net is 23% faster on Rockchip RK3288 and 3.77 ? faster On DP2000. Besides less FLOPs operations, we believe this speed improvement is also brought by two factors: 1) Convolutions are highly optimized by existing tool chains that are widely used to deploy models into these resource constrained devices; 2) Compared with convolutions, transformers are more data bandwith demanding as computing the attention map involves two large matrices K and Q, whereas in convolutions the kernel is a rather small matrix compared with the input feature map. In case the bandwith requirement exceeds that of the chip design, the CPU will be left idle waiting for data, resulting in lower CPU utilization and overall slower inference speed;</p><p>Results in rows 3-10 show that our proposed ParC-Net universally improves performances of typical light weight models. MobileViT-S has much higher FLOPs but achieves good trade-off between model size and accuracy, which excels in its own application purpose. By applying our ParC-Net designs on MobleViT-S, ParC-Net-S achieve better balance between model size, FLOPs and accuracy. Results on ResNet50, MobileNetV2 and ConvNext-T shows that models which focus on optimizing FLOPs-accuracy trade-offs can also benefit from our ParC-Net designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, for edge computing devices, we present ParC-Net, a pure ConvNet, which inherits advantages of ConvNet and integrated structure characteristics of ViT. To evaluate the performances, we apply the proposed model on three popular vision tasks, image classification, object detection and semantic segmentation. The proposed model achieves better performance on all three tasks, while having fewer parameters compared with other ConvNet, ViT and hybrid models. Experimental results on low power devices Rockchip RK3288 and our in house processor DP2000 show that the proposed ParC-Net does inherit ConvNets and it is well supported by edge computing devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>ParC block. (a) A residual block that is widely used in ConvNets; (b) A ViT block; (c) An ParC block</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Illustration of the position aware circular convolution. (a) ParC-V; (b) ParC-H. F , EV and EH are explained in equations 1 and 2 x 0 x 1 ... x w x 0 x 1 ... x w x 0 x 1 ... x Illustration of global circular convolution on horizontal direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>module or local feature extraction structure T : Transformer module or global feature extraction structue F : Fusing local and glocal features Three main hybrid structures. (a) serial structure; (b) parallel structure; (c) bifurcate structure name the proposed convolution as the circular convolution. The proposed ParC introduces three modifications:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Frameworks</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Object detection results on MS-COCO. (a) mAP vs model size. (b) Comparison results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Semantic segmentation experiments on PASCAL VOC. (a) mIOU vs model size.(b) Comparison results with more models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study. BK, MF, CA and PE denote big kernel, meta-former architecture, channel wise attention and position embedding.</figDesc><table><row><cell>BK 1/4 and</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Meta-former architecture. In experiments of abandoning Meta-former architecture, we integrate ParC</figDesc><table><row><cell cols="2">Row Models</cell><cell cols="5"># param FLOPs Devices Speed (ms) Top1 (%)</cell></row><row><cell>1</cell><cell>MobileViT-S</cell><cell>5.6M</cell><cell>4.0G</cell><cell>RK3288</cell><cell>457</cell><cell>78.4</cell></row><row><cell>2</cell><cell>ParC-Net-S</cell><cell>5.0M</cell><cell>3.5G</cell><cell>RK3288</cell><cell>353</cell><cell>78.6</cell></row><row><cell>3</cell><cell>MobileViT-S</cell><cell>5.6M</cell><cell>4.0G</cell><cell>DP2000</cell><cell>368</cell><cell>78.4</cell></row><row><cell>4</cell><cell>ParC-Net-S</cell><cell>5.0M</cell><cell>3.5G</cell><cell>DP2000</cell><cell>98</cell><cell>78.6</cell></row><row><cell>5</cell><cell>ResNet50 *</cell><cell>26 M</cell><cell>4.1G</cell><cell>CPU</cell><cell>98</cell><cell>78.8</cell></row><row><cell>6</cell><cell>ParC-ResNet50 *</cell><cell>24 M</cell><cell>4.0G</cell><cell>CPU</cell><cell>98</cell><cell>79.6</cell></row><row><cell>7</cell><cell>MobileNetV2*</cell><cell>3.5M</cell><cell>0.6G</cell><cell>CPU</cell><cell>24</cell><cell>70.2</cell></row><row><cell>8</cell><cell>ParC-MobileNetV2*</cell><cell>3.5M</cell><cell>0.6G</cell><cell>CPU</cell><cell>27</cell><cell>71.1</cell></row><row><cell>9</cell><cell>ConvNext-T(0.5?W)*</cell><cell>7.4M</cell><cell>1.1G</cell><cell>CPU</cell><cell>47</cell><cell>77.5</cell></row><row><cell>10</cell><cell>ParC-ConvNext-T(0.5?W)*</cell><cell>7.4M</cell><cell>1.1G</cell><cell>CPU</cell><cell>48</cell><cell>78.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Applying ParC-Net designs on different backbones and comparing inference speeds of different models. CPU used here is Xeon E5-2680 v4. DP2000 is the code name of a in house unpublished low power neural network processor that highly optimizes the convolutions. *denotes the models are trained under convnext hyperparameters settings, which may not be the optimal. W means network width. Latency is measured with batch size 1.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://github.com/onnx/onnx,2019.9" />
		<title level="m">Open neural network exchange</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Tinynet: a lightweight, modular, and unified network architecture for the internet of things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gonglong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGCOMM 2019 conference posters and demos</title>
		<meeting>the ACM SIGCOMM 2019 conference posters and demos</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.05895</idno>
		<title level="m">Mobileformer: Bridging mobilenet and transformer</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coatnet: Marrying convolution and attention for all data sizes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="2286" to="2296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sma</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cki</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cmt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.06263</idno>
		<title level="m">Convolutional neural networks meet vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ghostnet: More features from cheap operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1580" to="1589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking spatial dimensions of vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11936" to="11945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mnn: A universal and efficient inference engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiliu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhang</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Micronet: Improving image recognition with extremely low flops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="468" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yuan</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
	</analytic>
	<monogr>
		<title level="m">Trevor Darrell, and Saining Xie. A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Mobilevit: lightweight, general-purpose, and mobile-friendly vision transformer. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Rastegari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Acceleration of stochastic approximation by averaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anatoli B Juditsky</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on control and optimization</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="838" to="855" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno>PMLR, 2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongchan</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cvt: Introducing convolutions to vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="22" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Early convolutions help transformers see better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mannat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Mintun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Metaformer is actually what you need for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11418</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
