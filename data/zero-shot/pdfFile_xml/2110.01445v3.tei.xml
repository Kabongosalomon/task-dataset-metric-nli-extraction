<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust and Decomposable Average Precision for Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Ramzi</surname></persName>
							<email>elias.ramzi@cnam.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Conservatoire National des Arts et M?tiers</orgName>
								<orgName type="institution">CEDRIC</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Coexya</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
							<email>nicolas.thome@cnam.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Conservatoire National des Arts et M?tiers</orgName>
								<orgName type="institution">CEDRIC</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Rambour</surname></persName>
							<email>clement.rambour@cnam.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Conservatoire National des Arts et M?tiers</orgName>
								<orgName type="institution">CEDRIC</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Audebert</surname></persName>
							<email>nicolas.audebert@cnam.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Conservatoire National des Arts et M?tiers</orgName>
								<orgName type="institution">CEDRIC</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bitot</surname></persName>
							<email>xavier.bitot@coexya.eu</email>
							<affiliation key="aff1">
								<orgName type="institution">Coexya</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust and Decomposable Average Precision for Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In image retrieval, standard evaluation metrics rely on score ranking, e.g. average precision (AP). In this paper, we introduce a method for robust and decomposable average precision (ROADMAP) addressing two major challenges for end-to-end training of deep neural networks with AP: non-differentiability and non-decomposability. Firstly, we propose a new differentiable approximation of the rank function, which provides an upper bound of the AP loss and ensures robust training. Secondly, we design a simple yet effective loss function to reduce the decomposability gap between the AP in the whole training set and its averaged batch approximation, for which we provide theoretical guarantees. Extensive experiments conducted on three image retrieval datasets show that ROADMAP outperforms several recent AP approximation methods and highlight the importance of our two contributions. Finally, using ROADMAP for training deep models yields very good performances, outperforming state-of-the-art results on the three datasets. Code and instructions to reproduce our results will be made publicly available at https://github.com/elias-ramzi/ROADMAP. Such tasks are usually evaluated with rank-based metrics, e.g. Recall@k, Normalized Discounted Cumulative Gain (NDCG), and Average Precision (AP). AP is also the de facto metric used in several vision tasks implying a large imbalance between positive and negative samples, e.g. object detection.</p><p>In this paper, we address the problem of direct AP training with stochastic gradient-based optimization, e.g. using deep neural networks, which poses two major challenges.</p><p>Firstly, the AP loss L AP = 1 ? AP is not differentiable and is thus not directly amenable to gradientbased optimization. There has been a rich literature for providing smooth and upper bound surrogate 35th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The task of 'query by example' is a major prediction problem, which consists in learning a similarity function able to properly rank all the instances in a retrieval set according to their relevance to the query, such that relevant items have the largest similarity. In computer vision, it drives several major applications, e.g. content-based image retrieval, face recognition or person re-identification.  losses for L AP <ref type="bibr" target="#b52">[50,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b6">6,</ref><ref type="bibr" target="#b28">28]</ref>. More recently, smooth differentiable rank approximations have been proposed <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b2">2]</ref>, but generally lose the important L AP upper bound property.</p><p>The second important issue of AP optimization relates to its non-decomposability: L B AP averaged over batches underestimates L AP on the whole training dataset, which we refer as the decomposability gap. In image retrieval, the attempts to circumvent the problem involve ad hoc methods based on batch sampling strategies <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b35">35]</ref>, or storing all training representations/scores <ref type="bibr" target="#b46">[44,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b28">28]</ref>, leading to complex models with a large computation and memory overhead.</p><p>In this paper, we introduce a method for RObust And DecoMposable Average Precision (ROADMAP), which explicitly addresses the aforementioned challenges of AP optimization.</p><p>Our first contribution is to propose a new surrogate loss L SupAP for L AP . In particular, we introduce a smooth approximation of the rank function, with a different behaviour for positive and negative examples. By this design, L SupAP provides an upper bound of L AP , and always back-propagates gradients when the correct ranking is not satisfied. These two features illustrated in the the toy example on Figure are not fulfilled by binning approaches <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b32">32]</ref> or by SmoothAP <ref type="bibr" target="#b2">[2]</ref>.</p><p>As a second contribution, we propose to improve the non-decomposability in AP training. To this end, we introduce a simple yet effective training objective L calibr. , which calibrates the scores among different batches by controlling the absolute value of positive and negative samples. We provide a theoretical analysis showing that L calibr. decreases the decomposability gap. <ref type="figure" target="#fig_1">Figure 1b</ref> illustrates how L calibr. can be leveraged to improve the overall ranking.</p><p>We provide a thorough experimental validation including three standard image retrieval datasets and show that ROADMAP outperforms state-of-the-art methods. We also report the large and consistent gain compared to rank/AP approximation baselines, and we highlight in the ablation studies the importance of our two contributions. Finally, ROADMAP does not entail any memory or computation overhead and remains competitive even with small batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>We discuss here the literature in image retrieval dedicated to AP optimization, and compare to other approaches based on optimizing representations <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b53">51,</ref><ref type="bibr" target="#b55">53,</ref><ref type="bibr" target="#b38">38]</ref> in the experiments.</p><p>Smooth AP approximations Studying smooth surrogate losses for AP has a long history. The widely used surrogate for retrieval is to consider constraints based on pairs <ref type="bibr" target="#b49">[47,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b31">31]</ref>, triplets <ref type="bibr" target="#b11">[11]</ref>, quadruplets <ref type="bibr" target="#b20">[20]</ref> or n-uplets <ref type="bibr" target="#b35">[35]</ref> to enforce partial ranking. These metric learning methods optimize a very coarse upper bound on AP and need complex post-processing and tricks to be effective.</p><p>One option for training with AP is to design smooth upper bounds on the AP loss. Seminal works are based on structural SVMs <ref type="bibr" target="#b52">[50,</ref><ref type="bibr" target="#b23">23]</ref>, with extensions to speed-up the "loss-augmented inference" <ref type="bibr" target="#b24">[24]</ref> or to adapt to weak supervision <ref type="bibr" target="#b6">[6]</ref>. Recently, a generic blackbox combinatorial solver has been introduced <ref type="bibr" target="#b28">[28]</ref> and applied to AP optimization <ref type="bibr" target="#b33">[33]</ref>. To overcome the brittleness of AP with respect to small score variations, an ad hoc perturbation is applied to positive and negative scores during training. These methods provide elegant AP upper bounds, but generally are coarse AP approximations.</p><p>Other approaches rely on designing smooth approximations of the the rank function. This is done in soft-binning techniques <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b41">40,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b32">32]</ref> by using a smoothed discretization of similarity scores. Other approaches rely on explicitly approximating the non-differentiable rank functions using neural networks <ref type="bibr" target="#b8">[8]</ref>, or with a sum of sigmoid functions in the recent SmoothAP approach <ref type="bibr" target="#b2">[2]</ref>. These approaches enable accurate AP approximations by providing tight and smooth approximations of the rank function. However, they do not guarantee that the resulting loss is an AP loss upper bound. The L SupAP introduced in this work is based on a smooth approximation of the rank function leading to an upper bound on the AP loss, making our approach both accurate and robust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Decomposability in AP optimization</head><p>Batch training is mandatory in deep learning. However, the non-decomposability of AP is a severe issue, since it yields an inconsistent AP gradient estimator.</p><p>Non-decomposability is related to sampling informative constraints in simple AP surrogates, e.g. triplet losses, since the constraints' cardinality on the whole training set is prohibitive. This has been addressed by efficient batch sampling <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b37">37]</ref> or selecting informative constraints within mini-batches <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b37">37]</ref>. In cross-batch memory technique <ref type="bibr" target="#b46">[44]</ref>, the authors assume a slow drift in learned representations to store them and compute global mining in pair-based deep metric learning.</p><p>In AP optimization, the non-decomposability has essentially been addressed by a brute force increase of the batch size <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b28">28]</ref>. This includes an important overhead in computation and memory, generally involving a two-step approach for first computing the AP loss and subsequently re-computing activations and back-propagating gradients. In contrast, our loss L calibr. does not add any overhead and enables good performances for AP optimization even with small batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Robust and decomposable AP training</head><p>We present here our method for RObust And DecoMposable AP (ROADMAP) dedicated to direct optimization of a smooth surrogate of AP with stochastic gradient descent (SGD), see <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>Training context Let us consider a retrieval set ? = {x j } j? 1;N composed of N elements, and a set of M queries included in ?, i.e. Q = {q i } i? 1;M ? ?. For each query q i , each element in ? is assigned a label y(x j , q i ) ? {+1; ?1}, such that y(x j , q i ) = 1 (resp. y(x j , q i ) = ?1) if x j is relevant (resp. irrelevant) with respect to q i . This defines a query-dependent partitioning of ? such that ? = P i ? N i , where P i := {x j ? ?|y(x j , q i ) = +1} and N i := {x j ? ?|y(x j , q i ) = ?1}.</p><p>For each x j ? ?, we define a prediction model parametrized by parameters ?, e.g. a deep neural network, which provides a vectorial embedding v qi ? R d of each element, i.e.: v qi := f ? (q i ). In the embedded space R d , we compute a similarity score between each query q i and each element in ?, e.g. by using the cosine similarity:</p><formula xml:id="formula_0">s(q i , x j ) = vq i T v j ||vq i || 2 ||v j || 2 .</formula><p>During training, our goal is to optimize, for each query q i , the model parameters ? such that positive elements are ranked before negatives. More precisely, we aim at minimizing the AP loss L APi for each query q i in the retrieval set ?.</p><p>Our overall AP loss L AP is averaged over all queries:</p><formula xml:id="formula_1">L AP (?) = 1 ? 1 M M i=1 AP i (?), AP i (?) = 1 |P i | k?Pi Pre(k, ?) = 1 |P i | k?Pi rank + (k, ?) rank(k, ?)<label>(1)</label></formula><p>where Pre(k, ?) is the precision for the k th positive example x k , rank + (k, ?) its rank among positives P i , and the rank(k, ?) its rank over ? = P i ? N i .</p><p>As previously mentioned, there are two main challenges with SGD optimization of AP in Eq. (1): i) AP(?) is not differentiable with respect to ?, and ii) AP does not linearly decompose into batches. ROADMAP addresses both issues: we introduce the robust differentiable L SupAP surrogate (Section 3.1), and add the L calibr. loss (Section 3.2) to improve AP decomposability. Our final loss L ROADMAP is a linear combination of L SupAP and L calibr. , weighted by the hyperparameter ?: </p><formula xml:id="formula_2">L ROADMAP (?) = (1 ? ?) ? L SupAP (?) + ? ? L calibr. (?)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Robustness in smooth rank approximation</head><p>The non-differentiablity in Eq (1) comes from the ranking operator, which can be viewed as counting the number of instances that have a similarity score greater than the considered instance, i.e. <ref type="bibr" target="#b0">1</ref> :</p><formula xml:id="formula_3">rank + (k) = 1 + j?Pi\{k} H(s j ? s k ), where H(t) = 1 if t ? 0 0 otherwise rank(k) = rank + (k) + j?Ni H(s j ? s k ) = rank + (k) + rank ? (k)<label>(3)</label></formula><p>From Eq. (3) it becomes clear that the non-differentiablity is due to the Heaviside (step) function H, whose derivative is either zero or undefined. Note that the computation of rank + (k) and rank ? (k) in Eq. (3) relates to the rank of positive instances x k ? P i : the score s k in Eq. (3) is always the score of a positive, whereas s j can either be a negative's or positive's score.</p><p>Smooth loss L SupAP To provide a smooth approximation of L AP in Eq. (1), we introduce a smooth approximation of the rank function. In particular, we propose a different behaviour between rank + (k) and rank ? (k) in Eq. (3) by defining two functions H + and H ? .</p><p>For rank + (k), we choose to keep the Heaviside (step) function, i.e. H + = H (see <ref type="figure" target="#fig_3">Fig. 3a</ref>), which consists in ignoring rank + (k) in gradient-based AP optimization. This is done on purpose since ?AP ? rank + (k) = rank ? (k) (rank + (k)+rank ? (k)) 2 ? 0: the gradient would tend to increase rank + (k) and to decrease the score of s k . Reminding x k is always a positive instance, this behaviour is undesirable.</p><p>For rank ? (k), we define the following smooth surrogate H ? for H, shown in <ref type="figure" target="#fig_3">Fig 3b</ref>: where ? and ? are hyperparameters, and ? is defined such that the sigmoidal part of H ? reaches the saturation regime and is fixed for the rest of the paper (see supplementary Sec. A). From the H ? smooth approximation defined in Eq. (4), we obtain the following smooth approximation rank ? s (k) = j?Ni H ? (s j ? s k ), leading to the following smooth AP loss approximation:</p><formula xml:id="formula_4">H ? (t) = ? ? ? ?( t ? ) if t ? 0, where ? is the sigmoid function (Fig. 3c) ?( t ? ) + 0.5 if t ? [0; ?] with ? ? 0 ? ? (t ? ?) + ?( ? ? ) + 0.5 if t &gt; ?<label>(4)</label></formula><formula xml:id="formula_5">(a) H + (x) = H(x) in Eq. (3) (b) H ? (x) in Eq. (4) (c) Sigmoid used in [2]</formula><formula xml:id="formula_6">L SupAP (?) = 1 ? 1 M M i=1 1 |P i | k?Pi rank + (k) rank + (k) + rank ? s (k)<label>(5)</label></formula><p>L SupAP in Eq. (5) fulfills two main features for AP optimization:</p><p>1 L SupAP is an upper bound of L AP in Eq. (1). Since H ? in Eq. (4) is an upper bound of a step function <ref type="figure" target="#fig_3">(Fig 3b)</ref>, it is easy to see that L SupAP ? L AP . This is a very important property, since it ensures that the model keeps training until the correct ranking is obtained. It is worth noting that existing smooth rank approximations in the literature <ref type="bibr" target="#b41">[40,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b2">2]</ref> do not fulfill this property.</p><p>2 L SupAP brings training gradients until the correct ranking plus a margin is fulfilled. When the ranking is incorrect, the negative x j is ranked before the positive x k , thus s j &gt; s k and H ? (s j ? s k ) in Eq. (4) has a non-null derivative. We use a sigmoid to have a large gradient when s j ? s k is small. To overcome vanishing gradients of the sigmoid for large values s j ? s k , we use a linear function ensuring constant ? derivative. When the ranking is correct (s j &lt; s k ), we enforce robustness by imposing a margin parametrized by ? (sigmoid in Eq. <ref type="formula" target="#formula_4">(4)</ref>). This margin overcomes the brittleness of rank losses, which vanish as soon as the ranking is correct <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b28">28]</ref>. <ref type="bibr" target="#b2">[2]</ref> by i) providing an upper bound on L AP , ii) improving the gradient flow ( <ref type="figure" target="#fig_3">Fig. 3b vs Fig. 3c</ref>), and iii) overcoming adverse effects of the sigmoid for rank + , as shown in <ref type="figure" target="#fig_1">Fig. 1a</ref> (and in supplementary sec. A). We experimentally verify the consistent gain brought out by L SupAP over L SmoothAP .</p><formula xml:id="formula_7">Comparison to SmoothAP [2] L SupAP differs from L SmoothAP in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decomposable Average Precision</head><p>In Eq. (1), AP decomposes linearly between queries q i , but AP i does not decomposes linearly between samples. We therefore focus our analysis of the non-decomposability on a single query. For a retrieval set ? of N elements, we consider {B b } b?{1:K} batches of size B, such that N/B = K ? N. Let AP b i (?) be the AP in batch b for query q i , we define the "decomposability gap" DG AP as follows:</p><formula xml:id="formula_8">DG AP (?) = 1 K K b=1 AP b i (?) ? AP i (?)<label>(6)</label></formula><p>DG AP in Eq. <ref type="formula" target="#formula_8">(6)</ref> is a direct measure of the non-decomposability of AP (see supplementary Sec. A). Our motivation here is to decrease DG AP , i.e. to have the average AP over the batches as close as possible to the AP computed over the whole training set. To this aim, we introduce the following loss during training:</p><formula xml:id="formula_9">L calibr. (?) = 1 M M i=1 1 |P i | xj ?Pi [? ? s j ] + L + calibr. + 1 |N i | xj ?Ni [s j ? ?] + L ?</formula><p>calibr. <ref type="bibr" target="#b7">(7)</ref> where [x] + = max(0, x). The loss L + calibr. enforces the score of the positive x i ? P i to be larger than ?, and L ? calibr. enforces the score of the negative x j ? N i to be smaller than ? &lt; ?. L calibr. is a standard pair-based loss <ref type="bibr" target="#b12">[12]</ref>, which we revisit in our context to "calibrate" the values of the scores between mini-batches: intuitively, the fact that the positive (resp. negative) scores are above (resp. below) a threshold in the mini-batches makes the average AP closer to the AP on the whole dataset.</p><p>Upper bound on the decomposabilty gap To formalize this idea, we provide a theoretical analysis of the impact on the global ranking of L calibr. in Eq. <ref type="bibr" target="#b7">(7)</ref>. Firstly, we can see that if L ? calibr. = L + calibr. = 0, on each batch, the overall AP and the AP in batches is null, i.e. DG AP (?) = 0 and we get a decomposable AP. In a more general setting, we show that minimizing L calibr. on each batch reduces the decomposability gap, hence improving the decomposability of the AP.</p><formula xml:id="formula_10">Let's consider K batches {B b } b?{1:K} of batch size B divided in P b i positive instances and N b i negative instances w.r.t. the query q i .</formula><p>To give some insight we assume that the AP of each batch is one (i.e. AP b i = 1), and give the following upper bound of DG AP :</p><formula xml:id="formula_11">0 ? DG AP ? 1 ? 1 K b=1 |P b i | ? ? K b=1 B j=1 j + |P 1 i | + ? ? ? + |P b?1 i | j + |P 1 i | + ? ? ? + |P b?1 i | + |N 1 i | + ? ? ? + |N b?1 i | ? ? (8)</formula><p>This upper bound of the decomposability gap is given in the worst case for the global AP : the global ranking is built from the juxtaposition of the batches (see supplementary Sec. A).</p><p>We can refine this upper bound by introducing the calibration loss L calibr. and constraining the scores of positive and negative instances to be well calibrated. On each batch we define the following quantities</p><formula xml:id="formula_12">E ? b = j?N ? i 1(s j &gt; ?)</formula><p>which are the negative instances that do not respect the constraints and</p><formula xml:id="formula_13">G ? b = j?N ? i 1(s j ? ?)</formula><p>the negative instances that do. We similarly define E + b and G + b . We then have the following upper bound on the decomposability gap : </p><formula xml:id="formula_14">0 ? DG AP ? 1 ? 1 K b=1 |P b i | K b=1 G + b j=1 j + G + 1 + ? ? ? + G + b?1 j + G + 1 + ? ? ? + G + b?1 + E ? 1 + . . . E ? b?1 + (9) E + b j=1 j + G + b + |P 1 i | + ? ? ? + |P b?1 i | j + G + b + |P 1 i | + ? ? ? + |P b?1 i | + |N 1 i | + ? ? ? + |N b?1 i |</formula><formula xml:id="formula_15">? b , E + b , E + b , G + b )</formula><p>, making it tighter, hence improving the decomposability of the AP (see supplementary Sec. A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experimental setup We evaluate ROADMAP on the following three image retrieval datasets: CUB-200-2011 <ref type="bibr" target="#b43">[42]</ref> contains 11 788 images of birds classified into 200 fine-grained classes. We follow the standard protocol and use the first (resp. last) 100 classes for training (resp. evaluation). Stanford Online Product (SOP) <ref type="bibr" target="#b36">[36]</ref> is a dataset with 120 053 images of 22 634 objects classified into 12 categories (e.g. bikes, coffee makers). We use the reference train and test splits from <ref type="bibr" target="#b36">[36]</ref>. INaturalist-2018 <ref type="bibr" target="#b42">[41]</ref> is a large scale dataset of 461 939 wildlife animals images classified into 8142 classes. We use the splits from <ref type="bibr" target="#b2">[2]</ref> with 70% of the classes in the train set and the rest in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ROADMAP settings</head><p>For all experiments in Section 4.1 and Section 4.2, we use ? = 0.5 for L ROADMAP in Eq. (2), ? = 0.01 and ? = 100 for L SupAP in Eq. (5), ? = 0.9 and ? = 0.6 for L calibr.</p><p>in Eq. <ref type="bibr" target="#b7">(7)</ref>. We study more in depth the impact of those parameters in Section 4.3. Deep models are trained using Adam <ref type="bibr" target="#b19">[19]</ref> for ResNet-50 backbones and AdamW <ref type="bibr" target="#b21">[21]</ref> for DeiT transformers <ref type="bibr" target="#b40">[39]</ref>. Test protocol Methods are evaluated using the standard recall at k (R@k) and mean average precision at R <ref type="bibr" target="#b26">[26]</ref> (mAP@R) metrics (see supplementary Sec. B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ROADMAP validation</head><p>In this section, all models are trained in the same setting (ResNet-50 backbone, embedding size 512, batch size 64). The comparisons thus directly measures the impact of the training loss.</p><p>Comparison to AP approximations. In <ref type="table" target="#tab_1">Table 1</ref>, we compare ROADMAP on the three datasets to recent AP loss approximations including the soft-binning approaches FastAP <ref type="bibr" target="#b3">[3]</ref> and SoftBinAP <ref type="bibr" target="#b32">[32]</ref>, the generic solver BlackBox <ref type="bibr" target="#b33">[33]</ref>, and the smooth rank approximation <ref type="bibr" target="#b2">[2]</ref>. We use the publicly available PyTorch implementations of all these baselines. We can see that ROADMAP outperforms all the current AP approximations by a large margin. The gain is especially pronounced on the large scale dataset INaturalist. This highlights the importance our two contributions, i.e. our robust smooth AP upper bound and our AP decomposability improvement (see supplementary Sec. B). Comparison to memory methods.</p><p>XBM stores the embeddings of previously seen batches to alleviate complex batch sampling and better approximate AP on the whole dataset. Although XBM has a low memory overhead (a few hundreds megabytes on SOP), it is time consuming. We ran experiments storing the entire dataset for SOP (60k embeddings), but for INaturalist we could not train while storing all the dataset in tractable time. We chose to store the same amount of embeddings as for SOP : 60k embeddings which is about 17% of the training set.</p><p>We can see in <ref type="table" target="#tab_2">Table 2</ref> that XBM is approximately 3 times longer to train than ROADMAP. This becomes critical on INaturalist, where training while storing 60k images takes about 3 days, and reaches only a R@1 of 60. Consequently, ROADMAP outperforms XBM on both datasets; there is a ?+2pt increase on both metrics for SOP and an especially large gap on INaturalist. In the latter, not being able to store all the embeddings affects drastically the performances of the XBM in a negative way. There is a 5pt difference in R@1 and more than 6pt in mAP@R. This demonstrates the suitability of ROADMAP on large-scale settings. Ablation study. To study more in depth the impact of our contributions, we perform ablation studies in <ref type="table" target="#tab_3">Table 3</ref>. We show the improvement against SmoothAP <ref type="bibr" target="#b2">[2]</ref> when changing the sigmoid by H + and H ? for L SupAP in Eq. (5), and the use of L calibr. in Eq. <ref type="bibr" target="#b7">(7)</ref>. We can see that L SupAP consistently improves performances over L SmoothAP (0.9pt on CUB, 0.5pt on SOP and 1.5pt on INaturalist). L SupAP and L calibr. equally contribute to the overall gain in CUB and SOP, but the gain of L calibr. is much more important on INaturalist. This is explained by the fact that the batch vs. dataset ratio size B N is tiny ( 1), making the decomposability gap in Eq. (6) huge. We can see that L calibr. is very effective for reducing this gap and brings a gain of more than 3pt. We compare ROADMAP to other state of the art methods across three image retrieval datasets and report the results in <ref type="table" target="#tab_5">Table 4</ref>. We divide competitor methods into three categories: metric learning <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b45">43,</ref><ref type="bibr" target="#b54">52,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b46">44,</ref><ref type="bibr" target="#b50">48]</ref>, classification losses for image retrieval <ref type="bibr" target="#b55">[53,</ref><ref type="bibr" target="#b53">51,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b38">38]</ref>, and AP approximations <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b2">2]</ref>. ROADMAP falls in the latter category. We use the same setup as in Switching the backbone to the more recent vision transformer architecture DeiT <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b40">39]</ref>, further lifts the performances of ROADMAP by several point, from 3 to 9 points depending on the dataset, with a smaller embedding size (384 vs 512). The decomposable AP approximation ROADMAP also outperforms by a significant margin IRT R , the DeiT architecture for image retrieval introduced in <ref type="bibr" target="#b7">[7]</ref> trained with a contrastive loss. Overall ROADMAP achieves state-of-the-art performances across all three datasets by a significant margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Analysis</head><p>We show in <ref type="figure" target="#fig_5">Fig. 4</ref> the impact of the main ROADMAP hyperparameters on INaturalist. The relative weighting ? from Eq. (2) controls the balance between our two training objectives L SupAP and L calibr. : ? = 0 reduces L ROADMAP to L SupAP while ? = 1 to L calibr. . We can see in <ref type="figure" target="#fig_5">Fig. 4a</ref> that training with the complete L ROADMAP with both L calibr. and L SupAP is always better than using only one of the two losses. Note that results are stable in the [0.2, 0.8] range with a consistent ?1.5pt increase, demonstrating the robustness of ROADMAP to this hyperparameter tuning.     <ref type="figure" target="#fig_5">. 4b</ref>, the improvement is important and stable in <ref type="bibr" target="#b10">[10,</ref><ref type="bibr">100]</ref>. Note that ? &gt; 0 already improves the results compared to ? = 0 in <ref type="bibr" target="#b2">[2]</ref>. There is an important decrease when ? 100 probably due to the high gradient that takes over the signal for correctly ranked samples. The impact of the margin ? ? ? in L calibr. is shown in <ref type="figure" target="#fig_5">Fig. 4c</ref>. Once again, ROADMAP exhibits a robust behaviour w.r.t. the values of its hyperparameters: any margin in the [0.1, 0.6] range results in an improvement in mAP@R compared to the L SupAP baseline without the decomposability loss. Best results are achieved with smaller margins 0.1 &lt; ? ? ? &lt; 0.4. <ref type="figure" target="#fig_7">Fig. 5</ref> shows the improvement in mAP@R on the three datasets when adding L calibr. to L SupAP . We can see that the increase becomes larger as the batch size gets smaller. This confirms our intuition that the decomposability in L calibr. has a stronger effect on smaller batch sizes, for which the AP estimation is noisier and DG AP larger. This is critical on the large-scale dataset INaturalist where the batch AP on usual batch sizes is a very poor approximation of the global AP. <ref type="bibr" target="#b32">32</ref>   As a qualitative assessment, we show in <ref type="figure" target="#fig_9">Fig. 6</ref> some results of ROADMAP on INaturalist. We show the queries (in purple) and the 4 most similar retrieved images (in green). We can appreciate the semantic quality of the retrieval. More qualitative results are provided in supplementary Sec. C.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper introduces the ROADMAP method for gradient-based optimization of average precision. ROADMAP is based on a smooth rank approximation, leading to the L SupAP being both accurate and robust. To overcome the lack of decomposability in AP, ROADMAP is equipped with a calibration loss L calibr. which aims at reducing the decomposability gap. We provide theoretical guarantees as well as experiments to assess this behavior. Experiments show that ROADMAP can combine the strength of ranking methods with the simplicity of a batch strategy. Without bells and whistles, ROADMAP is able to outperform state-of-the-art performances on three datasets, and remains effective even with small batch sizes.</p><p>As any work on image retrieval, our contribution could be applied to critical applications in surveillance scenarios, e.g. face recognition or person re-identification. ROADMAP is neither worse nor better than previous work in this regard. Our work is also a data-driven learning method, and thus inherits the risk of perpetuating dataset biases. Future work will focus on improving fair and accurate retrieval by reducing dataset biases. We also plan to relax the need for full supervision to tackle situations more representative to in-the-wild scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A ROADMAP model A.1 Properties of SupAP &amp; comparison to SmoothAP</head><p>We further discuss and give additional explanations of the property of our L SupAP loss function, and especially its comparison with respect to the SmoothAP <ref type="bibr" target="#b2">[2]</ref> baseline.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.a of the main paper, and discussed in Section 3.1 ("Comparison to SmoothAP"), the smooth rank approximation in <ref type="bibr" target="#b2">[2]</ref> has several drawbacks, that we show below: <ref type="figure">Figure 8</ref>: Limitation of the smooth rank approximation in <ref type="bibr" target="#b2">[2]</ref>: contradictory gradient flow for the positives samples x 1 and x 2 (in green), vanishing gradient for the negative example x 3 (in red), and no guarantees of having an upper bound of L AP .</p><p>Specifically, we explain in more detail the following three limitations identified in the main paper for SmoothAP <ref type="bibr" target="#b2">[2]</ref>, which comes from the use of the sigmoid function to approximate the Heaviside (step) function for computing the rank:</p><p>i Contradictory gradient flow for positives samples: Firstly we can see on the toy dataset of <ref type="figure">Fig. 8</ref> that the gradients of the two positive examples (in green) with SmoothAP have opposite directions. The positive with the lowest rank x 1 has a gradient in the good direction, since it leads to increase x 1 's score because the correct ordering is not reached (the negative instance x 3 has a better rank). But the gradient of the positive with the highest rank x 2 is on the wrong direction, since it tends to decrease x 2 's score. This is an undesirable behaviour, which comes from the use of the sigmoid in L SmoothAP . In the example of <ref type="figure">Fig. 8</ref>, we can actually show that</p><formula xml:id="formula_16">?L SmoothAP ?s 1 = ? ?L SmoothAP ?s 2</formula><p>To see this we write :</p><formula xml:id="formula_17">?L SmoothAP ?s 1 = ?L SmoothAP ? rank + (x 1 ) ? ? rank + (x 1 ) ?s 1 + ?L SmoothAP ? rank + (x 2 ) ? ? rank + (x 2 ) ?s 1 + ?L SmoothAP ? rank ? (x 1 ) ? ? rank ? (x 1 ) ?s 1 + ?L SmoothAP ? rank ? (x 2 ) ? ? rank ? (x 2 ) ?s 1 Because rank ? (x 2 ) = ?( s3?s2 ? ), we have ? rank ? (x2) ?s1 = 0 and ? rank ? (x1) ?s1</formula><p>= 0 in the example of <ref type="figure">Fig. 8</ref>, because rank ? (x 1 ) = ?( s3?s1 ? ) and s 3 ? s 1 falls into the saturation regime of the sigmoid. We get a similar result for the derivative of L SmoothAP wrt. s 2 :</p><formula xml:id="formula_18">?L SmoothAP ?s 2 = ?L SmoothAP ? rank + (x 1 ) ? ? rank + (x 1 ) ?s 2 + ?L SmoothAP ? rank + (x 2 ) ? ? rank + (x 2 ) ?s 2</formula><p>Furthermore we have : (b) Our LSupAP has gradients that do not stop until the correct ranking is achieved. <ref type="figure">Figure 9</ref>: We illustrates the different steps to built L SupAP . On <ref type="figure">Fig. 9a</ref> we change H + to be the true Heaviside (step) function. On <ref type="figure">Fig. 9b</ref> we replace the sigmoid by H ? defined in Eq. (4) of the main paper. Using H + and H ? , L SupAP is an upper bound of L AP . and s 2 also have opposite signs: ? rank + (x2)</p><formula xml:id="formula_19">? rank + (x 1 ) ?s 1 = ? ? rank + (x 1 ) ?s 2 Indeed rank + (x 1 ) = 1 + ?( s2?s1 ? ), such that ? rank + (x1) ?s1 = ?? ? ?( s2?s1 ? ) 1 ? ?( s2?s1 ? ) and ? rank + (x1) ?s2 = ? ? ?( s2?s1 ? ) 1 ? ?( s2?s1 ?</formula><formula xml:id="formula_20">?s1 = ? ? rank + (x2) ?s2</formula><p>. It concludes the proof that</p><formula xml:id="formula_21">?LSmoothAP ?s1 = ? ?LSmoothAP ?s2</formula><p>.</p><p>ii Vanishing gradients: Secondly, SmoothAP <ref type="bibr" target="#b2">[2]</ref> has vanishing gradients due to its use of the sigmoid function. This is illustrated on the toy dataset in <ref type="figure">Fig. 8</ref>. The negative instance x 3 has a high score s 3 , but does not receive any gradient, which does not enable it to lower its score although it would improve the overall ranking. This is because the score difference between x 3 and x 2 is large, i.e. s 3 ? s 2 = 0.13. Similarly, s 3 ? s 1 = 0.14. Consequently, both s 3 ? s 2 and s 3 ? s 1 fall into the saturation regime of the sigmoid, preventing to propagate any gradient (see <ref type="figure" target="#fig_3">Fig. 3c</ref>. in the main paper).</p><p>iii Finally, L SmoothAP is not an upper bound of L AP . The use of the sigmoid means that both rank + and rank ? can be over or under estimated. If rank + is overestimated (resp. underestimated) L SmoothAP underestimates L AP (resp. overestimates). And if rank ? is overestimated (resp. underestimated) L SmoothAP overestimates L AP (resp. overestimated). Therefore, L SmoothAP can be larger or lower than L AP in general. In the example of <ref type="figure">Fig. 8</ref>, we show that L SmoothAP is lower than L AP .</p><p>We address those three issues with L SupAP :</p><p>i Using the the true Heaviside (step) function H + for rank + allows to have the expected behaviour regarding the gradients of positives. When Changing H + for rank + in <ref type="figure">Fig. 9a</ref>, we can see that we fix the problem of opposite gradients for the positive examples x 1 and x 2 -although the gradient is zero.</p><p>ii Using H ? for rank ? overcomes vanishing gradients. By using H ? in Eq. (4) in submission, we design a linear function for positive (s j ? s k ) values, where s j (resp. s k ) is the score of a negative (resp. positive) example -see <ref type="figure" target="#fig_3">Fig. 3b</ref> in the main paper. We can see in <ref type="figure">Fig. 9b</ref> that this change enables to have gradients in the correct directions for the two positive instances x 1 and x 2 (tending to increase their scores), and for the negative instance x 3 (tending to decrease its score).</p><p>iii L SupAP is an upper bound of L AP . By the proposed design of H ? in Eq. (4) in submission, we have rank ? s (k) ? rank ? (k). Since we do not approximate rank + (k) by keeping the Heaviside function, it leads to</p><formula xml:id="formula_22">rank + (k) rank + (k)+rank ? s (k) ? rank + (k)</formula><p>rank + (k)+rank ? (k) , and therefore L SupAP ? L AP .</p><p>Overall, L SupAP has all the desired properties : i) A correct gradient flow during training, ii) No vanishing gradients while the correct ranking is not reached, iii) Being an upper bound on the AP loss L AP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Properties of the L calibr. loss function</head><p>We remind the reader of the definition of the decomposability gap given in Eq. (6) of the main paper.</p><formula xml:id="formula_23">DG AP (?) = 1 K K b=1 AP b i (?) ? AP i (?)</formula><p>We illustrates the decomposability gap, DG AP with the toy dataset of <ref type="figure" target="#fig_1">Fig. 10</ref>. The decomposability gap comes from the fact that the AP is not decomposable in mini-batches as we discuss in the Sec. 3.2 of the main paper. The motivation behind L calibr. is thus to force the scores of the different batches to aligned as illustrated in the <ref type="figure" target="#fig_2">Fig. 2b</ref> of the main paper. This upper bound is given in the worst case: when the AP has the lowest value guaranteed by the AP on each batch. We illustrate this case in <ref type="figure" target="#fig_1">Fig. 11</ref>.</p><p>In Eq. (8) from the main paper the 1 in the right hand term comes from the average of AP over all batches:</p><formula xml:id="formula_24">1 K K b=1 AP b i (?) = 1</formula><p>We then justify the term in the parenthesis of Eq. (8) in the main paper, which is the lower bound of the AP. In the global ordering the positive instances are ranked after all the positive instances from previous batches giving the following rank + : j + |P 1 i | + ? ? ? + |P b?1 i |, with j the rank + in the batch, Positive instances are also ranked after all negative instances from previous batches giving rank ? :</p><formula xml:id="formula_25">|N 1 i | + ? ? ? + |N b?1 i |.</formula><p>Therefore we obtain the resulting upper bound of Eq. (8) of the main paper:</p><formula xml:id="formula_26">0 ? DG AP ? 1 ? 1 K b=1 |P b i | ? ? K b=1 B j=1 j + |P 1 i | + ? ? ? + |P b?1 i | j + |P 1 i | + ? ? ? + |P b?1 i | + |N 1 i | + ? ? ? + |N b?1 i | ? ?</formula><p>Proof of Eq. (9): Upper bound on the DG AP with L AP In the main paper we refine the upper bound on DG AP in Eq. (9) by adding L calibr. which calibrates the absolute scores across the minibatches.</p><p>We now write that each positive instance that respects the constraint of L calibr. is ranked after the positive instances of previous batch that respect the constraint giving the following rank + : <ref type="figure" target="#fig_1">Figure 11</ref>: The worst case when computing the global AP would be that each batch is juxtaposed.</p><formula xml:id="formula_27">j + G + 1 + ? ? ? + G + b?1 ,</formula><p>with j the rank + in the current batch. Positive instances are also ranked after the negative instances of previous batches that do not respect the constraints yielding rank ? :</p><formula xml:id="formula_28">E ? 1 + ? ? ? + E ? b?1 .</formula><p>We then write that positive instances that do not respect the constraints are ranked after all positive instances from previous batches and the positive instances respecting the constraints of the current batch giving rank</p><formula xml:id="formula_29">+ : j + G + b |P 1 i | + ? ? ? + |P b?1 i |.</formula><p>They also are ranked after all the negative instances from previous batches giving rank ? :</p><formula xml:id="formula_30">|N 1 i | + ? ? ? + |N b?1 i |.</formula><p>Resulting in Eq. (9) from the main paper:</p><formula xml:id="formula_31">0 ? DG AP ? 1 ? 1 K b=1 |P b i | K b=1 G + b j=1 j + G + 1 + ? ? ? + G + b?1 j + G + 1 + ? ? ? + G + b?1 + E ? 1 + . . . E ? b?1 + E + b j=1 j + G + b + |P 1 i | + ? ? ? + |P b?1 i | j + G + b + |P 1 i | + ? ? ? + |P b?1 i | + |N 1 i | + ? ? ? + |N b?1 i | A.3 Choice of ?</formula><p>In the main paper we introduce ? in Eq. (4) to define H ? . We choose ? as the point where the gradient of the sigmoid function becomes low &lt; , and we then have ? = ? ? ln 1? . This is illustrated in <ref type="figure" target="#fig_1">Fig. 12</ref>. For our experiments we use = 10 ?2 giving ? 0.05. computed on the R first instances retrieved, with R being set to the number of positive instances wrt. a query. mAP@R is a lower bound of the AP (mAP@R = AP when the correct ranking is achieved, i.e. mAP@R = AP = 1).</p><formula xml:id="formula_32">mAP @R i = 1 R R j=1 P (j),</formula><p>where P (j) = precision at j if the jth retrieval is correct 0 otherwise (11)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Detail on experimental setup</head><p>In this section, we describe the experimental setup used in the Sec. 4.1 of the main paper, and the Sec. B of the supplementary.</p><p>We use standard data augmentation strategy during training: images are resized so that their shorter side has a size of 256, we then make a random crop that has a size between 40 and 256, and aspect ratio between 3/4 and 4/3. This crop is then resized to 224x224, and flipped horizontally with a 50% chance. During evaluation, images are resized to 256 and then center cropped to 224.</p><p>We use two different strategy to sample each mini-batch. On CUB and INaturalist we choose a batch size (e.g. 128) and a number of samples per classes (e.g. 4). We then randomly sample classes (e.g. 32) to construct our batches. For SOP we use the hard sampling strategy from <ref type="bibr" target="#b3">[3]</ref>. For each pair of category (e.g. bikes and coffee makers) we use the preceding sampling strategy. This sampling techniques is used because it yields harder and more informative batches. The intuition behind this sampling is that it will be harder to discriminate two bikes from one another, than a bike and a sofa.</p><p>We train the ResNet-50 models using Adam <ref type="bibr" target="#b19">[19]</ref>. On CUB we train our models with a learning rate of 10 ?6 for 200 epochs. For SOP and INaturalist we take the same scheduling as in <ref type="bibr" target="#b2">[2]</ref>. We set the learning rate for the backbone to 10 ?5 and the double for the added linear projection layer. We drop the learning rate by 70% on the epochs 30 and 70. Finally the models are trained for 100 epochs on SOP and 90 on INaturalist (as in <ref type="bibr" target="#b2">[2]</ref>).</p><p>We train the DeiT transformers models using AdamW <ref type="bibr" target="#b21">[21]</ref> as in <ref type="bibr" target="#b7">[7]</ref>. On INaturalist we use the same schedule as when training ResNet-50, with a learning rate of 10 ?5 . On SOP we train for 75 epochs with a learning rate of 10 ?5 which is dropped by 70% at epochs 25 and 50. Finally on CUB we train the models for about 100 epochs with a learning rate of 10 ?6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Details of the backbones used</head><p>We briefly describe the backbones used throughout out the experiments presented in the main paper and the supplementary.</p><p>ResNet-50 <ref type="bibr" target="#b14">[14]</ref> We use the well-known convolutional neural network ResNet-50. We remove the linear classification layer. We also add a linear projection layer to reduce the dimension (e.g. from 2048 to 512).</p><p>DeiT <ref type="bibr" target="#b40">[39]</ref> Recently transformer models have been introduced for computer vision <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b40">39]</ref>. They establish new state-of-the-art performances on computer vision tasks. We use the DeiT-S from <ref type="bibr" target="#b40">[39]</ref> which has less parameters than the ResNet-50 (? 21 million for DeiT vs 25 for ResNet-50). We use the pretrained version with distillation from <ref type="bibr" target="#b40">[39]</ref> and its implementation in the timm library <ref type="bibr" target="#b47">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ROADMAP validation</head><p>Comparison to AP approximations We compare in <ref type="table" target="#tab_8">Table 5</ref> ROADMAP vs other ranking losses on different settings : a batch size of 128 and two backbones (ResNet-50 and DeiT). We conduct this comparison on 5 runs to show the statistical improvement of our method compared to other ranking losses baselines.</p><p>We observe that our method outperforms recent ranking losses on the two backbones and the three datasets. On SOP and CUB, ROADMAP has a high increase for the mAP@R, of +1pt on CUB and +2pt on SOP. The performance improvement is greater on the large scale dataset INaturalist with ?+3.5pt with a ResNet-50 backbone and ?+2pt with a DeiT backbone of mAP@R. This trend is the same as in the comparison of the main paper <ref type="table" target="#tab_1">(Table 1</ref>).   Comparison to state of the art method We show in <ref type="table" target="#tab_11">Table 8</ref> the impact of increasing the embedding dimension when using ResNet-50. All metrics improve on the three datasets when the embedding dimension increases. We observe a gain particularly important on CUB and SOP with ?+1pt in R@1 and mAP@R.</p><p>Choosing an embedding size of 2048 further boost the performances of ROADMAP, yielding competitive performances on CUB and state-of-the-art performances for SOP and INaturalist.  <ref type="table" target="#tab_12">Table 9</ref> preliminary experiments to evaluate ROADMAP on ROxford and RParis <ref type="bibr" target="#b30">[30]</ref>, by training our model on the SfM-120k dataset and using the standard GitHub code for evaluation 2 .</p><p>We can see that ROADMAP is significantly better than <ref type="bibr" target="#b7">[7]</ref> with the DeiT-S <ref type="bibr" target="#b40">[39]</ref> on ROxford and RParis medium protocol, and has similar performances for RParis hard protocol. This highlights the relevance of using ROADMAP instead of the contrastive loss used in <ref type="bibr" target="#b7">[7]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Model analysis</head><p>Hyperparameters In <ref type="figure" target="#fig_1">Fig. 13</ref> we show the impact of the hyperparameters of L SupAP . We plot the mAP@R vs ? in <ref type="figure" target="#fig_1">Fig. 13a</ref> and mAP@R vs ? in <ref type="figure" target="#fig_1">Fig. 13b</ref>. The experiments are conducted on SOP with a batch size of 128.</p><p>We observe on <ref type="figure" target="#fig_1">Fig. 13a</ref> that L SupAP is stable with small values of ? , i.e. in the range [0.001, 0.05]. As a reminder we use the default value ? = 0.01 in all our results, as it was the suggested value from the SmoothAP paper <ref type="bibr" target="#b2">[2]</ref>.</p><p>We conduct a study of the impact of ? in <ref type="figure" target="#fig_1">Fig. 13b</ref>. We find that L SupAP is very stable wrt. this hyperparameter. Performances are improving with a greater value of ? before dropping after 10 4 . The trend follows what was observed in the <ref type="figure" target="#fig_5">Fig. 4b</ref> of the main paper, although this time using a value if ? = 10 4 yields better performances. Using cross-validation to choose an optimal value for ? may lead to even better performances for L SupAP .  Decomposability gap In <ref type="table" target="#tab_1">Table 10</ref> we measure the relative decrease of the decomposability gap DG AP on SOP and CUB test sets. On both datasets we can see that L calibr. decreases the decomposability gap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Source code</head><p>We describe in this section the software used for our work, and discuss the computation costs associated with training models presented in this paper. <ref type="figure" target="#fig_1">Figure 14</ref>: Qualitative results on CUB: a query (purple) with the 10 most similar instances. Relevant (resp. irrelevant) instances are in green (resp. red).  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) LSupAP ? LAP and ?LSupAP &gt; 0 in this example, in contrast to SmoothAP [2]. This ensures robust training and comes from a new approximation of the rank function. (b) LAP non-decomposability: LAP = 0 in all batches B i despite LAP = 0 over the whole i B i . Lcalibr. controls the absolute scores between batches, such that LROADMAP = 0 in each batch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Our robust and decomposable Average Precision training (ROADMAP) includes (a) a smooth loss L SupAP upper-bounding L AP , and (b) a calibration loss L calibr. supporting decomposability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>ROADMAP training: we optimize parameters ? of a deep neural networks to minimize a smooth surrogate of L APi (?) between the query q i and the retrieval set ?. Our smooth rank approximations H + and H ? enables L SupAP to be both accurate and robust (sec 3.1), and L calibr. enables an implicit batch scores comparison for better decomposability without additional storing (sec 3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Proposed surrogate losses for the Heaviside (step): with H + (x) in Fig. 3a and H ? (x) in Fig. 3b, L SupAP in Eq. (5)is an upper bound of L AP . In addition, H ? (x) back-propagates gradients until the correct ranking is satisfied, in contrast to the sigmoid used in<ref type="bibr" target="#b2">[2]</ref> (Fig. 3c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>mAP@R vs ? ? ? for Lcalibr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of ROADMAP hyperparameters on INaturalist (batch size 224).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 4b shows</head><label></label><figDesc>the influence of the slope ? that controls the linear regime in H ? and determines the amount of gradient backpropagated for negative samples with a (wrong) high score. As shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Relative increase of the mAP@R vs batch size when adding L calibr. to L SupAP .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7</head><label>7</label><figDesc>shows another qualitative assessment on INaturalist, where ROADMAP corrects some failing cases of the SmoothAP baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>Results on INaturalist: a query (purple) with the 4 most similar retrieved images (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Results on INaturalist: a query (purple) with the 9 most similar retrieved images, green for relevant images, red otherwise. Top line results with ROADMAP. Bottom line results with SmoothAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Illustration of the decomposability gap on a toy dataset. Proof of Eq. (8): Upper bound on the DG AP with no L AP We choose a setting for the proof of the upper bound similar to the one used for training, i.e. all the batch have the same size, and the number of positive instances per batch (i.e. P b i ) is the same. Eq. (8) from the main paper gives an upper bound for DG AP .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Gradient of the temperature scaled sigmoid (? = 0.01) vs the difference of scores s k ? s j of a negative pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>mAP@R vs ? for LSupAP.10 ?1 10 0 10 1 10 2 10 3 10 4 mAP@R vs ? for LSupAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 :</head><label>13</label><figDesc>Analysis of L SupAP hyperparameters on SOP (batch size 128).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 :</head><label>15</label><figDesc>Qualitative results on SOP: a query (purple) with the 10 most similar instances. Relevant (resp. irrelevant) instances are in green (resp. red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 :</head><label>16</label><figDesc>Qualitative results on INaturalist: a query (purple) with the 10 most similar instances. Relevant (resp. irrelevant) instances are in green (resp. red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This refined upper bound is tighter than the upper bound of Eq.<ref type="bibr" target="#b8">(8)</ref>. Our new L calibr. loss directly optimizes this upper bound (by explicitly optimizing E</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison between ROADMAP and state-of-the-art AP ranking based methods.</figDesc><table><row><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>SOP</cell><cell cols="2">INaturalist</cell></row><row><cell>Method</cell><cell cols="6">R@1 mAP@R R@1 mAP@R R@1 mAP@R</cell></row><row><cell>FastAP [3]</cell><cell>58.9</cell><cell>22.9</cell><cell>78.2</cell><cell>51.3</cell><cell>53.5</cell><cell>19.6</cell></row><row><cell>SoftBin [32]</cell><cell>61.2</cell><cell>24.0</cell><cell>80.1</cell><cell>53.5</cell><cell>56.6</cell><cell>20.1</cell></row><row><cell cols="2">BlackBox [33] 62.6</cell><cell>23.9</cell><cell>80.0</cell><cell>53.1</cell><cell>52.3</cell><cell>15.2</cell></row><row><cell cols="2">SmoothAP [2] 62.1</cell><cell>23.9</cell><cell>80.9</cell><cell>54.6</cell><cell>59.8</cell><cell>20.7</cell></row><row><cell>ROADMAP</cell><cell>64.2</cell><cell>25.3</cell><cell>82.0</cell><cell>56.5</cell><cell>64.5</cell><cell>25.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Our method compared to cross batch memory<ref type="bibr" target="#b46">[44]</ref>. The unit of time is m/e which stands for minutes per epoch.</figDesc><table><row><cell></cell><cell></cell><cell>SOP</cell><cell></cell><cell></cell><cell>INaturalist</cell><cell></cell></row><row><cell>Method</cell><cell cols="6">R@1 mAP@R time? R@1 mAP@R time?</cell></row><row><cell>XBM [44]</cell><cell>80.6</cell><cell>54.9</cell><cell>6</cell><cell>59.3</cell><cell>18.5</cell><cell>34</cell></row><row><cell cols="2">ROADMAP (ours) 82.0</cell><cell>56.5</cell><cell>2</cell><cell>64.5</cell><cell>25.1</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study for the impact of our two contribution on and the SmoothAP baseline.</figDesc><table><row><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>SOP</cell><cell cols="2">INaturalist</cell></row><row><cell>Method</cell><cell cols="6">H ? L calibr. R@1 mAP@R R@1 mAP@R R@1 mAP@R</cell></row><row><cell>SmoothAP [2]</cell><cell>62.1</cell><cell>23.9</cell><cell>80.9</cell><cell>54.6</cell><cell>59.7</cell><cell>20.7</cell></row><row><cell>SupAP</cell><cell>62.9</cell><cell>24.6</cell><cell>81.4</cell><cell>55.3</cell><cell>61.2</cell><cell>21.3</cell></row><row><cell>ROADMAP</cell><cell>64.2</cell><cell>25.3</cell><cell>82.0</cell><cell>56.5</cell><cell>64.5</cell><cell>25.1</cell></row><row><cell cols="2">4.2 State of the art comparison</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of state of the art performances from the literature on SOP, CUB and INaturalist with the proposed ROADMAP (recall@k). Except for the DeiT category, all methods rely on a standard convolutional backbone (generally ResNet-50).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SOP</cell><cell></cell><cell cols="2">CUB</cell><cell></cell><cell></cell><cell>INaturalist</cell></row><row><cell></cell><cell>Method</cell><cell>dim</cell><cell>1</cell><cell cols="2">10 100 1</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>1</cell><cell>4</cell><cell>16 32</cell></row><row><cell></cell><cell>Triplet SH [46]</cell><cell cols="9">512 72.7 86.2 93.8 63.6 74.4 83.1 90.0 58.1 75.5 86.8 90.7</cell></row><row><cell>Metric learning</cell><cell>LiftedStruct [36] MIC [34] MS [43] SEC [52] HORDE [17] XBM [44]</cell><cell cols="8">512 62.1 79.8 91.3 47.2 58.9 70.2 80.2 -512 77.2 89.4 95.6 66.1 76.8 85.6 --512 78.2 90.5 96.0 65.7 77.0 86.3 91.2 -512 78.7 90.8 96.6 68.8 79.4 87.2 92.5 -512 80.1 91.3 96.2 66.8 77.4 85.1 91.0 -128 80.6 91.6 96.2 65.8 75.9 84.0 89.9 -</cell><cell>------</cell><cell>------</cell><cell>------</cell></row><row><cell></cell><cell cols="9">Triplet SCT [48] 512/64 81.9 92.6 96.8 57.7 69.8 79.6 87.0 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ProxyNCA [25]</cell><cell cols="3">512 73.7 -</cell><cell cols="6">-49.2 61.9 67.9 72.4 61.6 77.4 87.0 90.6</cell></row><row><cell>Classification</cell><cell cols="9">ProxyGML [53] NSoftmax [51] NSoftmax [51] Cross-Entropy [1] 2048 81.1 91.7 96.3 69.2 79.2 86.9 91.6 -512 78.0 90.6 96.2 66.6 77.6 86.4 --512 78.2 90.6 96.2 61.3 73.9 83.5 90.0 -2048 79.5 91.5 96.7 65.3 76.7 85.4 91.8 -ProxyNCA++ [38] 512 80.7 92.0 96.7 69.0 79.8 87.3 92.7 -</cell><cell>-----</cell><cell>-----</cell><cell>-----</cell></row><row><cell></cell><cell cols="9">ProxyNCA++ [38] 2048 81.4 92.4 96.9 72.2 82.0 89.2 93.5 -</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FastAP [3]</cell><cell cols="4">512 76.4 89.0 95.1 -</cell><cell>-</cell><cell>-</cell><cell cols="3">-60.6 77.0 87.2 90.6</cell></row><row><cell>AP loss</cell><cell cols="10">BlackBox [33] SmoothAP [2] SoftBin * [32] ROADMAP (ours) 512 83.1 92.7 96.3 68.5 78.7 86.6 91.9 69.1 83.1 91.3 93.9 512 78.6 90.5 96.0 64.0 75.3 84.1 90.6 62.9 79.4 88.7 91.7 512 80.1 91.5 96.6 ----67.2 81.8 90.3 93.1 512 80.6 91.3 96.1 61.2 73.14 83.0 89.5 64.2 77.1 82.7 91.7</cell></row><row><cell>DeiT</cell><cell cols="10">IRT R [7] ROADMAP (ours) 384 86.0 94.4 97.6 77.4 85.5 91.4 95.0 73.6 86.2 93.1 95.2 384 84.2 93.7 97.3 76.6 85.0 91.1 94.3 ----</cell></row><row><cell>Fig</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison between ROADMAP and state-of-the-art AP ranking based losses on three image retrieval datasets. Bck in the first column stands for bakcbone. Models are trained with a batch size of 128.</figDesc><table><row><cell></cell><cell></cell><cell>CUB</cell><cell>SOP</cell><cell></cell><cell cols="2">INaturalist</cell></row><row><cell>Bck Method</cell><cell>R@1</cell><cell>mAP@R</cell><cell>R@1</cell><cell>mAP@R</cell><cell>R@1</cell><cell>mAP@R</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Ablation study for the impact of our two contribution vs the SmoothAP baseline for the three datasets and different batch sizes, with a ResNet-50 backbone<ref type="bibr" target="#b14">[14]</ref> </figDesc><table><row><cell>CUB</cell><cell>SOP</cell><cell>INaturalist</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Ablation study for the impact of our two contribution vs the SmoothAP baseline for the three</figDesc><table><row><cell cols="6">datasets and different batch sizes, with a DeiT backbone [39]</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>SOP</cell><cell cols="2">INaturalist</cell></row><row><cell cols="2">BS Method</cell><cell>H ? L calibr.</cell><cell>R@1</cell><cell>mAP@R</cell><cell>R@1</cell><cell>mAP@R</cell><cell>R@1</cell><cell>mAP@R</cell></row><row><cell></cell><cell>SmoothAP</cell><cell></cell><cell>76.2</cell><cell>34.7</cell><cell>84.16</cell><cell>60.18</cell><cell>69.83</cell><cell>29.49</cell></row><row><cell>128</cell><cell>SupAP</cell><cell></cell><cell>76.33</cell><cell>34.91</cell><cell>84.74</cell><cell>61.29</cell><cell>71.12</cell><cell>30.5</cell></row><row><cell></cell><cell>ROADMAP</cell><cell></cell><cell>77.09</cell><cell>35.76</cell><cell>85.44</cell><cell>62.57</cell><cell>72.82</cell><cell>31.36</cell></row><row><cell></cell><cell>SmoothAP</cell><cell></cell><cell>76.38</cell><cell>35.33</cell><cell>84.3</cell><cell>60.49</cell><cell>70.55</cell><cell>30.25</cell></row><row><cell>224</cell><cell>SupAP</cell><cell></cell><cell>76.47</cell><cell>35.67</cell><cell>84.77</cell><cell>61.38</cell><cell>71.9</cell><cell>31.31</cell></row><row><cell></cell><cell>ROADMAP</cell><cell></cell><cell>77.14</cell><cell>36.18</cell><cell>85.56</cell><cell>62.75</cell><cell>73.64</cell><cell>31.82</cell></row><row><cell></cell><cell>SmoothAP</cell><cell></cell><cell>76.72</cell><cell>35.86</cell><cell>84.66</cell><cell>61.26</cell><cell>71.09</cell><cell>30.89</cell></row><row><cell>384</cell><cell>SupAP</cell><cell></cell><cell>77.13</cell><cell>36.17</cell><cell>85.01</cell><cell>61.76</cell><cell>72.55</cell><cell>31.89</cell></row><row><cell></cell><cell>ROADMAP</cell><cell></cell><cell>77.38</cell><cell>36.23</cell><cell>85.35</cell><cell>62.29</cell><cell>73.64</cell><cell>32.12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Difference in performance when using an embedding size of 512 vs 2048 with a ResNet-50 backbone, on the three datasets. Performances are obtained with the same setup as described in the Sec. 4.2 of the main paper.Preliminary results on Landmarks retrieval We show in</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CUB</cell><cell></cell><cell>SOP</cell><cell cols="2">INaturalist</cell></row><row><cell>Method</cell><cell>dim</cell><cell cols="6">R@1 mAP@R R@1 mAP@R R@1 mAP@R</cell></row><row><cell cols="2">ROADMAP (ours) 512</cell><cell>68.5</cell><cell>27.97</cell><cell>83.19</cell><cell>58.05</cell><cell>69.19</cell><cell>27.85</cell></row><row><cell cols="3">ROADMAP (ours) 2048 69.87</cell><cell>28.8</cell><cell>83.77</cell><cell>59.38</cell><cell>69.62</cell><cell>27.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Comparison of ROADMAP vs IRT<ref type="bibr" target="#b7">[7]</ref> on ROxford and RParis<ref type="bibr" target="#b30">[30]</ref>. Models are DeiT-S<ref type="bibr" target="#b40">[39]</ref>, ROADMAP is trained with a batch size of 128.</figDesc><table><row><cell>Method</cell><cell cols="4">ROxford Medium Hard Medium Hard RParis</cell></row><row><cell>IRT [7]</cell><cell>34.5</cell><cell>15.8</cell><cell>65.8</cell><cell>42.0</cell></row><row><cell>ROADMAP (ours)</cell><cell>38.9</cell><cell>20.7</cell><cell>67.5</cell><cell>42.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Relative decrease of the decomposability gap when adding L calibr. to L SupAP (ROADMAP). Dataset decrease of DG AP</figDesc><table><row><cell>CUB</cell><cell>3.7%</cell></row><row><cell>SOP</cell><cell>5.4%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For the sake of readability we drop in the following the dependence on ? for the rank, i.e. rank(k) := rank(k, ?) and on the query for the similarity, i.e. sj := s(qi, xj).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/filipradenovic/cnnimageretrieval-pytorch</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/naver/deep-image-retrieval 4 https://github.com/Andrew-Brown1/Smooth_AP 5 https://github.com/martius-lab/blackbox-backprop 6 https://github.com/kunhe/FastAP-metric-learning</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was done under a grant from the the AHEAD ANR program (ANR-20-THIA-0002). It was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012645 made by GENCI.</p><p>Librairies We use several Python libraries often used in image retrieval.</p><p>We use PyTorch <ref type="bibr" target="#b29">[29]</ref> as a general framework to implement our neural networks, losses and training loops. We use several utilities from PyTorch Metric Learing <ref type="bibr" target="#b27">[27]</ref>, an open-source Python library focused on helping researcher working on image retrieval and metric learning. We use Faiss <ref type="bibr" target="#b18">[18]</ref> to compute metrics (i.e. to perform nearest neighbours search), which is a Python library often used in image retrieval to compute the rankings or the similarity matrix. To load and use the transformer models we use timm <ref type="bibr" target="#b47">[45]</ref>, a library implementing recent computer vision models, with pretrained weights for most of them. To handle all our config files, we use Hydra <ref type="bibr" target="#b51">[49]</ref>, this library makes it possible to combine the use of Yaml configuration files and overriding them using the command line.</p><p>We use the publicly available implementation of SoftBinAP 3 <ref type="bibr" target="#b32">[32]</ref> which is under a BSD-3 license. The original codes of SmoothAP 4 <ref type="bibr" target="#b2">[2]</ref>, BlackBox 5 <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b33">33]</ref> are under an MIT license. For FastAP <ref type="bibr" target="#b3">[3]</ref> we use the implementation from <ref type="bibr" target="#b27">[27]</ref> (MIT license), the original implementation of FastAP 6 is also under an MIT license.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiments B.1 Metrics</head><p>We detail here the performance metrics that we use to evaluate our models.</p><p>Recall@K The Recall@K metrics (Eq. (10)) is often used in the literature. For a single query the Recall@K is 1 if a positive instance is in the K nearest neighbors, and 0 otherwise. The Recall@K is then averaged on all the queries. Researcher use different values of K for a given dataset (e.g. 1, 2, 4, 8 on CUB).</p><p>r(i), where r(i) = 1 if a positive instance has a ranking smaller than i 0 otherwise</p><p>mAP@R Recently, the mAP@R (Eq. (11)) has been introduced in <ref type="bibr" target="#b26">[26]</ref>. The authors show that this metric is less noisy and better captures the performance of a model. The mAP@R is a partial AP,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compute costs</head><p>We use mixed-precision learning offered within PyTorch <ref type="bibr" target="#b29">[29]</ref>. The time and memory consumption are reduced by a factor between 2 and 3/2 with no notable difference in performances. We could train all models on 16GiB GPUs, except for models trained with a batch size of 384 which requires a 32GiB GPU.</p><p>CUB Models take between 30 minutes and 1 hour to train on a Nvidia Quadro RTX 5000 with 16GiB.</p><p>SOP Models take between 4 and 8 hours to train on a Nvidia Quadro RTX 5000 with 16GiB.</p><p>INaturalist To train models on INaturalist we were granted access to the IDRIS HPC cluster with Tesla V-100 GPUs (of 16GiB or 32GiB). Models train for approximately 20 hours.</p><p>We could not train models with mixed-precision when using BlackBox <ref type="bibr" target="#b33">[33]</ref>. Models trained with it took longer to train (e.g. 30 hours on INaturalist) and are more demanding on memory (almost 16GiB with a batch size of 128 while models trained with other loss functions required less than 10Gib).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative results</head><p>CUB As a qualitative assessment, we show in <ref type="figure">Fig. 14 some results</ref> of ROADMAP on CUB. We show the queries (in purple) and the 10 most similar retrieved images, with relevant instances in green and irrelevant instances in red.</p><p>SOP In <ref type="figure">Fig. 15</ref> we perform the same assessment for SOP. In SOP there are fewer relevant instances per query (in average 5). So even for queries that retrieved all the relevant instances, there will be negative instances that have high ranks (in <ref type="figure">Fig. 15</ref> ranks that are lower than 10).</p><p>INaturalist Finally we show on <ref type="figure">Fig. 16</ref> some examples of queries and the 10 most similar instances for a model trained with ROADMAP on INaturalist.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unifying mutual information view of metric learning: cross-entropy vs. pairwise losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename><surname>Boudiaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Rony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Imtiaz Masud Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="548" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smooth-ap: Smoothing the path towards large-scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicky</forename><surname>Kalogeiton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="677" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1861" to="1870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval in the cooking context: Learning semantic text-image embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micael</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cad?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</title>
		<editor>Kevyn Collins-Thompson, Qiaozhu Mei, Brian D. Davison, Yiqun Liu, and Emine Yilmaz</editor>
		<meeting><address><addrLine>Ann Arbor, MI, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018-07-08" />
			<biblScope unit="page" from="35" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting negative evidence for deep latent structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="351" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Training vision transformers for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05644</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sodeep: A sorting deep net to learn ranking loss surrogates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engilberge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Chevallier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">VSE++: improving visualsemantic embeddings with hard negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep metric learning with hierarchical triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end learning of deep visual representations for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smart mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Harwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>corr abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hashing as tie-aware learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Kun He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><forename type="middle">Adel</forename><surname>Cakir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Bargal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Local descriptors optimized for average precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Metric learning with horde: High-order regularizer for deep embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6539" to="6548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Billion-scale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08734</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons between quadruplets of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><forename type="middle">T</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="65" to="94" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2859" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Metric learning to rank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gert</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual International Conference on Machine Learning (ICML</title>
		<meeting>the 27th annual International Conference on Machine Learning (ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient optimization for rank-based loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pritish</forename><surname>Mohapatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Rol?nek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Jawahar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Pawan</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A metric learning reality check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="681" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Pytorch metric learning</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">V?t Musil, Georg Martius, and Michal Rol?nek. Differentiation of blackbox combinatorial solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Vlastelica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Paulus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Revisiting oxford and paris: Large-scale image retrieval benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Giorgos Tolias, Yannis Avrithis, and Ond?ej Chum</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CNN image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning with average precision: Training image retrieval with a listwise loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar Roberto De</forename><surname>Rafael S Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Souza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5107" to="5116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Optimizing rank-based metrics with blackbox differentiation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rol?nek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V?t</forename><surname>Musil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Vlastelica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7620" to="7630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mic: Mining interclass characteristics for improved metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biagio</forename><surname>Brattoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjorn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8000" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stochastic class-based hard example mining for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumin</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Proxynca++: Revisiting and revitalizing proxy neighborhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Eu Wern Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham W</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning deep embeddings with histogram loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD Birds</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dataset</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5022" to="5030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-batch memory for embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6388" to="6397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2840" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Becker, S. Thrun, and K. Obermayer</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Hard negative examples are hard, but useful</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaotong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="126" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Hydra -a framework for elegantly configuring complex applications. Github</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omry</forename><surname>Yadan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A support vector method for optimizing average precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07</title>
		<meeting>the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Making classification competitive for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1811.12649</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep metric learning with spherical embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="18772" to="18783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Fewer is more: A deep graph metric learning perspective using fewer proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuehua</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muli</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17792" to="17803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Softbinap</surname></persName>
		</author>
		<idno>32] 61.70?0.10 24.29?0.16 80.30?0.21 53.69?0.27 60.88?0.06 23.22?0.05</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blackboxap</surname></persName>
		</author>
		<idno>33] 61.96?0.28 23.83?0.14 80.97?0.07 54.49?0.15 59.53?0.12 19.62?0.02</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title/>
		<idno>ours) 64.05?0.51 25.27?0.12 82.20? 0.09 56.64?0.09 68.15?0.10 27.01?0.10</idno>
	</analytic>
	<monogr>
		<title level="j">ROADMAP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Softbinap</surname></persName>
		</author>
		<idno>32] 74.84?0.11 33.57?0.08 84.09?0.05 60.53?0.07 65.97?0.13 27.57?0.09</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blackboxap</surname></persName>
		</author>
		<idno>33] 75.45?0.22 33.97?0.10 84.07?0.09 60.20?0.05 70.29?0.10 29.44?0.06</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
		<idno>ours) 77.14?0.12 36.30?0.08 85.44? 0.06 62.73?0.06 72.81?0.11 31.31?0.10</idno>
	</analytic>
	<monogr>
		<title level="j">ROADMAP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">We perform a paired student t-test to further asses the statistical significance of the performance boost obtained with ROADMAP. We compute the p-values for both the R@1 and mAP@R: it turns out that the p-values are never larger than 0.001, meaning that the gain is statistically significant</title>
		<imprint/>
	</monogr>
	<note>with a risk less than 0.1%)</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">On all settings L SupAP outperforms the L SmoothAP baseline by almost ?+0.5pt consistently, and almost +1pt on every setting for INaturalist. When we add L calibr. the gain is further increased</title>
	</analytic>
	<monogr>
		<title level="m">Ablation studies In Table 6 we extend the ablation studies of the main paper</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>Table 2 of main paper) to other settings, including more batch sizes (32, 128, 224, 384) and two backbones (ResNet-50 and DeiT). main paper) the gain when adding L calibr. is particularly noticeable on the large in Table 6. L SupAP is consistently better than the L SmoothAP baseline, with gain up to more than 1pt (e.g. on batch size 128 on INaturalist. L calibr. further lifts the performances on the three datasets and all batch sizes</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
