<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Simple Yet Strong Pipeline for HotpotQA</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Groeneveld</surname></persName>
							<email>dirkg@allenai.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
							<email>tushark@allenai.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename><forename type="middle">?</forename></persName>
							<email>mausam@cse.iitd.ac.in</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
							<email>ashishs@allenai.org</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Allen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute for AI</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">U.S.A</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Indian Institute of Technology</orgName>
								<address>
									<settlement>Delhi</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Simple Yet Strong Pipeline for HotpotQA</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>State-of-the-art models for multi-hop question answering typically augment large-scale language models like BERT with additional, intuitively useful capabilities such as named entity recognition, graph-based reasoning, and question decomposition.</p><p>However, does their strong performance on popular multihop datasets really justify this added design complexity? Our results suggest that the answer may be no, because even our simple pipeline based on BERT, named QUARK, performs surprisingly well. Specifically, on Hot-potQA, QUARK outperforms these models on both question answering and support identification (and achieves performance very close to a RoBERTa model). Our pipeline has three steps: 1) use BERT to identify potentially relevant sentences independently of each other; 2) feed the set of selected sentences as context into a standard BERT span prediction model to choose an answer; and 3) use the sentence selection model, now with the chosen answer, to produce supporting sentences. The strong performance of QUARK resurfaces the importance of carefully exploring simple model designs before using popular benchmarks to justify the value of complex techniques.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Textual Multi-hop Question Answering (QA) is the task of answering questions by combining information from multiple sentences or documents. This is a challenging reasoning task that requires QA systems to identify relevant pieces of information in the given text and learn to compose them to answer a question. To enable progress in this area, many datasets <ref type="bibr" target="#b11">(Welbl et al., 2018;</ref><ref type="bibr" target="#b9">Talmor and Berant, 2018;</ref><ref type="bibr" target="#b14">Yang et al., 2018;</ref><ref type="bibr" target="#b2">Khot et al., 2020)</ref> and models <ref type="bibr" target="#b5">(Min et al., 2019b;</ref><ref type="bibr" target="#b13">Xiao et al., 2019;</ref><ref type="bibr" target="#b10">Tu et al., 2019)</ref>  focuses on HotpotQA <ref type="bibr" target="#b14">(Yang et al., 2018)</ref>, which contains 105,257 multi-hop questions derived from two Wikipedia paragraphs, where the correct answer is a span in these paragraphs or yes/no. Due to the multi-hop nature of this dataset, it is natural to assume that the relevance of a sentence for a question would depend on the other sentences considered to be relevant. E.g., the relevance of "Obama was born in Hawaii." to the question "Where was the 44 th President of USA born?" depends on the other relevant sentence: "Obama was the 44 th President of US." As a result, many approaches designed for this task focus on jointly identifying the relevant sentences (or paragraphs) via mechanisms such as cross-document attention, graph networks, and entity linking.</p><p>Our results question this basic assumption. We show that a simple model, QUARK (see <ref type="figure" target="#fig_0">Fig. 1</ref>), that first identifies relevant sentences from each paragraph independent of other paragraphs, is surprisingly powerful on this task: in 90% of the questions, QUARK's relevance module recovers all gold supporting sentences within the top-5 sentences. For QA, it uses a standard BERT <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref> span prediction model (similar to current published models) on the output of this module. Additionally, QUARK exploits the inherent similarity between the relevant sentence identification task and the task of generating an explanation given an answer produced by the QA module: it uses the same architecture for both tasks.</p><p>We show that this independent sentence scoring model results in a simple QA pipeline that outperforms all other BERT models in both 'distractor' and 'fullwiki' settings of HotpotQA. In the distractor setting (10 paragraphs, including two gold, provided as context), QUARK achieves joint scores (answer and support prediction) within 0.75% of the current state of the art. Even in the fullwiki setting (all 5M Wikipedia paragraphs as context), by combining our sentence selection approach with a commonly used paragraph selection approach <ref type="bibr" target="#b6">(Nie et al., 2019)</ref>, we outperform all previously published BERT models. In both settings, the only models scoring higher use RoBERTa , a more robustly trained language model that is known to outperform BERT across various tasks.</p><p>While our design uses multiple transformer models (now considered a standard starting point in NLP), our contribution is a simple pipeline without any bells and whistles, such as NER, graph networks, entity linking, etc.</p><p>The closest effort to QUARK is by <ref type="bibr" target="#b4">Min et al. (2019a)</ref>, who also propose a simple QA model for HotpotQA. Their approach selects answers independently from each paragraph to achieve competitive performance on the question-answering subtask of HotpotQA (they do not address the support identification subtask). We show that while relevant sentences can be selected independently, operating jointly over these sentences chosen from multiple paragraphs can lead to state-of-the-art questionanswering results, outperforming independent answer selection by several points.</p><p>Finally, our ablation study demonstrates that the sentence selection module benefits substantially from using context from the corresponding paragraph. It also shows that running this module a second time, with the chosen answer as input, results in more accurate support identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Most approaches for HotpotQA attempt to capture the interactions between the paragraphs by either relying on cross-attention between documents or sequentially selecting paragraphs based on the previously selected paragraphs.</p><p>While <ref type="bibr" target="#b7">Nishida et al. (2019)</ref> also use a standard Reading Comprehension (RC) model, they combine it with a special Query Focused Extractor (QFE) module to select relevant sentences for QA and explanation. The QFE module sequentially identifies relevant sentences by updating a RNN state representation in each step, allowing the model to capture the dependency between sentences across time-steps. <ref type="bibr" target="#b13">Xiao et al. (2019)</ref> propose a Dynamically Fused Graph Networks (DFGN) model that first extracts entities from paragraphs to create an entity graph, dynamically extract subgraphs and fuse them with the paragraph representation. The Select, Answer, Explain (SAE) model <ref type="bibr" target="#b10">(Tu et al., 2019)</ref> is similar to our approach in that it also first selects relevant documents and uses them to produce answers and explanations. However, it relies on a self-attention over all document representations to capture potential interactions. Additionally, they rely on a Graph Neural Network (GNN) to answer the questions. Hierarchical Graph Network (HGN) model <ref type="bibr" target="#b1">(Fang et al., 2019)</ref> builds a hierarchical graph with three levels: entities, sentences and paragraphs to allow for joint reasoning. DecompRC <ref type="bibr" target="#b5">(Min et al., 2019b)</ref> takes a completely different approach of learning to decompose the question (using additional annotations) and then answer the decomposed questions using a standard single-hop RC system.</p><p>Others such as <ref type="bibr" target="#b4">Min et al. (2019a)</ref> have also noticed that many HotpotQA questions can be answered just based on a single paragraph. Our findings are both qualitatively and quantitatively different. They did not consider the support identification task, and showed strong (but not quite SoTA) QA performance by running a QA model independently on each paragraph. We, on the other hand, show that interaction is not essential for selecting relevant sentences but actually valuable for QA! Specifically, by using a context of relevant sentences spread across multiple paragraphs in steps 2 and 3, our simple BERT model outperforms previous models with complex entity-and graph-based interactions on top of BERT. We thus view QUARK as a different, stronger baseline for multi-hop QA.</p><p>In the fullwiki setting, each question has no associated context and models are expected to select paragraphs from Wikipedia. To be able to scale to such a large corpus, the proposed systems often select the paragraphs independent of each other. A recent retrieval method in this setting is Semantic Retrieval <ref type="bibr" target="#b6">(Nie et al., 2019)</ref> where first the paragraphs are selected based on the question, followed by individual sentences from these paragraphs. However, unlike our approach, they do not use the paragraph context to select the sentences, missing key context needed to identify relevance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pipeline Model: QUARK</head><p>Our model works in three steps. First, we score individual sentences from an input set of paragraphs D based on their relevance to the question. Second, we feed the highest-scoring sentences to a span prediction model to produce an answer to the question. Third, we score sentences from D a second time to identify the supporting sentences using the answer. These three steps are implemented using the two modules described next in Sections 3.1 and 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sentence Scoring Module</head><p>In the distractor setting, HotpotQA provides 10 context paragraphs that have an average length of 41.4 sentences and 1106 tokens. This is too long for standard language-model based span-predictionmost models scale quadratically with the number of tokens, and some are limited to 512 tokens. This motivates selecting a few relevant sentences E to reduce the size of the input to the span-prediction model without losing important context. In a similar vein, the support identification subtask of Hot-potQA also involves selecting a few sentences that best explain the chosen answer. We solve both of these problems with the same transformer-based sentence scoring module, with slight variation in its input.</p><p>Our sentence scorer uses the BERT-Large-Cased model <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref> trained with wholeword masking, with an additional linear layer over the [CLS] token. Here, whole word masking refers to a BERT variant that masks entire words instead of word pieces during pre-training.</p><p>We score every sentence s from every paragraph p ? D independently by feeding the following sequence to the model: [CLS] question [SEP] p [SEP] answer <ref type="bibr">[SEP]</ref>. This sequence is the same for every sentence in the para-graph, but the sentence being classified is indicated using a segment IDs: It is set to 1 for tokens from the sentence and to 0 for the rest. If a paragraph has more than 512 tokens, we restrict the input to the first 512. Each annotated support sentence forms a positive example and all other sentences from D form the negative examples. Note that our classifier scores each sentence independently and never sees sentences from two paragraphs at the same time.</p><p>(See Appendix A.1 for further detail.)</p><p>We train two variants of this model: (1) r na (s) is trained to score sentences given a question but no answer (answer is replaced with a [MASK] token); and (2) r a (s) is trained to score sentences given a question and its gold answer. We use r na (s) for relevant sentence selection and r a (s) for support identification (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Question Answering Module</head><p>To find answers to questions, we use <ref type="bibr" target="#b12">Wolf et al. (2019)</ref>'s implementation of <ref type="bibr" target="#b0">Devlin et al. (2019)</ref>'s span prediction model. To achieve our best score, we use their BERT-Large-Cased model with whole-word masking and SQuAD (Rajpurkar et al., 2016) fine-tuning. <ref type="bibr">1</ref> We fine-tune this model on the HotpotQA dataset with input QA context E from r na (s). Since BERT models have a hard limit of 512 word-pieces, we use r na (s) to select the most relevant sentences that can fit within this limit, as described next. (See Appendix A.2 for training details.)</p><p>To accomplish this, we compute the score r na (s) for each sentence in the input D. Then we add sentences in decreasing order of their scores to the QA context E, until we have filled no more than 508 word-pieces (incl. question word-pieces). For every new paragraph considered, we also add its first sentence, and the title of the article (enclosed in &lt;t&gt;&lt;/t&gt;). This ensures that our span-prediction model has the right co-referential information from each paragraph. We arrange these paragraphs in the order of their highest-scoring sentence, so the most relevant sentences come earlier -a signal that could be exploited by our model.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Bringing it Together: Distractor Setting</head><p>Given a question along with 10 distractor paragraphs D, we use the r na (s) variant of our sentence scoring module to score each sentence s in D, again without looking at other paragraphs. In the second step, the selected sentences are fed as context E into the QA module (as described in Section 3.2) to choose an answer. In the final step, to find sentences supporting the chosen answer, we use r a (s) to score each sentence in D, this time with the chosen answer as part of the input. <ref type="bibr">2</ref> We define the score n(S) of a set of sentences S ? D to be the sum of the individual sentence scores; that is, n(S) = s?S r a (s). 3 In HotpotQA, supporting sentences always come from exactly two paragraphs. We compute this score for all possible S satisfying this constraint and take the highest scoring set of sentences as our support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Bringing it Together: Fullwiki Setting</head><p>Since there are too many paragraphs in the fullwiki setting, we use paragraphs from the SR-MRS system <ref type="bibr" target="#b6">(Nie et al., 2019)</ref> as our context D for each question. On the Dev set, we found QUARK to perform best with a paragraph score threshold of ?8.0 in MRS. Neither the sentence scorers r na (s), r a (s) nor the QA module were retrained in this setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate on both the distractor and fullwiki settings of HotpotQA with the following goal: Can a simple pipeline model outperform previous, more complex, approaches? We present the EM (Exact Match) and F1 scores on the evaluation metrics proposed for HotpotQA:</p><p>(1) answer selection, (2) support selection, and (3) joint score. <ref type="table" target="#tab_0">Table 1</ref> shows that on the distractor setting, QUARK outperforms all previous models based on BERT, including HGN, which like us also uses whole word masking for contextual embeddings. Moreover, we are within 1 point of models that use RoBERTa embeddings-a much stronger language model that has shown improvements of 1.5 to 6 points in previous HotpotQA models.</p><p>QUARK also performs better than the recent single-paragraph approach for the QA subtask <ref type="bibr" target="#b4">(Min et al., 2019a)</ref> by 14 points F1. While most of this gain comes from using a larger language model, QUARK scores 2 points higher even with a language model of the same size (BERT-Base).</p><p>We observe a similar trend in the fullwiki setting <ref type="table" target="#tab_1">(Table 2)</ref> where QUARK again outperforms previous approaches (except HGN with RoBERTa). While we rely on retrieval from SR-MRS <ref type="bibr" target="#b6">(Nie et al., 2019)</ref> for our initial paragraphs, we outperform the original work. We attribute this improvement to two factors: our sentence selection capitalizing on the sentence's paragraph context leading to better support selection, and a better span selection model  <ref type="table">Table 3</ref>: Ablation study on sentence selection in the distractor setting. top-n indicates the number of sentences required to cover the annotated support sentences in 90% of the questions.</p><p>leading to improved QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation</head><p>To evaluate the impact of context on our sentence selection model in isolation, we look at the number of sentences that score at least as high as the lowestscoring annotated support sentence. In other words, this is the number of sentences we must send to the QA model to ensure all annotated support is included. <ref type="table">Table 3</ref> shows that providing the model with the context from the paragraph gives a substantial boost on this metric, bringing it down from 10 to only 6 when using BERT-Base (an oracle would need 3 sentences). It further shows that this boost carries over to the downstream tasks of span selection and choosing support sentences (improving it by 9 points to 83%). Finally, the table shows the value of running the sentence selection model a second time: with BERT-Large, r a (s) outperforms r na (s) by 1.62% on the Support F1 metric. Looking deeper, we analyzed the accuracy of our third stage, r a (s), as a function of the correctness of the QA stage. When QA finds the correct gold answer, r a (s) obtains the right support in 65.9% of the cases. If the answer from QA is incorrect, the success rate of r a (s) is only 50.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our work shows that on the HotpotQA tasks, a simple pipeline model can do as well as or better than more complex solutions. Powerful pre-trained models allow us to score sentences one at a time, without looking at other paragraphs. By operating jointly over these sentences chosen from multiple paragraphs, we arrive at answers and supporting sentences on par with state-of-the-art approaches. This result shows that retrieval in HotpotQA is not itself a multi-hop problem, and suggests focusing on other multi-hop datasets to demonstrate the value of more complex techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Training the sentence scoring model</head><p>Both r na (s) and r a (s) are trained the same way. We use the 90447 questions from the HotpotQA training set, shuffle them, and train for 4 epochs. Both models are trained in the distractor setting only, but evaluated in both settings. We construct positive and negative examples by choosing the two paragraphs containing the annotated support sentences, plus two more randomly chosen paragraphs. All sentences from the chosen paragraphs become instances for the model.</p><p>During training, we follow the fine-tuning advice from <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref>, with two exceptions. We ramp up the learning rate from 0 to 10 ?5 over the first 10% of the batches, and then linearly decrease it again to 0.</p><p>To avoid biasing the training towards questions with many context sentences, we create batches at the question level. Three questions make up one batch, regardless of how many sentences they contain. We cap the batch size at 5625 tokens for practical purposes. If a batch exceeds this size, we drop sentences at random until the batch is small enough. As is standard for BERT classifiers, we use a cross-entropy loss with two classes, one for positive examples, and one for negative examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training the span prediction model</head><p>We train the BERT span prediction model on the output paragraphs from r na (s). We use a batch size of 16 questions and maximum sequence length of 512 word-pieces. We use the same optimizer settings as the sentence selection model with an additional weight decay of 0.01. The model is trained for a fixed number of epochs (set to 3) and the final model is used for evaluation. Under the hood, this model consists of two classifiers that run at the same time. One finds the first token of potential spans, and one finds the last token of potential spans. Each classifier uses a cross entropy loss. The final loss is the average loss of the two classifiers. We train one model on the output from our best r na (s) selection model and use it in all our experiments (and ablations).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>with varying complexities have been proposed over the past few years. Our work score sentences Overview of the QUARK model, with a question and context paragraphs as input. In both blue boxes, sentences are scored independently from one another. r na (s) and r a (s) use the same model architecture with different weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The final four tokens are a separator, plus the words yes, no, and noans. This allows the model to answer yes/no comparison questions, or give no answer at all. HotpotQA's distractor setting, Dev set. The bottom two models use larger language models than QUARK.</figDesc><table><row><cell>QA Model</cell><cell cols="2">Answer</cell><cell cols="2">Support</cell><cell></cell><cell>Joint</cell></row><row><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>Single-paragraph (Min et al., 2019a)</cell><cell>-</cell><cell>67.08</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>QFE (Nishida et al., 2019)</cell><cell>53.70</cell><cell>68.70</cell><cell>58.80</cell><cell>84.70</cell><cell>35.40</cell><cell>60.60</cell></row><row><cell>DFGN (Xiao et al., 2019)</cell><cell>55.66</cell><cell>69.34</cell><cell>53.10</cell><cell>82.24</cell><cell>33.68</cell><cell>59.86</cell></row><row><cell>SAE (Tu et al., 2019)</cell><cell>61.32</cell><cell>74.81</cell><cell>58.06</cell><cell>85.27</cell><cell>39.89</cell><cell>66.45</cell></row><row><cell>HGN (Fang et al., 2019)</cell><cell>-</cell><cell>79.69</cell><cell>-</cell><cell>87.38</cell><cell>-</cell><cell>71.45</cell></row><row><cell>QUARK (Ours)</cell><cell>67.75</cell><cell>81.21</cell><cell>60.72</cell><cell>86.97</cell><cell>44.35</cell><cell>72.26</cell></row><row><cell>SAE (RoBERTa) (Tu et al., 2019)</cell><cell>67.70</cell><cell>80.75</cell><cell>63.30</cell><cell>87.38</cell><cell>46.81</cell><cell>72.75</cell></row><row><cell>HGN (RoBERTa) (Fang et al., 2019)</cell><cell>-</cell><cell>81.00</cell><cell>-</cell><cell>87.93</cell><cell>-</cell><cell>73.01</cell></row><row><cell>QA Model</cell><cell></cell><cell cols="2">Answer</cell><cell cols="2">Support</cell><cell>Joint</cell></row><row><cell></cell><cell></cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell><cell>EM</cell><cell>F1</cell></row><row><cell>QFE (Nishida et al., 2019)</cell><cell></cell><cell>28.66</cell><cell>38.06</cell><cell>14.20</cell><cell>44.35</cell><cell>8.69</cell><cell>23.10</cell></row><row><cell>SR-MRS (Nie et al., 2019)</cell><cell></cell><cell>45.32</cell><cell>57.34</cell><cell>38.67</cell><cell>70.83</cell><cell>25.14</cell><cell>47.60</cell></row><row><cell>QUARK + SR-MRS (Ours)</cell><cell></cell><cell>55.50</cell><cell>67.51</cell><cell>45.64</cell><cell>72.95</cell><cell>32.89</cell><cell>56.23</cell></row><row><cell cols="2">HGN (RoBERTa) + SR-MRS (Fang et al., 2019)</cell><cell>56.71</cell><cell>69.16</cell><cell>49.97</cell><cell>76.39</cell><cell>35.36</cell><cell>59.86</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>HotpotQA's fullwiki setting, Test set. The bottom-most model uses a larger language model than QUARK.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While we use the model fine-tuned on SQuAD, ablations show that this only adds 0.2% to the final score.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We simply append the answer string to the question even if it is "yes" or "no".3 Note that ra(s) is the logit score and can be negative, so adding a sentence may not always improve this score.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical graph network for multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1911.03631</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">QASC: A dataset for question answering via sentence composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Guerquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="m">RoBERTa: A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compositional questions do not necessitate multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-hop reading comprehension through question decomposition and rescoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><forename type="middle">S</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Answering while summarizing: Multi-task learning for multi-hop QA with evidence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<editor>NAACL-HLT</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jui-Ting</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bufang</forename><surname>Zhou</surname></persName>
		</author>
		<idno>abs/1911.00484</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R&amp;apos;emi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Brew</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
